{"issue_number": 6090, "issue_title": "Autogen website accessibility issues (low severity)", "issue_body": "What happened?\nNote: Some of these issues may be the duplicates of the same underlying problem. If you are unable to reproduce an issue, please mark the issue as fixed and leave a comment to that effect!\n\n (9)\tClose button is not defined inside the 'hamburger' dialog. Problem Image\n (12)\tNo underline or change in color is appearing, when keyboard focus lands on links control. Problem Video -- @AndreaTang123\n (15)\tNo underline is provided on left navigation controls. Problem Image\n (16)\tScreen reader is announcing the information 'Navigation' twice when focus lands on left navigation control 'Installation'. Problem Video\n (17)\tScreen reader is announcing the information 'list' twice when focus lands on link control 'Trace Logger Name'. Problem Video\n (18)\tMultiple <H1> is defined on the page. Problem Video\n (21)\tUnderline/ Chevron is not defined for links present in \u2018On This Page\u2019 section. Problem Image\n (23)\tScreen reader is announcing unwanted position information '1 of 1' when focus lands on Collapse button. Problem Video\n (25)\tBack to Top control is not accessible with keyboard. Problem Video\n (28)\tAt 320*256 settings, Horizontal scrollbar appears on the page. Problem Image\n (30)\tIncorrect role is defined for the 'Venv' and 'Conda' controls. Problem Video\n (32)\tIn browse mode, screen reader is focus moving to hidden after 'Discover and create your own' link control. Problem Video\n (37)\tScreen reader not announcing out of list information. Problem Video\n (38)\tScreen reader announcing unnecessary information as document when focus lands on copy control. Problem Image\n (39)\tScreen reader announcing blank after 'Discover community projects' text. Problem Image\n (42)\tIn browse mode, screen reader focus order is not logical on tabs controls. Problem Video\n\nWhich packages was the bug in?\nPython Core (autogen-core)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-24", "closed_at": null, "labels": ["documentation", "help wanted"], "State": "open", "Author": "peterychang"}
{"issue_number": 6089, "issue_title": "Unable to view outputs of tool based agents in tracing tools like LangFuse, OpenLit and Microsoft Tracing", "issue_body": "What happened?\nDescribe the bug\n\nWe are unable to view outputs of tool based agents in Tracing tools like LangFuse, OpenLit and Microsoft Tracing.\nWe are encountering an error and unable to export traces when using the code provided in #5992\n\nTo Reproduce\n\nCode for reproducing Bug 1\n\nimport os\nimport base64\nimport openlit\nimport asyncio\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\nfrom opentelemetry import trace\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom autogen_agentchat.messages import AgentEvent, ChatMessage\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import AzureOpenAIChatCompletionClient\nfrom autogen_core import SingleThreadedAgentRuntime\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nos.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"...\"\nos.environ[\"LANGFUSE_SECRET_KEY\"] = \"...\"\nos.environ[\"LANGFUSE_HOST\"] = \"...\"\nLANGFUSE_AUTH = base64.b64encode(\n    f\"{os.environ.get('LANGFUSE_PUBLIC_KEY')}:{os.environ.get('LANGFUSE_SECRET_KEY')}\".encode()\n).decode()\nos.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = os.environ.get(\"LANGFUSE_HOST\") + \"/api/public/otel\"\nos.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {LANGFUSE_AUTH}\"\n \ntrace_provider = TracerProvider()\ntrace_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))\ntrace.set_tracer_provider(trace_provider)\ntracer = trace.get_tracer(__name__)\nopenlit.init(tracer=tracer, disable_batch=True, disable_metrics=True)\n\nasync def Moral() -> str:\n    return \"No morals to this story\"\n\nasync def main():\n    API_KEY = os.getenv(\"api_key\")\n    Model_Name = os.getenv(\"model-name\")\n    Api_Version = os.getenv(\"api-version\")\n    Azure_Endpoint = os.getenv(\"azure_endpoint\")\n    Deployment_Name = os.getenv(\"deployment-name\")\n\n    az_model_client = AzureOpenAIChatCompletionClient(\n        azure_deployment=Deployment_Name,\n        model=Model_Name,\n        api_version=Api_Version,\n        azure_endpoint=Azure_Endpoint,\n        api_key=API_KEY\n    )\n    planning_agent = AssistantAgent(\n    \"PlanningAgent\",\n    description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n    model_client=az_model_client,\n    system_message=\"\"\"\n        You are a planning agent.\n        Your job is to break down complex tasks into smaller, manageable subtasks.\n        Your team members are:\n            Story_writer: Writes story and make corrections.\n            Story_reviewer: Checks if the story is for kids and Provides constructive feedback on Kids stories to add a positive impactful ending. It doesn't write the story, only provide feedback and improvements.\n            Story_moral: Finally, adds the moral to the story.\n\n        You only plan and delegate tasks - you do not execute them yourself. You can engage team members multiple times so that a perfect story is provided.\n\n        When assigning tasks, use this format:\n        1. <agent> : <task>\n\n        After all tasks are complete, summarize the findings and end with \"TERMINATE\".\n        \"\"\",\n    )\n\n    # Create the Writer agent.\n    Story_writer = AssistantAgent(\n        \"Story_writer\",\n        model_client=az_model_client,\n        system_message=\"You are a helpful AI assistant which write the story. Keep the story short.\"\n    )\n\n    # Create the Reviewer agent.\n    Story_reviewer = AssistantAgent(\n        \"Story_reviewer\",\n        model_client=az_model_client,\n        system_message=\"You are a helpful AI assistant which checks if the story is for kids and provides constructive feedback on Kids stories to have a postive impactful ending\",\n    )\n\n    # Story Moral Agent.\n    Story_moral = AssistantAgent(\n        \"Story_moral\",\n        model_client=az_model_client,\n        system_message=\"\"\"You are a helpful AI assistant which add the moral of the story using the 'Moral' tool, it has to be written by a seperation ' ========moral of the story==========='\"\"\",\n        tools=[Moral],\n        reflect_on_tool_use=True\n    )\n\n    text_mention_termination = TextMentionTermination(\"TERMINATE\")\n    max_messages_termination = MaxMessageTermination(max_messages=10)\n    termination = text_mention_termination | max_messages_termination\n\n    runtime = SingleThreadedAgentRuntime(tracer_provider=trace_provider)\n\n    with tracer.start_as_current_span(\"Sample-Trace\") as span:\n        runtime.start()\n        team = SelectorGroupChat(\n            [planning_agent, Story_writer, Story_reviewer, Story_moral],\n            model_client=az_model_client,\n            termination_condition=termination,\n            runtime = runtime\n        )\n        span.set_attribute(\"langfuse.user.id\", \"user-A\")\n        span.set_attribute(\"langfuse.session.id\", \"28\")\n        span.set_attribute(\"langfuse.tags\", [\"semantic-kernel\", \"demo-9\"])\n        span.set_attribute(\"langfuse.prompt.name\", \"test-9\")\n        await Console(\n            team.run_stream(task=\"write a story on rocket crash\")\n        )\nawait runtime.close()\nawait model_client.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\nCode for reproducing Bug 2\n\nimport os\nimport base64\nimport asyncio\nimport openlit\nfrom dotenv import load_dotenv\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\nfrom opentelemetry.sdk.resources import Resource\nfrom autogen_core import SingleThreadedAgentRuntime\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n\nload_dotenv()\nos.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"...\"\nos.environ[\"LANGFUSE_SECRET_KEY\"] = \"...\" \nLANGFUSE_AUTH = base64.b64encode(\n    f\"{os.environ.get('LANGFUSE_PUBLIC_KEY')}:{os.environ.get('LANGFUSE_SECRET_KEY')}\".encode()\n).decode()\n\ndef configure_oltp_tracing():\n    langfuse_exporter = OTLPSpanExporter(endpoint=\"...\", headers={\"Authorization\" : f\"Basic {LANGFUSE_AUTH}\"})\n    tracer_provider = TracerProvider(resource=Resource({\"service.name\" : \"autogen-test-agentchat\"}))\n    span_processor = SimpleSpanProcessor(langfuse_exporter)\n    tracer_provider.add_span_processor(span_processor)\n    trace.set_tracer_provider(tracer_provider)\n    return tracer_provider\n\ndef search_web_tool(query: str) -> str:\n    if \"2006-2007\" in query:\n        return \"\"\"Here are the total points scored by Miami Heat players in the 2006-2007 season:\n        Udonis Haslem: 844 points\n        Dwayne Wade: 1397 points\n        James Posey: 550 points\n        ...\n        \"\"\"\n    elif \"2007-2008\" in query:\n        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\"\n    elif \"2008-2009\" in query:\n        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\"\n    return \"No data found.\"\n\ndef percentage_change_tool(start: float, end: float) -> float:\n    return ((end - start) / start) * 100\n\nasync def main():\n    API_KEY = os.getenv(\"api_key\")\n    Model_Name = os.getenv(\"model-name\")\n    Deployment_Name = os.getenv(\"deployment-name\")\n    Api_Version = os.getenv(\"api-version\")\n    Azure_Endpoint = os.getenv(\"azure_endpoint\")\n    model_client = AzureOpenAIChatCompletionClient(\n        azure_deployment=Deployment_Name,\n        model=Model_Name,\n        api_version=Api_Version,\n        azure_endpoint=Azure_Endpoint,\n        api_key=API_KEY\n    )\n\n    planning_agent = AssistantAgent(\n        \"PlanningAgent\",\n        description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n        model_client=model_client,\n        system_message=\"\"\"\n        You are a planning agent.\n        Your job is to break down complex tasks into smaller, manageable subtasks.\n        Your team members are:\n            WebSearchAgent: Searches for information\n            DataAnalystAgent: Performs calculations\n\n        You only plan and delegate tasks - you do not execute them yourself.\n\n        When assigning tasks, use this format:\n        1. <agent> : <task>\n\n        After all tasks are complete, summarize the findings and end with \"TERMINATE\".\n        \"\"\",\n    )\n\n    web_search_agent = AssistantAgent(\n        \"WebSearchAgent\",\n        description=\"An agent for searching information on the web.\",\n        tools=[search_web_tool],\n        model_client=model_client,\n        system_message=\"\"\"\n        You are a web search agent.\n        Your only tool is search_tool - use it to find information.\n        You make only one search call at a time.\n        Once you have the results, you never do calculations based on them.\n        \"\"\",\n    )\n\n    data_analyst_agent = AssistantAgent(\n        \"DataAnalystAgent\",\n        description=\"An agent for performing calculations.\",\n        model_client=model_client,\n        tools=[percentage_change_tool],\n        system_message=\"\"\"\n        You are a data analyst.\n        Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.\n        If you have not seen the data, ask for it.\n        \"\"\",\n    )\n\n    text_mention_termination = TextMentionTermination(\"TERMINATE\")\n    max_messages_termination = MaxMessageTermination(max_messages=25)\n    termination = text_mention_termination | max_messages_termination\n\n    selector_prompt = \"\"\"Select an agent to perform task.\n\n    {roles}\n\n    Current conversation context:\n    {history}\n\n    Read the above conversation, then select an agent from {participants} to perform the next task.\n    Make sure the planner agent has assigned tasks before other agents start working.\n    Only select one agent.\n    \"\"\"\n\n    task = \"Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\"\n    tracer_provider = configure_oltp_tracing()\n    openlit.init(tracer=tracer_provider, disable_batch=True, disable_metrics=True)\n    runtime = SingleThreadedAgentRuntime(tracer_provider=tracer_provider)\n    tracer = trace.get_tracer(\"autogen-test-agentchat\")\n    with tracer.start_as_current_span(\"runtime\") as span:\n        runtime.start()\n        team = SelectorGroupChat(\n            [planning_agent, web_search_agent, data_analyst_agent],\n            model_client=model_client,\n            termination_condition=termination,\n            selector_prompt=selector_prompt,\n            allow_repeated_speaker=True,  # Allow an agent to speak multiple turns in a row.\n            runtime=runtime,\n        )\n        span.set_attribute(\"langfuse.user.id\", \"user-A\")\n        span.set_attribute(\"langfuse.session.id\", \"26\")\n        span.set_attribute(\"langfuse.tags\", [\"semantic-kernel\", \"demo-9\"])\n        span.set_attribute(\"langfuse.prompt.name\", \"test-9\")\n        await Console(team.run_stream(task=task))\n    \n    await runtime.close()\n    await model_client.close()\n\nasyncio.run(main())\n\nScreenshots\n\nScreenshot for bug 1\n\n\n\nScreenshot for bug 2\n\n\nTags\n#5995\n@victordibia\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\n0.4.9.2\nOther library version.\nNo response\nModel used\ngpt-4o\nModel provider\nAzure OpenAI\nOther model provider\nNo response\nPython version\n3.11\n.NET version\nNone\nOperating system\nWindows", "created_at": "2025-03-24", "closed_at": null, "labels": ["needs-triage"], "State": "open", "Author": "Arjun-Prakash10"}
{"issue_number": 6088, "issue_title": "Session State Leaking Between Different Browser Sessions in AutoGen", "issue_body": "What happened?\nSession State Leaking Between Different Browser Sessions in AutoGen\nDescription\nI've discovered an issue where conversation state is being shared between different browser sessions despite having unique session IDs and unique agent names. When a user connects to my application from a second browser, the second session's agent has access to messages from the first session, creating a privacy concern.\nObserved Behavior\nWhen two different browser sessions connect to the same WebSocket endpoint, they are assigned different session IDs and I create agents with different names for each session. However, the second session's state contains messages from the first session, indicating state leakage between sessions.\nHere are the saved states showing the problem:\nSession 1 State:\n{\n  \"type\": \"TeamState\",\n  \"version\": \"1.0.0\",\n  \"agent_states\": {\n    \"group_chat_manager/6b5cf9d6-3d5a-48c5-b09d-775510e7401c\": {\n      \"type\": \"RoundRobinManagerState\",\n      \"version\": \"1.0.0\",\n      \"message_thread\": [\n        {\n          \"source\": \"user\",\n          \"models_usage\": null,\n          \"metadata\": {},\n          \"content\": \"Hi\",\n          \"type\": \"TextMessage\"\n        },\n        {\n          \"source\": \"AgentSowWriter_s0e2fefd4\",\n          \"models_usage\": {\n            \"prompt_tokens\": 964,\n            \"completion_tokens\": 26\n          },\n          \"metadata\": {},\n          \"content\": \"Hello! How can I assist you today in writing a Due Diligence Scope of Work (SOW) document?\",\n          \"type\": \"TextMessage\"\n        }\n      ],\n      \"current_turn\": 0,\n      \"next_speaker_index\": 0\n    },\n    \"collect_output_messages/6b5cf9d6-3d5a-48c5-b09d-775510e7401c\": {},\n    \"AgentSowWriter_s0e2fefd4/6b5cf9d6-3d5a-48c5-b09d-775510e7401c\": {\n      \"type\": \"ChatAgentContainerState\",\n      \"version\": \"1.0.0\",\n      \"agent_state\": {\n        \"type\": \"AssistantAgentState\",\n        \"version\": \"1.0.0\",\n        \"llm_context\": {\n          \"messages\": [\n            {\n              \"content\": \"Hi\",\n              \"source\": \"user\",\n              \"type\": \"UserMessage\"\n            },\n            {\n              \"content\": \"Hello! How can I assist you today in writing a Due Diligence Scope of Work (SOW) document?\",\n              \"thought\": null,\n              \"source\": \"AgentSowWriter_s0e2fefd4\",\n              \"type\": \"AssistantMessage\"\n            }\n          ]\n        }\n      },\n      \"message_buffer\": []\n    }\n  },\n  \"team_id\": \"6b5cf9d6-3d5a-48c5-b09d-775510e7401c\"\n}\nSession 2 State:\n{\n  \"type\": \"TeamState\",\n  \"version\": \"1.0.0\",\n  \"agent_states\": {\n    \"group_chat_manager/fdd33bfd-345f-494d-a73b-99c257bd3a60\": {\n      \"type\": \"RoundRobinManagerState\",\n      \"version\": \"1.0.0\",\n      \"message_thread\": [\n        {\n          \"source\": \"user\",\n          \"models_usage\": null,\n          \"metadata\": {},\n          \"content\": \"Hi\",\n          \"type\": \"TextMessage\"\n        },\n        {\n          \"source\": \"AgentSowWriter_s00010d6e\",\n          \"models_usage\": {\n            \"prompt_tokens\": 1010,\n            \"completion_tokens\": 25\n          },\n          \"metadata\": {},\n          \"content\": \"Hello again! How can I assist you with the Due Diligence Scope of Work (SOW) document?\",\n          \"type\": \"TextMessage\"\n        }\n      ],\n      \"current_turn\": 0,\n      \"next_speaker_index\": 0\n    },\n    \"collect_output_messages/fdd33bfd-345f-494d-a73b-99c257bd3a60\": {},\n    \"AgentSowWriter_s00010d6e/fdd33bfd-345f-494d-a73b-99c257bd3a60\": {\n      \"type\": \"ChatAgentContainerState\",\n      \"version\": \"1.0.0\",\n      \"agent_state\": {\n        \"type\": \"AssistantAgentState\",\n        \"version\": \"1.0.0\",\n        \"llm_context\": {\n          \"messages\": [\n            {\n              \"content\": \"Hi\",\n              \"source\": \"user\",\n              \"type\": \"UserMessage\"\n            },\n            {\n              \"content\": \"Hello! How can I assist you today in writing a Due Diligence Scope of Work (SOW) document?\",\n              \"thought\": null,\n              \"source\": \"AgentSowWriter_s0e2fefd4\",\n              \"type\": \"AssistantMessage\"\n            },\n            {\n              \"content\": \"Hi\",\n              \"source\": \"user\",\n              \"type\": \"UserMessage\"\n            },\n            {\n              \"content\": \"Hello again! How can I assist you with the Due Diligence Scope of Work (SOW) document?\",\n              \"thought\": null,\n              \"source\": \"AgentSowWriter_s00010d6e\",\n              \"type\": \"AssistantMessage\"\n            }\n          ]\n        }\n      },\n      \"message_buffer\": []\n    }\n  },\n  \"team_id\": \"fdd33bfd-345f-494d-a73b-99c257bd3a60\"\n}\nNotice that in Session 2's llm_context, it contains messages from both AgentSowWriter_s0e2fefd4 (Session 1) and AgentSowWriter_s00010d6e (Session 2).\nExpected Behavior\nEach browser session should have its own isolated state. Session 2 should only contain messages from its own conversation, not messages from Session 1.\nCode\nHere's the relevant code for creating separate teams for each session:\nasync def get_team(assistant: str, session_id: str) -> RoundRobinGroupChat:\n    # Create a model client\n    az_model_client = AzureOpenAIChatCompletionClient(\n        model=os.getenv(\"AZURE_MODEL_NAME\"),\n        api_version=os.getenv(\"AZURE_API_VERSION\"),\n        azure_endpoint=os.getenv(\"AZURE_ENDPOINT_URL\"),\n        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n    )\n\n    # Each session has a unique ID\n    import hashlib\n    session_hash = hashlib.md5(session_id.encode()).hexdigest()[:8]\n    unique_agent_name = f\"{assistant}_s{session_hash}\"\n\n    if assistant not in assistant_configs:\n        raise ValueError(f\"Invalid assistant name: {assistant}\")\n\n    config = assistant_configs[assistant]\n    agent = AssistantAgent(name=unique_agent_name, model_client=az_model_client, **config)\n\n    termination_condition = TextMessageTermination(unique_agent_name)\n\n    team = RoundRobinGroupChat(\n        [agent],\n        termination_condition=termination_condition,\n    )\n\n    # Get session-specific paths\n    paths = get_session_path(session_id, assistant)\n    state_path = paths[\"state_path\"]\n\n    # Load state if it exists\n    if os.path.exists(state_path):\n        async with aiofiles.open(state_path, \"r\") as file:\n            state = json.loads(await file.read())\n        await team.load_state(state)\n\n    return team\nSteps to Reproduce\n\nSet up a WebSocket server as in the code provided\nConnect from Browser 1, send a message, and get a response\nConnect from Browser 2 (using a different session)\nSend a message from Browser 2\nCheck the state for Browser 2's session - it will contain messages from Browser 1's session\n\nImpact\nThis is a critical issue for multi-user applications as it creates a privacy breach where users could potentially see messages from other users' conversations.\nAttempted Workarounds\nI've tried:\n\nCreating agents with unique names for each session\nStoring state in session-specific files\n\nNone of these approaches have successfully isolated the state between sessions.\nAny help would be greatly appreciated.\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.8)\nAutoGen library version.\nOther (please specify)\nOther library version.\n0.4.8\nModel used\ngpt-4o-mini\nModel provider\nAzure OpenAI\nOther model provider\nNo response\nPython version\n3.11\n.NET version\nNone\nOperating system\nWindows", "created_at": "2025-03-24", "closed_at": "2025-03-26", "labels": ["needs-triage"], "State": "closed", "Author": "divyansh-23906"}
{"issue_number": 6085, "issue_title": "Get current message thread from a group chat team.", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nFor BaseGroupChat, support a public method for getting all the messages happened so far via an async send_message to the group chat manager agent. This will require a new GroupChatGetThread RPC event type, and handled by the BaseGroupChatManager.", "created_at": "2025-03-24", "closed_at": null, "labels": ["good first issue", "help wanted", "proj-agentchat"], "State": "open", "Author": "ekzhu"}
{"issue_number": 6084, "issue_title": "When Call MultimodalWebSurfer after using tool, error occurred", "issue_body": "What happened?\nDescribe the bug\nWhen using MultimodalWebSurfer after a tool call, the agent crashes due to an unhandled message type in on_messages_stream.\nTo Reproduce\nfrom autogen_ext.agents.web_surfer import MultimodalWebSurfer\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom autogen_agentchat.ui import Console\n\n\ntermination = TextMentionTermination(\"TERMINATE\")\n\ndef test():\n    return \"Hello, World!\"\n\nmulti_modal_web_surfer = MultimodalWebSurfer(\n    name=\"web_surfer\",\n    description=\"A web surfer agent that can surf the web for information.\",\n    model_client=anthropic_client,\n    # system_message=\"You are a web surfer agent and your task is to surf the web for information. Return `TERMINATE` once the information is found.\",\n)\n\ntest_agent = AssistantAgent(\n    name=\"test_agent\",\n    description=\"A test agent that returns a string.\",\n    model_client=anthropic_client,\n    system_message=\"You are a test agent that run test function.\",\n    tools = [test],\n)\n\nteam = RoundRobinGroupChat(\n    participants=[test_agent, multi_modal_web_surfer],\n    termination_condition=termination,\n)\n\nasync def team_run():\n    await Console(\n        team.run_stream(\n            task=\"try test function\"\n        )\n    )\n\nasyncio.run(team_run())\nObserved Behavior The agent throws:\n/Users/cysong/Documents/MOTOV_\u110b\u1165\u11b8\u1106\u116e/codes/TIPS/ai-agent/autogen/python/packages/autogen-ext/src/autogen_ext/models/openai/_openai_client.py:397: UserWarning: Missing required field 'structured_output' in ModelInfo. This field will be required in a future version of AutoGen.\n  validate_model_info(self._model_info)\n---------- user ----------\ntry test function\n---------- test_agent ----------\nI'll help you run the test function. Let me do that for you now.\n---------- test_agent ----------\nError processing publish message for web_surfer_d5a78cc6-f002-46ef-9eae-ab0ccfe113a9/d5a78cc6-f002-46ef-9eae-ab0ccfe113a9\nTraceback (most recent call last):\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_single_threaded_agent_runtime.py\", line 533, in _on_message\n    return await agent.on_message(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_base_agent.py\", line 113, in on_message\n    return await self.on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 67, in on_message_impl\n    return await super().on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_routed_agent.py\", line 485, in on_message_impl\n    return await h(self, message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_routed_agent.py\", line 268, in wrapper\n    return_value = await func(self, message, ctx)  # type: ignore\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 69, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-ext/src/autogen_ext/agents/web_surfer/_multimodal_web_surfer.py\", line 438, in on_messages_stream\n    raise ValueError(f\"Unexpected message in MultiModalWebSurfer: {chat_message}\")\nValueError: Unexpected message in MultiModalWebSurfer: source='test_agent' models_usage=None metadata={} content='Hello, World!' type='ToolCallSummaryMessage'\n[FunctionCall(id='toolu_01PgBqKGtjgWJ4ib6x2jx5T5', arguments='{}', name='test')]\n---------- test_agent ----------\n[FunctionExecutionResult(content='Hello, World!', name='test', call_id='toolu_01PgBqKGtjgWJ4ib6x2jx5T5', is_error=False)]\n---------- test_agent ----------\nHello, World!\nTraceback (most recent call last):\n  File \"/Users/cysong/Documents/ai-agent/test-agent/my-app/errortest.py\", line 72, in <module>\n    asyncio.run(team_run())\n  File \"/Users/cysong/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/asyncio/runners.py\", line 190, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/test-agent/my-app/errortest.py\", line 66, in team_run\n    await Console(\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-agentchat/src/autogen_agentchat/ui/_console.py\", line 117, in Console\n    async for message in stream:\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_base_group_chat.py\", line 482, in run_stream\n    await shutdown_task\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_base_group_chat.py\", line 426, in stop_runtime\n    await self._runtime.stop_when_idle()\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_single_threaded_agent_runtime.py\", line 769, in stop_when_idle\n    await self._run_context.stop_when_idle()\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_single_threaded_agent_runtime.py\", line 120, in stop_when_idle\n    await self._run_task\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_single_threaded_agent_runtime.py\", line 109, in _run\n    await self._runtime._process_next()  # type: ignore\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_single_threaded_agent_runtime.py\", line 604, in _process_next\n    raise e from None\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_single_threaded_agent_runtime.py\", line 551, in _process_publish\n    await asyncio.gather(*responses)\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_single_threaded_agent_runtime.py\", line 546, in _on_message\n    raise e\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_single_threaded_agent_runtime.py\", line 533, in _on_message\n    return await agent.on_message(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_base_agent.py\", line 113, in on_message\n    return await self.on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 67, in on_message_impl\n    return await super().on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_routed_agent.py\", line 485, in on_message_impl\n    return await h(self, message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_routed_agent.py\", line 268, in wrapper\n    return_value = await func(self, message, ctx)  # type: ignore\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 69, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-ext/src/autogen_ext/agents/web_surfer/_multimodal_web_surfer.py\", line 438, in on_messages_stream\n    raise ValueError(f\"Unexpected message in MultiModalWebSurfer: {chat_message}\")\nValueError: Unexpected message in MultiModalWebSurfer: source='test_agent' models_usage=None metadata={} content='Hello, World!' type='ToolCallSummaryMessage'\n\nExpected behavior\nRoot Cause In _multimodal_web_surfer.py:\n    async def on_messages_stream(\n        self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken\n    ) -> AsyncGenerator[AgentEvent | ChatMessage | Response, None]:\n        for chat_message in messages:\n            if isinstance(chat_message, TextMessage | MultiModalMessage):\n                self._chat_history.append(UserMessage(content=chat_message.content, source=chat_message.source))\n            else:\n                raise ValueError(f\"Unexpected message in MultiModalWebSurfer: {chat_message}\")\nCurrently, only TextMessage and MultiModalMessage are allowed. However, in team-based workflows (especially after tool calls), messages like ToolCallSummaryMessage, FunctionExecutionResultMessage may also be streamed.\nExpected Behavior MultimodalWebSurfer should either:\nGracefully skip unsupported message types, or\nHandle/append fallback content from those messages (e.g., str(chat_message.content))\nSuggestion Would it make sense to generalize this logic to handle all ChatMessage types, or at least skip the unrecognized ones with a warning?\nAlso, could you clarify the rationale for limiting _chat_history to only TextMessage and MultiModalMessage? Would broader support break assumptions elsewhere?\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nAnthropic\nOther model provider\nNo response\nPython version\n3.11\n.NET version\nNone\nOperating system\nMacOS", "created_at": "2025-03-24", "closed_at": "2025-03-26", "labels": ["needs-triage"], "State": "closed", "Author": "SongChiYoung"}
{"issue_number": 6083, "issue_title": "Anthorpic models have error when send empty content", "issue_body": "What happened?\nDescribe the bug\nWhen send empty content to anthropic model, it will be response error.\nIt's same error with #5762\nTo Reproduce\nclients =  [\n    anthropic_client,\n    openai_client,\n    anthropic_openai_client,\n    gemini_openai_client,\n]\n\nmessages = [\n    UserMessage(content=\"\", source=\"user\"),\n]\n\nfor client in clients:\n    try:\n        text = asyncio.run(client.create(messages=messages))\n        # print(text)\n    except Exception as e:\n        print(e)\n        print(f\"Error with {client.model_info.get(\"family\")}\")\nresult\nError code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'messages.0: all messages must have non-empty content except for the optional final assistant message'}}\nError with claude-3.7-sonnet\nError code: 400 - {'error': {'code': 'invalid_request_error', 'message': 'messages.0: all messages must have non-empty content except for the optional final assistant message', 'type': 'invalid_request_error', 'param': None}}\nError with claude-3.7-sonnet\n\nWhen send empty content, anthropic response error.\nExpected behavior\nWhen request to anthropic model, change empty to whitespace(\" \")\nSame error as OpenAI SDK and Anthropic SDK too\nAdditional context\nHow about fix this error with that PR #6063\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nAnthropic\nOther model provider\nNo response\nPython version\n3.11\n.NET version\nNone\nOperating system\nMacOS", "created_at": "2025-03-24", "closed_at": "2025-03-31", "labels": ["needs-triage"], "State": "closed", "Author": "SongChiYoung"}
{"issue_number": 6081, "issue_title": "Autogen when tried for Gemini model", "issue_body": "What happened?\nDescribe the bug\nI am trying to use Gemini model from vertex AI in autogen\nUsed the documentation (Use Autogen with Gemini via VertexAI)\nTo Reproduce\nCreated a project in vertex ai, used that model in Config in autogen.\nCode used:\nconfig=[{\n\"model\": \"gemini-1.5-flash-002\",\n\"api_type\": \"google\",\n\"project_id\": PROJECT,\n\"location\": \"us-central1\"\n}\n]\nImageAgent= MultimodalConversableAgent(\"assitant\", llm_config={\"config_list\": config}, max_consecutive_auto_reply =1)\nuser_proxy = UserProxyAgent(\"userproxy\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=0)\nuser_proxy.initiate_chat(ImageAgent,\nmessage=\"\"\"Describe the image clearly\"\n. \"\"\")\nExpected behavior\nExpected the response but getting\nResourceExhausted: 429 received Metadata size exceeded hard limit (value length 637301 vs. 16384).\nIt seems like grpc error.\nI followed documentation code thoroughly but still facing issues\nWhich packages was the bug in?\nPython Core (autogen-core)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nGemini flash 1.5\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-24", "closed_at": null, "labels": ["0.2", "needs-triage"], "State": "open", "Author": "purvaja2015"}
{"issue_number": 6076, "issue_title": "app_agent.py sample requires several undocumented dependencies", "issue_body": "What is the doc issue?\nDescribe the issue\nWhile trying to run the app_agent.py sample inside python/samples/agentchat_fastapi, I encountered several missing dependencies and setup hurdles that are not documented anywhere in the README or sample guide.\nWhat do you want to see in the doc?\nWhen running the sample at python/samples/agentchat_fastapi/app_agent.py, several important dependencies were missing and not documented. As a new contributor, I ran into multiple ModuleNotFoundError issues which were resolved only after trial-and-error.\nI\u2019d love to see an updated README or setup guide for the agentchat_fastapi sample that clearly lists all the required installation steps, including:\npip install aiofiles pyyaml fastapi uvicorn\ncd python/packages/autogen-core\npip install -e \".[dev]\"\ncd ../autogen-agentchat\npip install -e \".[dev]\"\nN/A \u2014 the issue is purely CLI-based, but here's a log-style sample of what I encountered:\nModuleNotFoundError: No module named 'aiofiles'\nModuleNotFoundError: No module named 'yaml'\nModuleNotFoundError: No module named 'autogen_agentchat'\nModuleNotFoundError: No module named 'fastapi'\nThis covers every step I took, all the errors I encountered, and the exact fixes I used. Happy to submit a PR if you'd like help improving the sample documentation!\nSuggested Fix / Working Setup\nClone & activate env\ngit clone https://github.com/microsoft/autogen.git\ncd autogen\npython3 -m venv env\nsource env/bin/activate\nInstall packages\ncd python/packages/autogen-core\npip install -e \".[dev]\"\ncd ../autogen-agentchat\npip install -e \".[dev]\"\npip install aiofiles pyyaml fastapi uvicorn\nRun sample\ncd ../../samples/agentchat_fastapi\npython3 app_agent.py\nLink to the doc page, if applicable\n//", "created_at": "2025-03-23", "closed_at": null, "labels": ["documentation", "good first issue", "help wanted"], "State": "open", "Author": "ZivGrinblat"}
{"issue_number": 6074, "issue_title": "Can't get the same result as the example in the Doc about Jaeger", "issue_body": "What happened?\nDescribe the bug\nI'm trying to use Jaeger to record the agent, but after I follow the guide and run the official example, I can't get the same result as is shown in the doc. The example agent is not traced successfully.\nTo Reproduce\nRun Jaeger\ndocker run -d --name jaeger \\\n  -e COLLECTOR_OTLP_ENABLED=true \\\n  -p 16686:16686 \\\n  -p 4317:4317 \\\n  -p 4318:4318 \\\n  jaegertracing/all-in-one:latest\n\njust the example in the doc\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\n\notel_exporter = OTLPSpanExporter(endpoint=\"http://localhost:4317\", insecure=True)\ntracer_provider = TracerProvider(resource=Resource({\"service.name\": \"autogen-test-agentchat\"}))\nspan_processor = BatchSpanProcessor(otel_exporter)\ntracer_provider.add_span_processor(span_processor)\ntrace.set_tracer_provider(tracer_provider)\n\n# we will get reference this tracer later using its service name\ntracer = trace.get_tracer(\"autogen-test-agentchat\")\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_core import SingleThreadedAgentRuntime\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\ndef search_web_tool(query: str) -> str:\n    if \"2006-2007\" in query:\n        return \"\"\"Here are the total points scored by Miami Heat players in the 2006-2007 season:\n        Udonis Haslem: 844 points\n        Dwayne Wade: 1397 points\n        James Posey: 550 points\n        ...\n        \"\"\"\n    elif \"2007-2008\" in query:\n        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\"\n    elif \"2008-2009\" in query:\n        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\"\n    return \"No data found.\"\n\n\ndef percentage_change_tool(start: float, end: float) -> float:\n    return ((end - start) / start) * 100\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n\n    planning_agent = AssistantAgent(\n        \"PlanningAgent\",\n        description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n        model_client=model_client,\n        system_message=\"\"\"\n        You are a planning agent.\n        Your job is to break down complex tasks into smaller, manageable subtasks.\n        Your team members are:\n            WebSearchAgent: Searches for information\n            DataAnalystAgent: Performs calculations\n\n        You only plan and delegate tasks - you do not execute them yourself.\n\n        When assigning tasks, use this format:\n        1. <agent> : <task>\n\n        After all tasks are complete, summarize the findings and end with \"TERMINATE\".\n        \"\"\",\n    )\n\n    web_search_agent = AssistantAgent(\n        \"WebSearchAgent\",\n        description=\"An agent for searching information on the web.\",\n        tools=[search_web_tool],\n        model_client=model_client,\n        system_message=\"\"\"\n        You are a web search agent.\n        Your only tool is search_tool - use it to find information.\n        You make only one search call at a time.\n        Once you have the results, you never do calculations based on them.\n        \"\"\",\n    )\n\n    data_analyst_agent = AssistantAgent(\n        \"DataAnalystAgent\",\n        description=\"An agent for performing calculations.\",\n        model_client=model_client,\n        tools=[percentage_change_tool],\n        system_message=\"\"\"\n        You are a data analyst.\n        Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.\n        If you have not seen the data, ask for it.\n        \"\"\",\n    )\n\n    text_mention_termination = TextMentionTermination(\"TERMINATE\")\n    max_messages_termination = MaxMessageTermination(max_messages=25)\n    termination = text_mention_termination | max_messages_termination\n\n    selector_prompt = \"\"\"Select an agent to perform task.\n\n    {roles}\n\n    Current conversation context:\n    {history}\n\n    Read the above conversation, then select an agent from {participants} to perform the next task.\n    Make sure the planner agent has assigned tasks before other agents start working.\n    Only select one agent.\n    \"\"\"\n\n    task = \"Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\"\n\n    tracer = trace.get_tracer(\"autogen-test-agentchat\")\n    with tracer.start_as_current_span(\"runtime\"):\n        team = SelectorGroupChat(\n            [planning_agent, web_search_agent, data_analyst_agent],\n            model_client=model_client,\n            termination_condition=termination,\n            selector_prompt=selector_prompt,\n            allow_repeated_speaker=True,\n        )\n        await Console(team.run_stream(task=task))\n\n    # await model_client.close()\n\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\nExpected behavior\njust like the screenshot in the doc\n\nScreenshots\n\n\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\ngpt-4o-mini\nModel provider\nNone\nOther model provider\nNo response\nPython version\n3.13\n.NET version\nNone\nOperating system\nWindows", "created_at": "2025-03-23", "closed_at": "2025-03-23", "labels": ["proj-core"], "State": "closed", "Author": "HiFiChang"}
{"issue_number": 6070, "issue_title": "sqlite3.OperationalError: no such column: \"size\" - should this be a string literal in single-quotes?", "issue_body": "Description:\nWhen running the market analysis workflow using AG2 (pyautogen) with asynchronous swarm chat (via initiate_swarm_chat/a_initiate_swarm_chat), I encounter an SQLite OperationalError coming from the disk cache implementation. The error message is:\nsqlite3.OperationalError: no such column: \"size\" - should this be a string literal in single-quotes?\nThis error occurs during the caching process when an LLM response is being stored. The stack trace indicates that the error is raised in diskcache/core.py during the _row_insert operation, suggesting that the SQL query is referencing a column named size that does not exist in the cache table schema.\nSteps to Reproduce:\nUse AG2 version 0.8.3 (pyautogen installed from GitHub commit 338f89be05aebaaec6942ff11d6e9db8d29b18ad or similar).\nSet up a market analysis workflow that uses swarm chat (ainitiate_swarm_chat) with AssistantAgent instances.\nEnsure that the LLM client is caching responses using DiskCache.\nClear the disk cache (e.g., delete the .diskcache folder) to ensure no stale schema remains.\nRun the workflow. The error occurs when DiskCache attempts to insert a new row into the cache database.\nStack Trace Excerpt:\n...\n  File \".../autogen/oai/client.py\", line 1202, in create cache.set(key, response) File \".../autogen/cache/disk_cache.py\", line 69, in set self.cache.set(key, value) File \".../diskcache/core.py\", line 808, in set self._row_insert(db_key, raw, now, columns) File \".../diskcache/core.py\", line 857, in _row_insert sql( sqlite3.OperationalError: no such column: \"size\" - should this be a string literal in single-quotes?\nNotes:\nI have cleared the DiskCache directory (e.g., .diskcache), yet the error persists.\nThe error appears to be related to a mismatch between the expected cache table schema and the actual schema\u2014specifically, the missing \"size\" column.\nMy environment uses DiskCache version 5.6.3.\nThis error happens during a call to initiate_swarm_chat/a_initiate_swarm_chat when caching the LLM response.\nExpected Behavior:\nThe cache should be created with the correct schema, including a \"size\" column if required, so that caching operations complete successfully without raising an SQLite error.\nQuestions:\nIs this a known issue with the current version of autogen/pyautogen?\nShould the SQL statement in DiskCache be modified to use a literal (i.e., 'size') or is the schema itself supposed to include a column named size?\nAre there any recommended workarounds, such as a cache initialization step or version adjustment, to resolve this error?\ncode for reference:\nconfig file:\n...\nimport os\nfrom autogen import LLMConfig, config_list_from_dotenv\n\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\nenv_path = os.path.join(current_dir, '..', '..', '.env')\n\nconfig_list = config_list_from_dotenv(\n    dotenv_file_path=env_path,\n    model_api_key_map={\n        \"gpt-4o-mini\": \"OPENAI_API_KEY\"\n    }\n)\n\nllm_config = LLMConfig(\n    config_list=config_list         # example: set seed for reproducibility\n)\n\n\nagents file:\n\nfrom autogen import AssistantAgent\nfrom typing import Dict, List\nfrom .market_analyser_config import llm_config\nfrom .market_analyser_prompts import ORCHESTRATOR_PROMPT, RESEARCHER_PROMPT, ANALYST_PROMPT, REPORTER_PROMPT\n\nclass MarketAnalyserAgents:\n    \"\"\"Class containing the declarations of all agents needed for market analysis.\"\"\"\n    \n    def __init__(self, orchestrator_tools: Dict, researcher_tools: Dict, \n                 analyst_tools: Dict, reporter_tools: Dict):\n        \"\"\"Initialize the agents with their respective tools and prompts.\"\"\"\n        \n        # Orchestrator Agent - Coordinates the analysis process\n        self.orchestrator = AssistantAgent(\n            name=\"orchestrator\",\n            system_message=ORCHESTRATOR_PROMPT,\n            llm_config=llm_config,\n            function_map=orchestrator_tools\n        )\n        \n        # Researcher Agent - Gathers and validates market data\n        self.researcher = AssistantAgent(\n            name=\"researcher\",\n            system_message=RESEARCHER_PROMPT,\n            llm_config=llm_config,\n            function_map=researcher_tools\n        )\n        \n        # Analyst Agent - Performs technical and fundamental analysis\n        self.analyst = AssistantAgent(\n            name=\"analyst\",\n            system_message=ANALYST_PROMPT,\n            llm_config=llm_config,\n            function_map=analyst_tools\n        )\n        \n        # Reporter Agent - Compiles and formats the analysis report\n        self.reporter = AssistantAgent(\n            name=\"reporter\",\n            system_message=REPORTER_PROMPT,\n            llm_config=llm_config,\n            function_map=reporter_tools\n        )\n\n    @property\n    def all_agents(self) -> List[AssistantAgent]:\n        \"\"\"Get all agents in a list.\"\"\"\n        return [\n            self.orchestrator,\n            self.researcher,\n            self.analyst,\n            self.reporter\n        ]\n\nWorkflow file:\n\"\"\"Market Analysis Workflow using AG2's swarm capabilities.\"\"\"\n\nfrom typing import Dict, List, Optional, Union, Any\nfrom datetime import datetime\nimport asyncio\nimport json\nfrom autogen import (\n    AssistantAgent,\n    ConversableAgent,\n    GroupChat,\n    GroupChatManager,\n    OnCondition,\n    AfterWork,\n    AfterWorkOption,\n    register_hand_off,\n    SwarmResult,\n    ContextExpression,\n    OnContextCondition,\n    initiate_swarm_chat,\n    a_initiate_swarm_chat,\n)\n\n# Attempt to import register_swarm; if not available, define a dummy version.\ntry:\n    from autogen.agentchat.contrib.swarm_agent import register_swarm\nexcept ImportError:\n    def register_swarm(initial_agent, agents):\n        print(\"Warning: register_swarm is not available in this version. Skipping swarm registration.\")\n\n\nfrom .market_analyser_agents import MarketAnalyserAgents\nfrom .market_analyser_config import llm_config\nfrom .market_analyser_tools import get_tools_config\n\n\nclass MarketAnalysisWorkflow:\n    \"\"\"Manages the workflow for market analysis using a swarm of agents.\"\"\"\n\n    def __init__(self, tools_config: Dict[str, Dict]):\n        \"\"\"\n        Initialize the workflow with tools configuration for each agent.\n\n        Args:\n            tools_config: Dictionary containing tool configurations for each agent.\n        \"\"\"\n        self.agents = MarketAnalyserAgents(**tools_config)\n        self._setup_workflow()\n\n    def _setup_workflow(self):\n        \"\"\"Set up the workflow by registering handoffs between agents.\"\"\"\n        # Orchestrator -> Researcher handoffs\n        register_hand_off(\n            agent=self.agents.orchestrator,\n            hand_to=[\n                OnContextCondition(\n                    target=self.agents.researcher,\n                    condition=ContextExpression(\"${data_collection_needed} == True\"),\n                    available=\"data_collection_needed\",\n                ),\n                AfterWork(self.agents.reporter),  # Default to reporter if no conditions met\n            ],\n        )\n\n        # Researcher -> Analyst handoffs\n        register_hand_off(\n            agent=self.agents.researcher,\n            hand_to=[\n                OnContextCondition(\n                    target=self.agents.analyst,\n                    condition=ContextExpression(\"${data_collection_complete} == True\"),\n                    available=\"data_collection_complete\",\n                ),\n                OnCondition(\n                    target=self.agents.orchestrator,\n                    condition=\"Need more guidance on data collection\",\n                ),\n                AfterWork(self.agents.analyst),  # Default to analyst after data collection\n            ],\n        )\n\n        # Analyst -> Reporter handoffs\n        register_hand_off(\n            agent=self.agents.analyst,\n            hand_to=[\n                OnContextCondition(\n                    target=self.agents.reporter,\n                    condition=ContextExpression(\"${analysis_complete} == True\"),\n                    available=\"analysis_complete\",\n                ),\n                OnCondition(\n                    target=self.agents.researcher,\n                    condition=\"Need additional market data for analysis\",\n                ),\n                AfterWork(self.agents.reporter),  # Default to reporter after analysis\n            ],\n        )\n\n        # Reporter -> Orchestrator or Termination\n        register_hand_off(\n            agent=self.agents.reporter,\n            hand_to=[\n                OnContextCondition(\n                    target=self.agents.orchestrator,\n                    condition=ContextExpression(\"${report_ready} == False\"),\n                    available=\"report_ready\",\n                ),\n                AfterWork(AfterWorkOption.TERMINATE),  # End workflow after report is complete\n            ],\n        )\n\n    async def analyze_market_async(self, market_request: str) -> tuple:\n        \"\"\"\n        Initiate market analysis workflow with the given request.\n        \n        Args:\n            market_request: The market analysis request.\n        \n        Returns:\n            tuple: (chat_history, context_variables, last_active_agent)\n        \"\"\"\n        # Initialize context variables\n        initial_context = {\n            \"data_collection_needed\": True,  # Trigger initial Researcher handoff\n            \"data_collection_complete\": False,\n            \"analysis_complete\": False,\n            \"report_ready\": False,\n            \"market_request\": market_request,\n            \"timestamp\": datetime.now().isoformat(),\n            \"iteration_count\": 0,\n            \"last_agent\": None,\n            \"missing_data_points\": [],\n            \"analysis_status\": \"pending\",\n        }\n\n        try:\n            # Remove the register_swarm call because it's no longer required.\n            # Instead, use the internal swarm manager by setting after_work accordingly.\n            return await initiate_swarm_chat(\n                initial_agent=self.agents.orchestrator,\n                agents=self.agents.all_agents,\n                messages=[{\n                    \"role\": \"user\",\n                    \"content\": market_request\n                }],\n                context_variables=initial_context,\n                after_work=AfterWorkOption.SWARM_MANAGER,\n                swarm_manager_args={\"llm_config\": llm_config}\n            )\n        except Exception as e:\n            print(f\"\\nError in workflow execution: {str(e)}\")\n            import traceback\n            traceback.print_exc()\n            raise\n\n# Example usage (only runs if file is run directly)\nif __name__ == \"__main__\":\n    async def main():\n        # Get tools configuration\n        tools_config = get_tools_config()\n\n        # Create workflow instance\n        workflow = MarketAnalysisWorkflow(tools_config)\n\n        # Example market analysis request\n        market_request = \"\"\"\n        Analyze AAPL stock for the next week (1W) including:\n        1. Basic information: current price, volume, market cap\n        2. Technical indicators: moving averages (20, 50, 200-day), RSI, MACD\n        3. Fundamental data: P/E ratio, revenue growth, profit margins\n        4. Market sentiment: recent news, analyst ratings, institutional ownership\n\n        Please provide a comprehensive analysis with actionable insights.\n        \"\"\"\n\n        print(\"\\nStarting Market Analysis Workflow...\")\n        print(\"=====================================\")\n        print(f\"Request: {market_request}\")\n        print(\"=====================================\\n\")\n\n        try:\n            # Run the workflow\n            chat_history, context_variables, last_agent = await workflow.analyze_market_async(market_request)\n\n            print(\"\\nWorkflow Results:\")\n            print(\"=================\")\n            print(f\"Last Active Agent: {last_agent.name if last_agent else 'None'}\")\n            print(f\"Analysis Status: {context_variables.get('analysis_status', 'unknown')}\")\n            print(f\"Total Iterations: {context_variables.get('iteration_count', 0)}\")\n\n            print(\"\\nChat History:\")\n            print(\"=============\")\n            for message in chat_history:\n                print(f\"\\n{message['role'].upper()}:\")\n                print(message['content'])\n\n            print(\"\\nContext Variables:\")\n            print(\"==================\")\n            print(json.dumps(context_variables, indent=2))\n\n        except Exception as e:\n            print(f\"\\nError during workflow execution: {str(e)}\")\n            raise\n\n    # Run the async main function\n    asyncio.run(main())\n\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\ngpt-4o-mini\nModel provider\nOpenAI\nOther model provider\nNo response\nPython version\n3.11\n.NET version\nNone\nOperating system\nMacOS", "created_at": "2025-03-22", "closed_at": "2025-03-23", "labels": ["needs-triage"], "State": "closed", "Author": "yogyagit"}
{"issue_number": 6069, "issue_title": "Request contains an invalid argument Gemini", "issue_body": "What happened?\nWhen calling the Gemini API via OpenAIClient If tool has any other datatype than string type it fails.\nit seems that the OpenAI code base generates a JSON Schema that is not (yet) accepted by the Gemini API. Gemini\u2019s OpenAI compatibility is still marked as Experimental and its response_format is based on a select subset of the OpenAPI 3.03 specification which does not handle all attributes that OpenAI might generate.\nReferences:\nhttps://ai.google.dev/gemini-api/docs/openai#structured-output\n(important one!)\nThe Schema represents a select subset of the OpenAPI 3.0 Schema object\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=\"API_KEY\", base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n\ntools= [{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_valid_service_package_options\",\n            \"description\": \"Validates the service name provided by the customer against the list of valid service package options available at a specific salon branch. This ensures that the correct service name is consistently used throughout subsequent processes. This function returns validated service name, the list of valid service package options (if requested), or an error message if validation fails.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"Salons_Branch_Name\": {\n                        \"description\": \"A required parameter. Represents the valid salon location name selected by customer.\",\n                        \"title\": \"Salons Branch Name\",\n                        \"type\": \"string\",\n                    },\n                    \"Salon_Service\": {\n                        \"default\": [],\n                        \"description\": \"An optional parameter. Represents list of valid salon services selected by the customer. If parameter provided, the function will filter and return only stylists who offer these specific services.\",\n                        \"items\": {},\n                        \"title\": \"Salon Service\",\n                        \"type\": \"array\",\n                    },\n                },\n                \"required\": [\"Salons_Branch_Name\"],\n                \"additionalProperties\": False,\n            },\n            \"strict\": False,\n        },\n    }]\n\nmessages = [\n    {\n        \"content\": \"Role: You are a Receptionist at a Salon. Your job is to book an appointment.\",\n        \"role\": \"system\",\n    },\n    {\"content\": \"book appointemnt\", \"role\": \"user\", \"name\": \"user\"},\n]\nresponse = client.chat.completions.create(model=\"gemini-2.0-flash\", messages=messages, tools=tools, tool_choice=\"auto\")\n\nprint(response)\n\n\nWhich packages was the bug in?\nPython Core (autogen-core)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\ngemini-2.0 falsh\nModel provider\nGoogle Gemini\nOther model provider\nNo response\nPython version\n3.10\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-22", "closed_at": null, "labels": ["needs-triage"], "State": "open", "Author": "gilada-shubham"}
{"issue_number": 6064, "issue_title": "Improper handling of MultimodalWebSurfer content led to premature task termination", "issue_body": "What happened?\nDescribe the bug\nWhen using MultimodalWebSurfer in MagenticOneGroupChat, the task prematurely terminates after the first WebSurfer action. This is caused by improper handling of multimodal content (images + text) in the MagenticOneOrchestrator's progress assessment logic.\nTo Reproduce\nfrom autogen_ext.teams.magentic_one import MagenticOne\n\nasync def main():\n\n    client = xxx\n    autoctf = MagenticOne(client=client)\n    task = \"open bing.com search weather then click third result link\"\n    result = await autoctf.run_stream(task=task)\n\n# The task terminates after WebSurfer's first action without completing click\nThe issue occurs because:\n\nWebSurfer returns MultiModalMessage containing both text and image\nMagenticOneOrchestrator fails to properly process this multimodal content when assessing task progress\nThis leads to incorrect termination assessment in _orchestrate_step\n\nExpected behavior\n\nThe WebSurfer should be able to perform multiple actions as needed\nTask should continue until the actual goal is achieved\nThe orchestrator should properly handle multimodal content in progress assessment\n\nTechnical Details\nThe issue occurs in two key components:\n\nMagenticOneOrchestrator._thread_to_context:\n\n# Original problematic code\nif isinstance(m, (TextMessage, MultiModalMessage, ToolCallSummaryMessage)):\n    context.append(UserMessage(content=m.content, source=m.source))\n\nProgress assessment logic incorrectly processes multimodal content, leading to premature task completion judgment.\n\nFix Implementation\nThe fix involves properly handling MultiModalMessage content in the orchestrator:\n# Fixed version\nif isinstance(m, MultiModalMessage):\n    if isinstance(m.content, list) and len(m.content) > 0:\n        content = m.content[0] if isinstance(m.content[0], str) else str(m.content[0])\n    else:\n        content = str(m.content)\nelse:\n    content = m.content\ncontext.append(UserMessage(content=content, source=m.source))\nAdditional context\n\nThis issue specifically affects tasks that require multiple WebSurfer actions\nThe issue affects standard MagenticOneGroupChat implementations\n\nEnvironment\n\nAutoGen Studio Version: 0.4.2\nPython Version: 3.10\nOS: Tested on both Linux and Windows\n\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nqwen-vl-max-latest\nModel provider\nOther (please specify below)\nOther model provider\nQwen\nPython version\n3.10\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-22", "closed_at": "2025-04-01", "labels": ["needs-triage"], "State": "closed", "Author": "ode126"}
{"issue_number": 6060, "issue_title": "Allow registration of instances instead of factories", "issue_body": "Add to the runtime, the ability to register a single or multiple instances for an agent type.\nThis is mutually exclusive with factory based registration.", "created_at": "2025-03-21", "closed_at": null, "labels": [], "State": "open", "Author": "jackgerrits"}
{"issue_number": 6055, "issue_title": "AutoGen Studio 0.4.2: Copy-pasting in Tool Name field kicks user back and breaks editing", "issue_body": "What happened?\nDescribe the bug\nWhen I copy-paste into the \"Name\" field of a tool within an agent in AutoGen Studio (0.4.2), I am kicked back to the agent editing window. After that, trying to re-edit the tool results in the tool's name being repeatedly appended to the top of the window, rendering the tool and agent uneditable (see screenshot below).\nTo Reproduce\n\nCreate a new team in the AutoGen UI (Studio 0.4.2)\nEdit the default agent.\nClick to edit the default agent's \"calculator\" tool.\nCopy/paste using shortcuts (Ctrl+C / Ctrl+V) a new Python tool to replace the \"calculator\" tool.\nThen, try to copy-paste the name of the new tool to replace the name of the default tool.\nYou should then be kicked back to the agent editing window. Note: This step is not consistently reproducible; you may need to attempt copy-pasting (Ctrl+C / Ctrl+V) in the \"Name\" field multiple times.\nTry to re-edit the tool. You should observe the tool's name being repeatedly appended to the top of the window, and the tool/agent becoming uneditable.\n\nExpected behavior\nCopy-pasting into the tool's \"Name\" field should update the name without kicking the user back to the agent editing window and without rendering the tool/agent uneditable.\nScreenshots\n\nAdditional context\nThe issue is temporarily resolved by saving the agent and reopening the editing window. All changes, including the copy-pasted tool name, are saved.\nWhich packages was the bug in?\nAutoGen Studio (autogensudio)\nAutoGen library version.\nPython dev (main branch)\nPython version\n3.12\nOperating system\nWindows", "created_at": "2025-03-21", "closed_at": null, "labels": ["proj-studio"], "State": "open", "Author": "usag1e"}
{"issue_number": 6053, "issue_title": "Allow `SelectorGroupChat`'s `selector_func` function to terminate the group chat by returning a `StopMessage`", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nBecause it is often easier to implement termination logic directly when selecting speakers, and this also avoid additional messages from a \"terminator\" agent to be emitted to the stream.", "created_at": "2025-03-21", "closed_at": "2025-03-21", "labels": ["proj-agentchat"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 6052, "issue_title": "Support async `selector_func` and `candidate_func` in `SelectorGroupChat`", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nBecause model clients are async and we want to be able to use that in selector functions.", "created_at": "2025-03-21", "closed_at": "2025-03-22", "labels": ["help wanted", "proj-agentchat"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 6049, "issue_title": "Add `CreateResult.request_id` field for tracking model usage.", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nIf the model API returns a request_id field, we should assign it to CreateResult.request_id.", "created_at": "2025-03-21", "closed_at": "2025-04-01", "labels": ["good first issue", "help wanted", "proj-extensions"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 6048, "issue_title": "`AssistantAgent.metadata` for user/application identity information associated with the agent.", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nmetadata field should be a Dict[str, str] allowing the application to assign identities to agents.\nit should be also included in the config for proper serialization.", "created_at": "2025-03-21", "closed_at": "2025-03-23", "labels": ["good first issue", "help wanted", "proj-agentchat"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 6041, "issue_title": "Code Executor should by default use a temp directory for working directory rather than the current directory.", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nRight now, all code executors under python/packages/autogen-ext/src/autogen_ext/code_executors are defaulting to \".\" -- the current directory, as the working directory for reading and writing files.\nThis introduces a security issue as the current directory may contain code files and other data you don't want to expose to a model.\nTo fix this, the code executors' constructor parameter's default for work_dir should be None, and a temp directory should be created upon the start() method is called, and the temp directory is closed when stop() method is called.\nFor backward compatibility for LocalCommandLineExecutor and ACADynamicSessionsCodeExecutor, which didn't have start() and stop() methods prior to #6040, we should keep using \".\" as the default work directory if the start() method was not called and work directory was not provided by the user, and emit a DeprecationWarning to reminder user of the best practice.\nFor temporary directory, see Python's tempfile: https://docs.python.org/3/library/tempfile.html#tempfile.TemporaryDirectory", "created_at": "2025-03-20", "closed_at": "2025-04-01", "labels": ["help wanted", "code-execution", "proj-extensions"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 6037, "issue_title": "[MCP server issue] Cannot access MCP server with env", "issue_body": "What happened?\nDescribe the bug\nI installed a Stdio MCP server following the configuration:\n\"github.com/mendableai/firecrawl-mcp-server\": {\n\"command\": \"npx\",\n\"args\": [\"-y\", \"firecrawl-mcp\"],\n\"env\": {\n\"FIRECRAWL_API_KEY\": \"fc-294547563d9344ef8409bbb8e80f5307\"\n},\n\"disabled\": false,\n\"autoApprove\": []\n}\n}\nHowever, when I ran it in AutoGEN 0.4, it throws an unexpected error when registering MCP tools:\n        logger.info(\"Setting up fetch MCP server...\")\n        fetch_mcp_server = StdioServerParams(command=\"env\", args=[\"FIRECRAWL_API_KEY=fc-294547563d9344ef8409bbb8e80f5307\", \"npx\", \"-y\" ,\"firecrawl-mcp\"])\n        fetch_tools = await mcp_server_tools(fetch_mcp_server)\n        logger.info(f\"Fetch tools loaded: {len(fetch_tools)} tools\")\n\nPrinted on the console it shows the successful onboard of this tool but:\nFireCrawl MCP Server running on stdio\n/home/ec2-user/bedrock-access-gw/bedrock-access-gateway/test-script/venv/lib64/python3.12/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"schema\" in \"DynamicModel\" shadows an attribute in parent \"BaseModel\"\n  warnings.warn(\n2025-03-20 09:15:17,824 - ERROR\nAn error occurred: Each oneOf variant must have a type const\nTraceback (most recent call last):\n  File \"/home/ec2-user/bedrock-access-gw/bedrock-access-gateway/test-script/test_mcp.py\", line 99, in main\n    fetch_tools = await mcp_server_tools(fetch_mcp_server)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/bedrock-access-gw/bedrock-access-gateway/test-script/venv/lib64/python3.12/site-packages/autogen_ext/tools/mcp/_factory.py\", line 139, in mcp_server_tools\n    return [StdioMcpToolAdapter(server_params=server_params, tool=tool) for tool in tools.tools]\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/bedrock-access-gw/bedrock-access-gateway/test-script/venv/lib64/python3.12/site-packages/autogen_ext/tools/mcp/_stdio.py\", line 48, in __init__\n    super().__init__(server_params=server_params, tool=tool)\n  File \"/home/ec2-user/bedrock-access-gw/bedrock-access-gateway/test-script/venv/lib64/python3.12/site-packages/autogen_ext/tools/mcp/_base.py\", line 36, in __init__\n    input_model = create_model(tool.inputSchema)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/bedrock-access-gw/bedrock-access-gateway/test-script/venv/lib64/python3.12/site-packages/json_schema_to_pydantic/__init__.py\", line 42, in create_model\n    return builder.create_pydantic_model(schema, root_schema)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/bedrock-access-gw/bedrock-access-gateway/test-script/venv/lib64/python3.12/site-packages/json_schema_to_pydantic/model_builder.py\", line 62, in create_pydantic_model\n    field_type = self._get_field_type(field_schema, root_schema)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/bedrock-access-gw/bedrock-access-gateway/test-script/venv/lib64/python3.12/site-packages/json_schema_to_pydantic/model_builder.py\", line 92, in _get_field_type\n    return self.combiner_handler.handle_one_of(field_schema, root_schema)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/bedrock-access-gw/bedrock-access-gateway/test-script/venv/lib64/python3.12/site-packages/json_schema_to_pydantic/handlers.py\", line 135, in handle_one_of\n    raise CombinerError(\"Each oneOf variant must have a type const\")\njson_schema_to_pydantic.exceptions.CombinerError: Each oneOf variant must have a type const\nTraceback (most recent call last):\n  File \"/home/ec2-user/bedrock-access-gw/bedrock-access-gateway/test-script/test_mcp.py\", line 173, in <module>\n    asyncio.run(main())\n  File \"/usr/lib64/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib64/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib64/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/bedrock-access-gw/bedrock-access-gateway/test-script/test_mcp.py\", line 99, in main\n    fetch_tools = await mcp_server_tools(fetch_mcp_server)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/bedrock-access-gw/bedrock-access-gateway/test-script/venv/lib64/python3.12/site-packages/autogen_ext/tools/mcp/_factory.py\", line 139, in mcp_server_tools\n    return [StdioMcpToolAdapter(server_params=server_params, tool=tool) for tool in tools.tools]\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/bedrock-access-gw/bedrock-access-gateway/test-script/venv/lib64/python3.12/site-packages/autogen_ext/tools/mcp/_stdio.py\", line 48, in __init__\n    super().__init__(server_params=server_params, tool=tool)\n  File \"/home/ec2-user/bedrock-access-gw/bedrock-access-gateway/test-script/venv/lib64/python3.12/site-packages/autogen_ext/tools/mcp/_base.py\", line 36, in __init__\n    input_model = create_model(tool.inputSchema)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/bedrock-access-gw/bedrock-access-gateway/test-script/venv/lib64/python3.12/site-packages/json_schema_to_pydantic/__init__.py\", line 42, in create_model\n    return builder.create_pydantic_model(schema, root_schema)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/bedrock-access-gw/bedrock-access-gateway/test-script/venv/lib64/python3.12/site-packages/json_schema_to_pydantic/model_builder.py\", line 62, in create_pydantic_model\n    field_type = self._get_field_type(field_schema, root_schema)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/bedrock-access-gw/bedrock-access-gateway/test-script/venv/lib64/python3.12/site-packages/json_schema_to_pydantic/model_builder.py\", line 92, in _get_field_type\n    return self.combiner_handler.handle_one_of(field_schema, root_schema)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/bedrock-access-gw/bedrock-access-gateway/test-script/venv/lib64/python3.12/site-packages/json_schema_to_pydantic/handlers.py\", line 135, in handle_one_of\n    raise CombinerError(\"Each oneOf variant must have a type const\")\njson_schema_to_pydantic.exceptions.CombinerError: Each oneOf variant must have a type const\n\nTo Reproduce\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\n-mcp\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\n3.12\n.NET version\nNone\nOperating system\nCentOS", "created_at": "2025-03-20", "closed_at": null, "labels": ["needs-triage", "awaiting-op-response"], "State": "open", "Author": "Yuanqi-babe"}
{"issue_number": 6035, "issue_title": "Issue: TypeError in MultimodalWebSurfer (and maybe.. sending Image too) when using Anthropic models (at newist version of anthropic)", "issue_body": "What happened?\nDescribe the bug\nWhen using the MultimodalWebSurfer with an Anthropic model, a TypeError: Cannot instantiate typing.Union occurs. This error arises because the typing.Union type hint is being incorrectly used as a class constructor within the _anthropic_client.py file, specifically when attempting to convert a user message to the format required by the Anthropic API(at newist version:=after 0.48.0 of anthropic).\nTo Reproduce\nfrom autogen_ext.agents.web_surfer import MultimodalWebSurfer\nfrom autogen_ext.models.anthropic import AnthropicChatCompletionClient\nfrom autogen_agentchat.messages import TextMessage, MultiModalMessage\nfrom autogen_agentchat.ui import Console\nfrom autogen_core import CancellationToken\n# Assuming you have an Anthropic API key set as an environment variable.\n\n# Create an Anthropic model client.  The specific configuration\n# (model name, etc.) might need adjustment to match your setup.\n\nanthropic_client = AnthropicChatCompletionClient(\n    model=\"claude-3-7-sonnet-latest\",\n    api_key=\"[OWN API KEY]\",\n)\n\n# Create a MultimodalWebSurfer_agent.\nagent = MultimodalWebSurfer(name=\"MultimodalWebSurfer\", model_client=anthropic_client)\n\n# Attempt to send a message that requires multimodal processing (e.g., an image).\n# The exact message structure will depend on how the agent is used, but the\n# key is that it needs to trigger the code path that leads to the TypeError.\n# The following is a placeholder; you'll likely need to adapt it.\nasync def assistant_run() -> None:\n    response = await agent.on_messages(\n        [\n            TextMessage(content=\"Search 2025/03/01 weather\", source=\"user\"),\n        ],\n        cancellation_token=CancellationToken(),\n    )\n    print(response.inner_messages)\n    print(response.chat_message)\n\nimport asyncio\nasyncio.run(assistant_run())\nStack Trace\nsource='MultimodalWebSurfer' models_usage=None metadata={} content='Web surfing error:\\n\\nTraceback (most recent call last):\\n  File \"/home/cysong/work/TIPS/test-agent/.venv/lib/python3.12/site-packages/autogen_ext/agents/web_surfer/_multimodal_web_surfer.py\", line 442, in on_messages_stream\\n    content = await self._generate_reply(cancellation_token=cancellation_token)\\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/home/cysong/work/TIPS/test-agent/.venv/lib/python3.12/site-packages/autogen_ext/agents/web_surfer/_multimodal_web_surfer.py\", line 612, in _generate_reply\\n    response = await self._model_client.create(\\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/home/cysong/work/TIPS/test-agent/.venv/lib/python3.12/site-packages/autogen_ext/models/anthropic/_anthropic_client.py\", line 451, in create\\n    anthropic_message = to_anthropic_type(message)\\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/home/cysong/work/TIPS/test-agent/.venv/lib/python3.12/site-packages/autogen_ext/models/anthropic/_anthropic_client.py\", line 256, in to_anthropic_type\\n    return user_message_to_anthropic(message)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/home/cysong/work/TIPS/test-agent/.venv/lib/python3.12/site-packages/autogen_ext/models/anthropic/_anthropic_client.py\", line 165, in user_message_to_anthropic\\n    source=Source(\\n           ^^^^^^^\\n  File \"/home/cysong/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/typing.py\", line 1184, in __call__\\n    result = self.__origin__(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/home/cysong/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/typing.py\", line 501, in __call__\\n    raise TypeError(f\"Cannot instantiate {self!r}\")\\nTypeError: Cannot instantiate typing.Union\\n' type='TextMessage'\n\nExpected behavior\nThe MultimodalWebSurfer should process multimodal inputs (including images) correctly when using an Anthropic model, without raising a TypeError.  The agent should be able to communicate with the Anthropic API and handle the response.\nScreenshots\n(Not applicable, as the error is a traceback.)\nAdditional context\nPython version: 3.12 (from the traceback)\nOS: Linux (from the traceback, x86_64) (It could reproduce at macOS too)\nautogen_core.version: '0.4.9.2'\nautogen_ext.version: '0.4.9.2'\nanthropic.version:'0.49.0'\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nOther (please specify)\nOther library version.\nPython 0.4.9.2\nModel used\nclaude-3-7-sonnet-latest\nModel provider\nAnthropic\nOther model provider\nNo response\nPython version\n3.12\n.NET version\nNone\nOperating system\nUbuntu", "created_at": "2025-03-20", "closed_at": "2025-03-22", "labels": [], "State": "closed", "Author": "SongChiYoung"}
{"issue_number": 6034, "issue_title": "Make usage of `name` field in OpenAI messages optional in `OpenAIChatCompletionClient`", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nMany \"openai-compatible\" model providers don't support the name field in message.\ne.g, https://console.groq.com/docs/openai\nAdd include_name_in_message parameter to BaseOpenAIChatCompletionClient, and add to BaseOpenAIClientConfiguration. Default should be True.\nThen, in user_message_to_oai and assistant_message_to_oai, we decide whether to use the name field based on the option set.\n\n\n\nautogen/python/packages/autogen-ext/src/autogen_ext/models/openai/_openai_client.py\n\n\n        Lines 182 to 183\n      in\n      4835321\n\n\n\n\n\n\n name=message.source, \n\n\n\n ) \n\n\n\n\n\n\n\n\nautogen/python/packages/autogen-ext/src/autogen_ext/models/openai/_openai_client.py\n\n\n         Line 222\n      in\n      4835321\n\n\n\n\n\n\n name=message.source, \n\n\n\n\n\nFor model APIs that don't support this field, we document in the usage guide about this option.", "created_at": "2025-03-20", "closed_at": null, "labels": ["good first issue", "help wanted", "proj-extensions"], "State": "open", "Author": "ekzhu"}
{"issue_number": 6032, "issue_title": "Introduce `OpenAIAgent` backed by the Response API in Extensions.", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nAssistant API will be deprecated in 2026, replaced by Response API: https://platform.openai.com/docs/guides/responses-vs-chat-completions.\nWe should start support the Response API by introducing OpenAIAgent that is backed by the Response API, which can be stateful.\nThe new agent should conform to the behavior of AssistantAgent in AgentChat, while backed directly by the openai library.\nThoughts, feedback welcome!", "created_at": "2025-03-20", "closed_at": null, "labels": ["help wanted", "proj-extensions"], "State": "open", "Author": "ekzhu"}
{"issue_number": 6031, "issue_title": "MCP tool should have a timeout setting.", "issue_body": "The code is as follows\nmcp_email_server = StdioServerParams(\n    command=\"python\", args=[\"-m\", \"mcp_servers.mcp_mail_processor\"]\n)\ntools_email_processor = await mcp_server_tools(mcp_email_server)\nIf an error occurs, like this\nC:\\Users\\jialiu3\\AppData\\Local\\miniforge3\\envs\\office\\python.exe: No module named mcp_servers.mcp_mail_processor\nQuestion:\nI don't know why the program freezes when an error is detected, even with try-except. Can anyone help me?\nI hope that when such an error is reported, the program can exit normally or execute the following code in sequence.\nOriginally posted by @Septa2112 in #6030\nWe need to add a timeout_seconds setting to this:\n\n\n\nautogen/python/packages/autogen-ext/src/autogen_ext/tools/mcp/_session.py\n\n\n        Lines 13 to 14\n      in\n      855bcd7\n\n\n\n\n\n\n server_params: McpServerParams, \n\n\n\n ) -> AsyncGenerator[ClientSession, None]: \n\n\n\n\n\nand add timeout setting to McpToolAdapter and mcp_server_tools.", "created_at": "2025-03-20", "closed_at": "2025-03-26", "labels": ["good first issue", "help wanted", "proj-extensions"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 6028, "issue_title": "\"TypeError: BaseGroupChat.run() called with 2 arguments when RoundRobinGroupChat.run() expects 1\"", "issue_body": "What happened?\nThe versions of autogen-core and autogen-agentchat (0.4.9.2).\nThe output of the check_signature.py script:\nRoundRobinGroupChat.run argument count: 1\nError during team.run(): BaseGroupChat.run() takes 1 positional argument but 2 were given.\nCode snippet:\nimport asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_core.model_context import BufferedChatCompletionContext\nfrom config import gemini_model\nasync def minimal_example():\nevaluator = AssistantAgent(\nname=\"Evaluator\",\nmodel_client=gemini_model,\nsystem_message=\"You are a test agent.\",\nmodel_context=BufferedChatCompletionContext(buffer_size=10),\n)\ndecision_maker = AssistantAgent(\nname=\"DecisionMaker\",\nmodel_client=gemini_model,\nsystem_message=\"You are a test agent.\",\nmodel_context=BufferedChatCompletionContext(buffer_size=10),\n)\ntermination_condition = TextMentionTermination(\"APPROVE|REJECT\")\nteam = RoundRobinGroupChat([evaluator, decision_maker], termination_condition=termination_condition)\n\ntest_message = TextMessage(content=\"Test message\", source=\"user\")\ntry:\n    await team.run([test_message])\nexcept TypeError as e:\n    print(f\"Error: {e}\")\n\nif name == \"main\":\nasyncio.run(minimal_example())\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-20", "closed_at": "2025-03-20", "labels": ["needs-triage"], "State": "closed", "Author": "avps03042000"}
{"issue_number": 6021, "issue_title": "Should be able to simply provide a team to autogen bench", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nProvide a team instead of defining a dockerfile for ag bench", "created_at": "2025-03-19", "closed_at": null, "labels": ["proj-autogenbench"], "State": "open", "Author": "jackgerrits"}
{"issue_number": 6020, "issue_title": "Out of the box benchmarks for autogen bench", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nUsed to measure safety as well as existing benchmarks like human eval", "created_at": "2025-03-19", "closed_at": null, "labels": ["proj-autogenbench"], "State": "open", "Author": "jackgerrits"}
{"issue_number": 6019, "issue_title": "AutoGenBench UX improvements", "issue_body": "Improve usage experience of AGBench", "created_at": "2025-03-19", "closed_at": null, "labels": ["proj-autogenbench"], "State": "open", "Author": "jackgerrits"}
{"issue_number": 6018, "issue_title": "AutoGenBench should include evaluation framework", "issue_body": "Include a system which the executing benchmark can provide performance metrics", "created_at": "2025-03-19", "closed_at": null, "labels": ["proj-autogenbench"], "State": "open", "Author": "jackgerrits"}
{"issue_number": 6017, "issue_title": "Guardrails and Safety", "issue_body": "Tracking issue for work related to agent guardrails and safety.", "created_at": "2025-03-19", "closed_at": null, "labels": ["epic"], "State": "open", "Author": "jackgerrits"}
{"issue_number": 6016, "issue_title": "Docker Code Executor should be Cancellable", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nDocker Code Executor is not cancellable. This can be problematic as there is no easy way to interrupt code execution.  @ekzhu", "created_at": "2025-03-19", "closed_at": "2025-03-19", "labels": ["code-execution"], "State": "closed", "Author": "husseinmozannar"}
{"issue_number": 6015, "issue_title": "Add `start` and `stop` methods to `CodeExecutor` base class and implements context manager interface", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nSo, all code executors can have explicit start and and stop as well as work as context managers.", "created_at": "2025-03-19", "closed_at": "2025-03-20", "labels": ["code-execution"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 6014, "issue_title": "Agent Networks", "issue_body": "Tracking issue for work centered around decentralized, distributed and/or networks of agents interoperating.\n\n[Exploration] Decentralized communication layer\n[Exploration] Discovery protocol for agents to find each other For communication protocol:\n[Exploration] Guardrails and verification techniques\nInteragent communication standard\n", "created_at": "2025-03-19", "closed_at": null, "labels": ["epic"], "State": "open", "Author": "jackgerrits"}
{"issue_number": 6013, "issue_title": "Support cancellation for `DockerCommandLineCodeExecutor`.", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nCurrent it doesn't. We need to support it via setting the cancellation_token.", "created_at": "2025-03-19", "closed_at": "2025-03-20", "labels": ["code-execution", "proj-extensions"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 6012, "issue_title": "Model support tiers", "issue_body": "In order for users to better understand what models are supported and in what capacity we should define model support tiers. Roughly to the effect of:\n\nWhere: Dedicated client, via semantic kernel adapter, openai compatible api, community client, none\nSupport level: Best effort by AutoGen team, or community supported\nTesting: Automatically tested in CI, tests in test project, no tests\nFeature support: Streaming, tool calls, etc\n\nThis should be decided for each common or requested model and documented on the docs site", "created_at": "2025-03-19", "closed_at": null, "labels": [], "State": "open", "Author": "jackgerrits"}
{"issue_number": 6011, "issue_title": "Model Support", "issue_body": "Tracking issue for overall area of model support in AutoGen. See sub-issues for specific areas as well as following areas:\n\nWe need to ensure that expectations are clear for which models are supported, and in what capacity.\nWhat model providers need support added?\n[Exploration] Automatic model selection\n[Exploration] Prompt fine tuning\n", "created_at": "2025-03-19", "closed_at": null, "labels": ["epic"], "State": "open", "Author": "jackgerrits"}
{"issue_number": 6009, "issue_title": "OperationalError: no such column: \"size\" - should this be a string literal in single-quotes?", "issue_body": "What happened?\nI encountered the following error when running the code:\nOperationalError                          Traceback (most recent call last)\nCell In[3], line 9\n6 user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config=False)\n8 # Start the chat\n----> 9 user_proxy.initiate_chat(\n10     assistant,\n11     message=\"Tell me a joke about NVDA and TESLA stock prices.\",\n12 )\nFile ~/miniconda3/envs/thangcn/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:1503, in ConversableAgent.initiate_chat(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\n1501     else:\n1502         msg2send = self.generate_init_message(message, **kwargs)\n-> 1503     self.send(msg2send, recipient, silent=silent)\n1504 summary = self._summarize_chat(\n1505     summary_method,\n1506     summary_args,\n1507     recipient,\n1508     cache=cache,\n1509 )\n1510 for agent in [self, recipient]:\nFile ~/miniconda3/envs/thangcn/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:1187, in ConversableAgent.send(self, message, recipient, request_reply, silent)\n1185 valid = self._append_oai_message(message, \"assistant\", recipient, is_sending=True)\n1186 if valid:\n...\n873         value,\n874     ),\n875 )\nOperationalError: no such column: \"size\" - should this be a string literal in single-quotes?\nMy code:\nimport os\nfrom autogen import AssistantAgent, UserProxyAgent\n\nllm_config = { \"config_list\": [{ \"model\": \"gpt-4o\", \"api_key\": os.environ.get(\"OPENAI_API_KEY\") }] }\nassistant = AssistantAgent(\"assistant\", llm_config=llm_config)\nuser_proxy = UserProxyAgent(\"user_proxy\", code_execution_config=False)\n\n# Start the chat\nuser_proxy.initiate_chat(\n    assistant,\n    message=\"Tell me a joke about NVDA and TESLA stock prices.\",\n)\nWhich packages was the bug in?\nV0.2 (autogen-agetnchat==0.2.*)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\n3.10\n.NET version\nNone\nOperating system\nUbuntu", "created_at": "2025-03-19", "closed_at": null, "labels": ["0.2", "needs-triage"], "State": "open", "Author": "thangcn1943"}
{"issue_number": 6006, "issue_title": "AutoGen Studio\u89c6\u9891\u6559\u7a0b-\u4e2d\u6587\u7248", "issue_body": "What is the doc issue?\nhttps://youtu.be/6ymRgLtsJZk\nLink to the doc page, if applicable\nhttps://youtu.be/6ymRgLtsJZk", "created_at": "2025-03-19", "closed_at": "2025-03-19", "labels": ["documentation", "needs-triage"], "State": "closed", "Author": "win4r"}
{"issue_number": 5994, "issue_title": "Autogen 0.4.9.2 doesn't work with OpenAI 1.66.3", "issue_body": "What happened?\nDescribe the bug\nWe are having issues with autogen 0.4.9 and openai 1.66.3. It says that there is a module error \"from openai.types.beta.vector_store import VectorStore ModuleNotFoundError: No module named 'openai.types.beta.vector_store'\". It says to do this \"Please install autogen-ext with the \"openai\" extra: pip install \"autogen-ext[openai]\" but that doesn't work.\nTo Reproduce\nUpgrade to open 1.66.3 and it will cause errors when trying to use the vector stores/ OpenAIAssistantAgent. It seems to be something in the openAI extension. OpenAIAssistantAgent class seems to not be working as it says there is module missing. The is in the error: from autogen_ext.agents.openai import OpenAIAssistantAgent\nExpected behavior\nExpected to not have this error show up and for it to call OpenAIAssistant\nWhich packages was the bug in?\nPython Extensions (autogen-ext), Python AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nAzure OpenAI\nOther model provider\nNo response\nPython version\n3.12\n.NET version\nNone\nOperating system\nMacOS", "created_at": "2025-03-18", "closed_at": "2025-03-19", "labels": ["needs-triage"], "State": "closed", "Author": "anabellewatt"}
{"issue_number": 5992, "issue_title": "Documentation on how to trace AgentChat", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nWe would like to have a guide in AgentChat doc for how to set up and use tracing for AgentChat.\n\nExplain tracing\nThe simplest way to set up tracing and sending OpenTelemetry traces to an observability service.\nHow to add your own traces, e.g., in your custom tools and agents.\nDistributed tracing via grpc agent runtime.\n\nThe new guide should be placed in https://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide either as a Jupyter Notebook or a Markdown file.\nWe can use this example for tracing a group chat:\nimport asyncio\n\n\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.sdk.resources import Resource\nfrom autogen_core import SingleThreadedAgentRuntime\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Tracing setup.\ndef configure_oltp_tracing():\n    jaeger_exporter = OTLPSpanExporter(endpoint=\"http://localhost:4317\", insecure=True)\n    tracer_provider = TracerProvider(\n        resource=Resource({\"service.name\": \"autogen-test-agentchat\"})\n    )\n    span_processor = BatchSpanProcessor(jaeger_exporter)\n    tracer_provider.add_span_processor(span_processor)\n    trace.set_tracer_provider(tracer_provider)\n    return tracer_provider\n\ndef search_web_tool(query: str) -> str:\n    if \"2006-2007\" in query:\n        return \"\"\"Here are the total points scored by Miami Heat players in the 2006-2007 season:\n        Udonis Haslem: 844 points\n        Dwayne Wade: 1397 points\n        James Posey: 550 points\n        ...\n        \"\"\"\n    elif \"2007-2008\" in query:\n        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\"\n    elif \"2008-2009\" in query:\n        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\"\n    return \"No data found.\"\n\n\ndef percentage_change_tool(start: float, end: float) -> float:\n    return ((end - start) / start) * 100\n\n\nasync def main():\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    planning_agent = AssistantAgent(\n        \"PlanningAgent\",\n        description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n        model_client=model_client,\n        system_message=\"\"\"\n        You are a planning agent.\n        Your job is to break down complex tasks into smaller, manageable subtasks.\n        Your team members are:\n            WebSearchAgent: Searches for information\n            DataAnalystAgent: Performs calculations\n\n        You only plan and delegate tasks - you do not execute them yourself.\n\n        When assigning tasks, use this format:\n        1. <agent> : <task>\n\n        After all tasks are complete, summarize the findings and end with \"TERMINATE\".\n        \"\"\",\n    )\n\n    web_search_agent = AssistantAgent(\n        \"WebSearchAgent\",\n        description=\"An agent for searching information on the web.\",\n        tools=[search_web_tool],\n        model_client=model_client,\n        system_message=\"\"\"\n        You are a web search agent.\n        Your only tool is search_tool - use it to find information.\n        You make only one search call at a time.\n        Once you have the results, you never do calculations based on them.\n        \"\"\",\n    )\n\n    data_analyst_agent = AssistantAgent(\n        \"DataAnalystAgent\",\n        description=\"An agent for performing calculations.\",\n        model_client=model_client,\n        tools=[percentage_change_tool],\n        system_message=\"\"\"\n        You are a data analyst.\n        Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.\n        If you have not seen the data, ask for it.\n        \"\"\",\n    )\n\n    text_mention_termination = TextMentionTermination(\"TERMINATE\")\n    max_messages_termination = MaxMessageTermination(max_messages=25)\n    termination = text_mention_termination | max_messages_termination\n\n    selector_prompt = \"\"\"Select an agent to perform task.\n\n    {roles}\n\n    Current conversation context:\n    {history}\n\n    Read the above conversation, then select an agent from {participants} to perform the next task.\n    Make sure the planner agent has assigned tasks before other agents start working.\n    Only select one agent.\n    \"\"\"\n\n    task = \"Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\"\n    tracer_provider = configure_oltp_tracing()\n    runtime = SingleThreadedAgentRuntime(tracer_provider=tracer_provider)\n    tracer = trace.get_tracer(\"autogen-test-agentchat\")\n    with tracer.start_as_current_span(\"runtime\"):\n        runtime.start()\n        team = SelectorGroupChat(\n            [planning_agent, web_search_agent, data_analyst_agent],\n            model_client=model_client,\n            termination_condition=termination,\n            selector_prompt=selector_prompt,\n            allow_repeated_speaker=True,  # Allow an agent to speak multiple turns in a row.\n            runtime=runtime,\n        )\n        await Console(team.run_stream(task=task))\n    \n    await runtime.close()\n    await model_client.close()\n\nasyncio.run(main())\nRelated: #5892", "created_at": "2025-03-18", "closed_at": "2025-03-22", "labels": ["documentation", "help wanted", "proj-agentchat"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 5990, "issue_title": "UserProxy in RoundRobinGroupChat not acting as expected.", "issue_body": "What happened?\nI was trying to setup a simple RoundRobinGroupChat with UserProxy being apart of the Team, but ran into some issue that the UserProxy's input prompt came too early.\nUse case: The RoundRobin seq is like this: [user_agent, assistant_agent, code_executor_agent], the user_agner(UserProxy) will block the output msg from code_executor_agent.\nTherefore, I follow the userguide from : Providing Feedback During a Run. And found the same result from the user-guide. To make the question more easier to understand, I use the user-guide code instead my code.\nFor example, the reference expected output are:\n---------- user ----------\nWrite a 4-line poem about the ocean.\n---------- assistant ----------\nIn endless blue where whispers play,  \nThe ocean's waves dance night and day.  \nA world of depths, both calm and wild,  \nNature's heart, forever beguiled.  \nTERMINATE\n---------- user_proxy ----------\nAPPROVE\n\nbut my results looks like this after running:\n---------- user ----------\nWrite a 4-line poem about the ocean.\n---------- assistant ----------\nEnter your response: The ocean's waves crash on the shore,\nA soothing sound that calms once more.\nIts depths are dark, its beauty bright,\nA treasure trove of wonder in sight.\n\nTERMINATE\n\nAnd after typing APPORVE, it then shows the following then end:\nAPPROVE\n---------- user_proxy ----------\nAPPROVE\n\nI am using autogen 0.4.9.1 and python 3.12.2 under Redhat 8 machine.\nAlso, I am using llama 3.3_70B through ollama.\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nllama_3.3_70B\nModel provider\nOllama\nOther model provider\nNo response\nPython version\n3.12\n.NET version\nNone\nOperating system\nOther", "created_at": "2025-03-18", "closed_at": null, "labels": ["needs-triage"], "State": "open", "Author": "Wei-Cheng881221"}
{"issue_number": 5982, "issue_title": "Unable to dump_component for OpenAIChatCompletionClient using a pydantic as response_format", "issue_body": "What happened?\nDescribe the bug\nIn order to export a gallery into autogen-studio we have a process to generate a gallery using from autogenstudio.gallery import builder which relies on dumping different components.\nWe have recently added structured outputs into some OpenAIChatCompletionClient and discovered we are not able to perform this component dump as we hit a pydantic validation (Reference)\nError:\npydantic_core._pydantic_core.ValidationError: 1 validation error for OpenAIClientConfigurationConfigModel\nresponse_format\n  Input should be a valid dictionary [type=dict_type, input_value=<class 'src.models.labour_order.LabourOrderModel'>, input_type=ModelMetaclass]\n    For further information visit https://errors.pydantic.dev/2.10/v/dict_type\n\nTo Reproduce\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom pydantic import BaseModel\n\nclass Response(BaseModel):\n    text: str\n    status: str\n\nmodel = OpenAIChatCompletionClient(\n    model=\"gpt-4o-mini\",\n    response_format=Response,  # type: ignore\n)\n\nprint(model.dump_component())\nExpected behavior\nComponent dump would yield a representation of the OpenAIChatCompletionClient that can be used by the\nScreenshots\nN/A\nAdditional context\nWe are creating the gallery with:\nfrom autogenstudio.gallery import builder as gallery_builder\nbuilder = gallery_builder.GalleryBuilder(id=\"...\", name=\"...\")\nbuilder.add_model(\n            model_instance.dump_component(), label=\"...\", description=\"...\"\n        )\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\ngpt-4o-mini\nModel provider\nOpenAI\nOther model provider\nNo response\nPython version\n3.12\n.NET version\nNone\nOperating system\nMacOS", "created_at": "2025-03-17", "closed_at": "2025-03-18", "labels": ["proj-studio"], "State": "closed", "Author": "jorge-wonolo"}
{"issue_number": 5979, "issue_title": "Ollama MultimodalWebSurfer crash using Ollama and llama3.2", "issue_body": "What happened?\nDescribe the bug\nWhen running the example of web surfer using ollama with model llama3.2 the MultimodalWebSurfer crashes\nTo Reproduce\nRun web surfer example provided in README.md with OllamaChatCompletionClient and model llama3.2\nimport asyncio\nfrom autogen_agentchat.agents import UserProxyAgent\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.ollama import OllamaChatCompletionClient\nfrom autogen_ext.agents.web_surfer import MultimodalWebSurfer\n\n\nasync def main() -> None:\n    model_client = OllamaChatCompletionClient(model=\"llama3.2\")\n\n    web_surfer = MultimodalWebSurfer(\n        \"web_surfer\", model_client, headless=False, animate_actions=True\n    )\n\n    user_proxy = UserProxyAgent(\"user_proxy\")\n\n    termination = TextMentionTermination(\"exit\", sources=[\"user_proxy\"])\n\n    team = RoundRobinGroupChat(\n        [web_surfer, user_proxy], termination_condition=termination\n    )\n\n    try:\n        await Console(\n            team.run_stream(\n                task=\"Find information about AutoGen and write a short summary.\"\n            )\n        )\n    finally:\n        await web_surfer.close()\n        await model_client.close()\n\n\nasyncio.run(main())\nExpected behavior\nProceed without errors\nStack Trace\nEnter your response: Web surfing error:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\nicoa\\Projects\\test\\.venv\\Lib\\site-packages\\autogen_ext\\agents\\web_surfer\\_multimodal_web_surfer.py\", line 442, in on_messages_stream\n    content = await self._generate_reply(cancellation_token=cancellation_token)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\nicoa\\Projects\\test\\.venv\\Lib\\site-packages\\autogen_ext\\agents\\web_surfer\\_multimodal_web_surfer.py\", line 612, in _generate_reply\n    response = await self._model_client.create(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\nicoa\\Projects\\test\\.venv\\Lib\\site-packages\\autogen_ext\\models\\ollama\\_ollama_client.py\", line 456, in create\n    self._client.chat(  # type: ignore\nTypeError: AsyncClient.chat() got an unexpected keyword argument 'tool_choice'\n\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nOther (please specify)\nOther library version.\n0.4.9.2\nModel used\nllama3.2\nModel provider\nOllama\nOther model provider\nNo response\nPython version\n3.12\n.NET version\n.NET 9\nOperating system\nWindows", "created_at": "2025-03-17", "closed_at": "2025-03-17", "labels": ["needs-triage"], "State": "closed", "Author": "NicoAvanzDev"}
{"issue_number": 5970, "issue_title": "Use structured output for speaker selection in SelectorGroupChat", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nUse structured output for selecting next speaker allows for more robust speaker selection behaviour.\nTo use structured output, we need to first check the model info of the model client used. The default prompt cannot be used in this case. We probably need to add another built in prompt for this.", "created_at": "2025-03-16", "closed_at": null, "labels": ["proj-agentchat", "structured output"], "State": "open", "Author": "ekzhu"}
{"issue_number": 5966, "issue_title": "MultimodalWebSurfer with Gemini fails \"Using cl100k_base encoding.\" when used in MagenticOneGroupChat & MagenticOne", "issue_body": "What happened?\nDescribe the bug\nRunning the example autogen.4 using Gemini-2.0-Flash model produces the error \"Model gemini-2.0-flash not found. Using cl100k_base encoding.\"  It doesns't ALWAYS fail, reading the console.stream the agent to agent handover of data fails at some point?\nsetup\nvscode 1.98.1\nnodejs 20.18.2\npython 3.11.9\nautogen-agentchat (0.4.9.2)\nautogen-ext[magentic-one,openai]  (0.4.9.2)\nautogen-core==0.4.9.2\nTo Reproduce\nUsing the example  with gemini .\nThe Surfer agent visits the webpage, MagenticOneOrchestrator askes to scroll down, back and forth a few times until error\nimport asyncio\nfrom autogen_agentchat.teams import MagenticOneGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.agents.web_surfer import MultimodalWebSurfer\n# from autogen_ext.agents.file_surfer import FileSurfer\n# from autogen_ext.agents.magentic_one import MagenticOneCoderAgent\n# from autogen_agentchat.agents import CodeExecutorAgent\n# from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(\n        model=\"gemini-2.0-flash\",\n        api_key=GOOGLE_API_KEY\n    )\n    surfer = MultimodalWebSurfer(\n        \"WebSurfer\",\n        model_client=model_client,\n    )\n\n    team = MagenticOneGroupChat([surfer], model_client=model_client)\n    await Console(team.run_stream(task=\"What is the UV index in Melbourne today?\"))\n\n    # # Note: you can also use  other agents in the team\n    # team = MagenticOneGroupChat([surfer, file_surfer, coder, terminal], model_client=model_client)\n    # file_surfer = FileSurfer( \"FileSurfer\",model_client=model_client)\n    # coder = MagenticOneCoderAgent(\"Coder\",model_client=model_client)\n    # terminal = CodeExecutorAgent(\"ComputerTerminal\",code_executor=LocalCommandLineCodeExecutor())\n\n\nasyncio.run(main())\ncallstack of model_client\n<autogen_ext.models.openai._openai_client.OpenAIChatCompletionClient object at 0x000001B6C1CEE310>\nSyntaxError('invalid syntax', ('<string>', 0, 0, '', 0, 0))\nSyntaxError('invalid syntax', ('<string>', 0, 0, '', 0, 0))\nSyntaxError('invalid syntax', ('<string>', 0, 0, '', 0, 0))\n{'vision': True, 'function_calling': True, 'json_output': True, 'family': 'gemini-2.0-flash'}\nNone\nNone\n'autogen_ext.models.openai.OpenAIChatCompletionClient'\n'model'\n1\n{'vision': True, 'function_calling': True, 'json_output': True, 'family': 'gemini-2.0-flash'}\n['component_config_schema', 'component_type']\nSyntaxError('invalid syntax', ('<string>', 1, 1, '<_abc._abc_data object at 0x000001B6C1D86CC0>', 1, 2))\nNameError(\"name 'RequestUsage' is not defined\")\nFalse\nSyntaxError('invalid syntax', ('<string>', 1, 1, '<openai.AsyncOpenAI object at 0x000001B6C1D40090>', 1, 2))\n{'model': 'gemini-2.0-flash'}\nSyntaxError('invalid syntax', ('<string>', 1, 1, '<bound method BaseOpenAIChatCompletionClient._create_stream_chunks of <autogen_ext.models.openai._openai_client.OpenAIChatCompletionClient object at 0x000001B6C1CEE310>>', 1, 2))\nSyntaxError('invalid syntax', ('<string>', 1, 1, '<bound method BaseOpenAIChatCompletionClient._create_stream_chunks_beta_client of <autogen_ext.models.openai._openai_client.OpenAIChatCompletionClient object at 0x000001B6C1CEE310>>', 1, 2))\nSyntaxError('invalid syntax', ('<string>', 1, 1, \"<bound method OpenAIChatCompletionClient._from_config of <class 'autogen_ext.models.openai._openai_client.OpenAIChatCompletionClient'>>\", 1, 2))\nSyntaxError('invalid syntax', ('<string>', 1, 1, \"<bound method ComponentFromConfig._from_config_past_version of <class 'autogen_ext.models.openai._openai_client.OpenAIChatCompletionClient'>>\", 1, 2))\nFalse\n{'vision': True, 'function_calling': True, 'json_output': True, 'family': 'gemini-2.0-flash'}\n{'model': 'gemini-2.0-flash', 'api_key': 'nonononotforu'}\n'gemini-2.0-flash'\nSyntaxError('invalid syntax', ('<string>', 1, 1, '<bound method OpenAIChatCompletionClient._to_config of <autogen_ext.models.openai._openai_client.OpenAIChatCompletionClient object at 0x000001B6C1CEE310>>', 1, 2))\nNameError(\"name 'RequestUsage' is not defined\")\n\nExpected behavior\nShould return the UV Index as example shows\nTerminal output\n---------- MagenticOneOrchestrator ----------\nPlease search for \"UV index Melbourne today\" and report your findings, focusing on information from reputable sources like the Bureau of Meteorology Australia.\n---------- WebSurfer ----------\nI typed 'UV index Melbourne today Bureau of Meteorology Australia' into the browser search bar.\n\nThe web browser is open to the page [UV index Melbourne today Bureau of Meteorology Australia - Search](https://www.bing.com/search?q=UV+index+Melbourne+today+Bureau+of+Meteorology+Australia&FORM=QBLH).\nThe viewport shows 32% of the webpage, and is positioned at the top of the page\nThe following text is visible in the viewport:\n---skipping copy/paste tons of json junk---\nHere is a screenshot of the page.\n<image>\n---------- MagenticOneOrchestrator ----------\nPlease confirm today's date in Melbourne, Australia. Based on the search results, it seems the UV Index is predicted to reach 8 (Very High) with sun protection recommended from 10:10 am to 4:50 pm. Please confirm this information from the Bureau of Meteorology for today's date.\nModel gemini-2.0-flash not found. Using cl100k_base encoding.\n\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0), Python Extensions (autogen-ext)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\ngemini-2.0-flash\nModel provider\nGoogle Gemini\nOther model provider\nNo response\nPython version\n3.11\n.NET version\n.NET 8\nOperating system\nWindows", "created_at": "2025-03-16", "closed_at": null, "labels": ["needs-triage"], "State": "open", "Author": "cattboy"}
{"issue_number": 5965, "issue_title": "Support model APIs that require strict alternating user-assistant roles", "issue_body": "          For the 400 server error about alternating user-assistant roles in messages, we need to handle this outside of model client, i.e., in `AssistantAgent` and `SelectorGroupChat`, where the model clients are used. Based on the model family in `model_info` field, we should inject an empty user message when there are consecutive assistant messages.\n\nOriginally posted by @ekzhu in #5961 (comment)\nReference of 400 error:\nopenai.BadRequestError: Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive user or assistant messages (messages[1] and messages[2] in your input). You should interleave the user/assistant messages in the message sequence.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n\nSteps:\n\nInvestigate what are the model families that require this strict alternating user-assistant roles? (DeepSeek R1, Mistral AI)\nIn AssistantAgent and SelectorGroupChat, where model client is used, ensure the messages are following the strict order when the above model families are involved. We can do this by concatenation of messages with repeated roles, or injecting empty message -- need to test them.\n", "created_at": "2025-03-16", "closed_at": null, "labels": ["help wanted", "proj-agentchat"], "State": "open", "Author": "ekzhu"}
{"issue_number": 5957, "issue_title": "Add support for structured output for AzureAIChatCompletionClient.", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nSee OpenAIChatCompletionClient for example.", "created_at": "2025-03-15", "closed_at": null, "labels": ["help wanted", "proj-extensions", "structured output"], "State": "open", "Author": "ekzhu"}
{"issue_number": 5953, "issue_title": "polymorphism bug in in AssistantAgent._process_model_result", "issue_body": "What happened?\nDescribe the bug\nIn the code shwon below, it should use cls._reflect_on_tool_use_flow and cls._summarize_tool_use to ensure proper polymorphism.\ncode\n        # STEP 4D: Reflect or summarize tool results\n        if reflect_on_tool_use:\n            async for reflection_response in AssistantAgent._reflect_on_tool_use_flow(\n                system_messages=system_messages,\n                model_client=model_client,\n                model_client_stream=model_client_stream,\n                model_context=model_context,\n                agent_name=agent_name,\n                inner_messages=inner_messages,\n            ):\n                yield reflection_response\n        else:\n            yield AssistantAgent._summarize_tool_use(\n                executed_calls_and_results=executed_calls_and_results,\n                inner_messages=inner_messages,\n                handoffs=handoffs,\n                tool_call_summary_format=tool_call_summary_format,\n                agent_name=agent_name,\n            )\nTo Reproduce\nCreate a child class of AssistantAgent and override methods such as _reflect_on_tool_use_flow and _summarize_tool_use. When you create a group chat using the child class, it still calls the parent class's methods.\nExpected behavior\nThe override methods should be be called.\nScreenshots\nIf applicable, add screenshots to help explain your problem.\nAdditional context\nAdd any other context about the problem here.\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-15", "closed_at": "2025-03-19", "labels": ["needs-triage"], "State": "closed", "Author": "ZacharyHuang"}
{"issue_number": 5951, "issue_title": "Bad Request Error  - Groupchat with DBRX Foundation Models", "issue_body": "What happened?\nPython Code\nThis code is exactly taken from the following example\nhttps://microsoft.github.io/autogen/0.2/docs/notebooks/agentchat_groupchat/\nimport autogen\nuser_proxy = autogen.UserProxyAgent(\n    name=\"User_proxy\",\n    system_message=\"A human admin.\",\n    code_execution_config={\n        \"last_n_messages\": 2,\n        \"work_dir\": \"groupchat\",\n        \"use_docker\": False,\n    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n    human_input_mode=\"TERMINATE\",\n)\ncoder = autogen.AssistantAgent(\n    name=\"Coder\",\n    llm_config=llm_config,\n)\npm = autogen.AssistantAgent(\n    name=\"Product_manager\",\n    system_message=\"Creative in software product ideas.\",\n    llm_config=llm_config,\n)\ngroupchat = autogen.GroupChat(agents=[user_proxy, coder, pm], messages=[], max_round=12)\nmanager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n\n\n\n\n**To Reproduce\nuser_proxy.initiate_chat(\n    manager, message=\"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\"\n)\n# type exit to terminate the chat\n\nExpected behavior\nThe agents should write code and lookup articles in arxiv\nNote that his code works successfully when using  OPEN AI models such as GPT 4o\nHowever, when running with DBRX databricks-meta-llama-3-70b-instruct then a BadRequestError is generated\nBadRequestError: Error code: 400 - {'error_code': 'BAD_REQUEST', 'message': \"Bad request: rpc error: code = InvalidArgument desc = Chat message input must end with a 'user', 'assistant', or 'tool' role\\n\"}\n\nthe DBRX foundation models work fine in most autogen examples but not for GroupChat\nWhich packages was the bug in?\nV0.2 (autogen-agetnchat==0.2.*)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\ndatabricks-meta-llama-3-70b-instruct\nModel provider\nNone\nOther model provider\nNo response\nPython version\n3.10\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-14", "closed_at": "2025-03-15", "labels": ["needs-triage"], "State": "closed", "Author": "Aljgutier"}
{"issue_number": 5950, "issue_title": "Update AgBench documentation", "issue_body": "What is the doc issue?\nThe README of agbench suggests creating ENV.json but the code uses ENV.yaml.\nWe need to improve this. I can help with this after I can run the bench (e.g., see #5949)\nLink to the doc page, if applicable\nhttps://github.com/microsoft/autogen/tree/main/python/packages/agbench", "created_at": "2025-03-14", "closed_at": null, "labels": ["documentation"], "State": "open", "Author": "gagb"}
{"issue_number": 5949, "issue_title": "AgBench is hardcoded to use azure_token_provider", "issue_body": "What happened?\nI tried running the example in AgBench's README and got the following error, despite setting an OpenAI client in the config.yml of HumanEval\n\nSome investigation revealed that this is because run_cmd.py is hardcoded to use the azure token provider.\ncc. @afourney\nWhich packages was the bug in?\nAutoGen Bench (agbench)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-14", "closed_at": null, "labels": [], "State": "open", "Author": "gagb"}
{"issue_number": 5946, "issue_title": "Authentication Bug in integration with Azure Dynamicsessionpool", "issue_body": "What happened?\nDescribe the bug\nwhen I want to create a ACASessionsExecutor instance and execute some code, the default library imported does not work. It always returns: \"ClientAuthenticationError: Authentication failed: AADSTS70011: The provided request must include a 'scope' input parameter. The provided value for the input parameter 'scope' is not valid. The scope https://dynamicsessions.io is not valid. Trace ID: d75efa58-8be7-44ef-8839-aacfdc850600 Correlation ID: a8e4d859-92da-4fbe-a8e0-05116323ab55 Timestamp: 2025-03-14 14:15:09Z\"\nTo Reproduce\nSimply use the official sample code to create a ACASessionsExecutor instance, it will report the error\nimport os\nimport re\n\nimport dotenv\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.responses import JSONResponse, RedirectResponse\nfrom autogen import ConversableAgent, config_list_from_json\nfrom aca_sessions_executor import ACASessionsExecutor\n\naca_pool_management_endpoint = os.getenv(\"POOL_MANAGEMENT_ENDPOINT\")\naca_sessions_executor = ACASessionsExecutor(aca_pool_management_endpoint)\n\n# Initialize the CodeExecutor agent\ncode_executor_agent = ConversableAgent(\n    name=\"CodeExecutor\",\n    llm_config=False,\n    code_execution_config={\"executor\": aca_sessions_executor},\n    human_input_mode=\"NEVER\",\n    is_termination_msg=lambda msg: \"TERMINATE\" in msg.get(\"content\", \"\").strip().upper()\n)\nSo I have to create a Custom method to change the scope in _ensure_access_token to be \"https://dynamicsessions.io/.default\" rather than \"\"https://dynamicsessions.io/\" and use the new method to create instances, it worked.\nclass CustomACADynamicSessionsCodeExecutor(ACADynamicSessionsCodeExecutor):\n    def _ensure_access_token(self) -> None:\n        if not self._access_token:\n            scope = \"https://dynamicsessions.io/.default\"\n            self._access_token = self._credential.get_token(scope).token\n\nexecutor = CustomACADynamicSessionsCodeExecutor(\n        pool_management_endpoint=POOL_MANAGEMENT_ENDPOINT, credential=credential\n    )\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nAzure OpenAI\nOther model provider\nNo response\nPython version\n3.12\n.NET version\nNone\nOperating system\nMacOS", "created_at": "2025-03-14", "closed_at": "2025-03-21", "labels": ["needs-triage"], "State": "closed", "Author": "EdwinInnovation"}
{"issue_number": 5944, "issue_title": "Connection error with AzureOpenAI after latest merge", "issue_body": "What happened?\nDescribe the bug\nThe following error appears in the latest version when querying an AzureOpenAI model:\nTypeError: Header value must be str or bytes, not <class 'pydantic.types.SecretStr'>\nSeems related to the last PR #5939\nTo Reproduce\nInstall the latest version with\npip install autogen-ext\nExpected behavior\nThe previous version works well\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-14", "closed_at": "2025-03-14", "labels": ["needs-triage"], "State": "closed", "Author": "victorgim87"}
{"issue_number": 5943, "issue_title": "UI Bug: Team Builder/Playground Blank in Chrome - Resolved by Hard Refresh", "issue_body": "What happened?\nDescribe the bug\n\nAutoGen Studio UI (Team Builder and Playground) initially fails to load in Google Chrome, displaying blank sections.\nThe Gallery section loads correctly.\nA hard refresh (Ctrl+Shift+R) in Google Chrome resolves the issue. I guess this is a caching problem. The UI functions correctly in Microsoft Edge without requiring a hard refresh.\nAdditionnaly, with version >=0.4.2.dev, after the hard refresh, there are issues on both Edge and Chrome when clicking on Team Builder or New team for example (see error below)\n\nTo Reproduce the caching problem\nI tried with AGS version: 0.4.1.11 to 0.4.2.dev3\nDefault Browser: Google Chrome (Version 134.0.6998.89 (Official Build) (64-bit)).\n\nTeam Builder and Playground are blank. Gallery loads.\nHard refresh (Ctrl+Shift+R): Team Builder and Playground should load.\nTry creating a new team.\n\nTo Reproduce the error with creating a New Team\nThis issue appears to be specific to the 0.4.2.devX versions.  AGS version 0.4.1.11 seems to function correctly after a hard refresh.\nUsing a version >=0.4.2.dev.\n\n(After a hard refresh if using Chrome) Click on \"Team Builder\" or attempt to create a new team.\nYou should see an error in the terminal (see \"Additional Context\").\n\nExpected behavior\nAll UI sections (Team Builder, Playground, and Gallery) should load correctly in Google Chrome on the initial load, without requiring a hard refresh.  Creating new teams and interacting with the UI should function without errors.\nScreenshots versions from 0.4.1.11 to 0.4.2.dev3\nwith Chrome without hard refresh\n\nPlayground (same for Team Builder)\n\n\n\nGallery\n\n\nwith Edge without hard refresh\nPlayground\n\nScreenshots using a version >=0.4.2.dev\n** with Chrome with hard refresh**\n\nAdditional context\n\nI believe this issue started after a recent Chrome update, although I cannot pinpoint the exact version when it began.\nFor version >=0.4.2.dev before AND after hard refresh: this ERROR shows up in the Terminal when i click on Team Builder for example\n\n2025-03-14 08:14:46.756 | INFO     | autogenstudio.web.app:lifespan:35 - Application startup complete. Navigate to http://127.0.0.1:8081\n2025-03-14 08:15:05.031 | ERROR    | autogenstudio.database.db_manager:upsert:174 - Error while updating/creating Gallery: (builtins.TypeError) Object of type SecretStr is not JSON serializable\n[SQL: INSERT INTO gallery (created_at, updated_at, user_id, version, config) VALUES (?, ?, ?, ?, ?)]\n[parameters: [{'created_at': datetime.datetime(2025, 3, 14, 8, 15, 5, 29947), 'version': '0.0.1', 'user_id': 'guestuser@gmail.com', 'config': {'id': 'gallery_defaul ... (59702 characters truncated) ... NLY RETURN THE ROLE.\", 'allow_repeated_speaker': True, 'max_selector_attempts': 3}}]}}, 'updated_at': datetime.datetime(2025, 3, 14, 8, 15, 5, 29947)}]]\n\nWhich packages was the bug in?\nAutoGen Studio (autogensudio)\nOperating system\nWindows", "created_at": "2025-03-14", "closed_at": null, "labels": ["proj-studio"], "State": "open", "Author": "usag1e"}
{"issue_number": 5934, "issue_title": "Add `output_content_type` parameters to `AssistantAgent`", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nDep: #5933, #5131\n\nAdd output_content parameter to the constructor of AssistantAgent. Validate whether the model client supports structured_output in its model info.\nWhen calling model client in the first step, use output_content_type defined in the agent to generate a structured output and create a structured message.\nIf the parameter is not defined, default to TextMessage.\n", "created_at": "2025-03-13", "closed_at": "2025-04-01", "labels": ["proj-agentchat", "structured output"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 5933, "issue_title": "Add structured output to model client API", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\n\noutput_type to ChatCompletionClient APIs.\nAdd structured_output as a new ModelInfo field.\nAdd structured_output field to existing built-in model infos.\nUse the new output_type argument in built-in model clients.\n", "created_at": "2025-03-13", "closed_at": "2025-03-15", "labels": ["proj-core", "proj-extensions", "structured output"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 5931, "issue_title": "Support customizing Swarm handoff trigger beyond model-based function call", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nCurrently handoff in AssistantAgent is through model-driven function call. The handoff is converted into a tool which the model can choose to call. We would like to be able to trigger handoff with a user-defined condition, which can take the input of the current model context or the latest model inference result.\nThis allows more robust handoff, especially when structured output is used.", "created_at": "2025-03-13", "closed_at": null, "labels": ["proj-agentchat", "swarm"], "State": "open", "Author": "ekzhu"}
{"issue_number": 5929, "issue_title": "Dockerfile for AutoGen Studio fails to build due to UID conflict", "issue_body": "What happened?\nDescribe the bug\nThe Dockerfile for Autogen Studio at python/packages/autogen-studio/Dockerfile fails to build.\nThe issue occurs at the step that adds a user with UID 1000 (RUN useradd -m -u 1000 user). Since the base image (mcr.microsoft.com/devcontainers/python:3.10) is a Dev Container, it already includes a user with UID 1000.\nThis causes the build to fail with error code 4 - UID already in use.\nTo Reproduce\n\nDownload the Dockerfile from python/packages/autogen-studio/Dockerfile into a local directory.\nBuild the Docker image with docker build .\nObserve error\n\nERROR: failed to solve: process \"/bin/sh -c useradd -m -u 1000 user\" did not complete successfully: exit code: 4\n\nExpected behavior\nThe Docker image should build successfully without errors.\nScreenshots\nN/A\nAdditional context\nThis issue is observed on the current main branch. I'm also not sure if this would be resolved as part of the ongoing effort to rewrite AutoGen Studio to AutoGen 0.4 API (#4006)\nWhich packages was the bug in?\nAutoGen Studio (autogensudio)\nAutoGen library version.\nStudio 0.4.1\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\n3.10\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-13", "closed_at": "2025-03-13", "labels": ["proj-studio"], "State": "closed", "Author": "gunt3001"}
{"issue_number": 5927, "issue_title": "Port Send/Publish changes to the .NET gRPC IAgentRuntime implementation", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nThis tracks the work of porting #5923 and #5916 (if there is any) to the gRPC runtime in .NET", "created_at": "2025-03-13", "closed_at": null, "labels": [], "State": "open", "Author": "lokitoth"}
{"issue_number": 5915, "issue_title": "Deadlock in logically recurrent calls to SendMessageAsync", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nIf a message handler invoked during SendMessage then attempts to send a message and await its result, the InProcessRuntime will deadlock.", "created_at": "2025-03-12", "closed_at": "2025-03-12", "labels": ["dotnet", "proj-core"], "State": "closed", "Author": "lokitoth"}
{"issue_number": 5912, "issue_title": "Not able to load Swarm state after HandoffTermination", "issue_body": "After the Swarm state is saved when it is terminated by HandoffTermination , I am not able to load it.\nHere is the error:\nERROR - HTTPException occurred: The existing handoff target user is not one of the participants ['agent1']. If you are resuming Swarm with a new task make sure to include in your task a HandoffMessage with a valid participant as the target. For example, if you are resuming from a HandoffTermination, make sure the new task is a HandoffMessage with a valid participant as the target.\nWhile I have handoffs=[\"user\"] in declaration of agent1\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython 0.4.8\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-12", "closed_at": "2025-03-13", "labels": ["needs-triage"], "State": "closed", "Author": "amd-srijaroy"}
{"issue_number": 5910, "issue_title": "Ollama throw \"Object of type Message is not JSON serializable\" error", "issue_body": "What happened?\nDescribe the bug\nWhen testing the code in https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html#ollama-experimental with logging enabled.\nIt show error: TypeError: Object of type Message is not JSON serializable\nTo Reproduce\nOllama is installed.\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.ollama import OllamaChatCompletionClient\nimport asyncio\nimport logging\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\nlogger = logging.getLogger(__name__)\n\nasync def main():\n    ollama_model_client = OllamaChatCompletionClient(model=\"llama3.2\")\n    response = await ollama_model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n    logger.info(f\"Sending message: {response}\")\n\nasyncio.run(main())\nThe output is as below:\n2025-03-12 22:15:56 - autogen_core.events - INFO - Dropped the following unrecognized keys from create_args: []\n2025-03-12 22:15:57 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n--- Logging error ---\nTraceback (most recent call last):\n  File \".pyenv/versions/3.12.8/lib/python3.12/logging/__init__.py\", line 1160, in emit\n    msg = self.format(record)\n          ^^^^^^^^^^^^^^^^^^^\n  File \".pyenv/versions/3.12.8/lib/python3.12/logging/__init__.py\", line 999, in format\n    return fmt.format(record)\n           ^^^^^^^^^^^^^^^^^^\n  File \".pyenv/versions/3.12.8/lib/python3.12/logging/__init__.py\", line 703, in format\n    record.message = record.getMessage()\n                     ^^^^^^^^^^^^^^^^^^^\n  File \".pyenv/versions/3.12.8/lib/python3.12/logging/__init__.py\", line 390, in getMessage\n    msg = str(self.msg)\n          ^^^^^^^^^^^^^\n  File \"venv/lib/python3.12/site-packages/autogen_core/logging.py\", line 64, in __str__\n    return json.dumps(self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \".pyenv/versions/3.12.8/lib/python3.12/json/__init__.py\", line 231, in dumps\n    return _default_encoder.encode(obj)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".pyenv/versions/3.12.8/lib/python3.12/json/encoder.py\", line 200, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".pyenv/versions/3.12.8/lib/python3.12/json/encoder.py\", line 258, in iterencode\n    return _iterencode(o, 0)\n           ^^^^^^^^^^^^^^^^^\n  File \".pyenv/versions/3.12.8/lib/python3.12/json/encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type Message is not JSON serializable\nCall stack:\n  File \"tests/test_ollama.py\", line 21, in <module>\n    asyncio.run(main())\n  File \".pyenv/versions/3.12.8/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n  File \".pyenv/versions/3.12.8/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n  File \".pyenv/versions/3.12.8/lib/python3.12/asyncio/base_events.py\", line 673, in run_until_complete\n    self.run_forever()\n  File \".pyenv/versions/3.12.8/lib/python3.12/asyncio/base_events.py\", line 640, in run_forever\n    self._run_once()\n  File \".pyenv/versions/3.12.8/lib/python3.12/asyncio/base_events.py\", line 1992, in _run_once\n    handle._run()\n  File \".pyenv/versions/3.12.8/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"tests/test_ollama.py\", line 17, in main\n    response = await ollama_model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n  File \"venv/lib/python3.12/site-packages/autogen_ext/models/ollama/_ollama_client.py\", line 485, in create\n    logger.info(\nMessage: <autogen_core.logging.LLMCallEvent object at 0x10711f650>\nArguments: ()\n2025-03-12 22:15:57 - __main__ - INFO - Sending message: finish_reason='unknown' content='The capital of France is Paris.' usage=RequestUsage(prompt_tokens=32, completion_tokens=8) cached=False logprobs=None thought=None\n\nExpected behavior\nDo not show any error in log.\nWhich packages was the bug in?\nPython Core (autogen-core)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nllama3.2\nModel provider\nOllama\nOther model provider\nNo response\nPython version\n3.12\n.NET version\nNone\nOperating system\nMacOS", "created_at": "2025-03-12", "closed_at": "2025-03-12", "labels": ["needs-triage"], "State": "closed", "Author": "treenwang1"}
{"issue_number": 5909, "issue_title": "ImportError: cannot import name 'StdioServerParams' from partially initialized module 'autogen_ext.tools.mcp' (most likely due to a circular import)", "issue_body": "What happened?\nDescribe the bug\nImportError: cannot import name 'StdioServerParams' from partially initialized module 'autogen_ext.tools.mcp' (most likely due to a circular import)\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_ext.tools.mcp import StdioServerParams, mcp_server_tools\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\n3.10\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-12", "closed_at": "2025-03-13", "labels": ["proj-extensions"], "State": "closed", "Author": "yueyongyue"}
{"issue_number": 5907, "issue_title": "ERROR when using deepseek r1", "issue_body": "What happened?\nDescribe the bug\n\n{\"error\":{\"message\":\"deepseek-reasoner does not support successive user or assistant messages (messages[1] and messages[2] in your input). You should interleave the user/assistant messages in the message sequence.\",\"type\":\"invalid_request_error\",\"param\":null,\"code\":\"invalid_request_error\"}}\n\nPrompt \"deepseek-reasoner does not support continuous user or assistant messages\". The reason for this problem is that the roles (user and assistant) in the message sequence must strictly alternate, and messages with the same role cannot appear continuously.\nTo Reproduce\nanalyst_model_client = OpenAIChatCompletionClient(model=\"deepseek-reasoner\",\n                                                  temperature=1.0,\n                                                  api_key=analyst_model[\"api_key\"],\n                                                  base_url=analyst_model[\"base_url\"],\n                                                  max_tokens=analyst_model[\"max_tokens\"],\n                                                  model_capabilities={\n                                                      \"vision\": False,\n                                                      \"function_calling\": False,\n                                                      \"json_output\": False,\n                                                      \"family\": ModelFamily.R1,\n                                                  })\n\nagents = get_basic_financials_agent(planning_model_client, analyst_model_client)\n\nresearch_team = RoundRobinGroupChat(agents, max_turns=2)\nExpected behavior\nDeepSeek V3 is ok.\nScreenshots\nNA\nAdditional context\nNA\nWhich packages was the bug in?\nPython Core (autogen-core), Python AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython 0.4.7\nOther library version.\n0.4.8.2\nModel used\nDeepSeek R1\nModel provider\nNone\nOther model provider\nDeepSeek\nPython version\n3.12\n.NET version\nNone\nOperating system\nUbuntu", "created_at": "2025-03-12", "closed_at": "2025-03-18", "labels": ["needs-triage"], "State": "closed", "Author": "xiyan18"}
{"issue_number": 5899, "issue_title": "Pydantic 2.10 error when using SQLDatabasechain", "issue_body": "What happened?\nDescribe the bug\nPydantic 2.10 is a requirement for AutoGen 0.4, but cause below error for Langchain when attempting to use SQLDatabaseChain:\n\nPydanticUserError: SQLDatabaseChain is not fully defined; you should define BaseCache, then call SQLDatabaseChain.model_rebuild().\n\nTo Reproduce\n\nimport autogen\nfrom langchain_community.utilities.sql_database import SQLDatabase\nfrom langchain_experimental.sql import SQLDatabaseChain\nfrom langchain_openai import AzureChatOpenAI\nimport os\nfrom dotenv import load_dotenv\nimport psycopg2\n\n# Load environment variables from the .env file from the same directory as notebook \nload_dotenv()\n\n# Retrieve environment variables\nPOSTGRES_USER = os.getenv('POSTGRES_USER')\nPOSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD')\nPOSTGRES_HOST = os.getenv('POSTGRES_HOST')\nPOSTGRES_PORT = os.getenv('POSTGRES_PORT')\nPOSTGRES_DB = os.getenv('POSTGRES_DB')\nAZURE_OPENAI_KEY = os.getenv('AZURE_OPENAI_KEY')\nAZURE_OPENAI_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')\nAZURE_OPENAI_DEPLOYMENT = os.getenv('AZURE_OPENAI_DEPLOYMENT')\n\n\n# # Construct the database URI\nshipment_db_uri = f\"postgresql+psycopg2://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}\"\ncrm_db_uri = f\"postgresql+psycopg2://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}\"\n\n# Establish database connections\nshipment_db = SQLDatabase.from_uri(shipment_db_uri)\ncrm_db = SQLDatabase.from_uri(crm_db_uri)\n\n# Initialize the Azure OpenAI language model\nazure_llm = AzureChatOpenAI(\n  azure_endpoint = AZURE_OPENAI_ENDPOINT,\n  api_key=AZURE_OPENAI_KEY,\n  api_version=\"2024-10-21\",\n  deployment_name=AZURE_OPENAI_DEPLOYMENT,\n)\n\n\n# Initialize the database chains\nshipment_chain = SQLDatabaseChain(llm=azure_llm, database=shipment_db, verbose=True)\ncrm_chain = SQLDatabaseChain(llm=azure_llm, database=crm_db, verbose=True)\n\n** Error trace **\n ---------------------------------------------------------------------------\nPydanticUserError                         Traceback (most recent call last)\nCell In[1], line 43\n     34 azure_llm = AzureChatOpenAI(\n     35   azure_endpoint = AZURE_OPENAI_ENDPOINT,\n     36   api_key=AZURE_OPENAI_KEY,\n     37   api_version=\"2024-10-21\",\n     38   deployment_name=AZURE_OPENAI_DEPLOYMENT,\n     39 )\n     42 # Initialize the database chains\n---> 43 shipment_chain = SQLDatabaseChain(llm=azure_llm, database=shipment_db, verbose=True)\n     44 crm_chain = SQLDatabaseChain(llm=azure_llm, database=crm_db, verbose=True)\n\nFile ~\\py11\\Lib\\site-packages\\langchain_core\\load\\serializable.py:125, in Serializable.__init__(self, *args, **kwargs)\n    123 def __init__(self, *args: Any, **kwargs: Any) -> None:\n    124     \"\"\"\"\"\"\n--> 125     super().__init__(*args, **kwargs)\n\n    [... skipping hidden 1 frame]\n\nFile ~\\py11\\Lib\\site-packages\\pydantic\\_internal\\_mock_val_ser.py:100, in MockValSer.__getattr__(self, item)\n     98 # raise an AttributeError if `item` doesn't exist\n     99 getattr(self._val_or_ser, item)\n--> 100 raise PydanticUserError(self._error_message, code=self._code)\n\nPydanticUserError: `SQLDatabaseChain` is not fully defined; you should define `BaseCache`, then call `SQLDatabaseChain.model_rebuild()`.\n\nFor further information visit https://errors.pydantic.dev/2.10/u/class-not-fully-defined\nExpected behavior\nI expected this to run without throwing errors and chain objects successfully being created.\nScreenshots\nIf applicable, add screenshots to help explain your problem.\nAdditional context\nAdd any other context about the problem here.\nWhich packages was the bug in?\nPython Core (autogen-core)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\n0.4.8.1\nModel used\ngpt-4 with api-version=2024-10-21\nModel provider\nAzure OpenAI\nOther model provider\nNo response\nPython version\n3.11\n.NET version\nNone\nOperating system\nWindows", "created_at": "2025-03-11", "closed_at": null, "labels": ["needs-triage"], "State": "open", "Author": "mehrsa"}
{"issue_number": 5897, "issue_title": "When using Autogen Studio to configure the Model in Ollama, a 502 error occurs when executing the Agent.", "issue_body": "What happened?\nI used Autogen Studio to configure the model, and the configuration JSON is as follows:\n{\n  \"provider\": \"autogen_ext.models.openai.OpenAIChatCompletionClient\",\n  \"component_type\": \"model\",\n  \"version\": 1,\n  \"component_version\": 1,\n  \"description\": \"deepseek-r1:1.5b\",\n  \"label\": \"deepseek-r1:1.5b\",\n  \"config\": {\n    \"model\": \"deepseek-r1:1.5b\",\n    \"model_info\": {\n      \"vision\": false,\n      \"function_calling\": true,\n      \"json_output\": false,\n      \"family\": \"unknown\"\n    },\n    \"base_url\": \"http://127.0.0.1:11434\",\n    \"api_key\": \"ollama\"\n  }\n}\n\nAnd the status of \"Validate Team\" is \"success\"\nBut when I use \"Run Team\" and type some input like \"Hello,\" it got error \"Error code: 502.\"\nThe base_url is ok, because I can link it in browser and get \"Ollama is running.\" And I also can use this modle from Python:\nllm = ChatOllama(model='deepseek-r1:1.5b',base_url = '127.0.0.1:11434')\nllm.invoke(\"Hi~\")\n\nSo I don't know why I get this error code: 502.\nThe log in conda shows:\nTraceback (most recent call last):\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\autogenstudio\\web\\managers\\connection.py\", line 106, in start_stream\n    async for message in team_manager.run_stream(\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\autogenstudio\\teammanager\\teammanager.py\", line 117, in run_stream\n    async for message in team.run_stream(task=task, cancellation_token=cancellation_token):\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat.py\", line 453, in run_stream\n    await shutdown_task\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat.py\", line 413, in stop_runtime\n    await self._runtime.stop_when_idle()\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 745, in stop_when_idle\n    await self._run_context.stop_when_idle()\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 120, in stop_when_idle\n    await self._run_task\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 109, in _run\n    await self._runtime._process_next()  # type: ignore\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 580, in _process_next\n    raise e from None\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 527, in _process_publish\n    await asyncio.gather(*responses)\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 522, in _on_message\n    raise e\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 509, in _on_message\n    return await agent.on_message(\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\autogen_core\\_base_agent.py\", line 113, in on_message\n    return await self.on_message_impl(message, ctx)\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 48, in on_message_impl\n    return await super().on_message_impl(message, ctx)\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\autogen_core\\_routed_agent.py\", line 485, in on_message_impl\n    return await h(self, message, ctx)\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\autogen_core\\_routed_agent.py\", line 268, in wrapper\n    return_value = await func(self, message, ctx)  # type: ignore\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 53, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 748, in on_messages_stream\n    async for inference_output in self._call_llm(\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 856, in _call_llm\n    async for chunk in model_client.create_stream(\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 760, in create_stream\n    async for chunk in chunks:\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 904, in _create_stream_chunks\n    stream = await stream_future\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 1927, in create\n    return await self._post(\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\openai\\_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\openai\\_base_client.py\", line 1461, in request\n    return await self._request(\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\openai\\_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\openai\\_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\openai\\_base_client.py\", line 1547, in _request\n    return await self._retry_request(\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\openai\\_base_client.py\", line 1594, in _retry_request\n    return await self._request(\n  File \"C:\\Users\\Omar\\anaconda3\\envs\\autogen\\lib\\site-packages\\openai\\_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.InternalServerError: Error code: 502\n\nWhich packages was the bug in?\nAutoGen Studio (autogensudio)\nAutoGen library version.\nStudio 0.4.1\nOther library version.\nNo response\nModel used\ndeepseek-r1:1.5b\nModel provider\nOllama\nOther model provider\nNo response\nPython version\n3.10\n.NET version\nNone\nOperating system\nWindows", "created_at": "2025-03-11", "closed_at": "2025-03-11", "labels": ["proj-studio"], "State": "closed", "Author": "Omarchen-Good"}
{"issue_number": 5895, "issue_title": "Trace LLM call spans for both create and create_stream", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nGive user a clear time line of LLM usage and understanding of model usage and performance.", "created_at": "2025-03-11", "closed_at": null, "labels": ["proj-extensions"], "State": "open", "Author": "ekzhu"}
{"issue_number": 5894, "issue_title": "Improving OTEL tracing by adding more details about each message", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nAs discussed here: #5853 (comment)\nWe need to add more attributes to the OTEL traces emitted by the runtime to make it easier to understand what are inside the messages.", "created_at": "2025-03-11", "closed_at": null, "labels": ["proj-core"], "State": "open", "Author": "ekzhu"}
{"issue_number": 5892, "issue_title": "Create a sample showing how to set up OTEL and see results in Jaeger UI", "issue_body": "\n\nThe sample should follow the same layout as other samples.\n\n\nThe sample has a README showing how to set up Jaeger UI using Docker run.\n\n\nInclude a screenshot for the resulting spans.\n        In a future PR, let's update the OTEL documentation page with an actual example and illustration of the spans. \n\n\n\nimport asyncio\n\n\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.sdk.resources import Resource\nfrom autogen_core import SingleThreadedAgentRuntime\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n# Tracing setup.\ndef configure_oltp_tracing():\n    jaeger_exporter = OTLPSpanExporter(endpoint=\"http://localhost:4317\", insecure=True)\n    tracer_provider = TracerProvider(\n        resource=Resource({\"service.name\": \"autogen-test-agentchat\"})\n    )\n    span_processor = BatchSpanProcessor(jaeger_exporter)\n    tracer_provider.add_span_processor(span_processor)\n    trace.set_tracer_provider(tracer_provider)\n    return tracer_provider\n\ndef search_web_tool(query: str) -> str:\n    if \"2006-2007\" in query:\n        return \"\"\"Here are the total points scored by Miami Heat players in the 2006-2007 season:\n        Udonis Haslem: 844 points\n        Dwayne Wade: 1397 points\n        James Posey: 550 points\n        ...\n        \"\"\"\n    elif \"2007-2008\" in query:\n        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\"\n    elif \"2008-2009\" in query:\n        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\"\n    return \"No data found.\"\n\n\ndef percentage_change_tool(start: float, end: float) -> float:\n    return ((end - start) / start) * 100\n\n\nasync def main():\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    planning_agent = AssistantAgent(\n        \"PlanningAgent\",\n        description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n        model_client=model_client,\n        system_message=\"\"\"\n        You are a planning agent.\n        Your job is to break down complex tasks into smaller, manageable subtasks.\n        Your team members are:\n            WebSearchAgent: Searches for information\n            DataAnalystAgent: Performs calculations\n\n        You only plan and delegate tasks - you do not execute them yourself.\n\n        When assigning tasks, use this format:\n        1. <agent> : <task>\n\n        After all tasks are complete, summarize the findings and end with \"TERMINATE\".\n        \"\"\",\n    )\n\n    web_search_agent = AssistantAgent(\n        \"WebSearchAgent\",\n        description=\"An agent for searching information on the web.\",\n        tools=[search_web_tool],\n        model_client=model_client,\n        system_message=\"\"\"\n        You are a web search agent.\n        Your only tool is search_tool - use it to find information.\n        You make only one search call at a time.\n        Once you have the results, you never do calculations based on them.\n        \"\"\",\n    )\n\n    data_analyst_agent = AssistantAgent(\n        \"DataAnalystAgent\",\n        description=\"An agent for performing calculations.\",\n        model_client=model_client,\n        tools=[percentage_change_tool],\n        system_message=\"\"\"\n        You are a data analyst.\n        Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.\n        If you have not seen the data, ask for it.\n        \"\"\",\n    )\n\n    text_mention_termination = TextMentionTermination(\"TERMINATE\")\n    max_messages_termination = MaxMessageTermination(max_messages=25)\n    termination = text_mention_termination | max_messages_termination\n\n    selector_prompt = \"\"\"Select an agent to perform task.\n\n    {roles}\n\n    Current conversation context:\n    {history}\n\n    Read the above conversation, then select an agent from {participants} to perform the next task.\n    Make sure the planner agent has assigned tasks before other agents start working.\n    Only select one agent.\n    \"\"\"\n\n    task = \"Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\"\n    tracer_provider = configure_oltp_tracing()\n    runtime = SingleThreadedAgentRuntime(tracer_provider=tracer_provider)\n    tracer = trace.get_tracer(\"autogen-test-agentchat\")\n    with tracer.start_as_current_span(\"runtime\"):\n        runtime.start()\n        team = SelectorGroupChat(\n            [planning_agent, web_search_agent, data_analyst_agent],\n            model_client=model_client,\n            termination_condition=termination,\n            selector_prompt=selector_prompt,\n            allow_repeated_speaker=True,  # Allow an agent to speak multiple turns in a row.\n            runtime=runtime,\n        )\n        await Console(team.run_stream(task=task))\n    \n    await runtime.close()\n    await model_client.close()\n\nasyncio.run(main())\nOriginally posted by @ekzhu in #5889 (review)", "created_at": "2025-03-11", "closed_at": "2025-04-13", "labels": ["help wanted", "sample-request"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 5891, "issue_title": "Support Approval Func in BaseTool in AgentChat", "issue_body": "Background\nAgents can act via tools. The BaseTool interface in AutoGen provides an excellent scaffold for building tools across the framework. Extensions of BaseTool like FunctionTool, PythonCodeExecutionTool, and others demonstrate the flexibility and value of this tool.\nThe Approval Challenge and Opportunity\nAn important capability for a framework is the ability to allow users to \"approve\" actions before they are \"executed\". The opportunity here is to build this into the BaseTool library by adding an approval mechanism that maintains backward compatibility.\nProposed Design Sketch [discussion welcome]\n1. Add Approval Function to BaseTool\n# Add to BaseTool init\ndef __init__(\n    self,\n    args_type: Type[ArgsT],\n    return_type: Type[ReturnT],\n    name: str,\n    description: str,\n    strict: bool = False,\n    approval_func: Optional[Callable[[str, Mapping[str, Any]], Awaitable[ToolApprovalResult]]] = None,\n) -> None:\n    # ... existing initialization ...\n    self._approval_func = approval_func or default_approval\n2. Create an Approval Result Model\nclass ToolApprovalResult(BaseModel):\n    \"\"\"Result of a tool approval request.\"\"\"\n    approved: bool\n    reason: Optional[str] = None\n\n# Default approval function that always approves\nasync def default_approval(tool_name: str, arguments: Mapping[str, Any]) -> ToolApprovalResult:\n    return ToolApprovalResult(approved=True)\n3. Modify run_json to Check Approval\nasync def run_json(self, args: Mapping[str, Any], cancellation_token: CancellationToken) -> Any:\n    # Approval check\n    approval_result = await self._approval_func(self.name, args)\n    \n    # Log the approval event\n    event = ToolCallApprovalEvent(\n        tool_name=self.name,\n        arguments=dict(args),\n        approved=approval_result.approved,\n        reason=approval_result.reason\n    )\n    logger.info(event)\n    \n    # If not approved, return early\n    if not approval_result.approved:\n        return f\"Tool execution not approved: {approval_result.reason or 'No reason provided'}\" \n   \n        \n    # Execute tool if approved (existing functionality)\n    return_value = await self.run(self._args_type.model_validate(args), cancellation_token)\n    # ... existing logging ...\n    return return_value\n4. Add to AssistantAgent\ndef __init__(\n    self,\n    # ... existing parameters ...\n    tool_approval_func: Optional[Callable[[str, Mapping[str, Any]], Awaitable[ToolApprovalResult]]] = None,\n):\n    # ... existing initialization ...\n    self._tool_approval_func = tool_approval_func\n    \n    # When processing tools, apply the approval func\n    for tool in tools:\n        if isinstance(tool, BaseTool) and self._tool_approval_func:\n            tool._approval_func = self._tool_approval_func\n        elif callable(tool) and self._tool_approval_func:\n            # For function tools, wrap with approval\n            function_tool._approval_func = self._tool_approval_func\nBenefits\n\nMaintains backward compatibility\nProvides consistent approval mechanism across all tools\nAllows for centralized approval at agent level or per-tool\nEnables logging of approval decisions\n\nImplementation Notes / Open Questions\n\nDefault behavior should match current behavior (auto-approve)\nShould emit events to notify of approval requests and decisions\nBoth BaseTool and AssistantAgent should accept approval functions\nWe need to check for side effects (e.g, if reflect_on_tool call , we should skpi this if tool denied)\nShould we add tool_call_approval_func to assistant agent?\nwhat happens with parrallel tool calls?\ncan approval/disapproval be constrained .. e.g,. provide feedback e.g., on poor tool call parameters or let the approval function modify the parameters ?\nHow do we pass context to the approval_function? E.g., if the function is soemthign that uses an LLM, we might need to provide plumbing for the context to be passed to the approval func\n\nThoughts welcome @ekzhu @jackgerrits @husseinmozannar", "created_at": "2025-03-10", "closed_at": null, "labels": ["tool-usage", "proj-core", "needs-design"], "State": "open", "Author": "victordibia"}
{"issue_number": 5881, "issue_title": "Add pause/resume APIs for agents in AgentChat", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nAgents can be in I/O or compute intensive tasks that requires pause and resume that allows for graceful stop, in addition to brute cancellation.", "created_at": "2025-03-08", "closed_at": "2025-03-12", "labels": ["proj-agentchat"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 5880, "issue_title": "Allow `BaseGroupChat.save_state` without requiring the team to stop running.", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nPeriodic checkpointing is useful.", "created_at": "2025-03-08", "closed_at": "2025-03-10", "labels": ["proj-agentchat"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 5879, "issue_title": "Convert `BaseGroupChat` into a general `GroupChat` team and make group chat manager customizable", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nBecause there are many different orchestrations that share ChatMessage context.", "created_at": "2025-03-08", "closed_at": null, "labels": ["proj-agentchat"], "State": "open", "Author": "ekzhu"}
{"issue_number": 5873, "issue_title": "Update documentations and samples to properly close model clients", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nWe should be closing the model clients in our docs and samples.\nStart working on this after: #5871", "created_at": "2025-03-07", "closed_at": "2025-03-20", "labels": ["documentation", "help wanted"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 5872, "issue_title": "AGS 0.4.2.dev01 Termination Conditions", "issue_body": "What happened?\nDescribe the bug\nIn autogenstudio v0.4.2.dev1, I am unable to edit the termination conditions (e.g choose the MaxMessageTermination or edit the  TextMentionTermination).\nTo Reproduce\nTry to edit the termination conditions of the default RoudRobin team (without switching to the JSON editor)\nExpected behavior\nI except to be able to edit the Termination condition via the UI\nScreenshots\nClicking the edit icon has no effect.\n\nWhich packages was the bug in?\nAutoGen Studio (autogensudio)", "created_at": "2025-03-07", "closed_at": "2025-03-11", "labels": ["proj-studio"], "State": "closed", "Author": "usag1e"}
{"issue_number": 5870, "issue_title": "Update documentation to replace usage of `TextMentionTermination` that relies on models in favor of more robust conditions", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nFor example, we can ask user to terminate via user proxy or use other robust and deterministic conditions.\nSee discussion:\n#5836", "created_at": "2025-03-07", "closed_at": null, "labels": ["documentation", "proj-agentchat"], "State": "open", "Author": "ekzhu"}
{"issue_number": 5868, "issue_title": "Publish Anaconda package", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nhttps://github.com/marketplace/actions/build-and-publish-conda-packages-to-anaconda-org", "created_at": "2025-03-07", "closed_at": null, "labels": [], "State": "open", "Author": "ekzhu"}
{"issue_number": 5866, "issue_title": "ModuleNotFoundError: No module named 'autogen_agentchat'", "issue_body": "What happened?\nDescribe the bug\nAfter following the installation steps, getting No module found error.\nTo Reproduce\nimport os\nimport asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.teams import MagenticOneGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.agents.web_surfer import MultimodalWebSurfer\nfrom autogen_ext.agents.file_surfer import FileSurfer\nfrom autogen_ext.agents.video_surfer import VideoSurfer\nfrom autogen_agentchat.agents import UserProxyAgent\nfrom pathlib import Path\nfrom autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n\n\n\n\n\n\n\n\n\n\nasync def main() -> None:\n\n    model_client = AzureOpenAIChatCompletionClient(\n    azure_deployment=\"gpt-4o\",\n    model=\"gpt-4o\",\n    api_version=\"2023-05-15\",\n    azure_endpoint=\"example url\",\n    api_key=api_key\n)\n\n    surfer = MultimodalWebSurfer(\n        \"WebSurfer\",\n        model_client=model_client,\n    )\n    filesurfer = FileSurfer('FileSurfer',model_client=model_client)\n    search_agent = UserProxyAgent(name=\"User\")\n    \n\n\n    team = MagenticOneGroupChat([surfer,search_agent, filesurfer], model_client=model_client)\n\n    \n\n\n    # task = \" some web task \"\n    await Console(team.run_stream(task=task))\n\n\n\nasyncio.run(main())\n\n\nExpected behavior\nImporting autogen_agentchat shouldnt raise error after following installation steps\nScreenshots\n\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nOther (please specify)\nOther library version.\n0.4.8\nModel used\ngpt-4o using Azure\nModel provider\nAzure OpenAI\nOther model provider\nNo response\nPython version\n3.12\n.NET version\nNone\nOperating system\nWindows\nThis is my pip list output. Even though the package is installed, importing it causes No  module found error -\n\n(autogen) C:\\Users\\pandsan\\OneDrive - acuitykp\\codes\\VisualAgent\\bolt_automate>pip list\nPackage            Version\n\naiofiles           24.1.0\nannotated-types    0.7.0\nanyio              4.8.0\nautogen-agentchat  0.4.8.1\nautogen-core       0.4.8.1\nautogen-ext        0.4.8.1\nazure-ai-inference 1.0.0b9\nazure-core         1.32.0\nazure-identity     1.20.0\ncertifi            2025.1.31\ncffi               1.17.1\ncharset-normalizer 3.4.1\ncolorama           0.4.6\ncryptography       44.0.2\nDeprecated         1.2.18\ndistro             1.9.0\nh11                0.14.0\nhttpcore           1.0.7\nhttpx              0.28.1\nidna               3.10\nimportlib_metadata 8.5.0\nisodate            0.7.2\njiter              0.8.2\njsonref            1.1.0\nmsal               1.31.1\nmsal-extensions    1.2.0\nopenai             1.65.4\nopentelemetry-api  1.30.0\npillow             11.1.0\npip                25.0\nportalocker        2.10.1\nprotobuf           5.29.3\npycparser          2.22\npydantic           2.10.6\npydantic_core      2.27.2\nPyJWT              2.10.1\npywin32            308\nregex              2024.11.6\nrequests           2.32.3\nsetuptools         75.8.0\nsix                1.17.0\nsniffio            1.3.1\ntiktoken           0.9.0\ntqdm               4.67.1\ntyping_extensions  4.12.2\nurllib3            2.3.0\nwheel              0.45.1\nwrapt              1.17.2\nzipp               3.21.0\n", "created_at": "2025-03-07", "closed_at": "2025-03-13", "labels": ["needs-triage", "awaiting-op-response"], "State": "closed", "Author": "sandeep-acuity"}
{"issue_number": 5856, "issue_title": "Anthropic client docs missing", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nNeed to add client to doc generation", "created_at": "2025-03-06", "closed_at": "2025-03-09", "labels": ["documentation"], "State": "closed", "Author": "jackgerrits"}
{"issue_number": 5852, "issue_title": "Document usage of custom runtime in AgentChat", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nAdd documentation to API doc as well as usage guide.", "created_at": "2025-03-06", "closed_at": null, "labels": ["documentation", "proj-agentchat"], "State": "open", "Author": "ekzhu"}
{"issue_number": 5851, "issue_title": "Stop consuming output queue when there is an exception from agent when using custom runtime", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nMake sure this test work for runtime fixture:\n\n\n\nautogen/python/packages/autogen-agentchat/tests/test_group_chat.py\n\n\n        Lines 346 to 363\n      in\n      7e5c115\n\n\n\n\n\n\n # TODO: add runtime fixture for testing with custom runtime once the issue regarding \n\n\n\n # hanging on exception is resolved. \n\n\n\n @pytest.mark.asyncio \n\n\n\n async def test_round_robin_group_chat_with_exception_raised() -> None: \n\n\n\n agent_1 = _EchoAgent(\"agent_1\", description=\"echo agent 1\") \n\n\n\n agent_2 = _FlakyAgent(\"agent_2\", description=\"echo agent 2\") \n\n\n\n agent_3 = _EchoAgent(\"agent_3\", description=\"echo agent 3\") \n\n\n\n termination = MaxMessageTermination(3) \n\n\n\n team = RoundRobinGroupChat( \n\n\n\n participants=[agent_1, agent_2, agent_3], \n\n\n\n termination_condition=termination, \n\n\n\n     ) \n\n\n\n \n\n\n\n with pytest.raises(ValueError, match=\"I am a flaky agent...\"): \n\n\n\n await team.run( \n\n\n\n task=\"Write a program that prints 'Hello, world!'\", \n\n\n\n         ) \n\n\n\n \n\n\n\n\n", "created_at": "2025-03-06", "closed_at": "2025-04-01", "labels": ["proj-agentchat"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 5850, "issue_title": "pass a dictionary as input and call a function in autogen", "issue_body": "What is the doc issue?\nDescribe the issue\nA clear and concise description of what the issue is. What is missing or incorrect?\nWhat do you want to see in the doc?\nScreenshots\nIf applicable, add screenshots to help explain your problem.\nAdditional context\nAdd any other context about the problem here.\nLink to the doc page, if applicable\nhttps://github.com/microsoft/autogen/issues/new?template=2-doc_issue.yml", "created_at": "2025-03-06", "closed_at": "2025-03-07", "labels": ["documentation", "needs-triage"], "State": "closed", "Author": "Zohreh6384NKH"}
{"issue_number": 5840, "issue_title": "pip install -U autogen-ext[video-surfer] in windows python virutal environment installation error", "issue_body": "What happened?\nDescribe the bug\npip install -U autogen-ext[video-surfer] in windows 11 python 3.12 virtual environment errors with:\nRequirement already satisfied: autogen-ext[video-surfer] in c:\\code\\autogenstudio\\autogenstudiocode.venv\\lib\\site-packages (0.4.8)\nRequirement already satisfied: autogen-core==0.4.8 in c:\\code\\autogenstudio\\autogenstudiocode.venv\\lib\\site-packages (from autogen-ext[video-surfer]) (0.4.8)\nRequirement already satisfied: autogen-agentchat==0.4.8 in c:\\code\\autogenstudio\\autogenstudiocode.venv\\lib\\site-packages (from autogen-ext[video-surfer]) (0.4.8)\nRequirement already satisfied: ffmpeg-python in c:\\code\\autogenstudio\\autogenstudiocode.venv\\lib\\site-packages (from autogen-ext[video-surfer]) (0.2.0)\nCollecting openai-whisper (from autogen-ext[video-surfer])\nUsing cached openai-whisper-20240930.tar.gz (800 kB)\nInstalling build dependencies ... done\nGetting requirements to build wheel ... error\nerror: subprocess-exited-with-error\n\u00d7 Getting requirements to build wheel did not run successfully.\n\u2502 exit code: 1\n\u2570\u2500> [25 lines of output]\n:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\nTraceback (most recent call last):\nFile \"c:\\Code\\autogenstudio\\autogenstudiocode.venv\\Lib\\site-packages\\pip_vendor\\pyproject_hooks_in_process_in_process.py\", line 389, in \nmain()\n~~~~^^\nFile \"c:\\Code\\autogenstudio\\autogenstudiocode.venv\\Lib\\site-packages\\pip_vendor\\pyproject_hooks_in_process_in_process.py\", line 373, in main\njson_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n~~~~^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"c:\\Code\\autogenstudio\\autogenstudiocode.venv\\Lib\\site-packages\\pip_vendor\\pyproject_hooks_in_process_in_process.py\", line 143, in get_requires_for_build_wheel\nreturn hook(config_settings)\nFile \"C:\\Users\\babal\\AppData\\Local\\Temp\\pip-build-env-jrq5d563\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 334, in get_requires_for_build_wheel\nreturn self._get_build_requires(config_settings, requirements=[])\n~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\babal\\AppData\\Local\\Temp\\pip-build-env-jrq5d563\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 304, in _get_build_requires\nself.run_setup()\n~~~~~~~~~~~~~~^^\nFile \"C:\\Users\\babal\\AppData\\Local\\Temp\\pip-build-env-jrq5d563\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 522, in run_setup\nsuper().run_setup(setup_script=setup_script)\n~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\babal\\AppData\\Local\\Temp\\pip-build-env-jrq5d563\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 320, in run_setup\nexec(code, locals())\n~~~~^^^^^^^^^^^^^^^^\nFile \"\", line 21, in \nFile \"\", line 11, in read_version\nKeyError: 'version'\n[end of output]\nnote: This error originates from a subprocess, and is likely not a problem with pip.\nerror: subprocess-exited-with-error\n\u00d7 Getting requirements to build wheel did not run successfully.\n\u2502 exit code: 1\n\u2570\u2500> See above for output.\nnote: This error originates from a subprocess, and is likely not a problem with pip.\nTo Reproduce\nSteps to reproduce the behavior. Please include code and outputs such as stacktrace.\npip install -U autogen-ext[video-surfer]\n\nIf your input is just \"I tried X, and it didn't work\" or\n\"X is not working\", your issue will be ignored.\nIf your input is not well formatted, it will hurt readability and\nmay be ignored as well.\n\nExpected behavior\nA clear and concise description of what you expected to happen.\nScreenshots\nIf applicable, add screenshots to help explain your problem.\nAdditional context\nAdd any other context about the problem here.\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\n0.4.8\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-05", "closed_at": null, "labels": [], "State": "open", "Author": "balakreshnan"}
{"issue_number": 5835, "issue_title": "CLAUDE_3_7_SONNET  error", "issue_body": "What happened?\nDescribe the bug\nA clear and concise description of what the bug is.\nIf it is a question or suggestion, please use Discussions\ninstead.\nTo Reproduce\nSteps to reproduce the behavior. Please include code and outputs such as stacktrace.\n\nIf your input is just \"I tried X, and it didn't work\" or\n\"X is not working\", your issue will be ignored.\nIf your input is not well formatted, it will hurt readability and\nmay be ignored as well.\n\nExpected behavior\nA clear and concise description of what you expected to happen.\nScreenshots\nIf applicable, add screenshots to help explain your problem.\nAdditional context\nAdd any other context about the problem here.\nWhich packages was the bug in?\nPython Core (autogen-core)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nAnthropic\nOther model provider\nNo response\nPython version\n3.10\n.NET version\nNone\nOperating system\nUbuntu", "created_at": "2025-03-05", "closed_at": "2025-03-06", "labels": ["proj-core"], "State": "closed", "Author": "moseshu"}
{"issue_number": 5832, "issue_title": "Local LLM example error", "issue_body": "What happened?\nWhen I use the example [https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/local-llms-ollama-litellm.html] and run litellm successfully. But there is a error:\nraise self._make_status_error_from_response(err.response) from None\nopenai.InternalServerError: Error code: 502\nWhat is wrong with it?\nMy code is like this:\nfrom dataclasses import dataclass\nimport asyncio\nfrom autogen_core import (\nAgentId,\nDefaultTopicId,\nMessageContext,\nRoutedAgent,\nSingleThreadedAgentRuntime,\ndefault_subscription,\nmessage_handler,\n)\nfrom autogen_core.model_context import BufferedChatCompletionContext\nfrom autogen_core.models import (\nAssistantMessage,\nChatCompletionClient,\nSystemMessage,\nUserMessage,\n)\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\ndef get_model_client() -> OpenAIChatCompletionClient:  # type: ignore\n\"Mimic OpenAI API using Local LLM Server.\"\nreturn OpenAIChatCompletionClient(\nmodel=\"llama3.2:1b\",\napi_key=\"NotRequiredSinceWeAreLocal\",\nbase_url=\"http://0.0.0.0:4000\",\nmodel_capabilities={\n\"json_output\": False,\n\"vision\": False,\n\"function_calling\": True,\n},\n)\n@DataClass\nclass Message:\ncontent: str\n@default_subscription\nclass Assistant(RoutedAgent):\ndef init(self, name: str, model_client: ChatCompletionClient) -> None:\nsuper().init(\"An assistant agent.\")\nself._model_client = model_client\nself.name = name\nself.count = 0\nself._system_messages = [\nSystemMessage(\ncontent=f\"Your name is {name} and you are a part of a duo of comedians.\"\n\"You laugh when you find the joke funny, else reply 'I need to go now'.\",\n)\n]\nself._model_context = BufferedChatCompletionContext(buffer_size=5)\n@message_handler\nasync def handle_message(self, message: Message, ctx: MessageContext) -> None:\n    self.count += 1\n    await self._model_context.add_message(UserMessage(content=message.content, source=\"user\"))\n    result = await self._model_client.create(self._system_messages + await self._model_context.get_messages())\n\n    print(f\"\\n{self.name}: {message.content}\")\n\n    if \"I need to go\".lower() in message.content.lower() or self.count > 2:\n        return\n\n    await self._model_context.add_message(AssistantMessage(content=result.content, source=\"assistant\"))  # type: ignore\n    await self.publish_message(Message(content=result.content), DefaultTopicId())  # type: ignore\n\nasync def run_number_agents():\nruntime = SingleThreadedAgentRuntime()\ncathy = await Assistant.register(\nruntime,\n\"cathy\",\nlambda: Assistant(name=\"Cathy\", model_client=get_model_client()),\n)\njoe = await Assistant.register(\nruntime,\n\"joe\",\nlambda: Assistant(name=\"Joe\", model_client=get_model_client()),\n)\nruntime.start()\nawait runtime.send_message(\nMessage(\"Joe, tell me a joke.\"),\nrecipient=AgentId(joe, \"default\"),\nsender=AgentId(cathy, \"default\"),\n)\nawait runtime.stop_when_idle()\nasyncio.run(run_number_agents())\nWhich packages was the bug in?\nPython Core (autogen-core)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\n0.4.8\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\n3.12\n.NET version\nNone\nOperating system\nWindows", "created_at": "2025-03-05", "closed_at": null, "labels": ["awaiting-op-response"], "State": "open", "Author": "Ben3-Github"}
{"issue_number": 5831, "issue_title": "Infinite loop appears when using swarm with human-computer dialogue", "issue_body": "What happened?\nDescribe the bug\nI used Swarm in autogen_autochat involves human-computer interaction. It obtains user input through handoffs. When the agent failed to understand the prompt well \u2014resulting in no handoff target\u2014the call fell into an infinite loop. In which both the context messages and content were empty. I suspect the root cause lies in either the swarm\u2019s handoff management logic or a multithreading issue.\nTo Reproduce\nI ran the demo Swarm Quickstart with LLM qwen-max.\nThe agents didn't follow the prompt to output query sentence firstly before handoff to user. The query sentence and handoff function call were returned in the same message.\n---------- user ----------\nI need to refund my flight.\n---------- travel_agent ----------\n[FunctionCall(id='call_9a676070361d469686ca6a', arguments='{}', name='transfer_to_flights_refunder')]\n---------- travel_agent ----------\n[FunctionExecutionResult(content='Transferred to flights_refunder, adopting the role of flights_refunder immediately.', call_id='call_9a676070361d469686ca6a')]\n---------- travel_agent ----------\nTransferred to flights_refunder, adopting the role of flights_refunder immediately.\nC:\\work\\code\\vs\\cloud-lab\\autogentest\\autochat\\.venv\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py:416: UserWarning: Both tool_calls and content are present in the message. This is unexpected. content will be ignored, tool_calls will be used.\n  model_result = await self._model_client.create(\n---------- flights_refunder ----------\n[FunctionCall(id='call_f4394b367fd24fcca2d1ab', arguments='{}', name='transfer_to_user')]\n---------- flights_refunder ----------\n[FunctionExecutionResult(content='Transferred to user, adopting the role of user immediately.', call_id='call_f4394b367fd24fcca2d1ab')]\n---------- flights_refunder ----------\nTransferred to user, adopting the role of user immediately.\nUser:\nTherefore, I modified the prompt with \"If you need information from the user, you must first only send your message, then you can handoff to the user.\"\nNow the query sentence is output as expected, but the infinite loop occurs.\n---------- user ----------\nI need to refund my flight.\n---------- travel_agent ----------\n[FunctionCall(id='call_34c97c23cca444098ddd71', arguments='{}', name='transfer_to_flights_refunder')]\n---------- travel_agent ----------\n[FunctionExecutionResult(content='Transferred to flights_refunder, adopting the role of flights_refunder immediately.', call_id='call_34c97c23cca444098ddd71')]\n---------- travel_agent ----------\nTransferred to flights_refunder, adopting the role of flights_refunder immediately.\n---------- flights_refunder ----------\nSure, I can help with that. Could you please provide me with the flight reference number?\n---------- flights_refunder ----------\n\n---------- flights_refunder ----------\n\n---------- flights_refunder ----------\nExpected behavior\n---------- user ----------\nI need to refund my flight.\n---------- travel_agent ----------\n[FunctionCall(id='call_ZQ2rGjq4Z29pd0yP2sNcuyd2', arguments='{}', name='transfer_to_flights_refunder')]\n[Prompt tokens: 119, Completion tokens: 14]\n---------- travel_agent ----------\n[FunctionExecutionResult(content='Transferred to flights_refunder, adopting the role of flights_refunder immediately.', call_id='call_ZQ2rGjq4Z29pd0yP2sNcuyd2')]\n---------- travel_agent ----------\nTransferred to flights_refunder, adopting the role of flights_refunder immediately.\n---------- flights_refunder ----------\nCould you please provide me with the flight reference number so I can process the refund for you?\n[Prompt tokens: 191, Completion tokens: 20]\n---------- flights_refunder ----------\n[FunctionCall(id='call_1iRfzNpxTJhRTW2ww9aQJ8sK', arguments='{}', name='transfer_to_user')]\n[Prompt tokens: 219, Completion tokens: 11]\n---------- flights_refunder ----------\n[FunctionExecutionResult(content='Transferred to user, adopting the role of user immediately.', call_id='call_1iRfzNpxTJhRTW2ww9aQJ8sK')]\n---------- flights_refunder ----------\nTransferred to user, adopting the role of user immediately.\n---------- user ----------\nUser:\nIssures\nI would appreciate it if you could answer:\n\nWhy does this infinite loop occur, and how can it be avoided?\nHow can we resolve the issue that the query sentence doesn't show before user input?\nIs it possible to provide a toggle parameter so that the content is not ignored when both tool_calls and content are present in the message?\n\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython 0.4.5\nOther library version.\nNo response\nModel used\nqwen-max\nModel provider\nOther (please specify below)\nOther model provider\nalibaba\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-05", "closed_at": "2025-03-07", "labels": [], "State": "closed", "Author": "Columboom"}
{"issue_number": 5828, "issue_title": "`candidate_func` in `SelectorGroupChat`", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nIn SelectorGroupChat, the selector_func allows for customizing an exact selection of agents to speak next.\nIn many cases, we still want to rely on the model to make a selection, but with a narrowed-down pool of candidates. So it is helpful to introduce a new parameter candidate_func which returns a list of potential candidates for the model to select from.\nThis is also to replicate the same allowed_transitions graph parameter in v0.2.", "created_at": "2025-03-04", "closed_at": "2025-03-17", "labels": ["proj-agentchat"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 5826, "issue_title": "FileSurfer stops reading the file", "issue_body": "What happened?\nDescribe the bug\nI was trying to read my source code file which contains 1000 lines of code using FileSurfer. It was normal at the first beginning of FileSurfer, but it suddenly came up Assertion error without any reason.\nTo Reproduce\n... other agents and model configuration ...\n...\n\nfile_surfer = FileSurfer(\"FileSurfer\",model_client=model_client)\n\nteam = SelectorGroupChat(\n    [file_surfer, planning_agent, g_agent, i_agent, s_agent, c_agent, critic_agent],\n    model_client=model_client,\n    termination_condition=MaxMessageTermination(max_messages=20),\n    selector_prompt=selector_prompt,\n    allow_repeated_speaker=True,\n\n)\n\nasync def main() -> None:\n    source_code_path = \"test/xxx/xxx/source_code.c\"\n    stream = team.run_stream(task=f\"Please performance an action for the source code from line 1341 to line 1439 in {source_code_path}\")\n    await Console(stream)\n\nasyncio.run(main())\nExpected behavior\nfile surfer should keep reading the file correctly without any error.\nScreenshots\nThe error:\n---------- FileSurfer ----------\nFile surfing error:\n\nTraceback (most recent call last):\n  File \"/Users/mars/Desktop/xxx/auto_gen_test/.venv/lib/python3.13/site-packages/autogen_ext/agents/file_surfer/_file_surfer.py\", line 83, in on_messages\n    _, content = await self._generate_reply(cancellation_token=cancellation_token)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/mars/Desktop/xxx/auto_gen_test/.venv/lib/python3.13/site-packages/autogen_ext/agents/file_surfer/_file_surfer.py\", line 113, in _generate_reply\n    assert isinstance(last_message, UserMessage)\n           ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n\nAdditional context\nAdd any other context about the problem here.\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\ngpt-4o-2024-08-06\nModel provider\nOpenAI\nOther model provider\nNo response\nPython version\n3.13\n.NET version\nNone\nOperating system\nMacOS", "created_at": "2025-03-04", "closed_at": null, "labels": [], "State": "open", "Author": "MarsWangyang"}
{"issue_number": 5824, "issue_title": "Enable `CodeExecutorAgent` to generate its own code and executing it.", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nAdding optional parameters: system_message and model_client to CodeExecutorAgent so it can generate and execute its own code -- achieving similar behavior as OpenAI Assistant Agent with code interpreter.", "created_at": "2025-03-04", "closed_at": "2025-04-15", "labels": ["code-execution", "proj-agentchat"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 5823, "issue_title": "Autogen Studio - PrismJS DOM Clobbering vulnerability", "issue_body": "What happened?\nDescribe the bug\nDependabot vulnerability: https://github.com/microsoft/autogen/security/dependabot/36\nPackage\nAffected versions\nPatched version\nprismjs\n(npm)\n<= 1.29.0\nNone\nPrism (aka PrismJS) through 1.29.0 allows DOM Clobbering (with resultant XSS for untrusted input that contains HTML but does not directly contain JavaScript), because document.currentScript lookup can be shadowed by attacker-injected HTML elements.\nWhich packages was the bug in?\nAutoGen Studio (autogensudio)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-04", "closed_at": null, "labels": ["proj-studio"], "State": "open", "Author": "rysweet"}
{"issue_number": 5817, "issue_title": "AzureAIChatCompletionClient max_toens is too small", "issue_body": "What happened?\nDescribe the bug\nWhen I use AzureAIChatCompletionClient to create a model_client for deepseek-r1 to output in streaming, it only returns 20 tokens, then stop with reason=length.\nTo Reproduce\nrun below code, you can only get incomplete response.\nmodel_client=AzureAIChatCompletionClient(\n    endpoint=os.getenv(\"AZURE_AI_FOUNDRY_ENDPOINT\"),\n    model=\"deepseek-r1\",\n    credential=AzureKeyCredential(os.getenv(\"AZURE_AI_FOUNDRY_KEY\")),\n    model_info={\n        \"json_output\": False,\n        \"function_calling\": False,\n        \"vision\": False,\n        \"family\": \"r1\",\n    },\n)\nasync for chunk in model_client.create_stream(messages=[UserMessage(content=\"how many r in strawberry?\", source=\"user\")]):\n    if isinstance(chunk, str):\n        print(chunk, end=\"\")\n    else:\n        print(\"\\n\" + chunk.content)\nExpected behavior\nA complete response should be returned, but not stop due to max_tokens limitation.\nAdditional context\nin AzureAIChatCompletionClient.create_stream(), a fixed max_tokens=20 is passed, does this make sense?\nif len(tools) > 0:\n    converted_tools = convert_tools(tools)\n    task = asyncio.create_task(\n        self._client.complete(messages=azure_messages, tools=converted_tools, stream=True, **create_args)\n    )\nelse:\n    task = asyncio.create_task(\n        self._client.complete(messages=azure_messages, max_tokens=20, stream=True, **create_args)\n    )\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\ndeepseek-r1\nModel provider\nAzure AI Foundary (Azure AI Studio)\nOther model provider\nNo response\nPython version\n3.13\n.NET version\nNone\nOperating system\nWindows", "created_at": "2025-03-04", "closed_at": "2025-03-07", "labels": ["proj-extensions"], "State": "closed", "Author": "ZacharyHuang"}
{"issue_number": 5816, "issue_title": "Event loop is Close? How to fix", "issue_body": "What happened?\nimport os\nimport asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_core.models import ModelInfo, ModelFamily\nfrom autogen_agentchat.agents import AssistantAgent, SocietyOfMindAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat, BaseGroupChat\nfrom autogenstudio.teammanager import TeamManager\nfrom flask import Flask, request, jsonify\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport torch\nimport numpy as np\nimport json\napp = Flask(name)\n@app.route(\"/autogenprocess\", methods=[\"POST\"])\nasync def autogen():\nmanager = TeamManager()\nteam_config = json.load(open(\"team.json\"))\ntry:\n    \n    data = request.get_json()\n    teacher_question = data.get(\"teacher_question\", [])\n    teacher_answer = data.get(\"teacher_answer\", [])\n    student_answer = data.get(\"student_answer\", [])\n    maxscore = data.get(\"maxscore\", [])\n    room_id = data.get(\"room\", [])\n    \n    if isinstance(teacher_question, str):\n        teacher_question = [teacher_question]\n    if  isinstance(teacher_answer, str):\n        teacher_answer = [teacher_answer]\n    if isinstance(student_answer, str):\n        student_answer = [student_answer]\n    if isinstance(maxscore, str):\n        maxscore = [maxscore]\n        \n    print(\"teacher_question :\", teacher_question)\n    print(\"teacher_answer :\", teacher_answer)\n    print(\"student_answer :\", student_answer)\n    print(\"maxscore :\", maxscore)\n        \n    if len(teacher_question) != len(teacher_answer) or len(teacher_question) != len(student_answer) or len(teacher_question) != len(maxscore):\n        return jsonify({\"error\": \"The number of teacher's questions and student's answers must be the same\"}), 400\n    \n    # Now you can iterate over the messages and extract their content\n    response_content = []\n    \n    \"\"\"if os.path.exists(f\"./state/stateroom_{room_id}.json\"):\n        with open(f\"./state/stateroom_{room_id}.json\", \"r\") as f:\n            state = json.load(f)\n            await TeamManager.load_from_file(state)\"\"\"\n    \n    print(len(teacher_question))\n    for i in range(len(teacher_question)):\n        \n        task_text = (\n            f\"\u0e08\u0e07\u0e15\u0e23\u0e27\u0e08\u0e2a\u0e2d\u0e1a\u0e41\u0e25\u0e30\u0e43\u0e2b\u0e49\u0e04\u0e30\u0e41\u0e19\u0e19\u0e04\u0e33\u0e15\u0e2d\u0e1a\u0e02\u0e2d\u0e07\u0e19\u0e31\u0e01\u0e40\u0e23\u0e35\u0e22\u0e19 ({student_answer[i]}) \"\n            f\"\u0e42\u0e14\u0e22\u0e1e\u0e34\u0e08\u0e32\u0e23\u0e13\u0e32\u0e08\u0e32\u0e01\u0e04\u0e27\u0e32\u0e21\u0e16\u0e39\u0e01\u0e15\u0e49\u0e2d\u0e07 \u0e04\u0e27\u0e32\u0e21\u0e2a\u0e21\u0e40\u0e2b\u0e15\u0e38\u0e2a\u0e21\u0e1c\u0e25 \u0e41\u0e25\u0e30\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e2d\u0e14\u0e04\u0e25\u0e49\u0e2d\u0e07\u0e01\u0e31\u0e1a\u0e04\u0e33\u0e16\u0e32\u0e21 ({teacher_question[i]}) \"\n            f\"\u0e2d\u0e22\u0e48\u0e32\u0e43\u0e0a\u0e49\u0e04\u0e33\u0e15\u0e2d\u0e1a\u0e02\u0e2d\u0e07\u0e2d\u0e32\u0e08\u0e32\u0e23\u0e22\u0e4c ({teacher_answer[i]}) \u0e40\u0e1b\u0e47\u0e19\u0e40\u0e01\u0e13\u0e11\u0e4c\u0e40\u0e1b\u0e23\u0e35\u0e22\u0e1a\u0e40\u0e17\u0e35\u0e22\u0e1a\u0e42\u0e14\u0e22\u0e15\u0e23\u0e07 \u0e41\u0e15\u0e48\u0e43\u0e2b\u0e49\u0e43\u0e0a\u0e49\u0e40\u0e1b\u0e47\u0e19\u0e41\u0e19\u0e27\u0e17\u0e32\u0e07\u0e27\u0e48\u0e32\u0e04\u0e33\u0e15\u0e2d\u0e1a\u0e17\u0e35\u0e48\u0e40\u0e2b\u0e21\u0e32\u0e30\u0e2a\u0e21\u0e04\u0e27\u0e23\u0e21\u0e35\u0e25\u0e31\u0e01\u0e29\u0e13\u0e30\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e44\u0e23 \"\n            f\"\u0e43\u0e2b\u0e49\u0e04\u0e30\u0e41\u0e19\u0e19\u0e19\u0e31\u0e01\u0e40\u0e23\u0e35\u0e22\u0e19\u0e43\u0e19\u0e0a\u0e48\u0e27\u0e07 0 \u0e16\u0e36\u0e07 {maxscore[i]} \u0e04\u0e30\u0e41\u0e19\u0e19 \u0e41\u0e25\u0e30\u0e2d\u0e18\u0e34\u0e1a\u0e32\u0e22\u0e40\u0e2b\u0e15\u0e38\u0e1c\u0e25\u0e02\u0e2d\u0e07\u0e04\u0e30\u0e41\u0e19\u0e19\u0e17\u0e35\u0e48\u0e43\u0e2b\u0e49\"\n        )\n        \n        response = await manager.run(\n            team_config=\"team.json\",\n            task=task_text,\n        )\n\n                    \n        # Extract the task_result from the response\n        task_result = response.task_result\n\n        # Extract the messages from the task_result\n        messages = task_result.messages\n\n        for message in messages:\n            response_content.append({\n                \"source\": message.source,\n                \"content\": message.content if isinstance(message.content, str) else str(message.content)\n            })\n\n        # Convert response content into a JSON-formatted string\n        final_response = json.dumps(response_content, indent=4, ensure_ascii=False)\n        \n        print(final_response)\n        \n        \"\"\"with open(f\"./state/stateroom_{room_id}.json\", \"w\") as f:\n                team_state = await team.save_state()\n                json.dump(team_state, f, indent=4)\"\"\"\n        \n\n    return jsonify({\"model_results\": response_content})\n\nexcept Exception as e:\n    return jsonify({\"error\": str(e)})\n\n@app.route(\"/autogenprocessfeedback\", methods=[\"POST\"])\nasync def autogen_feedback():\nmanager = TeamManager()\nteam_config = json.load(open(\"team.json\"))\ntry:\n    \n    data = request.get_json()\n    teacher_question = data.get(\"question\", [])\n    teacher_answer = data.get(\"answer_teacher\", [])\n    student_answer = data.get(\"answer_student\", [])\n    teacher_score = data.get(\"score_ai\", [])\n    print(\"teacher_question :\", teacher_question)\n    \n    if isinstance(teacher_question, str):\n        teacher_question = [teacher_question]\n    if  isinstance(teacher_answer, str):\n        teacher_answer = [teacher_answer]\n    if isinstance(student_answer, str):\n        student_answer = [student_answer]\n    if isinstance(teacher_score, str):\n        teacher_score = [teacher_score]\n\n        \n    print(\"teacher_question :\", teacher_question)\n    print(\"teacher_answer :\", teacher_answer)\n    print(\"student_answer :\", student_answer)\n    print(\"teacher_score :\", teacher_score)\n    \n    \n        \n    if len(teacher_question) != len(teacher_answer) or len(teacher_question) != len(student_answer) or len(teacher_question) != len(teacher_score):\n        print(len(teacher_question))\n        print(len(teacher_answer))\n        print(len(student_answer))\n        print(len(teacher_score))\n        return jsonify({\"error\": \"The number of teacher's questions and student's answers must be the same\"}), 400\n    \n    # Now you can iterate over the messages and extract their content\n    response_content = []\n    \n    \"\"\"if os.path.exists(f\"./state/stateroom_{room_id}.json\"):\n        with open(f\"./state/stateroom_{room_id}.json\", \"r\") as f:\n            state = json.load(f)\n            await TeamManager.load_from_file(state)\"\"\"\n    \n    print(len(teacher_question))\n    for i in range(len(teacher_question)):\n        \n        task_text =f\"\u0e08\u0e32\u0e01\u0e42\u0e08\u0e17\u0e22\u0e4c\u0e02\u0e49\u0e2d\u0e2a\u0e2d\u0e1a ({teacher_question[i]}) \u0e41\u0e25\u0e30\u0e21\u0e35\u0e04\u0e33\u0e15\u0e2d\u0e1a\u0e02\u0e2d\u0e07\u0e2d\u0e32\u0e08\u0e32\u0e23\u0e22\u0e4c ({teacher_answer[i]}) \u0e04\u0e33\u0e15\u0e2d\u0e1a\u0e02\u0e2d\u0e07\u0e19\u0e31\u0e01\u0e40\u0e23\u0e35\u0e22\u0e19 ({student_answer[i]}) \u0e04\u0e30\u0e41\u0e19\u0e19\u0e17\u0e35\u0e48\u0e17\u0e35\u0e48\u0e2d\u0e32\u0e08\u0e32\u0e23\u0e22\u0e4c\u0e21\u0e2d\u0e1a\u0e43\u0e2b\u0e49\u0e19\u0e31\u0e01\u0e40\u0e23\u0e35\u0e22\u0e19\u0e04\u0e37\u0e2d ({teacher_score[i]}) \u0e08\u0e07\u0e2d\u0e18\u0e34\u0e1a\u0e32\u0e22\u0e40\u0e2b\u0e15\u0e38\u0e1c\u0e25\u0e02\u0e2d\u0e07\u0e04\u0e30\u0e41\u0e19\u0e19\u0e17\u0e35\u0e48\u0e43\u0e2b\u0e49 \u0e43\u0e19\u0e15\u0e2d\u0e19\u0e2a\u0e38\u0e14\u0e17\u0e49\u0e32\u0e22\u0e15\u0e49\u0e2d\u0e07\u0e21\u0e35\u0e01\u0e32\u0e23\u0e01\u0e25\u0e48\u0e32\u0e27\u0e16\u0e36\u0e07\u0e04\u0e30\u0e41\u0e19\u0e19\u0e41\u0e25\u0e30\u0e40\u0e2b\u0e15\u0e38\u0e1c\u0e25\u0e15\u0e49\u0e2d\u0e07\u0e40\u0e1b\u0e47\u0e19\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\"\n        \n        response = await manager.run(\n            team_config=\"feedbackteam.json\",\n            task=task_text,\n        )\n                    \n        # Extract the task_result from the response\n        task_result = response.task_result\n\n        # Extract the messages from the task_result\n        messages = task_result.messages\n\n        for message in messages:\n            response_content.append({\n                \"source\": message.source,\n                \"content\": message.content if isinstance(message.content, str) else str(message.content)\n            })\n\n        # Convert response content into a JSON-formatted string\n        final_response = json.dumps(response_content, indent=4, ensure_ascii=False)\n                    \n        \"\"\"with open(f\"./state/stateroom_{room_id}.json\", \"w\") as f:\n                team_state = await team.save_state()\n                json.dump(team_state, f, indent=4)\"\"\"\n        print(final_response)\n\n    return jsonify({\"model_results\": response_content})\n\nexcept Exception as e:\n    return jsonify({\"error\": str(e)})\n\nif name == \"main\":\napp.run(debug=True)\n\nTraceback (most recent call last):\nFile \"c:\\project.venv\\Lib\\site-packages\\httpx_client.py\", line 1985, in aclose\nawait self._transport.aclose()\nFile \"c:\\project.venv\\Lib\\site-packages\\httpx_transports\\default.py\", line 406, in aclose\nawait self._pool.aclose()\nFile \"c:\\project.venv\\Lib\\site-packages\\httpcore_async\\connection_pool.py\", line 353, in aclose\nawait self._close_connections(closing_connections)\nFile \"c:\\project.venv\\Lib\\site-packages\\httpcore_async\\connection_pool.py\", line 345, in _close_connections\nawait connection.aclose()\nFile \"c:\\project.venv\\Lib\\site-packages\\httpcore_async\\connection.py\", line 173, in aclose\nawait self._connection.aclose()\nFile \"c:\\project.venv\\Lib\\site-packages\\httpcore_async\\http11.py\", line 258, in aclose\nawait self._network_stream.aclose()\nFile \"c:\\project.venv\\Lib\\site-packages\\httpcore_backends\\anyio.py\", line 53, in aclose\nawait self._stream.aclose()\nFile \"c:\\project.venv\\Lib\\site-packages\\anyio\\streams\\tls.py\", line 201, in aclose\nawait self.transport_stream.aclose()\nFile \"c:\\project.venv\\Lib\\site-packages\\anyio_backends_asyncio.py\", line 1306, in aclose\nself._transport.close()\nFile \"C:\\Users\\griae\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\proactor_events.py\", line 109, in close\nself._loop.call_soon(self._call_connection_lost, None)\nFile \"C:\\Users\\griae\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 792, in call_soon\nself._check_closed()\nFile \"C:\\Users\\griae\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 539, in _check_closed\nraise RuntimeError('Event loop is closed')\nRuntimeError: Event loop is closed\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-04", "closed_at": "2025-03-04", "labels": ["awaiting-op-response"], "State": "closed", "Author": "keingkrai"}
{"issue_number": 5814, "issue_title": "OpenAI bad request error for Tools call with agentchat v0.4.7 | Working fine with agentchat v0.4.3", "issue_body": "What happened?\nBug\nI am evaluating the tools calling with v0.4.7. When configuring tools call, getting openai.BadRequestError: Error code: 400\nWhich worked fine with v0.4.3.\nReproduce\nSteps to reproduce the behavior.\nRun the below script.\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nimport asyncio\n\n\n# Define a tool\nasync def get_weather(city: str) -> str:\n    return f\"The weather in {city} is 73 degrees and Sunny.\"\n\n\nasync def main() -> None:\n    # Define an agent\n    weather_agent = AssistantAgent(\n        name=\"weather_agent\",\n        model_client=OpenAIChatCompletionClient(\n            model=<llm_model>,\n            api_key=<api_key>,\n            base_url=\"https://genai-api.example.com/v1\",\n            model_capabilities={\n                \"vision\": True,\n                \"function_calling\": True,\n                \"json_output\": True,\n            },\n        ),\n        tools=[get_weather],\n        system_message=\"Make tools call to get_weather(city) and return the result.\"\n    )\n\n    agent_team = RoundRobinGroupChat([weather_agent], max_turns=1)\n\n    while True:\n        user_input = input(\"Enter a message (type 'exit' to leave): \")\n        if user_input.strip().lower() == \"exit\":\n            break\n        stream = agent_team.run_stream(task=user_input)\n        await Console(stream)\n\nasyncio.run(main())\n\n\nOutput for agentchat v0.4.3\nEnter a message (type 'exit' to leave): How is whether in New York\n---------- user ----------\nHow is whether in New York\n---------- weather_agent ----------\n[FunctionCall(id='chatcmpl-tool-3041237d4c7a40af83ec482dd7709fce', arguments='{\"city\": \"New York\"}', name='get_weather')]\n---------- weather_agent ----------\n[FunctionExecutionResult(content='The weather in New York is 73 degrees and Sunny.', call_id='chatcmpl-tool-3041237d4c7a40af83ec482dd7709fce')]\n---------- weather_agent ----------\nThe weather in New York is 73 degrees and Sunny.\nEnter a message (type 'exit' to leave): exit\n\nOutput for agentchat v0.4.7\nopenai.BadRequestError: Error code: 400 - {'object': 'error', 'message': \"[{'type': 'extra_forbidden', 'loc': ('body', 'tools', 0, 'function', 'strict'), 'msg': 'Extra inputs are not permitted', 'input': False}]\", 'type': 'BadRequestError', 'param': None, 'code': 400}\n\nExpected behavior\nShould not throw openai bad request error.\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython 0.4.7\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-04", "closed_at": null, "labels": ["awaiting-op-response"], "State": "open", "Author": "avinashmihani"}
{"issue_number": 5803, "issue_title": "Port AgentChat Groups to .NET", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nPort remaining GroupChat/Manager implementations from Python AgentChat", "created_at": "2025-03-03", "closed_at": null, "labels": [], "State": "open", "Author": "lokitoth"}
{"issue_number": 5802, "issue_title": "Port AgentChat Agents to .NET", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nPort the built-in Agents in Python AgentChat to .NET", "created_at": "2025-03-03", "closed_at": null, "labels": ["dotnet"], "State": "open", "Author": "lokitoth"}
{"issue_number": 5801, "issue_title": "Port AgentChat Terminations to .NET", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nPort the TerminationCondition implementations to .NET", "created_at": "2025-03-03", "closed_at": "2025-03-13", "labels": [], "State": "closed", "Author": "lokitoth"}
{"issue_number": 5800, "issue_title": "Save/Load in .NET AgentChat", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nFlesh out implementation of Save/Load on top of new Save/Load APIs in .NET", "created_at": "2025-03-03", "closed_at": "2025-03-13", "labels": [], "State": "closed", "Author": "lokitoth"}
{"issue_number": 5799, "issue_title": "Base implementation for Reset for .NET AgentChat", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nEnable de-initialization of the GroupChat/Manager classes (and manage internal AutoGen runtime initialization appropriately) to enable Reset() operations.", "created_at": "2025-03-03", "closed_at": "2025-03-07", "labels": [], "State": "closed", "Author": "lokitoth"}
{"issue_number": 5797, "issue_title": "Autogen Studio -  issue with src/images/icon.png", "issue_body": "What happened?\nDescribe the bug\nI tried yarn build after setting up the environment using devcontainer for Autogen Studio.\nyarn build started throwing error saying\n\"gatsby-plugin-manifest\" threw an error while running the onPostBootstrap lifecycle:\nInput file contains unsupported image format\nError: Input file contains unsupported image format\nTo Reproduce\nrun the devcontainer then try yarn build\nAdditional context\nLater, I navigated to src/images and downloaded the raw icon.png from GitHub and then replaced it with the png in my local then it fixed it\nWhich packages was the bug in?\nAutoGen Studio (autogensudio)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nMacOS", "created_at": "2025-03-03", "closed_at": "2025-03-04", "labels": ["proj-studio"], "State": "closed", "Author": "msbarathraj"}
{"issue_number": 5795, "issue_title": "Skip SSL verification option missing while creating model client in v0.4.x", "issue_body": "What happened?\nDescribe the bug\nI was using v0.2.x earlier an agentic implementation. The agent to interact with LLM, was expecting LLM_CONFIG, which had an option to skip the SSL verification. Now in v0.4.x, the model client initiation does not have an option for skipping verification.\nTo Reproduce\nTo reproduce, use in v0.2 as below:\nclass MyHttpClient(httpx.Client):\n    def __deepcopy__(self, memo):\n        return self\n\nllm_config = {\n    \"config_list\": [\n        {\n            \"model\": \"some-model\",\n            \"api_key\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n            \"base_url\": \"https://genai-api.example.com/v1\",\n            \"http_client\": MyHttpClient(verify=False),\n        }\n    ],\n}\n\nIn v0.4, model client is initiated as below.\nmodel_client = OpenAIChatCompletionClient(\n            model=model,\n            api_key=api_key,\n            base_url=base_url,\n            model_capabilities={\"vision\": True, \"function_calling\": True, \"json_output\": True}\n        )\n\nExpected behavior\nIt is expected that in v0.4 to have an option within OpenAIChatCompletionClient module like llm_config in v0.2 to handle http_client.\nSample below:\nclass MyHttpClient(httpx.Client):\n    def __deepcopy__(self, memo):\n        return self\n\nmodel_client = OpenAIChatCompletionClient(\n            model=model,\n            api_key=api_key,\n            base_url=base_url,\n            model_capabilities={\"vision\": True, \"function_calling\": True, \"json_output\": True},\n            http_client=MyHttpClient(verify=False)\n        )\n\nScreenshots\nBelow is the error:\nhttpx.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)\n\n\nAdditional context\nWe need to bypass SSL verification for the custom model integration with our agents implementation from local machine.\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython 0.4.7\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-03", "closed_at": null, "labels": ["proj-extensions"], "State": "open", "Author": "avinashmihani"}
{"issue_number": 5793, "issue_title": "VideoSurfer unable to detect video at the given path", "issue_body": "What happened?\nDescribe the bug\nVideo Surfer is unable to read the video at the given path .\nTo Reproduce\nimport os\nimport asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.teams import MagenticOneGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.agents.web_surfer import MultimodalWebSurfer\nfrom autogen_ext.agents.file_surfer import FileSurfer\nfrom autogen_ext.agents.video_surfer import VideoSurfer\nfrom autogen_agentchat.agents import UserProxyAgent\nfrom pathlib import Path\nfrom autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n\n\nvideo_path =\"yahoo.mp4\"\nif not os.path.exists(video_path):\n    raise FileNotFoundError(f\"Video file not found at: {video_path}\")\nasync def main() -> None:\n    model_client = AzureOpenAIChatCompletionClient(\n    azure_deployment=\"gpt-4o\",\n    model=\"gpt-4o\",\n    api_version=\"2023-05-15\",\n    api_key=api_key\n)\n\n    surfer = MultimodalWebSurfer(\n        \"WebSurfer\",\n        model_client=model_client,\n    )\n    filesurfer = FileSurfer('FileSurfer',model_client=model_client)\n    videosurfer = VideoSurfer('VideoSurfer',model_client=model_client)\n    Path_agent = UserProxyAgent(name=\"User\")\n\n\n    team = MagenticOneGroupChat([Path_agent, filesurfer, videosurfer, surfer], model_client=model_client)\n    # team = MagenticOneGroupChat([ videosurfer], model_client=model_client)\n    \n    # Modify the task to include the full path to the video\n    task = f\" Use video {video_path} to understand the steps involved.\"\n    \n    await Console(team.run_stream(task=task))\n\n\nasyncio.run(main())\n\nExpected behavior\nThe agent reads the video and gives a description.\nScreenshots\n\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\ngpt-4o\nModel provider\nAzure OpenAI\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nWindows", "created_at": "2025-03-03", "closed_at": null, "labels": ["proj-extensions"], "State": "open", "Author": "sandeep-acuity"}
{"issue_number": 5790, "issue_title": "Add latency and token per second stats to `LLMCallEvent`.", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nIt's useful to tracing LLM call performance.\nWork on this once #5730 is completed so we can add this to all LLM related events.", "created_at": "2025-03-03", "closed_at": null, "labels": ["logging", "proj-core", "proj-extensions"], "State": "open", "Author": "ekzhu"}
{"issue_number": 5787, "issue_title": "Integrating AgentChat and Core", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\n\nCustomize runtime in AgentChat\nAllow AgentChat agents and teams to be part of a Core workflow\n", "created_at": "2025-03-03", "closed_at": null, "labels": ["epic", "proj-core", "proj-agentchat"], "State": "open", "Author": "ekzhu"}
{"issue_number": 5786, "issue_title": "Add documentation for `TextTerminationCondition`", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nUpdate documentation once #5742 is done.\n\nhttps://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/termination.html\nAdd a new subsection \"Single Agent Team\" before this subsection: https://microsoft.github.io/autogen/dev/user-guide/agentchat-user-guide/tutorial/teams.html#creating-a-team. And show the example above.\nAdd pointer from the agents doc to this example regarding the common question of how to run an agent repeatedly.\n", "created_at": "2025-03-02", "closed_at": "2025-03-04", "labels": ["documentation", "proj-agentchat"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 5782, "issue_title": "Doc update to code executor agent to mention PythonCodeExecutionTool", "issue_body": "\nThe assistant agent always outputs TERMINATE after generating code\n\nYou can customize the system message of AssistantAgent.\n\nIs there a way to ensure that the snippet to be part of a markdown block.\n\nHave you tried PythonCodeExecutionTool? It will run the code in the same agent rather than requiring the code to be inside markdown format.\nhttps://microsoft.github.io/autogen/stable/reference/python/autogen_ext.tools.code_execution.html#autogen_ext.tools.code_execution.PythonCodeExecutionTool\nThe two-agent chat with assistant and code executor is a legacy pattern from v0.2. It's not a top recommended way to do code execution now.\nOriginally posted by @ekzhu in #5767 (reply in thread)", "created_at": "2025-03-02", "closed_at": "2025-03-04", "labels": ["documentation"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 5780, "issue_title": "Support OpenAI's New Project-Based API Keys (sk-proj-*)", "issue_body": "What happened?\nWhen using the new OpenAI project-based keys (which begin with sk-proj-...), AutoGen fails to validate the key. The code currently seems to only accept keys starting with the legacy sk-... format.\nError:  code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-...'\nExpected Behavior\nAutoGen should accept both legacy sk-... keys and project-based sk-proj-... keys, since OpenAI is now issuing project keys by default.\nDisscussions\nExisting PR with fix: #2426\nPrevious discussion: #2432\nFixed In: autogen/oai/openai_utils.py\nIn Microsft/autogen repo there is no autogen/oai/openai_utils.py file exists!\nWhich packages was the bug in?\nAutoGen Studio (autogensudio)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\ngpt-40\nModel provider\nOpenAI\nOther model provider\nNo response\nPython version\n3.13\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-02", "closed_at": "2025-03-02", "labels": ["needs-triage"], "State": "closed", "Author": "tohid4n"}
{"issue_number": 5777, "issue_title": "Update AssistantAgent and ModelContext API doc to show example of custom model context for R1.", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nShow example of custom model context for R1 that filters out thought content when getting messages.\nFor reasoning models like R1, having thought tokens in the context can cause models to underperform.\nfrom typing import List\nfrom autogen_core.model_context import UnboundedChatCompletionContext\nfrom autogen_core.models import AssistantMessage, LLMMessage\n\n\nclass ReasoningModelContext(UnboundedChatCompletionContext):\n    \"\"\"A model context for reasoning models.\"\"\"\n\n    async def get_messages(self) -> List[LLMMessage]:\n        messages = await super().get_messages()\n        # Filter out thought field from AssistantMessage.\n        messages_out = []\n        for message in messages:\n            if isinstance(message, AssistantMessage):\n                message.thought = None\n            messages_out.append(message)\n        return messages_out", "created_at": "2025-03-02", "closed_at": "2025-03-04", "labels": ["documentation", "proj-core"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 5775, "issue_title": "how to get call funtion in variable", "issue_body": "i want to get call function but output is so long\n\nthank i try to learn in tech\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-01", "closed_at": "2025-03-02", "labels": ["needs-triage"], "State": "closed", "Author": "keingkrai"}
{"issue_number": 5774, "issue_title": "how to save and load state in  TeamManager", "issue_body": "i use team_config from outside team.JSON but i don't know how to save_state in 'TeamManager'\nimport os\nimport asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_core.models import ModelInfo, ModelFamily\nfrom autogen_agentchat.agents import AssistantAgent, SocietyOfMindAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogenstudio.teammanager import TeamManager\nfrom flask import Flask, request, jsonify\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport torch\nimport numpy as np\nimport json\napp = Flask(name)\n@app.route(\"/autogenprocess\", methods=[\"POST\"])\nasync def autogen():\nmanager = TeamManager()\ntry:\n    \n    data = request.get_json()\n    teacher_question = data.get(\"teacher_question\", [])\n    teacher_answer = data.get(\"teacher_answer\", [])\n    student_answer = data.get(\"student_answer\", [])\n    maxscore = data.get(\"maxscore\", [])\n    room_id = data.get(\"room\", [])\n    \n    if isinstance(teacher_question, str):\n        teacher_question = [teacher_question]\n    if  isinstance(teacher_answer, str):\n        teacher_answer = [teacher_answer]\n    if isinstance(student_answer, str):\n        student_answer = [student_answer]\n    if isinstance(maxscore, str):\n        maxscore = [maxscore]\n        \n    print(\"teacher_question :\", teacher_question)\n    print(\"teacher_answer :\", teacher_answer)\n    print(\"student_answer :\", student_answer)\n    print(\"maxscore :\", maxscore)\n        \n    if len(teacher_question) != len(teacher_answer) or len(teacher_question) != len(student_answer) or len(teacher_question) != len(maxscore):\n        return jsonify({\"error\": \"The number of teacher's questions and student's answers must be the same\"}), 400\n    \n    # Now you can iterate over the messages and extract their content\n    response_content = []\n    \n    if os.path.exists(f\"./state/stateroom_{room_id}.json\"):\n        with open(f\"./state/stateroom_{room_id}.json\", \"r\") as f:\n            state = json.load(f)\n            await TeamManager.load_from_file(state)\n    \n    print(len(teacher_question))\n    for i in range(len(teacher_question)):\n        \n        task_text = (\n            f\"\u0e08\u0e07\u0e40\u0e1b\u0e23\u0e35\u0e22\u0e1a\u0e40\u0e17\u0e35\u0e22\u0e1a\u0e04\u0e27\u0e32\u0e21\u0e04\u0e25\u0e49\u0e32\u0e22\u0e23\u0e30\u0e2b\u0e27\u0e48\u0e32\u0e07\u0e04\u0e33\u0e15\u0e2d\u0e1a\u0e02\u0e2d\u0e07\u0e2d\u0e32\u0e08\u0e32\u0e23\u0e22\u0e4c {teacher_answer[i]} \u0e41\u0e25\u0e30\u0e04\u0e33\u0e15\u0e2d\u0e1a\u0e02\u0e2d\u0e07\u0e19\u0e31\u0e01\u0e40\u0e23\u0e35\u0e22\u0e19 {student_answer[i]} \"\n            f\"\u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49\u0e04\u0e33\u0e15\u0e2d\u0e1a\u0e02\u0e2d\u0e07\u0e2d\u0e32\u0e08\u0e32\u0e23\u0e22\u0e4c\u0e40\u0e1b\u0e47\u0e19\u0e41\u0e01\u0e19\u0e2b\u0e25\u0e31\u0e01\u0e43\u0e19\u0e02\u0e2d\u0e1a\u0e40\u0e02\u0e15\u0e02\u0e2d\u0e07\u0e04\u0e33\u0e16\u0e32\u0e21 {teacher_question[i]} \"\n            f\"\u0e41\u0e25\u0e30\u0e43\u0e2b\u0e49\u0e04\u0e30\u0e41\u0e19\u0e19\u0e08\u0e32\u0e01 0 \u0e04\u0e30\u0e41\u0e19\u0e19 \u0e16\u0e36\u0e07 {maxscore[i]} \u0e04\u0e30\u0e41\u0e19\u0e19 \u0e15\u0e32\u0e21\u0e04\u0e27\u0e32\u0e21\u0e40\u0e2b\u0e21\u0e32\u0e30\u0e2a\u0e21\u0e02\u0e2d\u0e07\u0e04\u0e33\u0e15\u0e2d\u0e1a\"\n        )\n        \n        response = await manager.run(\n            team_config=\"team.json\",\n            task=task_text,\n        )\n        \n        \n        print(response)\n                    \n        # Extract the task_result from the response\n        task_result = response.task_result\n        print(\"sfsddfsdfsdfsdfsdf\",task_result)\n\n        # Extract the messages from the task_result\n        messages = task_result.messages\n\n        for message in messages:\n            response_content.append({\n                \"source\": message.source,\n                \"content\": message.content if isinstance(message.content, str) else str(message.content)\n            })\n\n        # Convert response content into a JSON-formatted string\n        final_response = json.dumps(response_content, indent=4, ensure_ascii=False)\n        \n        print(final_response)\n        \n        \"\"\"with open(f\"./state/stateroom_{room_id}.json\", \"w\") as f:\n                team_state = await fina.save_state()\n                json.dump(team_state, f, indent=4)\"\"\"\n        \n\n    return jsonify({\"model_results\": response_content})\n\nexcept Exception as e:\n    return jsonify({\"error\": str(e)})\n\nif name == \"main\":\nwith app.app_context():\napp.run(host=\"0.0.0.0\", port=5000, debug=True)\nWhich packages was the bug in?\nAutoGen Studio (autogensudio)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-01", "closed_at": "2025-03-02", "labels": ["proj-studio", "proj-agentchat"], "State": "closed", "Author": "keingkrai"}
{"issue_number": 5773, "issue_title": "Enable file upload as part of task in AGS ....", "issue_body": "Many use cases often involve integrating files as part of a the task\n\nwrite an analysis on the insights in this image\n\nHow\n\nmodify UI to support file upload. The idea is to yield a sequence of string and files\ne.g., if a command and an image are uploaded -> [\"what is in this image\", image, \"\"]\nUpdate task endpoint to process files and add them to the task\n\nWe need to convert the the sequence from frontend a sequence from\n# Note: task is of type `task: str | ChatMessage | Sequence[ChatMessage] | None = None,`\n\nChatMessage = Annotated[\n    TextMessage | MultiModalMessage | StopMessage | ToolCallSummaryMessage | HandoffMessage, Field(discriminator=\"type\")\n]\n\n\nSupport a specific set of filetypes and determine how they are processed before passed to model\n\nimages, convert them with pil and add directly\ntxt/pdf/ etc... save to a location , convert to markdown and add as string sequence\n\n\n", "created_at": "2025-03-01", "closed_at": "2025-04-09", "labels": ["proj-studio"], "State": "closed", "Author": "victordibia"}
{"issue_number": 5772, "issue_title": "Update migration guide regarding teachability and rag agent", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nWith #5308 snd #5227 completed. We need to update migration guide", "created_at": "2025-03-01", "closed_at": "2025-03-14", "labels": ["documentation", "proj-extensions"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 5771, "issue_title": "Update doc executor agent doc to mention the requirements", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nhttps://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.CodeExecutorAgent\nNeed to update to mention it requires the code in markdown blocks", "created_at": "2025-03-01", "closed_at": "2025-03-03", "labels": ["documentation"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 5768, "issue_title": "When i use custom agent in team -> Error Unable to extract tag using discriminator 'type'", "issue_body": "What happened?\nDescribe the bug\nI have nested chat operator , when i use this agent directly there is no error. But when i use as a team member then this error occurs\nError processing publish message for AdvisorNestedTeam/02d28544-5196-4dc3-89f4-3bb28febc819\nTraceback (most recent call last):\n  File \"/Users/iceberg/Desktop/uzair-ai/.venv/lib/python3.13/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 505, in _on_message\n    return await agent.on_message(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n    ...<2 lines>...\n    )\n    ^\n  File \"/Users/iceberg/Desktop/uzair-ai/.venv/lib/python3.13/site-packages/autogen_core/_base_agent.py\", line 113, in on_message\n    return await self.on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/iceberg/Desktop/uzair-ai/.venv/lib/python3.13/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 48, in on_message_impl\n    return await super().on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/iceberg/Desktop/uzair-ai/.venv/lib/python3.13/site-packages/autogen_core/_routed_agent.py\", line 485, in on_message_impl\n    return await h(self, message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/iceberg/Desktop/uzair-ai/.venv/lib/python3.13/site-packages/autogen_core/_routed_agent.py\", line 268, in wrapper\n    return_value = await func(self, message, ctx)  # type: ignore\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/iceberg/Desktop/uzair-ai/.venv/lib/python3.13/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 64, in handle_request\n    GroupChatMessage(message=msg), topic_id=DefaultTopicId(type=self._output_topic_type)\n    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n  File \"/Users/iceberg/Desktop/uzair-ai/.venv/lib/python3.13/site-packages/pydantic/main.py\", line 214, in __init__\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\npydantic_core._pydantic_core.ValidationError: 2 validation errors for GroupChatMessage\nmessage.tagged-union[ToolCallRequestEvent,ToolCallExecutionEvent,MemoryQueryEvent,UserInputRequestedEvent,ModelClientStreamingChunkEvent]\n  Unable to extract tag using discriminator 'type' [type=union_tag_not_found, input_value=TaskResult(messages=[Text...ge')], stop_reason=None), input_type=TaskResult]\n    For further information visit https://errors.pydantic.dev/2.10/v/union_tag_not_found\nmessage.tagged-union[TextMessage,MultiModalMessage,StopMessage,ToolCallSummaryMessage,HandoffMessage]\n  Unable to extract tag using discriminator 'type' [type=union_tag_not_found, input_value=TaskResult(messages=[Text...ge')], stop_reason=None), input_type=TaskResult]\n    For further information visit https://errors.pydantic.dev/2.10/v/union_tag_not_found\nThis is my NestedChat :\nclass AdvisorNestedTeam(Operator):\n   \n    def __init__(self,**kwargs) -> None:\n        super().__init__(\n            name=\"AdvisorNestedTeam\",\n            **kwargs,\n            description=(\n            \"\"\"\n                You are Blog Subject Advisor\n            \"\"\"\n            ),\n        )\n\n    async def on_messages(self, task: str) -> Response :\n       pass\n\n \n    async def on_messages_stream(self, task: str,cancellation_token:CancellationToken) -> AsyncGenerator[AgentEvent | ChatMessage | Response, None]:\n        print(\"AdvisorNestedTeam Running\")\n        try:\n            result_guide = await BlogAdvisorGuideOperator().run(task=f\"{task}\")\n            formatted_result_guide = result_guide.messages[-1].content\n            print(f\"BlogAdvisorGuideOperatorResult: {formatted_result_guide}\")\n            result_scraper = await SearchScraperTeam().run(task=f\"{formatted_result_guide} search these , current_date: {datetime.datetime.now()}\")\n            formatted_result_scraper = result_scraper.messages[-1].content\n            print(f\"SearchScraperTeamResult: {formatted_result_scraper}\")\n            async for chunk in BlogAdvisorOperator().run_stream(task=f\"{formatted_result_scraper}\"):\n                yield chunk\n        except Exception as e:\n            yield TextMessage(content=\"Error while running AdvisorNestedTeam\")\n            return\n\n    async def on_reset(self) -> None:\n        # Reset the inner team.\n        await self._counting_team.reset()\n\n    @property\n    def produced_message_types(self) -> Sequence[type[ChatMessage]]:\n        return (TextMessage,)\nif i use this nestedchat directly like this there is no error :\nasync for chunk in advisor_nested_chat.run_stream(task=prompt):\nbut when i use like this , at the above error occurs :\n        general_chat_operator = Operator(name=\"general_chat\",description=\"if uses's task not related with other operators \n        run this operator\")\n        advisor_team = AdvisorNestedTeam()\n        team = SelectorTeamLeader(max_turns=1, participants=[advisor_team,general_chat_operator])\n        async for chunk in team.run_stream(task=prompt):\n            async for processed_chunk in self.__process_stream(chunk):\n                yield processed_chunk\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\ngpt-4o\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-01", "closed_at": "2025-03-02", "labels": ["needs-triage"], "State": "closed", "Author": "emirhanyagci"}
{"issue_number": 5762, "issue_title": "Error when using Gemini : Unable to submit request because it has an empty text parameter", "issue_body": "What happened?\nI am getting an error when using Open AI compatible API with Gemini model. The error is as follows:\nopenai.BadRequestError: Error code: 400 - [{'error': {'code': 400, 'message': 'Unable to submit request because it has an empty text parameter. Add a value to the parameter and try again. Learn more: https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini', 'status': 'INVALID_ARGUMENT'}}]\nSample script to reproduce the error\nAPI key needs to be updated in the following code for Gemini\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.conditions import TextMentionTermination, HandoffTermination\nfrom autogen_agentchat.agents import AssistantAgent\nimport asyncio\nfrom autogen_agentchat.ui import Console\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(\n        model=\"gemini-2.0-flash\",\n        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n        api_key=<API_KEY>,\n        model_info={\n            \"vision\": True,\n            \"function_calling\": True,\n            \"json_output\": True,\n            \"family\": \"unknown\",\n        },\n    )\n\n    calculator = AssistantAgent(\n        name=\"calculator\",\n        description=\"Calculator agent whose task is to resolve mathematical expressions and give the output.\",\n        model_client=model_client,\n        handoffs=[\"user\"],\n        reflect_on_tool_use=True,\n        system_message=\"You are a calculator agent and your task is to solve mathematical expressions using BODMAS. Transfer to user if expression is not provided or is incorrect. Return `TERMINATE` once the calculation is done.\"\n    )\n\n    handoff_termination = HandoffTermination(target=\"user\")\n    text_termination = TextMentionTermination(\"TERMINATE\")\n    combined_termination = handoff_termination | text_termination\n\n    team = Swarm(\n        [calculator],\n        termination_condition=combined_termination,\n    )\n\n    stream = team.run_stream(task=\"I need the output for the mathematical expression 2*3+6.\")\n    await Console(stream)\n   \nasyncio.run(main())\n\nThe code is working fine with Open AI LLM but issue is occurring when integrating with Gemini. Following is the output obtained through GPT-4o for the same code:\nI need the output for the mathematical expression 2*3+6.\n---------- calculator ----------\nTo solve the expression \\(2 \\times 3 + 6\\) using BODMAS (Brackets, Orders, Division and Multiplication, Addition and Subtraction), we first perform the multiplication, then the addition:\n\n1. Multiply: \\(2 \\times 3 = 6\\)\n2. Add: \\(6 + 6 = 12\\)\n\nSo, the result is \\(12\\).\n\nTERMINATE```\n\n\n### Which packages was the bug in?\n\nPython Extensions (autogen-ext)\n\n### AutoGen library version.\n\nPython 0.4.7\n\n### Other library version.\n\n_No response_\n\n### Model used\n\ngemini-2.0-flash\n\n### Model provider\n\nGoogle Gemini\n\n### Other model provider\n\n_No response_\n\n### Python version\n\n3.12\n\n### .NET version\n\nNone\n\n### Operating system\n\nWindows\n", "created_at": "2025-02-28", "closed_at": "2025-03-31", "labels": [], "State": "closed", "Author": "satvikasura"}
{"issue_number": 5760, "issue_title": "running xlang app host on windows, python grpc exception: NotImplementedError", "issue_body": "What happened?\nDescribe the bug\nlogs from the python xlang agent:\n INFO:autogen_core:Put message in send queue\n2025-02-28T11:47:17\n DEBUG:grpc.aio._call:Client request_iterator raised exception:\n2025-02-28T11:47:17\n Traceback (most recent call last):\n2025-02-28T11:47:17\n   File \"C:\\Users\\rysweet\\source\\repos\\autogen\\python\\.venv\\Lib\\site-packages\\grpc\\aio\\_call.py\", line 450, in _consume_request_iterator\n2025-02-28T11:47:17\n     async for request in request_iterator:\n2025-02-28T11:47:17\n   File \"C:\\Users\\rysweet\\source\\repos\\autogen\\python\\packages\\autogen-ext\\src\\autogen_ext\\runtimes\\grpc\\_worker_runtime.py\", line 89, in __anext__\n2025-02-28T11:47:17\n     return await self._queue.get()\n2025-02-28T11:47:17\n            ^^^^^^^^^^^^^^^^^^^^^^^\n2025-02-28T11:47:17\n   File \"C:\\Users\\rysweet\\AppData\\Roaming\\uv\\python\\cpython-3.12.9-windows-x86_64-none\\Lib\\asyncio\\queues.py\", line 158, in get\n2025-02-28T11:47:17\n     await getter\n2025-02-28T11:47:17\n asyncio.exceptions.CancelledError\n2025-02-28T11:47:17\n \n2025-02-28T11:47:17\n DEBUG:grpc._cython.cygrpc:Failed to receive any message from Core\n2025-02-28T11:47:17\n Traceback (most recent call last):\n2025-02-28T11:47:17\n   File \"C:\\Users\\rysweet\\source\\repos\\autogen\\dotnet\\test\\Microsoft.AutoGen.Integration.Tests.AppHosts\\core_xlang_hello_python_agent\\hello_python_agent.py\", line 75, in <module>\n2025-02-28T11:47:17\n     asyncio.run(main())\n2025-02-28T11:47:17\n   File \"C:\\Users\\rysweet\\AppData\\Roaming\\uv\\python\\cpython-3.12.9-windows-x86_64-none\\Lib\\asyncio\\runners.py\", line 195, in run\n2025-02-28T11:47:17\n     return runner.run(main)\n2025-02-28T11:47:17\n            ^^^^^^^^^^^^^^^^\n2025-02-28T11:47:17\n   File \"C:\\Users\\rysweet\\AppData\\Roaming\\uv\\python\\cpython-3.12.9-windows-x86_64-none\\Lib\\asyncio\\runners.py\", line 118, in run\n2025-02-28T11:47:17\n     return self._loop.run_until_complete(task)\n2025-02-28T11:47:17\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2025-02-28T11:47:17\n   File \"C:\\Users\\rysweet\\AppData\\Roaming\\uv\\python\\cpython-3.12.9-windows-x86_64-none\\Lib\\asyncio\\base_events.py\", line 691, in run_until_complete\n2025-02-28T11:47:17\n     return future.result()\n2025-02-28T11:47:17\n            ^^^^^^^^^^^^^^^\n2025-02-28T11:47:17\n   File \"C:\\Users\\rysweet\\source\\repos\\autogen\\dotnet\\test\\Microsoft.AutoGen.Integration.Tests.AppHosts\\core_xlang_hello_python_agent\\hello_python_agent.py\", line 67, in main\n2025-02-28T11:47:17\n     await runtime.stop_when_signal()\n2025-02-28T11:47:17\n   File \"C:\\Users\\rysweet\\source\\repos\\autogen\\python\\packages\\autogen-ext\\src\\autogen_ext\\runtimes\\grpc\\_worker_runtime.py\", line 341, in stop_when_signal\n2025-02-28T11:47:17\n     loop.add_signal_handler(sig, signal_handler)\n2025-02-28T11:47:17\n   File \"C:\\Users\\rysweet\\AppData\\Roaming\\uv\\python\\cpython-3.12.9-windows-x86_64-none\\Lib\\asyncio\\events.py\", line 582, in add_signal_handler\n2025-02-28T11:47:17\n     raise NotImplementedError\n2025-02-28T11:47:17\n NotImplementedError\n\nTo Reproduce\nnavigate to the XlangAppHost project and run F5\nWhich packages was the bug in?\nPython Core (autogen-core)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\n3.13\n.NET version\n.NET 9\nOperating system\nWindows", "created_at": "2025-02-28", "closed_at": null, "labels": ["x-lang"], "State": "open", "Author": "rysweet"}
{"issue_number": 5759, "issue_title": "running xlang app host on windows, exception in the logs: System.IO.IOException: The client reset the request stream.", "issue_body": "What happened?\nDescribe the bug\n2025-02-28T11:47:17       Error reading message.\n2025-02-28T11:47:17       System.IO.IOException: The client reset the request stream.\n2025-02-28T11:47:17          at System.IO.Pipelines.Pipe.GetReadResult(ReadResult& result)\n2025-02-28T11:47:17          at System.IO.Pipelines.Pipe.GetReadAsyncResult()\n2025-02-28T11:47:17          at System.IO.Pipelines.Pipe.DefaultPipeReader.GetResult(Int16 token)\n2025-02-28T11:47:17          at Microsoft.AspNetCore.Server.Kestrel.Core.Internal.Http2.Http2MessageBody.ReadAsync(CancellationToken cancellationToken)\n2025-02-28T11:47:17          at System.Runtime.CompilerServices.PoolingAsyncValueTaskMethodBuilder`1.StateMachineBox`1.System.Threading.Tasks.Sources.IValueTaskSource<TResult>.GetResult(Int16 token)\n2025-02-28T11:47:17          at Grpc.AspNetCore.Server.Internal.PipeExtensions.ReadStreamMessageAsync[T](PipeReader input, HttpContextServerCallContext serverCallContext, Func`2 deserializer, CancellationToken cancellationToken)\n\nTo Reproduce\nnavigate to XlangApphost project and F5\nWindows 11, VS 2022, dotnet 9\nWhich packages was the bug in?\n.NET Core (Microsoft.AutoGen.Core)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\n.NET 9\nOperating system\nWindows", "created_at": "2025-02-28", "closed_at": null, "labels": ["dotnet"], "State": "open", "Author": "rysweet"}
{"issue_number": 5751, "issue_title": "Test Magentic One in AGS, Add Documentation", "issue_body": "\n Test and verify support for MagneticOne in studio .Part of this might be work to migrate filesurfer and to be declarative #5607  . There might be work to verify the UI parameters or MagenticOne and its agents are well supported beyond just the json file\n Update AGS documentaion with a guide.\n", "created_at": "2025-02-28", "closed_at": null, "labels": ["proj-studio"], "State": "open", "Author": "victordibia"}
{"issue_number": 5750, "issue_title": ".NET Grpc Agents with multiple pods and running in different service", "issue_body": "What happened?\nDescribe the bug\nI'm planning to write two agents. Both the agents will be gRPC agents hosted on Kubernetes. Hence, they will have different gRpc Http/2 address on SSL. I would like to say that the Agent of Type A => resides in address 50001 and the agent of Type B resides in address 70001. How would I modify the Modify and checker sample to accommodate it?\nI have another ask. Can there be way where the Agent runtime can do some load balancing between multiple pods of an agent deployment ?\nExpected behavior\n\nLoad balance multiple instance same agent\nDistributed agents with different address\n\nWhich packages was the bug in?\n.NET Core (Microsoft.AutoGen.Core)\nAutoGen library version.\n.NET dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\n.NET 8 +\nOperating system\nUbuntu", "created_at": "2025-02-28", "closed_at": "2025-02-28", "labels": ["dotnet"], "State": "closed", "Author": "VenkateshSrini"}
{"issue_number": 5748, "issue_title": "feature: DeepSeekR1+Ollama", "issue_body": "What happened?\n/Users/puzzle/PycharmProjects/ai-manage/.venv/bin/python /Users/puzzle/PycharmProjects/ai-manage/AutoGenV04Test/BasicTest/6_RunTeamDeepSeek.py\n---------- user ----------\n\u5199\u4e00\u9996\u5173\u4e8e\u79cb\u5b63\u7684\u77ed\u8bd7\n---------- primary ----------\n\u300a\u79cb\u65e5\u624b\u8bb0\u300b\n\u67ab\u53f6\u5728\u6668\u5149\u4e2d\u8212\u5c55\u9508\u8272\n\u98ce\u7684\u624b\u6307\u8638\u53d6\u9732\u6c34\n\u7ed9\u6bcf\u6839\u8349\u830e\u9540\u4e0a\u94f6\u8d28\u7b7e\u540d\n\u7a3b\u7a57\u5782\u9996\n\u5927\u5730\u6b63\u5728\u79f0\u91cf\u91d1\u5c5e\u7684\u53f9\u606f\n\u5019\u9e1f\u526a\u5f00\u4e91\u5c42\u65f6\n\u5929\u7a7a\u7684\u4f24\u53e3\u6e17\u51fa\u7425\u73c0\n\u6240\u6709\u7b49\u5f85\u90fd\u8737\u6210\u79cd\u7c7d\n\u5728\u5e74\u8f6e\u6df1\u5904\n\u523b\u4e0b\u5c1a\u672a\u6108\u5408\u7684\u7eb9\u8def\n/Users/puzzle/PycharmProjects/ai-manage/.venv/lib/python3.11/site-packages/autogen_ext/models/openai/_openai_client.py:855: UserWarning: Could not find .. field in model response content. No thought was extracted.\nthought, content = parse_r1_content(content)\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython 0.4.7\nOther library version.\nNo response\nModel used\ndeepseek-r1\nModel provider\nDeepSeek (Hosted)\nOther model provider\ntongyi qwen\nPython version\n3.11\n.NET version\nNone\nOperating system\nMacOS", "created_at": "2025-02-28", "closed_at": null, "labels": [], "State": "open", "Author": "y26s4824k264"}
{"issue_number": 5745, "issue_title": "For OTEL, we can follow what we do for LLMCallEvent and emit a `ToolCallEvent` in `BaseTool`", "issue_body": "For OTEL, we can follow what we do for LLMCallEvent and emit a ToolCallEvent in BaseTool:\nSee: \n\n\nautogen/python/packages/autogen-core/src/autogen_core/logging.py\n\n\n        Lines 9 to 10\n      in\n      dd0781a\n\n\n\n\n\n\n class LLMCallEvent: \n\n\n\n def __init__( \n\n\n\n\n\nAnd where it is emitted: \n\n\nautogen/python/packages/autogen-ext/src/autogen_ext/models/openai/_openai_client.py\n\n\n        Lines 559 to 560\n      in\n      dd0781a\n\n\n\n\n\n\n logger.info( \n\n\n\n LLMCallEvent( \n\n\n\n\n\nOriginally posted by @ekzhu in #5741 (comment)", "created_at": "2025-02-27", "closed_at": "2025-03-08", "labels": ["proj-core"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 5743, "issue_title": "Autogen Studio not using azure open ai API", "issue_body": "What happened?\nI am using Autogen studio and I am simply trying to run a single calculator agent using the provided examples. I have setup an Azure Open AI model and double checked the json is utilizing the right syntax for Azure Open AI. However, when I try to run the team in the playground I am not able to engage the agent because I am running into the following error in my powershell:\nError processing publish message for assistant_agent/c682597c-86a3-4646-84dc-dfc6f70205b8\nTraceback (most recent call last):\nFile \"C:\\Users\\dafay\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_core_single_threaded_agent_runtime.py\", line 505, in _on_message\nreturn await agent.on_message(\n^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\dafay\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_core_base_agent.py\", line 113, in on_message\nreturn await self.on_message_impl(message, ctx)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\dafay\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_agentchat\\teams_group_chat_sequential_routed_agent.py\", line 48, in on_message_impl\nreturn await super().on_message_impl(message, ctx)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\dafay\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_core_routed_agent.py\", line 485, in on_message_impl\nreturn await h(self, message, ctx)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\dafay\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_core_routed_agent.py\", line 268, in wrapper\nreturn_value = await func(self, message, ctx)  # type: ignore\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\dafay\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_agentchat\\teams_group_chat_chat_agent_container.py\", line 53, in handle_request\nasync for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\nFile \"C:\\Users\\dafay\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_agentchat\\agents_assistant_agent.py\", line 416, in on_messages_stream\nmodel_result = await self._model_client.create(\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\dafay\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\autogen_ext\\models\\openai_openai_client.py\", line 534, in create\nresult: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n^^^^^^^^^^^^\nFile \"C:\\Users\\dafay\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 1927, in create\nreturn await self._post(\n^^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\dafay\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai_base_client.py\", line 1856, in post\nreturn await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\dafay\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai_base_client.py\", line 1550, in request\nreturn await self._request(\n^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\dafay\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai_base_client.py\", line 1651, in _request\nraise self._make_status_error_from_response(err.response) from None\nopenai.NotFoundError: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}\nIT looks as if the backend code is still referencing OpenAI and not Azure Open AI\nWhich packages was the bug in?\nAutoGen Studio (autogensudio)\nAutoGen library version.\nPython 0.4.1\nOther library version.\nNo response\nModel used\ngpt 4o\nModel provider\nAzure AI Foundary (Azure AI Studio)\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-02-27", "closed_at": "2025-04-11", "labels": ["proj-studio"], "State": "closed", "Author": "dfay88"}
{"issue_number": 5740, "issue_title": "running xlang app host on windows - get multiple exceptions", "issue_body": "What happened?\nDescribe the bug\nA clear and concise description of what the bug is.\nIf it is a question or suggestion, please use Discussions\ninstead.\nTo Reproduce\nselect xlang app host project, F5\nExpected behavior\napp runs and messages show in the console\nMicrosoft.AspNetCore.Routing.EndpointMiddleware: Information: Executing endpoint 'gRPC - /agents.AgentRpc/AddSubscription'\nMicrosoft.AutoGen.Core.Grpc.GrpcAgentRuntime: Error: Error reading from channel.\nSystem.InvalidOperationException: ProtoData is null.\nat Microsoft.AutoGen.Core.Grpc.GrpcAgentRuntime.HandlePublish(CloudEvent evt, CancellationToken cancellationToken) in C:\\Users\\rysweet\\source\\repos\\autogen\\dotnet\\src\\Microsoft.AutoGen\\Core.Grpc\\GrpcAgentRuntime.cs:line 222\nat Microsoft.AutoGen.Core.Grpc.GrpcAgentRuntime.OnMessageAsync(Message message, CancellationToken cancellation) in C:\\Users\\rysweet\\source\\repos\\autogen\\dotnet\\src\\Microsoft.AutoGen\\Core.Grpc\\GrpcAgentRuntime.cs:line 484\nat Microsoft.AutoGen.Core.Grpc.GrpcMessageRouter.RunReadPump() in C:\\Users\\rysweet\\source\\repos\\autogen\\dotnet\\src\\Microsoft.AutoGen\\Core.Grpc\\GrpcMessageRouter.cs:line 145\nat Microsoft.AutoGen.Core.Grpc.GrpcMessageRouter.RunReadPump() in C:\\Users\\rysweet\\source\\repos\\autogen\\dotnet\\src\\Microsoft.AutoGen\\Core.Grpc\\GrpcMessageRouter.cs:line 137\n'HelloAgentTests.exe' (CoreCLR: clrhost): Loaded 'C:\\Program Files\\dotnet\\shared\\Microsoft.NETCore.App\\8.0.10\\Microsoft.Win32.Registry.dll'. Skipped loading symbols. Module is optimized and the debugger option 'Just My Code' is enabled.\nMicrosoft.AspNetCore.Routing.EndpointMiddleware: Information: Executed endpoint 'gRPC - /agents.AgentRpc/AddSubscription'\nMicrosoft.AspNetCore.Hosting.Diagnostics: Information: Request finished HTTP/2 POST http://localhost:50673/agents.AgentRpc/AddSubscription - 200 - application/grpc 26.4631ms\nMicrosoft.AspNetCore.Hosting.Diagnostics: Information: Request starting HTTP/2 POST https://localhost:53071/agents.AgentRpc/OpenChannel - application/grpc -\nGrpc.Net.Client.Internal.GrpcCall: Information: Call failed with gRPC error status. Status code: 'Cancelled', Message: 'gRPC call disposed.'.\nMicrosoft.AspNetCore.Routing.EndpointMiddleware: Information: Executing endpoint 'gRPC - /agents.AgentRpc/OpenChannel'\nMicrosoft.AutoGen.RuntimeGateway.Grpc.GrpcGateway: Information: Received new connection from ipv6:[::1]:60145.\nMicrosoft.AutoGen.RuntimeGateway.Grpc.GrpcGateway: Information: Attaching dangling registrations for 2db07151-38d8-4c07-b8cf-73c6c6d829c8.\nGrpc.AspNetCore.Server.ServerCallHandler: Information: Error reading message.\nSystem.IO.IOException: The client reset the request stream.\nat System.IO.Pipelines.Pipe.GetReadResult(ReadResult& result)\nat System.IO.Pipelines.Pipe.GetReadAsyncResult()\nat System.IO.Pipelines.Pipe.DefaultPipeReader.GetResult(Int16 token)\nat Microsoft.AspNetCore.Server.Kestrel.Core.Internal.Http2.Http2MessageBody.ReadAsync(CancellationToken cancellationToken)\nat System.Runtime.CompilerServices.PoolingAsyncValueTaskMethodBuilder1.StateMachineBox1.System.Threading.Tasks.Sources.IValueTaskSource.GetResult(Int16 token)\nat Grpc.AspNetCore.Server.Internal.PipeExtensions.ReadStreamMessageAsync[T](PipeReader input, HttpContextServerCallContext serverCallContext, Func`2 deserializer, CancellationToken cancellationToken)\nException thrown: 'System.IO.IOException' in System.Private.CoreLib.dll\nGrpc.AspNetCore.Server.ServerCallHandler: Information: Error reading message.\nSystem.IO.IOException: The client reset the request stream.\nat System.IO.Pipelines.Pipe.GetReadResult(ReadResult& result)\nat System.IO.Pipelines.Pipe.GetReadAsyncResult()\nat System.IO.Pipelines.Pipe.DefaultPipeReader.GetResult(Int16 token)\nat Microsoft.AspNetCore.Server.Kestrel.Core.Internal.Http2.Http2MessageBody.ReadAsync(CancellationToken cancellationToken)\nat System.Runtime.CompilerServices.PoolingAsyncValueTaskMethodBuilder1.StateMachineBox1.System.Threading.Tasks.Sources.IValueTaskSource.GetResult(Int16 token)\nat Grpc.AspNetCore.Server.Internal.PipeExtensions.ReadStreamMessageAsync[T](PipeReader input, HttpContextServerCallContext serverCallContext, Func`2 deserializer, CancellationToken cancellationToken)\nException thrown: 'System.IO.IOException' in System.Private.CoreLib.dll\nException thrown: 'System.OperationCanceledException' in System.Private.CoreLib.dll\nException thrown: 'System.IO.IOException' in System.Private.CoreLib.dll\nException thrown: 'System.IO.IOException' in System.Private.CoreLib.dll\nException thrown: 'System.OperationCanceledException' in System.Private.CoreLib.dll\nException thrown: 'System.IO.IOException' in System.Private.CoreLib.dll\nException thrown: 'System.OperationCanceledException' in System.Private.CoreLib.dll\nException thrown: 'System.IO.IOException' in System.Private.CoreLib.dll\nException thrown: 'System.OperationCanceledException' in System.Private.CoreLib.dll\nException thrown: 'System.IO.IOException' in System.Private.CoreLib.dll\nMicrosoft.AspNetCore.Routing.EndpointMiddleware: Information: Executed endpoint 'gRPC - /agents.AgentRpc/OpenChannel'\nMicrosoft.AspNetCore.Hosting.Diagnostics: Information: Request finished HTTP/2 POST https://localhost:53071/agents.AgentRpc/OpenChannel - 200 - application/grpc 1638.5383ms\nException thrown: 'System.IO.IOException' in System.Private.CoreLib.dll\nMicrosoft.AspNetCore.Routing.EndpointMiddleware: Information: Executed endpoint 'gRPC - /agents.AgentRpc/OpenChannel'\nMicrosoft.AspNetCore.Hosting.Diagnostics: Information: Request finished HTTP/2 POST http://localhost:50673/agents.AgentRpc/OpenChannel - 200 - application/grpc 630.5921ms\nThe thread '.NET TP Wait' (11376) has exited with code 0 (0x0).\nWindows 11 DevBox in Azure, VS2022 Pre, .NET 9.0.200\nWhich packages was the bug in?\n.NET Core (Microsoft.AutoGen.Core)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\n.NET 9\nOperating system\nWindows", "created_at": "2025-02-27", "closed_at": null, "labels": ["dotnet"], "State": "open", "Author": "rysweet"}
{"issue_number": 5736, "issue_title": "Bug: Function Arguments as Pydantic Models or Dataclasses Fail in AutoGen Tool Calls", "issue_body": "What happened?\nDescribe the bug\nWhen a tool function\u2019s parameter is annotated with a Pydantic model or a dataclass, AutoGen is expected to convert the incoming JSON (i.e. a raw dictionary) into an instance of that type before calling the function. However, the conversion is not taking place, and the function receives a raw dict instead. This leads to errors such as:\n[FunctionExecutionResult(content=\"Error: 'dict' object has no attribute 'x'\", call_id='', is_error=True)]\nTo Reproduce\nFor a complete reproducible example, please see the failing Colab notebook:\nGoogle Colab Notebook - Failing Case\n\nCreate a tool function using a Pydantic model as follows:\n\nfrom pydantic import BaseModel\n\nclass Add(BaseModel):\n    x: int\n    y: int\n\ndef add(input: Add) -> int:\n    return input.x + input.y\n\nRegister the tool function in an Agent and run it\n\nfrom google.colab import userdata\n\ngemini_api_key =\"\"\n\n# Define a model client. You can use other model client that implements\n# the `ChatCompletionClient` interface.\nmodel_client: OpenAIChatCompletionClient = OpenAIChatCompletionClient(\n    model=\"gemini-2.0-flash\",\n    api_key=gemini_api_key,\n)\n\n\nagent: AssistantAgent = AssistantAgent(\n    name=\"addition_agent\",\n    model_client=model_client,\n    tools=[add],\n    system_message=\"You are a helpful addition assistant.\",\n    reflect_on_tool_use=True,\n    model_client_stream=True,  # Enable streaming tokens from the model client.\n)\n\n# Create the team setting a maximum number of turns to 1.\nteam = RoundRobinGroupChat([agent], max_turns=1)\n\nasync def conversation():\n      # Run the conversation and stream to the console.\n      stream = team.run_stream(task=\"what is 2 and 3\")\n\n      await Console(stream)\n\ndef run_main():\n    asyncio.run(conversation())\nExpected behavior\nAutoGen should automatically convert the raw JSON into an instance of the specified Pydantic model or dataclass (e.g. using Add.model_validate(...) or via the dataclass constructor) so that the tool function receives a fully instantiated and validated object.\nAdditional context\nThis bug affects tool function calls in AutoGen\u2019s core where arguments are processed in the run() method. The proposed change is to enhance the conversion logic so that for any function parameter annotated as a Pydantic model or a dataclass, the input dictionary is converted to an instance before invocation. A working example using a similar approach in the LangGraph framework can be seen here:\nColab Notebook - WorkCase with LangGraph Framework\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0), Python Core (autogen-core)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\ngemini-2.0-flash\nModel provider\nGoogle Gemini\nOther model provider\nNo response\nPython version\n3.12\n.NET version\nNone\nOperating system\nMacOS", "created_at": "2025-02-27", "closed_at": "2025-03-04", "labels": ["tool-usage", "proj-core"], "State": "closed", "Author": "mjunaidca"}
{"issue_number": 5734, "issue_title": "need to document that full dotnet build requires python and uv because of xlang.", "issue_body": "What happened?\nthe full dotnet build requires uv and running the tests requires python because of the xlang tests.\nwe need to doc this somewhere. Ideally if we can make the build ensure uv and python thats better.\nWhich packages was the bug in?\n.NET Core (Microsoft.AutoGen.Core)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-02-27", "closed_at": null, "labels": ["dotnet"], "State": "open", "Author": "rysweet"}
{"issue_number": 5732, "issue_title": "`TextMessageTermination` Condition", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nA termination condition that stops the team when a TextMessage is emitted from a specific source, or any source.\nFor example, to terminate when a specific agent produced a TextMessage:\nTextMessageTermination(source=\"assistant\")\nTo terminate when any agent produced a TextMessage\nTextMessageTermination()\n\nIn Swarm group chat team, it is useful to terminate the team when an agent no longer hands off or producing tool call results. #5611 (comment)\nIn a single agent scenario, often useful to repeatedly call the same agent in a RoundRobinGroupChat until it stops producing tool call results. #5621 (comment)\n\nFor example\nassistant = AssistantAgent(...)\nteam = RoundRobinGroupChat([assistant], termination_condition=TextMessageTermination())\nresult = team.run(...)", "created_at": "2025-02-26", "closed_at": "2025-03-03", "labels": ["proj-agentchat"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 5731, "issue_title": "AssistantAgent should raise warning when reflection_on_tool_use=True and model family is Claude", "issue_body": "In addition, the warning message should explain how to perform reflection on tool usage with anthropic model client.\nIt should be based on the text termination condition: #5732, and point to the relevant part of the documentation.\n\n@lspinheiro @victordibia I think it is possible in the future Anthropic will support tool_choice=None just like OpenAI, or support not requiring the tools field when there is tool usage in the message history.\nBefore that happens, in AssistantAgent we can use model_info of the model client to determine if the model belongs to the Claude family. In that case, we raise an exception when reflect_on_tool_use=True, in the constructor.\nSo basically an AssistantAgent won't support reflection on tool usage when the model is Claude.\n\nOriginally posted by @ekzhu in #5675", "created_at": "2025-02-26", "closed_at": "2025-03-05", "labels": ["proj-agentchat"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 5730, "issue_title": "LLMStreamStartEvent and LLMStreamEndEvent for autogen_core.logging", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nWe need to create new events:\n\nLLMStreamStartEvent\nLLMStreamEndEvent\n\nin autogen_core.logging.\nThen, emit these events from model clients in their create_stream methods:\n\nOpenAIChatCompletionClient and Azure...\nAzureAIChatCompletionClient\nOllamaChatCompletionClient\nAnthropicChatCompletionClient\nSKChatCompletionAdapter\n", "created_at": "2025-02-26", "closed_at": "2025-03-11", "labels": ["logging", "proj-core", "proj-extensions"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 5728, "issue_title": "Ability to Test Model Components in AGS", "issue_body": "We have a few valuable debugging capabilities  in AGS today\n\nAbilitity to test agent teams in teambuilder\nAutomatic validation of entire team\n\nHowever, validation only gives us static checks for agent behavior (e.g a spec is valid .. valid provider, etc), it tells us nothing about actual functional behaviour e.g., an agent  might have the right provider, but have missing required fields eg a model client, or a model client look good but its baseulr incorrect etc.\nThis could be useful in the Gallery where developers can create components , test them and then reliably reuse them.\nThis PR is meant to add the ability to run a component as a form of live test.\nCurrently will focus on the ability to test model clients. Others can be tested relatively easily (e.g., agents) by using the test teams already available.\nHow\n\nExtend /validate endpoint to validate/test/component= component\nFor each coponent determine how we will test\n\nagent - task.run(\"what is x\")\nmodel - model.create (\"hi there\")\ntools - tool.run ...\n\n\n", "created_at": "2025-02-26", "closed_at": "2025-03-17", "labels": ["proj-studio"], "State": "closed", "Author": "victordibia"}
{"issue_number": 5717, "issue_title": "How can I access the information inside a variable without printing it to the console?", "issue_body": "What is the doc issue?\nDescribe the issue\nA clear and concise description of what the issue is. What is missing or incorrect?\nWhat do you want to see in the doc?\nScreenshots\nIf applicable, add screenshots to help explain your problem.\nAdditional context\nAdd any other context about the problem here.\nLink to the doc page, if applicable\nhttps://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/quickstart.html", "created_at": "2025-02-26", "closed_at": "2025-02-26", "labels": ["documentation", "needs-triage"], "State": "closed", "Author": "auserwn"}
{"issue_number": 5715, "issue_title": "Studio devcontainer fails to run `yarn build`", "issue_body": "What happened?\nDescribe the bug\nI tried installing studio from source in the dev container. The process failed at yarn build with the following error:\nError: `siteUrl` does not exist on `siteMetadata` in the data returned from the query\nTo Reproduce\nSteps to reproduce the behavior. Please include code and outputs such as stacktrace.\n# 1. clone autogen @main\n# 2. cd into studio package\ncd python/packages/autogen-studio\n# 3. in vscode, click on `Open in Container`\n# 4. wait for the new container to build\ncd frontend/\nyarn build\nScreenshots\n\nWhich packages was the bug in?\nAutoGen Studio (autogensudio)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-02-26", "closed_at": null, "labels": ["proj-studio"], "State": "open", "Author": "gagb"}
{"issue_number": 5713, "issue_title": "Use OmniParser to create a UI Surfer Agent", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\n\nUI tools for controlling with keyboards and mouse, as well as capturing screenshots. Considering a MCP server for possible implementation.\nUse OmniParser to convert screenshots to structured messages to store in model context.\n", "created_at": "2025-02-25", "closed_at": null, "labels": ["proj-extensions"], "State": "open", "Author": "ekzhu"}
{"issue_number": 5710, "issue_title": "uv.lock constantly breaks", "issue_body": "What happened?\nDescribe the bug\nthe uv.lock file is perpetually broken on commit\nTo Reproduce\nUncertain exactly what causes this issue. If I call uv sync on wsl, there's a pretty good chance that the file will be broken\nExpected behavior\nWe need to either prevent this issue from happening, or clearly document how to fix/avoid this issue\nWhich packages was the bug in?\nPython Core (autogen-core)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-02-25", "closed_at": "2025-02-25", "labels": ["needs-triage"], "State": "closed", "Author": "peterychang"}
{"issue_number": 5708, "issue_title": "Support Anthropic Client in AutoGen v0.4 and Studio", "issue_body": "Support Anthropic Client in AutoGen v0.4\nExpected behaviour below\nfrom dotenv import load_dotenv\nimport os \n\nload_dotenv()\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.models.anthropic import AnthropicChatCompletionClient \nmodel_client =   AnthropicChatCompletionClient(\n        model=\"claude-3-7-sonnet-20250219\" \n    )\n\nasync def get_weather(city: str) -> str:\n    \"\"\"Get the weather for a given city.\"\"\"\n    return f\"The weather in {city} is 73 degrees and Sunny.\"\n\n \nagent = AssistantAgent(\n    name=\"weather_agent\",\n    model_client=model_client,\n    tools=[get_weather],\n    system_message=\"You are a helpful assistant.\", \n    # model_client_stream=True,   \n)\n\n# Run the agent and stream the messages to the console.\nasync def main() -> None:\n    await Console(agent.run_stream(task=\"What is the weather in New York?\"))\nawait main()", "created_at": "2025-02-25", "closed_at": "2025-02-26", "labels": ["proj-core", "proj-agentchat"], "State": "closed", "Author": "victordibia"}
{"issue_number": 5706, "issue_title": "Separate documentation on nested chats", "issue_body": "What is the doc issue?\nWe should provide guidance beyond the migration document about how to handle nested chats.\nhttps://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/migration-guide.html#nested-chat\nLink to the doc page, if applicable\nNew doc", "created_at": "2025-02-25", "closed_at": null, "labels": ["documentation"], "State": "open", "Author": "jackgerrits"}
{"issue_number": 5703, "issue_title": "An agent using multi tool together", "issue_body": "What is the doc issue?\nDescribe the issue\nIs agents has a multi tool calling skill ,.I couldn't find any example fir this situation for example i have an agent that have googlesearch tool and newspaperk4k tool the agent first should googlesearch tool then run newspaper i tried this after google search ending procces.\nLink to the doc page, if applicable\n.", "created_at": "2025-02-25", "closed_at": "2025-02-25", "labels": ["documentation", "needs-triage"], "State": "closed", "Author": "emirhanyagci"}
{"issue_number": 5701, "issue_title": "How can i desing teams in teams", "issue_body": "What is the doc issue?\nDescribe the issue\nI want to use team as a participants of a team is that possible ?\nLink to the doc page, if applicable\n.", "created_at": "2025-02-25", "closed_at": "2025-02-25", "labels": ["documentation", "needs-triage"], "State": "closed", "Author": "emirhanyagci"}
{"issue_number": 5699, "issue_title": "agent function_call :openai.BadRequestError: Error code: 400 - {'error': {'message': \"Invalid value for 'content': expected a string, got null. (req", "issue_body": "What happened?\nDescribe the bug\nA clear and concise description of what the bug is.\nIf it is a question or suggestion, please use Discussions\ninstead.\nTo Reproduce\nSteps to reproduce the behavior. Please include code and outputs such as stacktrace.\n\nIf your input is just \"I tried X, and it didn't work\" or\n\"X is not working\", your issue will be ignored.\nIf your input is not well formatted, it will hurt readability and\nmay be ignored as well.\n\nExpected behavior\nA clear and concise description of what you expected to happen.\nScreenshots\nIf applicable, add screenshots to help explain your problem.\nAdditional context\nAdd any other context about the problem here.\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-02-25", "closed_at": null, "labels": ["0.2", "awaiting-op-response"], "State": "open", "Author": "auserwn"}
{"issue_number": 5697, "issue_title": "Incorrect Trace structure in autogen-core Otel support", "issue_body": "What happened?\nDescribe the bug\nThe tree structure of the span is not captured correctly in the Otel support of autogen-core.\nAlso, the span does not have useful attributes.\nTo Reproduce\nSetup a JAEGER collector\ndocker run --rm --name jaeger \\\n  -p 16686:16686 \\\n  -p 4317:4317 \\\n  -p 4318:4318 \\\n  -p 5778:5778 \\\n  -p 9411:9411 \\\n  -v /path/to/local/config.yaml:/jaeger/config.yaml \\\n  jaegertracing/jaeger:2.3.0 \\\n  --config /jaeger/config.yaml\nRun the following simple agent interaction with Otel exporter.\nfrom dataclasses import dataclass\nfrom typing import Callable\n\nfrom autogen_core import DefaultTopicId, MessageContext, RoutedAgent, default_subscription, message_handler\n\n\n@dataclass\nclass Message:\n    content: int\n\n\n@default_subscription\nclass Modifier(RoutedAgent):\n    def __init__(self, modify_val: Callable[[int], int]) -> None:\n        super().__init__(\"A modifier agent.\")\n        self._modify_val = modify_val\n\n    @message_handler\n    async def handle_message(self, message: Message, ctx: MessageContext) -> None:\n        val = self._modify_val(message.content)\n        print(f\"{'-'*80}\\nModifier:\\nModified {message.content} to {val}\")\n        await self.publish_message(Message(content=val), DefaultTopicId())  # type: ignore\n\n\n@default_subscription\nclass Checker(RoutedAgent):\n    def __init__(self, run_until: Callable[[int], bool]) -> None:\n        super().__init__(\"A checker agent.\")\n        self._run_until = run_until\n\n    @message_handler\n    async def handle_message(self, message: Message, ctx: MessageContext) -> None:\n        if not self._run_until(message.content):\n            print(f\"{'-'*80}\\nChecker:\\n{message.content} passed the check, continue.\")\n            await self.publish_message(Message(content=message.content), DefaultTopicId())\n        else:\n            print(f\"{'-'*80}\\nChecker:\\n{message.content} failed the check, stopping.\")\n\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\nfrom autogen_core import AgentId, SingleThreadedAgentRuntime\n\ndef configure_jaeger_tracing() -> trace.TracerProvider:\n    jaeger_exporter = OTLPSpanExporter(endpoint=\"http://localhost:4317\", insecure=True)\n    tracer_provider = TracerProvider(\n        resource=Resource({\"service.name\": \"autogen-test\"})\n    )\n    span_processor = BatchSpanProcessor(jaeger_exporter)\n    tracer_provider.add_span_processor(span_processor)\n\n    return tracer_provider\n    \ntracer_provider = configure_jaeger_tracing()\ntracer = tracer_provider.get_tracer(__name__)\nruntime = SingleThreadedAgentRuntime(tracer_provider=tracer_provider)\n\nwith tracer.start_as_current_span(\"runtime\"):\n    await Modifier.register(\n        runtime,\n        \"modifier\",\n        # Modify the value by subtracting 1\n        lambda: Modifier(modify_val=lambda x: x - 1),\n    )\n    \n    await Checker.register(\n        runtime,\n        \"checker\",\n        # Run until the value is less than or equal to 1\n        lambda: Checker(run_until=lambda x: x <= 1),\n    )\n    \n    # Start the runtime and send a direct message to the checker.\n    runtime.start()\n    await runtime.send_message(Message(3), AgentId(\"checker\", \"default\"))\n    await runtime.stop_when_idle()\nExpected behavior\nWe expect a trace with the structure like the one below:\n- trace root \n        - cheker 1\n        - modifier 1\n        - cheker 2\n        - modifier 2\n        - cheker 3\n\nHowever, the generated trace has a structure like this:\n- trace root \n        - cheker 1\n                 - modifier 1\n                        - cheker 2\n                                - modifier 2\n                                        - cheker 3\n\nMoreover, most spans only have messaging.destination and messaging.operation as their attributes, which is not enough for debugging and improving the system. At least we would like to see the input and output of each message handling.\nScreenshots\n\nAdditional context\nAlso, it would be great if your team could implement the Otel support on the agent chat library.\nWhich packages was the bug in?\nPython Core (autogen-core)\nAutoGen library version.\nPython 0.4.5\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\n3.10\n.NET version\nNone\nOperating system\nMacOS", "created_at": "2025-02-25", "closed_at": "2025-03-09", "labels": ["proj-core"], "State": "closed", "Author": "TomeHirata"}
{"issue_number": 5693, "issue_title": "bring \"agents\" package up to match python base agents", "issue_body": "What feature would you like to be added?\nreview python base agents and make sure we have equivalent\nWhy is this needed?\nalignment", "created_at": "2025-02-24", "closed_at": null, "labels": ["dotnet"], "State": "open", "Author": "rysweet"}
{"issue_number": 5692, "issue_title": "update dev team sample to work with latest code...", "issue_body": "What feature would you like to be added?\nbring dev team dotnet sample up to date\nWhy is this needed?\nhygiene", "created_at": "2025-02-24", "closed_at": null, "labels": ["dotnet", "example-app"], "State": "open", "Author": "rysweet"}
{"issue_number": 5688, "issue_title": "\"Object of type MemoryMimeType is not JSON serializable\" for TeamChat with Agent having memory", "issue_body": "What happened?\nDescribe the bug\nIn a RoundRobinGroupChat with Agent(s) with memory, the team save_state() is not Json serializable to save as Json file. Below is the error.\n\"Object of type MemoryMimeType is not JSON serializable\".\nTo Reproduce\nCreate a RoundRobinGroupChat with an Agent having memory (not None), Execute it and save the team state in persistent json file.\nExpected behavior\nShould have been saved into the JSON file as Agents or RoundRobinGroupChat  without memory.\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0), Python Core (autogen-core)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-02-24", "closed_at": "2025-02-26", "labels": [], "State": "closed", "Author": "amd-srijaroy"}
{"issue_number": 5685, "issue_title": "Add Default Model Client in AGS", "issue_body": "Add Default Model Client in AGS that can be used for several house keeping issues e.g,\n\nAutomatically summarise the session title in playground #3515\nAutomatically generate components (future capability)\n\nHow\n\nAdd as feature to app settings in DB\nAllow users view/modify default model in settings\n", "created_at": "2025-02-24", "closed_at": "2025-03-01", "labels": ["proj-studio"], "State": "closed", "Author": "victordibia"}
{"issue_number": 5680, "issue_title": "pc in offline, view json state is loading", "issue_body": "What happened?\nDescribe the bug\nA clear and concise description of what the bug is.\nIf it is a question or suggestion, please use Discussions\ninstead.\nTo Reproduce\nSteps to reproduce the behavior. Please include code and outputs such as stacktrace.\n\nIf your input is just \"I tried X, and it didn't work\" or\n\"X is not working\", your issue will be ignored.\nIf your input is not well formatted, it will hurt readability and\nmay be ignored as well.\n\nExpected behavior\nA clear and concise description of what you expected to happen.\nScreenshots\nIf applicable, add screenshots to help explain your problem.\nAdditional context\nAdd any other context about the problem here.\nWhich packages was the bug in?\nAutoGen Studio (autogensudio)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-02-24", "closed_at": "2025-02-24", "labels": ["needs-triage"], "State": "closed", "Author": "SocratesMing"}
{"issue_number": 6377, "issue_title": "Tool id string too long", "issue_body": "What happened?\nDescribe the bug\nThe tool id is too long and causes and context error. It is length 41 but max length 40 causing openai.BadRequestError: Error code: 400\nStack Trace\n`ERROR:autogen_core:Error processing publish message for python_analyst_5694c56d-6c52-4f8b-8da4-7e5b331e2210/5694c56d-6c52-4f8b-8da4-7e5b331e2210\nTraceback (most recent call last):\nFile \"/Users/rampotham/miniforge3/envs/sitewiz/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 533, in _on_message\nreturn await agent.on_message(\n^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/rampotham/miniforge3/envs/sitewiz/lib/python3.12/site-packages/autogen_core/_base_agent.py\", line 113, in on_message\nreturn await self.on_message_impl(message, ctx)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/rampotham/miniforge3/envs/sitewiz/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 67, in on_message_impl\nreturn await super().on_message_impl(message, ctx)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/rampotham/miniforge3/envs/sitewiz/lib/python3.12/site-packages/autogen_core/_routed_agent.py\", line 485, in on_message_impl\nreturn await h(self, message, ctx)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/rampotham/miniforge3/envs/sitewiz/lib/python3.12/site-packages/autogen_core/_routed_agent.py\", line 268, in wrapper\nreturn_value = await func(self, message, ctx)  # type: ignore\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/rampotham/miniforge3/envs/sitewiz/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 79, in handle_request\nasync for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\nFile \"/Users/rampotham/miniforge3/envs/sitewiz/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 793, in on_messages_stream\nasync for inference_output in self._call_llm(\nFile \"/Users/rampotham/miniforge3/envs/sitewiz/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 920, in _call_llm\nmodel_result = await model_client.create(\n^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/rampotham/miniforge3/envs/sitewiz/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py\", line 622, in create\nresult: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n^^^^^^^^^^^^\nFile \"/Users/rampotham/miniforge3/envs/sitewiz/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2000, in create\nreturn await self._post(\n^^^^^^^^^^^^^^^^^\nFile \"/Users/rampotham/miniforge3/envs/sitewiz/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\nreturn await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/rampotham/miniforge3/envs/sitewiz/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\nreturn await self._request(\n^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/rampotham/miniforge3/envs/sitewiz/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\nraise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: OpenAIException - Invalid 'messages[3].tool_calls[0].id': string too long. Expected a string with maximum length 40, but got a string with length 41 instead.\\nmodel=reasoning. context_window_fallbacks=None. fallbacks=None.\\n\\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=reasoning\\nAvailable Model Group Fallbacks=None\", 'type': None, 'param': None, 'code': '400'}}\nError in analyze_problem: BadRequestError: Error code: 400 - {'error': {'message': \"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: OpenAIException - Invalid 'messages[3].tool_calls[0].id': string too long. Expected a string with maximum length 40, but got a string with length 41 instead.\\nmodel=reasoning. context_window_fallbacks=None. fallbacks=None.\\n\\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=reasoning\\nAvailable Model Group Fallbacks=None\", 'type': None, 'param': None, 'code': '400'}}\nTraceback:\nTraceback (most recent call last):\nFile \"/Users/rampotham/miniforge3/envs/sitewiz/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 79, in handle_request\nasync for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\nFile \"/Users/rampotham/miniforge3/envs/sitewiz/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 793, in on_messages_stream\nasync for inference_output in self._call_llm(\nFile \"/Users/rampotham/miniforge3/envs/sitewiz/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 920, in _call_llm\nmodel_result = await model_client.create(\n^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/rampotham/miniforge3/envs/sitewiz/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py\", line 622, in create\nresult: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n^^^^^^^^^^^^\nFile \"/Users/rampotham/miniforge3/envs/sitewiz/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2000, in create\nreturn await self._post(\n^^^^^^^^^^^^^^^^^\nFile \"/Users/rampotham/miniforge3/envs/sitewiz/lib/python3.12/site-packages/openai/_base_client.py\", line 1767, in post\nreturn await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/rampotham/miniforge3/envs/sitewiz/lib/python3.12/site-packages/openai/_base_client.py\", line 1461, in request\nreturn await self._request(\n^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/rampotham/miniforge3/envs/sitewiz/lib/python3.12/site-packages/openai/_base_client.py\", line 1562, in _request\nraise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: OpenAIException - Invalid 'messages[3].tool_calls[0].id': string too long. Expected a string with maximum length 40, but got a string with length 41 instead.\\nmodel=reasoning. context_window_fallbacks=None. fallbacks=None.\\n\\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=reasoning\\nAvailable Model Group Fallbacks=None\", 'type': None, 'param': None, 'code': '400'}}\nTraceback (most recent call last):\nFile \"/Users/rampotham/Documents/GitHub/sitewiz/backend/agents/data_analyst_group/src/group_chat.py\", line 145, in analyze_problem\ntask_result, summary, evaluation_record, state_manager = await run_group_chat(options[\"type\"], chat, task)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/rampotham/Documents/GitHub/sitewiz/backend/agents/data_analyst_group/src/group_chat.py\", line 100, in run_group_chat\ntask_result = await state_manager.process_stream(stream, chat)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/rampotham/Documents/GitHub/sitewiz/backend/agents/data_analyst_group/promptOptimization/StateManager.py\", line 119, in process_stream\nasync for message in stream:\nFile \"/Users/rampotham/miniforge3/envs/sitewiz/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py\", line 503, in run_stream\nraise RuntimeError(str(message.error))\nRuntimeError: BadRequestError: Error code: 400 - {'error': {'message': \"litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: OpenAIException - Invalid 'messages[3].tool_calls[0].id': string too long. Expected a string with maximum length 40, but got a string with length 41 instead.\\nmodel=reasoning. context_window_fallbacks=None. fallbacks=None.\\n\\nSet 'context_window_fallback' - https://docs.litellm.ai/docs/routing#fallbacks. Received Model Group=reasoning\\nAvailable Model Group Fallbacks=None\", 'type': None, 'param': None, 'code': '400'}}`\nWhich packages was the bug in?\nPython Core (autogen-core), Python Extensions (autogen-ext)\nAutoGen library version.\nPython 0.5.4\nOther library version.\nNo response\nModel used\nvertex_ai/gemini-2.5-flash-preview-04-17 (through litellm proxy)\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-04-23", "closed_at": null, "labels": ["needs-triage"], "State": "open", "Author": "rapturt9"}
{"issue_number": 6376, "issue_title": "autogen-ext test is too slow...!", "issue_body": "What happened?\nDescribe the bug\nCheck each tests at autogen-ext\nautogen-ext/cache_store : 0.15s\nautogen-ext/code_executors : 203.89s\nautogen-ext/memory : 12s\nautogen-ext/models : 37s\nautogen-ext/tools: 14s\nautogen_ext/*.py : 146.02s\nI could find out slow tests\npackages/autogen-ext/tests/code_executors/test_docker_commandline_code_executor.py : 161.66s\npackages/autogen-ext/tests/test_openai_assistant_agent.py : 92.66s\n=============================================================== slowest more than 1sec durations ================================================================\n21.02s call     tests/code_executors/test_docker_commandline_code_executor.py::test_delete_tmp_files\n16.01s call     tests/test_openai_assistant_agent.py::test_file_retrieval[openai]\n13.08s call     tests/test_openai_assistant_agent.py::test_on_reset_behavior[openai]\n11.42s call     tests/code_executors/test_docker_commandline_code_executor.py::test_commandline_code_executor_timeout[docker]\n10.75s call     tests/code_executors/test_docker_commandline_code_executor.py::test_deprecated_warning\n10.69s call     tests/code_executors/test_docker_commandline_code_executor.py::test_docker_commandline_code_executor_serialization\n10.50s call     tests/code_executors/test_docker_commandline_code_executor.py::test_docker_commandline_code_executor_extra_args\n10.49s call     tests/code_executors/test_docker_commandline_code_executor.py::test_error_wrong_path\n10.38s call     tests/code_executors/test_docker_commandline_code_executor.py::test_docker_commandline_code_executor_start_stop\n10.34s call     tests/code_executors/test_docker_commandline_code_executor.py::test_docker_commandline_code_executor_start_stop_context_manager\n10.34s call     tests/code_executors/test_docker_commandline_code_executor.py::test_directory_creation_cleanup\n10.25s call     tests/code_executors/test_docker_jupyter_code_executor.py::test_canncellation[docker]\n10.18s teardown tests/code_executors/test_docker_commandline_code_executor.py::test_commandline_code_executor_cancellation[docker]\n10.12s teardown tests/code_executors/test_docker_commandline_code_executor.py::test_commandline_code_executor_timeout[docker]\n10.12s teardown tests/code_executors/test_docker_commandline_code_executor.py::test_valid_relative_path[docker]\n10.10s teardown tests/code_executors/test_docker_commandline_code_executor.py::test_execute_code[docker]\n10.10s teardown tests/code_executors/test_docker_commandline_code_executor.py::test_invalid_relative_path[docker]\n10.03s call     tests/test_worker_runtime.py::test_register_receives_publish_cascade_single_worker\n7.51s call     tests/test_websurfer_agent.py::test_run_websurfer\n6.44s call     tests/test_openai_assistant_agent.py::test_code_interpreter[openai]\n6.20s call     tests/models/test_llama_cpp_model_client.py::test_llama_cpp_integration_non_streaming_structured_output\n5.87s call     tests/models/test_llama_cpp_model_client.py::test_llama_cpp_integration_non_streaming\n3.96s call     tests/models/test_openai_model_client.py::test_model_client_with_function_calling[gpt-4.1-nano]\n3.71s call     tests/models/test_openai_model_client.py::test_model_client_basic_completion[gpt-4.1-nano]\n3.10s call     tests/memory/test_chroma_memory.py::test_initialization\n2.87s call     tests/models/test_openai_model_client.py::test_structured_output_with_streaming_tool_calls\n2.81s call     tests/code_executors/test_docker_jupyter_code_executor.py::test_timeout[docker]\n2.75s call     tests/models/test_openai_model_client.py::test_structured_output_with_streaming\n2.72s call     tests/code_executors/test_jupyter_code_executor.py::test_commandline_code_executor_timeout\n2.72s call     tests/code_executors/test_docker_jupyter_code_executor.py::test_execute_code_with_image_output\n2.39s call     tests/memory/test_chroma_memory.py::test_content_types\n2.01s call     tests/code_executors/test_docker_commandline_code_executor.py::test_commandline_code_executor_cancellation[docker]\n1.95s call     tests/tools/test_mcp_tools.py::test_mcp_server_fetch\n1.92s setup    tests/code_executors/test_docker_jupyter_code_executor.py::test_execute_code[docker]\n1.87s call     tests/code_executors/test_docker_jupyter_code_executor.py::test_execute_code_and_persist_variable[docker]\n1.82s call     tests/memory/test_chroma_memory.py::test_strict_matching\n1.72s call     tests/code_executors/test_jupyter_code_executor.py::test_commandline_code_executor_cancellation\n1.71s call     tests/test_playwright_controller.py::test_playwright_controller_click_id\n1.68s call     tests/models/test_openai_model_client.py::test_openai_structured_output_with_streaming_tool_calls[gpt-4.1-nano]\n1.64s call     tests/models/test_openai_model_client.py::test_openai_structured_output_with_tool_calls[gpt-4.1-nano]\n1.39s call     tests/memory/test_chroma_memory.py::test_basic_workflow\n1.34s call     tests/code_executors/test_jupyter_code_executor.py::test_jupyter_code_executor_serialization\n1.25s call     tests/memory/test_chroma_memory.py::test_model_context_update\n1.22s call     tests/memory/test_chroma_memory.py::test_metadata_handling\n1.21s call     tests/code_executors/test_jupyter_code_executor.py::test_execute_code_after_restart\n1.13s call     tests/code_executors/test_docker_jupyter_code_executor.py::test_start_stop\n1.09s call     tests/tools/test_mcp_tools.py::test_mcp_server_filesystem\n1.09s call     tests/models/test_openai_model_client.py::test_openai_structured_output[gpt-4.1-nano]\n1.04s call     tests/models/test_openai_model_client.py::test_openai_structured_output_with_streaming[gpt-4.1-nano]\n1.03s call     tests/models/test_openai_model_client.py::test_openai_structured_output_using_response_format[gpt-4.1-nano]\n1.01s call     tests/code_executors/test_commandline_code_executor.py::test_commandline_code_executor_timeout[local]\n1.01s call     tests/code_executors/test_commandline_code_executor.py::test_commandline_code_executor_cancellation\n1.00s setup    tests/code_executors/test_docker_jupyter_code_executor.py::test_canncellation[docker]\n1.00s setup    tests/code_executors/test_docker_jupyter_code_executor.py::test_execute_code_and_persist_variable[docker]\n\nTo Reproduce\npoe test\npytest python/packages/autogen-ext/tests\nExpected behavior\nMore fast...!\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-04-23", "closed_at": null, "labels": ["help wanted", "proj-extensions"], "State": "open", "Author": "SongChiYoung"}
{"issue_number": 6372, "issue_title": "Error in sse_reader for MCP", "issue_body": "What happened?\nDescribe the bug\nError in sse_reader: Expected response header Content-Type to contain 'text/event-stream', got 'text/html'\nTo Reproduce\nfrom autogen_ext.tools.mcp import SseMcpToolAdapter, SseServerParams, mcp_server_tools\nfrom autogen_agentchat.agents import AssistantAgent\n\n\n# Create server params for the remote MCP service\nserver_params = SseServerParams(\n    url=\"http://127.0.0.1:6274/\",\n    headers={\"Authorization\": \"Bearer your-api-key\"},\n    timeout=30,  # Connection timeout in seconds\n)\n\ntools = await mcp_server_tools(server_params)\nor\nadapter = await SseMcpToolAdapter.from_server_params(server_params, \"WikipediaAssistant\")\nError:  Error in sse_reader: Expected response header Content-Type to contain 'text/event-stream', got 'text/html'\nBut when I use the below (StdioServerParams), it works [both in local]\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_ext.tools.mcp import StdioServerParams, mcp_server_tools\n\n# Get the fetch tool from mcp-server-fetch.\nfetch_mcp_server = StdioServerParams(command=\"uvx\", args=[\"mcp-server-fetch\"])\ntools = await mcp_server_tools(fetch_mcp_server)\n\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython 0.5.4\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nAzure OpenAI\nOther model provider\nNo response\nPython version\n3.12\n.NET version\nNone\nOperating system\nUbuntu", "created_at": "2025-04-23", "closed_at": null, "labels": ["needs-triage"], "State": "open", "Author": "amd-srijaroy"}
{"issue_number": 6368, "issue_title": "Update `McpToolAdapter` to implement `return_value_as_string` ", "issue_body": "We need to update McpToolAdapter to implement return_value_as_string which will then properly format a readable output for the result list.\nOriginally posted by @ekzhu in #6367 (reply in thread)", "created_at": "2025-04-23", "closed_at": null, "labels": ["help wanted", "proj-extensions"], "State": "open", "Author": "ekzhu"}
{"issue_number": 6366, "issue_title": "The icons are not aligned vertically.", "issue_body": "What happened?\nDescribe the bug\nA clear and concise description of what the bug is.\nIf it is a question or suggestion, please use Discussions\ninstead.\nTo Reproduce\nSteps to reproduce the behavior. Please include code and outputs such as stacktrace.\n\nIf your input is just \"I tried X, and it didn't work\" or\n\"X is not working\", your issue will be ignored.\nIf your input is not well formatted, it will hurt readability and\nmay be ignored as well.\n\nExpected behavior\nA clear and concise description of what you expected to happen.\nScreenshots\nIf applicable, add screenshots to help explain your problem.\nAdditional context\nAdd any other context about the problem here.\nWhich packages was the bug in?\nAutoGen Studio (autogensudio)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-04-23", "closed_at": null, "labels": ["needs-triage"], "State": "open", "Author": "xionnon"}
{"issue_number": 6355, "issue_title": "**Problem Statement** Authorization specification for AutoGen clients", "issue_body": "Problem Statement\nEnterprises adopting autonomous AI agents using frameworks like AutoGen are aiming to automate operations from financial trading and insurance policy assessment to B2B supplier sourcing. However, without authorization frameworks, these agents (clients) lack ability to access protected resources (like outlook emails, Teams chat etc.) on behalf of resource owner.\nAn authorization server can issue a JWT access token in response to any authorization grant defined by  and subsequent extensions meant to result in an access token. If the request includes a \"resource\" parameter (as defined in ), the resulting JWT access token \"aud\" claim  have the same value as the \"resource\" parameter in the request.\nexample,\nget token request\nGET /as/authorization.oauth2?response_type=code\n&client_id=s6BhdRkqt3\n&state=xyz\n&scope=openid%20profile%20reademail\n&redirect_uri=https%3A%2F%2Fclient%2Eexample%2Ecom%2Fcb\n&resource=https%3A%2F%2Frs.example.com%2F HTTP/1.1\nHost: authorization-server.example.com\nResponse\nHeader:\n{\"typ\":\"at+JWT\",\"alg\":\"RS256\",\"kid\":\"RjEwOwOA\"}\nClaims:\n{\n\"iss\": \"https://authorization-server.example.com/\",\n\"sub\": \"5ba552d67\",\n\"aud\":   \"https://rs.example.com/\",\n\"exp\": 1639528912,\n\"iat\": 1618354090,\n\"jti\" : \"dbe39bf3a3ba4238a513f51d6e1691c4\",\n\"client_id\": \"s6BhdRkqt3\",\n\"scope\": \"openid profile reademail\"\n}\nOriginally posted by @yogitasrivastava in #5921", "created_at": "2025-04-22", "closed_at": null, "labels": [], "State": "open", "Author": "yogitasrivastava"}
{"issue_number": 6353, "issue_title": "Teams with agents that contain duplicated produced_message_types result in an ValueError exception", "issue_body": "What happened?\nDescribe the bug\nOn 0.53.0 output_content_type for agents is automatically registered into the team Code Reference.  When a team is comprised of agents that share the same output_content_type then a ValueError is raised.\nTo Reproduce\nAdd into a team 2 agents with the same Pydantic model as output_content_type.\nValueError: Message type <class 'autogen_agentchat.messages.StructuredMessage[<TestModel>]'> is already registered.\n\nExpected behavior\nOnly register structured messages that have not been registered previously.\nScreenshots\nIf applicable, add screenshots to help explain your problem.\nAdditional context\nI have added #6354 for your consideration\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython 0.5.3\nOther library version.\nNo response\nModel used\ngpt-4o\nModel provider\nOpenAI\nOther model provider\nNo response\nPython version\n3.11\n.NET version\nNone\nOperating system\nMacOS", "created_at": "2025-04-21", "closed_at": "2025-04-22", "labels": ["needs-triage"], "State": "closed", "Author": "jorge-wonolo"}
{"issue_number": 6352, "issue_title": "completion_tokens return None when result.usage is None", "issue_body": "What happened?\nDescribe the bug\nFor something reason when I use _model_client.create genereted of next bug :\n--- Logging error ---\nTraceback (most recent call last):\nFile \"C:\\Users\\anis.abdel\\AI\\seraph3\\utils\\llm_usage.py\", line 34, in emit\nself._completion_tokens += event.completion_tokens\nTypeError: unsupported operand type(s) for +=: 'int' and 'NoneType'\nI explored that situation and I founded the reason:\nusage = RequestUsage(\n            # TODO backup token counting\n            prompt_tokens=result.usage.prompt_tokens if result.usage is not None else 0,\n            completion_tokens=(result.usage.completion_tokens if result.usage is not None else 0),\n        )\n\n        logger.info(\n            LLMCallEvent(\n                messages=cast(List[Dict[str, Any]], create_params.messages),\n                response=result.model_dump(),\n                prompt_tokens=usage.prompt_tokens,\n                completion_tokens=usage.completion_tokens,\n            )\n        )\nThen completion_tokens=(result.usage.completion_tokens if result.usage is not None else 0),\nreturn None!!!\nBut result.usage is None, for some reason return None but the real answer is 0.\nI tried change the line 629 for the next:\n(result.usage.completion_tokens if result.usage is not None and not type(None) else 0)\nAnd the answer is sucessfully.\nTo Reproduce\nUse OpenAI Model 4.1\nPython 3.12\nawait self._model_client.create( ...)\nExpected behavior\nSuccesfull comparative:\ncompletion_tokens=(result.usage.completion_tokens if result.usage is not None else 0)\nWhen result.usage is Not None the correct return is 0.\nScreenshots\nIf applicable, add screenshots to help explain your problem.\nAdditional context\nAdd any other context about the problem here.\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython 0.5.3\nOther library version.\nNo response\nModel used\ngpt-4.1-2025-04-14\nModel provider\nOpenAI\nOther model provider\nNo response\nPython version\n3.12\n.NET version\nNone\nOperating system\nWindows", "created_at": "2025-04-21", "closed_at": null, "labels": ["help wanted", "proj-extensions"], "State": "open", "Author": "alejandroabdel"}
{"issue_number": 6341, "issue_title": "macOS/uv: venv tests fail (dyld error), not resolved by symlinks=False", "issue_body": "What happened?\nDescribe the bug\nThe pytest tests test_local_executor_with_custom_venv and test_local_executor_with_custom_venv_in_local_relative_path located in packages/autogen-ext/tests/code_executors/test_commandline_code_executor.py fail when run on macOS (aarch64) using a Python interpreter managed by uv (following the project's recommended development setup).\nThe failure occurs during the creation of a nested virtual environment using Python's standard venv.EnvBuilder. Specifically, the attempt to run ensurepip inside the newly created venv fails immediately with a SIGABRT signal. The root cause appears to be a dynamic library loading error (dyld error) where the Python executable inside the newly created venv cannot find its required libpythonX.Y.dylib shared library.\nThis issue persists even when explicitly setting symlinks=False in venv.EnvBuilder, which should force copying of files instead of symlinking.\nTo Reproduce\n\n\nEnvironment Setup:\n\nOS: macOS (tested on aarch64 / Apple Silicon)\nInstall uv (following AutoGen's specified version if applicable).\nClone the autogen repository.\nNavigate to the python directory.\nCreate and sync the development environment using uv sync --all-extras. This uses a uv-managed Python interpreter (e.g., cpython-3.11.11-macos-aarch64-none as seen in tracebacks).\nActivate the virtual environment: source .venv/bin/activate\n\n\n\nRun Tests:\n\nExecute the tests using poe test or directly target the specific file:\npytest packages/autogen-ext/tests/code_executors/test_commandline_code_executor.py\n\n\n\n\nObserve Failure: The tests mentioned above will fail.\n\n\nTraceback: The failure occurs within venv.EnvBuilder.create, specifically when calling _setup_pip -> _call_new_python -> subprocess.check_output (or subprocess.run). The key parts of the traceback are:\n# Condensed Traceback showing the failure point and error\npackages/autogen-ext/tests/code_executors/test_commandline_code_executor.py:175: in test_local_executor_with_custom_venv\n env_builder.create(temp_dir)\n<...>/python3.11/venv/__init__.py:77: in create\n self._setup_pip(context)\n<...>/python3.11/venv/__init__.py:359: in _setup_pip\n self._call_new_python(context, '-m', 'ensurepip', '--upgrade', '--default-pip')\n<...>/python3.11/venv/__init__.py:355: in _call_new_python\n subprocess.check_output(args, **kwargs)\n<...>/python3.11/subprocess.py:466: in check_output\n return run(*popenargs, stdout=PIPE, timeout=timeout, check=True, ...)\n<...>/python3.11/subprocess.py:571: in run\n raise CalledProcessError(retcode, process.args, output=stdout, stderr=stderr)\n\n# Captured stdout from the failing subprocess shows the dyld error:\n# stdout = b\"dyld[...]: Library not loaded: @executable_path/../lib/libpython3.11.dylib\\n  Referenced from: <...>/python3.11\\n  Reason: tried: '[...]/lib/libpython3.11.dylib' (no such file), '/private/var/folders/[...]/lib/libpython3.11.dylib' (no such file)\\n\"\n\n# The final exception:\nE subprocess.CalledProcessError: Command '['/var/folders/[...]/python3.11', '-m', 'ensurepip', '--upgrade', '--default-pip']' died with <Signals.SIGABRT: 6>.\n\n\n\nNote: Modifying the test code to use venv.EnvBuilder(with_pip=True, symlinks=False) results in the exact same error and traceback.\n\n\nExpected behavior\nThe tests test_local_executor_with_custom_venv and test_local_executor_with_custom_venv_in_local_relative_path should pass successfully on macOS when using the recommended uv-based development environment. The standard venv.EnvBuilder should be able to create a functional virtual environment where ensurepip can run, allowing the CommandlineCodeExecutor to utilize custom virtual environments as intended.\nScreenshots\nN/A (Traceback provides the necessary details)\nAdditional context\n\nRoot Cause Analysis: The failure stems from the Python executable within the venv created by venv.EnvBuilder being unable to locate its libpythonX.Y.dylib shared library via the relative path (@executable_path/../lib/). This occurs specifically when the base Python interpreter is one managed by uv on macOS. The exact reason why symlinks=False (copying) doesn't resolve this is unclear, but it suggests a deeper incompatibility in how the uv-provided Python is built/structured and how venv expects to create environments from it on macOS.\nPlatform Specificity: This issue appears specific to macOS. It's expected that these tests would pass on Linux using a similar uv-based setup due to differences in dynamic linking mechanisms.\nPotential Workarounds Discussed:\n\nModify Tests to Use uv venv: Change the test implementation to use subprocess to call uv venv --python ... instead of the venv module. This aligns with the project's tooling and works but slightly changes the test's scope.\nSkip Tests: Use @pytest.mark.skipif to skip these specific tests on the problematic platform/environment combination (macOS + uv Python). This allows the test suite to pass but avoids testing the functionality.\n\n\nQuestion: Is this considered a bug in the AutoGen test code (for not being compatible with the recommended dev setup on macOS), or does it point to an underlying issue in Python's venv module or the uv-provided Python builds for macOS that needs addressing upstream? How should these tests be handled for developers using the recommended setup on macOS?\n\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\n3.11\n.NET version\nNone\nOperating system\nMacOS", "created_at": "2025-04-19", "closed_at": null, "labels": ["needs-triage"], "State": "open", "Author": "SongChiYoung"}
{"issue_number": 6336, "issue_title": "Some agents do not deserialize model_context (set to None or Do not serialize too)", "issue_body": "What happened?\nDescribe the bug\nSome agent classes, including AssistantAgent and SocietyOfMindAgent and CodeExecutorAgent fail to properly deserialize the model_context field when reconstructing from their declarative config. Even though the model_context is correctly serialized via dump_component(), it is explicitly set to None during deserialization.\nThis leads to the silent loss of critical context configuration, especially when using model-specific conditional transformers or inference-time metadata handlers.\nTo Reproduce\n# at AssistantAgent\n# to_config: correct serialization\nmodel_context=self._model_context.dump_component(),\n\n# from_config: incorrect deserialization\nmodel_context=None,  # <-- should be ChatCompletionContext.load_component(config.model_context)\nSame omission is present in SocietyOfMindAgent, where model_context is neither serialized nor deserialized.\nExpected behavior\nAgent reconstruction via from_config() should faithfully restore all components, including model_context, to support complete behavioral parity with the original instance.\nAdditional context\n\ud83e\udde9 Affected classes:\n\nAssistantAgent: model_context is serialized but not deserialized.\nSocietyOfMindAgent: model_context is completely ignored (neither serialized nor deserialized).\nCodeExecutorAgent: model_context is completely ignored (neither serialized nor deserialized).\n\n\ud83e\uddea Missing test coverage:\n\nRound-trip serialization/deserialization test for agents with non-null model_context\nNegative test for unsupported ChatCompletionContext types\n\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-04-19", "closed_at": "2025-04-22", "labels": ["needs-triage"], "State": "closed", "Author": "SongChiYoung"}
{"issue_number": 6328, "issue_title": "No tools are passed in reflecting tool use flow and raise param error from llm server", "issue_body": "What happened?\nDescribe the bug\nIn processing model result of assistant agent, at the step of reflecting the tool use, no tools are passed and raise UnsupportedParamsError on the llm server which in turn again cause BadRequestError in autogen.\nTo Reproduce\nWhen I tried to run quick start example of agent chat with ollama and litellm backed llm server.\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nimport asyncio\nfrom autogen_core.models import ModelInfo, ModelFamily\n\n# Define a model client. You can use other model client that implements\n# the `ChatCompletionClient` interface.\nmodel_client = OpenAIChatCompletionClient(\n    model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    base_url=\"http://my_llm_server\",\n    api_key=\"my_api_key\",\n    model_info=ModelInfo(\n        vision=True,\n        function_calling=True,\n        json_output=True,\n        family=ModelFamily.CLAUDE_3_5_SONNET,\n        structured_output=False,\n    ),\n)\n\n\n# Define a simple function tool that the agent can use.\n# For this example, we use a fake weather tool for demonstration purposes.\nasync def get_weather(city: str) -> str:\n    \"\"\"Get the weather for a given city.\"\"\"\n    return f\"The weather in {city} is 73 degrees and Sunny.\"\n\n\n# Define an AssistantAgent with the model, tool, system message, and reflection enabled.\n# The system message instructs the agent via natural language.\nagent = AssistantAgent(\n    name=\"weather_agent\",\n    model_client=model_client,\n    tools=[get_weather, sum],\n    system_message=\"You are a helpful assistant.\",\n    reflect_on_tool_use=True,\n    model_client_stream=True,  # Enable streaming tokens from the model client.\n)\n\n\n# Run the agent and stream the messages to the console.\nasync def main() -> None:\n    await Console(agent.run_stream(task=\"What is the weather in New York?\"))\n    # Close the connection to the model client.\n    await model_client.close()\n\n\n# NOTE: if running this inside a Python script you'll need to use asyncio.run(main()).\nif __name__ == \"__main__\":\n    asyncio.run(main())\nThis is the logs.\n[Single-Agent Team](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/teams.html#single-agent-team) for more details.\n  agent = AssistantAgent(\n---------- TextMessage (user) ----------\nWhat is the weather in New York?\n---------- ModelClientStreamingChunkEvent (weather_agent) ----------\nCertainly! I can help you find out the weather in New York. To get this information, I'll use the get_weather function. Let me fetch that for you right away.\n---------- ToolCallRequestEvent (weather_agent) ----------\n[FunctionCall(id='tooluse_GCHoZfjoS1OiePVqLMZLFA', arguments='{\"city\": \"New York\"}', name='get_weather')]\n---------- ToolCallExecutionEvent (weather_agent) ----------\n[FunctionExecutionResult(content='The weather in New York is 73 degrees and Sunny.', name='get_weather', call_id='tooluse_GCHoZfjoS1OiePVqLMZLFA', is_error=False)]\nTraceback (most recent call last):\n  File \"/home/tris/Desktop/autogen-test2/main.py\", line 55, in <module>\n    asyncio.run(main())\n  File \"/home/tris/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/home/tris/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tris/.local/share/uv/python/cpython-3.12.9-linux-x86_64-gnu/lib/python3.12/asyncio/base_events.py\", line 691, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/home/tris/Desktop/autogen-test2/main.py\", line 48, in main\n    await Console(agent.run_stream(task=\"What is the weather in New York?\"))\n  File \"/home/tris/Desktop/autogen-test2/.venv/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py\", line 117, in Console\n    async for message in stream:\n  File \"/home/tris/Desktop/autogen-test2/.venv/lib/python3.12/site-packages/autogen_agentchat/agents/_base_chat_agent.py\", line 175, in run_stream\n    async for message in self.on_messages_stream(input_messages, cancellation_token):\n  File \"/home/tris/Desktop/autogen-test2/.venv/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 840, in on_messages_stream\n    async for output_event in self._process_model_result(\n  File \"/home/tris/Desktop/autogen-test2/.venv/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 1043, in _process_model_result\n    async for reflection_response in cls._reflect_on_tool_use_flow(\n  File \"/home/tris/Desktop/autogen-test2/.venv/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py\", line 1156, in _reflect_on_tool_use_flow\n    async for chunk in model_client.create_stream(\n  File \"/home/tris/Desktop/autogen-test2/.venv/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py\", line 809, in create_stream\n    async for chunk in chunks:\n  File \"/home/tris/Desktop/autogen-test2/.venv/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py\", line 1000, in _create_stream_chunks\n    stream = await stream_future\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/home/tris/Desktop/autogen-test2/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 2032, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/home/tris/Desktop/autogen-test2/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1805, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tris/Desktop/autogen-test2/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1495, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/tris/Desktop/autogen-test2/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1600, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"litellm.UnsupportedParamsError: Bedrock doesn't support tool calling without `tools=` param specified. Pass `tools=` param OR set `litellm.modify_params = True` // `litellm_settings::modify_params: True` to add dummy tool to the request.\\nReceived Model Group=anthropic.claude-3-5-sonnet-20240620-v1:0\\nAvailable Model Group Fallbacks=None\", 'type': 'None', 'param': None, 'code': '400'}}\n\nExpected behavior\nI tried to pass tools in _reflect_on_tool_use_flow which resolve the error but I am not sure it is the right solution since there are various llm that support tool calling or not.\nScreenshots\n\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nanthropic.claude-3-5-sonnet-20240620-v1:0\nModel provider\nOther (please specify below)\nOther model provider\nself hosted with ollama and litellm\nPython version\n3.12\n.NET version\nNone\nOperating system\nOther", "created_at": "2025-04-17", "closed_at": null, "labels": ["needs-triage"], "State": "open", "Author": "Khinyu2000"}
{"issue_number": 6325, "issue_title": "[BUG]: json description value in config object not linked to form editor when adding a tool", "issue_body": "What happened?\nDescribe the bug\nWhen adding via Gallery > Tools > Add Tool, editing the Description only changes the value of one occurence of the description key in the JSON Editor. The nested config object also has a description field that is not linked.\nTo Reproduce\nAdd a tool through Gallery > Tools > Add Tool, edit the description field, check the description value in the config object.\nExpected behavior\nThe nested description value changes as well.\nScreenshots\n\nWhich packages was the bug in?\nAutoGen Studio (autogensudio)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-04-17", "closed_at": "2025-04-17", "labels": ["proj-studio", "needs-triage"], "State": "closed", "Author": "pavelmdescamps1002"}
{"issue_number": 6323, "issue_title": "Support anyOf property types in convert_tools for Ollama clients", "issue_body": "What happened?\nDescribe the bug\nA KeyError: 'type' occurs when an Ollama\u2011backed agent attempts to load a tool schema that includes an anyOf clause without a top-level type field. This happens, for example, with the google_search tool used by Deep Research Team in Autogen Studio.\nThe convert_tools function should:\n\nDetect the absence of a top-level type and resolve it from the first non-null variant in anyOf.\nFall back to \"string\" when no explicit type can be inferred.\nMaintain compatibility with schemas that already specify a type.\n\nTo Reproduce\nSteps to reproduce the behavior:\n\nCreate or import a team in Autogen Studio using a research-assistant agent with the google_search tool (schema example below).\nConfigure the agent to run on an Ollama model that supports function-calling, e.g., PetrosStav/gemma3-tools:12b.\nTrigger a task that invokes the tool.\n\nExpected behavior\nThe agent should call the tool successfully.\nActual behavior\nThe runtime crashes with the following traceback:\nKeyError: 'type'\n  File \".../autogen_ext/models/ollama/_ollama_client.py\", line 277, in convert_tools\n\nThe failure stems from this part of the code, which directly accesses spec[\"type\"] without checking:\nollama_properties[prop_name] = OllamaTool.Function.Parameters.Property(\n    type=prop_schema[\"type\"],\n    description=prop_schema[\"description\"] if \"description\" in prop_schema else None,\n)\nScreenshots / Schema Example\nExample from the google_search schema\u2014note content_max_length uses anyOf but lacks a top-level type:\n{\n  \"name\": \"google_search\",\n  \"parameters\": {\n    \"properties\": {\n      \"content_max_length\": {\n        \"anyOf\": [\n          { \"type\": \"integer\" },\n          { \"type\": \"null\" }\n        ],\n        \"default\": 10000,\n        \"description\": \"content_max_length\"\n      }\n    }\n  }\n}\nAdditional context\nThanks for reviewing!\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nollama PetrosStav/gemma3-tools:12b\nModel provider\nOllama\nOther model provider\nNo response\nPython version\n3.10\n.NET version\nNone\nOperating system\nUbuntu", "created_at": "2025-04-17", "closed_at": "2025-04-22", "labels": ["needs-triage"], "State": "closed", "Author": "nfaranda"}
{"issue_number": 6322, "issue_title": "autogen-studio running error", "issue_body": "What happened?\nDescribe the bug\nAfter successfully starting Studio locally, entering \"Hello\" in the chat window will generate an abnormal error message in the background\nTo Reproduce\n1.After downloading the code, successfully run it using the \"###### A) Install from source manually\" method.\n2.After entering \"Hello\" in the chat window of the interface, an error message was generated in the background.\nExpected behavior\n1.May I ask if this is normal? Did I miss any steps?\nScreenshots\n\n\n\n\nAdditional context\nAdd any other context about the problem here.\nWhich packages was the bug in?\nAutoGen Studio (autogensudio)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\n3.11\n.NET version\nNone\nOperating system\nWindows", "created_at": "2025-04-17", "closed_at": null, "labels": ["proj-studio"], "State": "open", "Author": "xiaoSS-developer"}
{"issue_number": 6321, "issue_title": "Azure AI Search tool not working in v0.5.3", "issue_body": "What happened?\nDescribe the bug\nHi @ekzhu @jay-thakur  Thanks for providing the fix. I tried out the Azure AI Search tool in v0.5.3 and I am getting following issues now. II am still not able to use the tool, Could you provide some guidance ?\n---------- HandoffMessage (ResearchAgent) ----------\nTransferred to CodeAnalysisAgent, adopting the role of CodeAnalysisAgent immediately.\nWARNING:azure.search.documents._generated._serialization:k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n---------- ToolCallRequestEvent (CodeAnalysisAgent) ----------\n[FunctionCall(id='call_5cpYPh5Py8HRRl9FeGu1JH4d', arguments='{\"query\":\"screen 409 locomotive track element\"}', name='hybrid_search_tool')]\n---------- ToolCallExecutionEvent (CodeAnalysisAgent) ----------\n[FunctionExecutionResult(content=\"Error: Error from Azure AI Search: () The request is invalid. Details: Requested value 'hybrid' was not found.\\nCode: \\nMessage: The request is invalid. Details: Requested value 'hybrid' was not found.\", name='hybrid_search_tool', call_id='call_5cpYPh5Py8HRRl9FeGu1JH4d', is_error=True)]\nTo Reproduce\nSteps to reproduce the behavior.\nhybrid_search_tool = AzureAISearchTool.create_hybrid_search(\nname=\"hybrid_search_tool\",\nendpoint=AZURE_AI_SEARCH_ENDPOINT,\nindex_name=VLI_DB_SEARCH_INDEX,\ncredential=AzureKeyCredential(AZURE_AI_SEARCH_API_KEY),\nvector_fields=[\"contentVector\"],\nsearch_fields=[\"title\", \"content\", \"url\"],\nselect_fields=[\"title\", \"content\", \"url\"],\ntop=10,\n)\nOutput:\n---------- HandoffMessage (ResearchAgent) ----------\nTransferred to CodeAnalysisAgent, adopting the role of CodeAnalysisAgent immediately.\nWARNING:azure.search.documents._generated._serialization:k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n---------- ToolCallRequestEvent (CodeAnalysisAgent) ----------\n[FunctionCall(id='call_5cpYPh5Py8HRRl9FeGu1JH4d', arguments='{\"query\":\"screen 409 locomotive track element\"}', name='hybrid_search_tool')]\n---------- ToolCallExecutionEvent (CodeAnalysisAgent) ----------\n[FunctionExecutionResult(content=\"Error: Error from Azure AI Search: () The request is invalid. Details: Requested value 'hybrid' was not found.\\nCode: \\nMessage: The request is invalid. Details: Requested value 'hybrid' was not found.\", name='hybrid_search_tool', call_id='call_5cpYPh5Py8HRRl9FeGu1JH4d', is_error=True)]\n\nIf your input is just \"I tried X, and it didn't work\" or\n\"X is not working\", your issue will be ignored.\nIf your input is not well formatted, it will hurt readability and\nmay be ignored as well.\n\nExpected behavior\nA clear and concise description of what you expected to happen.\nScreenshots\nIf applicable, add screenshots to help explain your problem.\nAdditional context\nAdd any other context about the problem here.\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython 0.5.3\nOther library version.\nNo response\nModel used\ngpt-4o-mini\nModel provider\nNone\nOther model provider\nNo response\nPython version\n3.11\n.NET version\nNone\nOperating system\nWindows", "created_at": "2025-04-17", "closed_at": "2025-04-18", "labels": ["needs-triage"], "State": "closed", "Author": "amardeepjaiman"}
{"issue_number": 6317, "issue_title": "AgentChat messages should have an `id` field.", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nMessage id is useful to correlate ModelClientStreamingChunkEvent with the eventual completed message being produced.\nThis should be added to BaseChatMessage and BaseAgentEvent as an id: str field. For ModelClientStreamingChunkEvent, it should have a separate optional field full_message_id: str | None = None to reference the complete message that may come after the chunks. So the consumer of the stream don't need to double-print it.", "created_at": "2025-04-17", "closed_at": null, "labels": ["help wanted", "proj-agentchat"], "State": "open", "Author": "ekzhu"}
{"issue_number": 6313, "issue_title": "Latest OpenAI models (GPT-4.1, o3 and o4-mini) are missing", "issue_body": "What happened?\nDescribe the bug\nOpenAI has just released new models named GPT-4.1, o3 and o4-mini.\n\nhttps://openai.com/index/gpt-4-1/\nhttps://openai.com/index/introducing-o3-and-o4-mini/\n\nExpected behavior\nAutoGen have to support them in https://github.com/microsoft/autogen/blob/main/python/packages/autogen-ext/src/autogen_ext/models/openai/_model_info.py.\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-04-17", "closed_at": "2025-04-17", "labels": ["needs-triage"], "State": "closed", "Author": "withsmilo"}
{"issue_number": 6308, "issue_title": "AzureAISearchTool Not working as expected", "issue_body": "What happened?\nDescribe the bug\nGetting following error when I am trying to use Hybrid Search tool with Assistant Agent.\n{\n\"source\": \"CodeAnalysisAgent\",\n\"models_usage\": null,\n\"metadata\": {},\n\"content\": \"\\nTool Call Execution Results:\\n\\nError: Error from Azure AI Search: HTTP transport has already been closed. You may check if you're calling a function outside of the async with of your client creation, or if you called await close() on your client already.\",\n\"type\": \"ToolCallExecutionEvent\"\n}\nCode Snippet:\nhybrid_search = AzureAISearchTool.create_hybrid_search(\nname=\"hybrid_search\",\nendpoint=AZURE_AI_SEARCH_ENDPOINT,\nindex_name=VLI_DB_SEARCH_INDEX,\ncredential=AzureKeyCredential(AZURE_AI_SEARCH_API_KEY),\nvector_fields=[\"contentVector\"],\nsearch_fields=[\"title\", \"content\", \"url\"],\nselect_fields=[\"title\", \"content\", \"url\"],\ntop=10,\n)\ndb_analyze_agent = AssistantAgent(\nname=\"DatabaseAnalysisAgent\",\nmodel_client=az_model_client,\ntools=[hybrid_search],\n}\nTo Reproduce\nSteps to reproduce the behavior. Please include code and outputs such as stacktrace.\n\nIf your input is just \"I tried X, and it didn't work\" or\n\"X is not working\", your issue will be ignored.\nIf your input is not well formatted, it will hurt readability and\nmay be ignored as well.\n\nExpected behavior\nA clear and concise description of what you expected to happen.\nScreenshots\nIf applicable, add screenshots to help explain your problem.\nAdditional context\nAdd any other context about the problem here.\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython 0.5.2\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\n3.11\n.NET version\nNone\nOperating system\nWindows", "created_at": "2025-04-16", "closed_at": "2025-04-17", "labels": ["needs-triage"], "State": "closed", "Author": "amardeepjaiman"}
{"issue_number": 6304, "issue_title": "Installing Task Centric Memory related dependency causes protobuf version conflicting error", "issue_body": "What happened?\nDescribe the bug\nUser may run into following error after running pip install autogen-ext[task-centric-memory] to install dependency\nTypeError: Descriptors cannot be created directly.\nE   If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nE   If you cannot immediately regenerate your protos, some other possible workarounds are:\nE    1. Downgrade the protobuf package to 3.20.x or lower.\nE    2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nIt happens because Task Centric Memory(TCM) feature needs chromadb, chromadb has pulled opentelemetry-proto as dependency and older version of opentelemetry-proto used protobuf 3.x version to generate files which conflicts with the protobuf version in AutoGen(5.29.3).\nTo Reproduce\n\nRun pip install autogen-ext[task-centric-memory]\nRun test_playwright_controller.py you will see the error message above\n\nExpected behavior\nTest and TCM feature should run without error\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython 0.5.1\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-04-15", "closed_at": "2025-04-15", "labels": ["needs-triage"], "State": "closed", "Author": "cheng-tan"}
{"issue_number": 6302, "issue_title": "Add optional `device_requests` to DockerCommandLineCodeExecutor", "issue_body": "Discussed in #6237\n\nOriginally posted by millerh1 April  7, 2025\nIt seems like DockerCommandLineCodeExecutor doesn't use GPUs even when they are available. I think you would need to update _docker_code_executor.py to something like this:\n# Create a device request asking for all GPUs (equivalent to --gpus all)\ngpu_request = [DeviceRequest(count=-1, capabilities=[['gpu']])]\n\nself._container = await asyncio.to_thread(\n    client.containers.create,\n    self._image,\n    name=self.container_name,\n    entrypoint=shell_command,\n    command=command,\n    tty=True,\n    detach=True,\n    auto_remove=self._auto_remove,\n    volumes={str(self._bind_dir.resolve()): {\"bind\": \"/workspace\", \"mode\": \"rw\"}, **self._extra_volumes},\n    working_dir=\"/workspace\",\n    extra_hosts=self._extra_hosts,\n    # Pass the GPU request to the container creation call\n    device_requests=gpu_request,\n)\nawait asyncio.to_thread(self._container.start)\nI tried this locally and it worked nicely. Is this of interest?\n", "created_at": "2025-04-15", "closed_at": "2025-04-22", "labels": ["help wanted", "code-execution", "proj-extensions"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 6301, "issue_title": "`SelectorGroupChat` to add `model_context` parameter and make the group chat manager use the model context for selecting speaker", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nThis makes it possible to customize the selector and better context management.\nRelated #6292", "created_at": "2025-04-15", "closed_at": null, "labels": ["help wanted", "proj-agentchat"], "State": "open", "Author": "ekzhu"}
{"issue_number": 6295, "issue_title": "Thought content is lost when model returns a handoff tool call", "issue_body": "What happened?\nDescribe the bug\nWhen using AzureOpenAIChatCompletionClient, the model sometimes returns both a meaningful message and a tool call simultaneously. If the tool call is a handoff, the content of the response becomes the thought in the result, but it is not included in the HandoffMessage context that is sent to the target agent. As a result, the receiving agent loses access to the original thought.\nThis behavior occurs because _check_and_handle_handoff(...) in AssistantAgent only includes the thought when there's at least one tool call in addition to the handoff tool call.\nTo Reproduce\nUse AzureOpenAIChatCompletionClient with a model that returns both a message content and a single handoff tool call.\nObserve how the thought is not passed along in the HandoffMessage context.\nRelevant snippet from BaseOpenAIChatCompletionClient.create(...):\n...\nelif choice.message.tool_calls is not None and len(choice.message.tool_calls) > 0:\n    ...\n    if choice.message.content is not None and choice.message.content != \"\":\n        # Put the content in the thought field.\n        thought = choice.message.content\n...\nresponse = CreateResult(\n    finish_reason=normalize_stop_reason(finish_reason),\n    content=content,\n    usage=usage,\n    cached=False,\n    logprobs=logprobs,\n    thought=thought,\n)\n\nAnd from AssistantAgent._check_and_handle_handoff(...):\n...\nif len(tool_calls) > 0: \n    # Only includes the thought if there are other tool calls besides the handoff\n    handoff_context.append(\n        AssistantMessage(\n            content=tool_calls,\n            source=agent_name,\n            thought=getattr(model_result, \"thought\", None),\n        )\n    )\n\nIf there's only a single handoff tool call, and no other calls, the thought is not included.\nExpected behavior\nEven if the only tool call present is a handoff, the model's thought (i.e., the message content) should still be passed in the HandoffMessage context, so the receiving agent has full context.\nQuick Fix / Workaround\nAs a workaround, I subclassed AssistantAgent and patched _check_and_handle_handoff(...) to also include the thought as a UserMessage if no other tool calls are present:\n...\nif len(tool_calls) > 0:\n    handoff_context.append(\n        AssistantMessage(\n            content=tool_calls,\n            source=agent_name,\n            thought=getattr(model_result, \"thought\", None),\n        )\n    )\n    handoff_context.append(FunctionExecutionResultMessage(content=tool_call_results))\n\n# MANUAL QUICK FIX\nelif has_thought:\n    handoff_context.append(\n        UserMessage(\n            content=model_result.thought,\n            source=agent_name,\n        )\n    )\n\nreturn Response(\n    chat_message=HandoffMessage(\n        content=selected_handoff_message,\n        target=selected_handoff.target,\n        source=agent_name,\n        context=handoff_context,\n    ),\n    inner_messages=inner_messages,\n)\n\nAdditional context\n\nI initially attempted to prompt the model to emit a regular message before making a tool call, but this approach did only work out okayish\nThe manual fix works more robust and should not introduce any side-effects, thought it most likely is not the proper solution\n\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython 0.5.1\nOther library version.\nNo response\nModel used\ngpt4o\nModel provider\nAzure OpenAI\nOther model provider\nNo response\nPython version\n3.11\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-04-14", "closed_at": "2025-04-17", "labels": ["help wanted", "proj-agentchat"], "State": "closed", "Author": "ChrisBlaa"}
{"issue_number": 6290, "issue_title": "Some Team/Agent like SocietyOfMindAgent is trying to use multiple/separated system messages, but the current system does not allow that.", "issue_body": "What happened?\nDescribe the bug\nSome Team/Agent like SocietyOfMindAgent or SelectorGroupChat tries to use multiple or separated system_messages, but the current system doesn't support that. As a result, non-OpenAI model backends may break or crash depending on how they handle system_messages.\nCurrently:\n\nSelectorGroupChat manually downgrades the system_message to a user message unless the model is OpenAI-compatible.\nOther agents (like SocietyOfMindAgent) do not have such fallback, and will fail silently or cause exceptions depending on the model vendor.\n\nThis issue is especially problematic for models like Anthropic, which only support multiple/separated system_messages when invoked via OpenAI-compatible APIs, but not via their native SDK.\nThis situation makes some teams or agents effectively OpenAI-only, which is not an intentional design.\nTo Reproduce\nUse SocietyOfMindAgent or any GroupChat-style agent that passes multiple/separated system_messages to sub-agents, and test against various model backends.\nExample:\n    from autogen_agentchat.agents import AssistantAgent, SocietyOfMindAgent\n    from autogen_agentchat.teams import RoundRobinGroupChat\n    from autogen_ext.models.anthropic import AnthropicChatCompletionClient  # native SDK\n    from autogen_agentchat.ui import Console\n\n    client = AnthropicChatCompletionClient(model=\"claude-3-5-haiku-20241022\")\n\n    agent1 = AssistantAgent(name=\"agent1\", system_message=\"You are a writer.\", model_client=client)\n    agent2 = AssistantAgent(name=\"agent2\", system_message=\"You are a critic.\", model_client=client)\n\n    group = RoundRobinGroupChat([agent1, agent2], max_turns=1)\n\n    society = SocietyOfMindAgent(name=\"society\", team=group, model_client=client)\n    \n    async def team_run():\n        await Console(\n            society.run_stream(\n                task=\"Write a short story with critique.\"\n            )\n        )\n    \n    asyncio.run(team_run())\nDepending on the SDK, this may throw an error or silently misbehave (e.g., ignore one of the system_messages).\nExpected behavior\nAgents and teams that wish to use multiple/separated system_messages should:\n\nCheck model compatibility before applying that structure\nFallback gracefully (e.g., convert to user message) when unsupported\nNot crash or behave incorrectly on non-OpenAI models\n\nAdditional context\nWe propose the following as a longer-term structural fix:\n\n\nSurvey required\na. Which models (by SDK / backend) support multiple/separated system_messages\nb. Which Team/Agent types want to make use of this feature (e.g., SocietyOfMindAgent, SelectorGroupChat, custom GroupChats)\n\n\nIntroduce capability flags\nA new field (e.g., supports_multi_system_message) should be added in model_info or similar, and exposed through the model registry.\n\n\nPropagate model capabilities to agents\nAgent constructors or transformers should receive this flag and adjust behavior accordingly.\n\n\nConditional behavior per agent\nEach agent or team should gracefully fallback or reformat messages based on the capability flag.\n\n\nThese changes should be implemented across two or more separate PRs, and step 1a/1b will likely require community input \u2014 it's probably too much to handle alone.\nHowever, this issue is critical because some agents/teams silently become OpenAI-only, and we need to address this explicitly in the design.\n\nTODO...\nCheck list - is Agent/Team need to multi-system message?\nAgents\n\n Name : assistant_agent  Need it? : X\n Name : base_chat_agent  Need it? : X\n Name : code_executor_agent  Need it? : X\n Name : society_of_mind_agnet  Need it? : O\n Name : user_proxy_agent  Need it? : X\n\nTeams\n\n[?] Name : magentic_one  Need it? : ?\n Name : base_group_chat  Need it? : X\n Name : round_robin_group_chat  Need it? : X\n Name : selector_group_chat  Need it? : X\n[?] Name : swarm_group_chat  Need it? : ?\n\nModel SDK\n\n Name : anthropic Could Multi-System message : X\n Name : azure Could Multi-System message : ?\n Name : cache Could Multi-System message : ?\n Name : llama_cpp Could Multi-System message : ?\n Name : ollama Could Multi-System message : ?\n Name : replay Could Multi-System message : ?\n Name : semantic_kernel Could Multi-System message : ?\n Name : openai Could Multi-System message : go to down\n\nOpenAI API aware model\n\n Name : openAI Could Multi-System message : O\n Name : anthropic Could Multi-System message : O\n  Name : gemini  Could Multi-System message : X\n\n\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-04-13", "closed_at": null, "labels": ["needs-triage"], "State": "open", "Author": "SongChiYoung"}
{"issue_number": 6281, "issue_title": "Swarm flight refund demo is not working with ollama models", "issue_body": "There is a travel_agent/flight_refunder demo on the Swarm docs page. I've copied and pasted it into jupyter notebook and tried to run it with different local models via ollama using OpenAIChatCompletionClient (OpenAI like ollama endpoint) as well as OllamaChatCompletionClient. Both produced the same weird behavior when the swarm immediately hands off control to the user via transfer_to_user function call.\nSo, the demo run logs from the page look like this:\n---------- user ----------\nI need to refund my flight.\n---------- travel_agent ----------\n[FunctionCall(id='call_ZQ2rGjq4Z29pd0yP2sNcuyd2', arguments='{}', name='transfer_to_flights_refunder')]\n[Prompt tokens: 119, Completion tokens: 14]\n---------- travel_agent ----------\n[FunctionExecutionResult(content='Transferred to flights_refunder, adopting the role of flights_refunder immediately.', call_id='call_ZQ2rGjq4Z29pd0yP2sNcuyd2')]\n---------- travel_agent ----------\nTransferred to flights_refunder, adopting the role of flights_refunder immediately.\n---------- flights_refunder ----------\nCould you please provide me with the flight reference number so I can process the refund for you?\n[Prompt tokens: 191, Completion tokens: 20]\n---------- flights_refunder ----------\n[FunctionCall(id='call_1iRfzNpxTJhRTW2ww9aQJ8sK', arguments='{}', name='transfer_to_user')]\n[Prompt tokens: 219, Completion tokens: 11]\n---------- flights_refunder ----------\n[FunctionExecutionResult(content='Transferred to user, adopting the role of user immediately.', call_id='call_1iRfzNpxTJhRTW2ww9aQJ8sK')]\n---------- flights_refunder ----------\nTransferred to user, adopting the role of user immediately.\n---------- Summary ----------\nNumber of messages: 8\nFinish reason: Handoff to user from flights_refunder detected.\nTotal prompt tokens: 529\nTotal completion tokens: 45\nDuration: 2.05 seconds\n---------- user ----------\nSure, it's 507811\n.....\n\nWhat happens here is: the flights_refunder agent generates the message to the user \"Could you please provide me with the flight reference number so I can process the refund for you?\" and then asks for user input.\nMy multiple attempts to reproduce the demo using ollama hosted models (deepseek-r1 and gemma3 to be specific) produce the following results:\n---------- user ----------\nI need to refund my flight.\n---------- travel_agent ----------\n[FunctionCall(id='0', arguments='{}', name='transfer_to_user')]\n---------- travel_agent ----------\n[FunctionExecutionResult(content='Transferred to user, adopting the role of user immediately.', name='transfer_to_user', call_id='0', is_error=False)]\n---------- travel_agent ----------\nTransferred to user, adopting the role of user immediately.\n---------- user ----------\n\nSo instead of travel_agent saying what it expects the user to enter it asks for an input immediately.\nSame behavior I see when I try to port a couple of very basic agents from llamaindex based code with prompts that worked there.\nHere's my configuration for model_client:\nmodel_client = OpenAIChatCompletionClient(\n    model=\"gemma3-tools:27b\",\n    temperature=0.1,\n    base_url='https://.../v1/',\n    timeout=600,\n    model_info=ModelInfo(\n        vision=True,\n        function_calling=True,\n        structured_output=True,\n        json_output=True,\n        family='unknown',\n    ),\n    parallel_tool_calls=False, # type: ignore\n)\n\nThe rest is just the original demo code from the docs.\nI am not sure if it's a model specific problem.\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython 0.5.1\nOther library version.\nNo response\nModel used\nmultiple tried\nModel provider\nOllama\nOther model provider\nNo response\nPython version\n3.12\n.NET version\nNone\nOperating system\nMacOS", "created_at": "2025-04-11", "closed_at": null, "labels": ["needs-triage"], "State": "open", "Author": "antony66"}
{"issue_number": 6280, "issue_title": "Heavy use of broad Exceptions leads to incorrect error handling", "issue_body": "What happened?\nDescribe the bug\nI was working through the agentchat_fastapi examples\u2014specifically the WebSocket example (app_team.py). While the example works out of the box, I tried connecting using a custom WebSocket client. Submitting and receiving messages works just fine.\nHowever, when I close the socket on the client side, the server prints an error message and continues to listen.\n    raise RuntimeError(f\"Failed to get user input: {str(e)}\") from e\nRuntimeError: Failed to get user input: Failed to get user input: (1000, '')\nUnexpected error: Unexpected ASGI message 'websocket.send', after sending 'websocket.close' or response already completed\n\nTracking down the issue was fairly straightforward. The server tries one last time to send a message because it's still waiting for user input (the app_team.py wraps websocket.receive_json inside a UserProxy agent, so it's effectively waiting for input).\nWhat happens next is also simple:\n\nThe WebSocket raises a WebSocketDisconnect exception, as it should.\nThis exception is caught by a broad Exception, and then a RuntimeError is raised instead:\n\n    raise RuntimeError(\"Failed to get user input: )\n\nTrying to address this by subclassing the UserProxy agent revealed that this pattern\u2014i.e., catching broad exceptions\u2014is used quite frequently. This obscures the root cause of errors.\nIn this particular example (WebSocketDisconnect), it prevents the chat-endpoint in the agent_fastapi examples from returning gracefully after the client disconnects. (Note: there's a workaround using nested try/except blocks in the examples. However, the inner loop also uses a broad except that fails when trying to send on a closed socket.)\nI've found this pattern of obscuring exceptions throughout the codebase, not just here.\nTo Reproduce\nRun app_team.py from the agentchat_fastapi example and run the following script.\nYou can also kill the script instead of closing the socket.  Ensure that the Server waits for input after at least one message was sent.\nimport asyncio\nimport json\n\nimport websockets\n\n\nasync def wsclient(uri=\"ws://localhost:8002/ws/chat\"):\n    \"\"\"\n    1. send message to server\n    2. receive messages from server\n    3. server is waiting for input (UserProxyAgent)\n    3. close websocket on client side\n    \"\"\"\n    async with websockets.connect(uri) as websocket:\n        await websocket.send(json.dumps({\"source\": \"user\", \"content\": \"Hello World\"}))\n        async for response in websocket:\n\n            response = json.loads(response)\n            print(f\"{response['source']}\", response[\"type\"])\n\n            if response[\"type\"] == \"UserInputRequestedEvent\":\n\n                # causes incorrect errorhandling on the server side if\n                # close the socket while the server is waiting for input\n                await websocket.close()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(wsclient())\nExpected behavior\nAvoid catching of broad/blind Exceptions see:\n\nhttps://pylint.readthedocs.io/en/latest/user_guide/messages/warning/broad-exception-caught.html\nhttps://docs.astral.sh/ruff/rules/blind-except/\n\nThe WebSocketDisconnect  exception should be propagated so that the chat_endpoint can exit gracefully. No reraising other Exception classes, just raise or raise the same Exception Class e.g. raise WebSockentDisconnect('cusom message' from e\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython 0.5.1", "created_at": "2025-04-11", "closed_at": null, "labels": ["needs-triage", "proj-agentchat"], "State": "open", "Author": "rhoef"}
{"issue_number": 6279, "issue_title": "Not consistent response with Litellm API + Gemini + tools", "issue_body": "What happened?\npython version: 3.11.7\nautogen-agentchat version: 0.5.1\nautogen-ext: 0.5.1\nI have litellm hosted somewhere else and this is the way I am calling that API so the client\nclass OpenAILLM:\n    def __init__(self, logger=None):\n        self.client = self._before_authenticate_openai()\n        self.logger = logger\n\n    def _before_authenticate_openai(self):\n        http_client = httpx.AsyncClient(http2=True, verify=False)\n\n        model_info = ModelInfo(\n            name=\"Gemini\",\n            family=ModelFamily.GEMINI_2_0_FLASH,\n            vision=False,\n            function_calling=True,\n            json_output=True,\n            structured_output=True\n        )\n\n        return OpenAIChatCompletionClient(\n            model=\"gemini-1.5-flash\",\n            model_info=model_info,\n            api_key=\"**************\",\n            base_url=\"***************\",\n            http_client=http_client,\n        )\n\n    def get_model_client(self):\n        return self.client\n\nthen this definition for tools and the way I am calling it\nasync def get_stock_price(ticker: str, date: Annotated[str, \"Date in YYYY/MM/DD\"]) -> float:\n    return random.uniform(10, 200)\n\n\nstock_price_tool = FunctionTool(get_stock_price, description=\"Get the stock price.\")\n\n\nasync def run_tool():\n    cancellation_token = CancellationToken()\n    result = await stock_price_tool.run_json({\"ticker\": \"AAPL\", \"date\": \"2021/01/01\"}, cancellation_token)\n    print(stock_price_tool.schema)\n    print(stock_price_tool.return_value_as_string(result))\n\n\nasync def main():\n    # Optionally test the tool directly\n    # await run_tool()\n\n    client = OpenAILLM().get_model_client()\n\n    user_message = UserMessage(content=\"What is the stock price of AAPL on 2021/01/01?\", source=\"user\")\n\n    create_result = await client.create(\n        messages=[user_message],\n        tools=[stock_price_tool]\n    )\n    print(f\"create result : {create_result}\")\n    result = create_result.content\n    print(f\"FInal Result: {result}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\nI am following this example only : https://microsoft.github.io/autogen/stable//user-guide/core-user-guide/components/tools.html\nwhen I am running this code, the same code sometimes giving it\n[FunctionCall(id='call_tpJ5J1Xoxi84Sw4v0scH0qBM', arguments='{\"ticker\":\"AAPL\",\"date\":\"2021/01/01\"}', name='get_stock_price')]\n\nand sometimes its giving\nthe available tools lack the implementation to optain stock price, Therefore, I cannot answer your question\n\nnot sure what's wrong here but its realiable.\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython 0.5.1\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nGoogle Gemini\nOther model provider\nNo response\nPython version\n3.11\n.NET version\nNone\nOperating system\nMacOS", "created_at": "2025-04-11", "closed_at": "2025-04-15", "labels": ["needs-triage"], "State": "closed", "Author": "jaiswalvineet"}
{"issue_number": 6277, "issue_title": "adding TextMessage Dependency", "issue_body": "What is the doc issue?\nDescribe the issue\nText Message dependency is missing, in custom agents  Custom Agents\nWhat do you want to see in the doc?\nadd from autogen_agentchat.messages import TextMessage\nScreenshots\nIf applicable, add screenshots to help explain your problem.\nAdditional context\nAdd any other context about the problem here.\nLink to the doc page, if applicable\nhttps://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/custom-agents.html", "created_at": "2025-04-11", "closed_at": null, "labels": ["documentation", "needs-triage"], "State": "open", "Author": "swapnilxi"}
{"issue_number": 6271, "issue_title": "LLMs.txt for AutoGen Documentation Guides", "issue_body": "For each page in documentation, we can compile a comparable llms.txt (or llms.md??) with markdown compilation of the content. The links in it should also reference corresponding links in llms.txt. \u00a0\nDiscussed in #6262\n\nOriginally posted by cc0zh April 10, 2025\nHello AutoGen Team,\nI hope this message finds you well.\nI'd like to suggest adding an LLMs.txt file specifically for the AutoGen user guide documentation hosted at https://microsoft.github.io/autogen/stable/user-guide/.\nAn LLMs.txt file can provide valuable directives for large language models (LLMs) and AI agents interacting with the documentation. This guidance could include preferred crawl paths, sections suitable for summarization, disallowed areas, citation preferences, etc.", "created_at": "2025-04-11", "closed_at": null, "labels": ["documentation", "help wanted"], "State": "open", "Author": "ekzhu"}
{"issue_number": 6270, "issue_title": "Display message type in `Console` output", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nMake the Console also print out the type of messages. For example,\n--------- TextMessage (user) -----------\nxxx\n--------- ToolCallRequestEvent (assistant) --------\nxxxx\n--------- ToolCallExecutionEvent (assistant) --------\nxxxx\n--------- TextMessage (assistant) ---------\nxxxx\n", "created_at": "2025-04-11", "closed_at": "2025-04-17", "labels": ["good first issue", "help wanted", "proj-agentchat"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 6268, "issue_title": "Support repeated tool calls in a loop within `AssistantAgent`", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nMany models have been trained to call tools repeatedly until a reflection on the results. For example, Claude models have been trained to perform exactly this.\nIdea:\nWe can support this in AssistantAgent by introducing an optional tool_call_loop parameter to the constructor. By default, tool_call_loop=False which indicates the tool calls won't get into a loop -- a single tool call at most.\nUser can set tool_call_loop=True which indicates the tool call will run in a loop until the model:\n\nProduces non-tool call response, or\nProduces a handoff tool call.\n\nThis is a start. As the next step, introduce a ToolCallConfig class which controls how a tool call loop should behave, with parameters for setting maximum number of iterations, preset response text, etc. -- we can consider merging reflect_on_tool_call into it as well.\nPlease discuss and suggest ideas.\nRelated:\n#6261\n#5621", "created_at": "2025-04-10", "closed_at": null, "labels": ["help wanted", "proj-agentchat"], "State": "open", "Author": "ekzhu"}
{"issue_number": 6267, "issue_title": "Support embedding func in ChromaDB memory", "issue_body": "Current Status\nThe current implementation of ChromaDBVectorMemory in the AutoGen extension package doesn't expose parameters for setting custom embedding functions. It relies entirely on ChromaDB's default embedding function (Sentence Transformers all-MiniLM-L6-v2).\nGoal\nAllow users to customize the embedding function used by ChromaDBVectorMemory through a flexible, declarative configuration system that supports:\n\nDefault embedding function (current behavior)\nAlternative Sentence Transformer models\nOpenAI embeddings\nCustom user-defined embedding functions\n\nRough Sketch of an Implementation Plan\n1. Create Base Configuration Classes\nCreate a hierarchy of embedding function configurations:\nclass BaseEmbeddingFunctionConfig(BaseModel):\n    \"\"\"Base configuration for embedding functions.\"\"\"\n    function_type: Literal[\"default\", \"sentence_transformer\", \"openai\", \"custom\"]\n    \n\nclass DefaultEmbeddingFunctionConfig(BaseEmbeddingFunctionConfig):\n    \"\"\"Configuration for the default embedding function.\"\"\"\n    function_type: Literal[\"default\", \"sentence_transformer\", \"openai\", \"custom\"] = \"default\"\n\n\nclass SentenceTransformerEmbeddingFunctionConfig(BaseEmbeddingFunctionConfig):\n    \"\"\"Configuration for SentenceTransformer embedding functions.\"\"\"\n    function_type: Literal[\"default\", \"sentence_transformer\", \"openai\", \"custom\"] = \"sentence_transformer\"\n    model_name: str = Field(default=\"all-MiniLM-L6-v2\", description=\"Model name to use\")\n    \n\nclass OpenAIEmbeddingFunctionConfig(BaseEmbeddingFunctionConfig):\n    \"\"\"Configuration for OpenAI embedding functions.\"\"\"\n    function_type: Literal[\"default\", \"sentence_transformer\", \"openai\", \"custom\"] = \"openai\"\n    api_key: str = Field(default=\"\", description=\"OpenAI API key\")\n    model_name: str = Field(default=\"text-embedding-ada-002\", description=\"Model name\")\n2. Support Custom Embedding Functions\nAdd a configuration for custom embedding functions using the direct function approach:\nclass CustomEmbeddingFunctionConfig(BaseEmbeddingFunctionConfig):\n    \"\"\"Configuration for custom embedding functions.\"\"\"\n    function_type: Literal[\"default\", \"sentence_transformer\", \"openai\", \"custom\"] = \"custom\"\n    function: Callable[..., Any] = Field(description=\"Function that returns an embedding function\")\n    params: Dict[str, Any] = Field(default_factory=dict, description=\"Parameters\")\nNote: Using a direct function in the configuration will make it non-serializable. The implementation should include appropriate warnings when users attempt to serialize configurations that contain function references.\n3. Update ChromaDBVectorMemory Configuration\nExtend the existing ChromaDBVectorMemoryConfig class to include the embedding function configuration:\nclass ChromaDBVectorMemoryConfig(BaseModel):\n    # Existing fields...\n    embedding_function_config: BaseEmbeddingFunctionConfig = Field(\n        default_factory=DefaultEmbeddingFunctionConfig,\n        description=\"Configuration for the embedding function\"\n    )\n4. Implement Embedding Function Creation\nAdd a method to ChromaDBVectorMemory that creates embedding functions based on configuration:\ndef _create_embedding_function(self):\n    \"\"\"Create an embedding function based on the configuration.\"\"\"\n    from chromadb.utils import embedding_functions\n    \n    config = self._config.embedding_function_config\n    \n    if config.function_type == \"default\":\n        return embedding_functions.DefaultEmbeddingFunction()\n    \n    elif config.function_type == \"sentence_transformer\":\n        cfg = cast(SentenceTransformerEmbeddingFunctionConfig, config)\n        return embedding_functions.SentenceTransformerEmbeddingFunction(\n            model_name=cfg.model_name\n        )\n    \n    elif config.function_type == \"openai\":\n        cfg = cast(OpenAIEmbeddingFunctionConfig, config)\n        return embedding_functions.OpenAIEmbeddingFunction(\n            api_key=cfg.api_key,\n            model_name=cfg.model_name\n        )\n    \n    elif config.function_type == \"custom\":\n        cfg = cast(CustomEmbeddingFunctionConfig, config)\n        return cfg.function(**cfg.params)\n    \n    else:\n        raise ValueError(f\"Unsupported embedding function type: {config.function_type}\")\n5. Update Collection Initialization\nModify the _ensure_initialized method to use the embedding function:\ndef _ensure_initialized(self) -> None:\n    # ... existing client initialization code ...\n    \n    if self._collection is None:\n        try:\n            # Create embedding function\n            embedding_function = self._create_embedding_function()\n            \n            # Create or get collection with embedding function\n            self._collection = self._client.get_or_create_collection(\n                name=self._config.collection_name,\n                metadata={\"distance_metric\": self._config.distance_metric},\n                embedding_function=embedding_function\n            )\n        except Exception as e:\n            logger.error(f\"Failed to get/create collection: {e}\")\n            raise\nExample Usage\n# Using default embedding function\nmemory = ChromaDBVectorMemory(\n    config=PersistentChromaDBVectorMemoryConfig()\n)\n\n# Using a specific Sentence Transformer model\nmemory = ChromaDBVectorMemory(\n    config=PersistentChromaDBVectorMemoryConfig(\n        embedding_function_config=SentenceTransformerEmbeddingFunctionConfig(\n            model_name=\"paraphrase-multilingual-mpnet-base-v2\"\n        )\n    )\n)\n\n# Using OpenAI embeddings\nmemory = ChromaDBVectorMemory(\n    config=PersistentChromaDBVectorMemoryConfig(\n        embedding_function_config=OpenAIEmbeddingFunctionConfig(\n            api_key=\"sk-...\",\n            model_name=\"text-embedding-3-small\"\n        )\n    )\n)\n\n# Using a custom embedding function (direct function approach)\ndef create_my_embedder(param1=\"default\"):\n    # Return a ChromaDB-compatible embedding function\n    class MyCustomEmbeddingFunction(EmbeddingFunction):\n        def __call__(self, input: Documents) -> Embeddings:\n            # Custom embedding logic here\n            return embeddings\n    \n    return MyCustomEmbeddingFunction(param1)\n\nmemory = ChromaDBVectorMemory(\n    config=PersistentChromaDBVectorMemoryConfig(\n        embedding_function_config=CustomEmbeddingFunctionConfig(\n            function=create_my_embedder,\n            params={\"param1\": \"custom_value\"}\n        )\n    )\n)\n ", "created_at": "2025-04-10", "closed_at": null, "labels": ["help wanted", "proj-extensions"], "State": "open", "Author": "victordibia"}
{"issue_number": 6265, "issue_title": "ValueError: Dataclass has a union type, which is not supported. To use a union, use a Pydantic model", "issue_body": "What happened?\nDescribe the bug\nWhen I run the code in the cookbook:\n\nhttps://github.com/microsoft/autogen/blob/main/python/packages/autogen-core/docs/src/user-guide/core-user-guide/cookbook/llamaindex-agent.ipynb\nhttps://microsoft.github.io/autogen/stable/user-guide/core-user-guide/cookbook/llamaindex-agent.html\n\nOne error is encounted:\nValueError: Dataclass has a union type, which is not supported. To use a union, use a Pydantic model\nTo Reproduce\nSteps to reproduce the behavior. Please include code and outputs such as stacktrace.\nJust follow the cookbook, step by step, and you will see\nExpected behavior\nNo error\nScreenshots\n\nAdditional context\nautogen_core version: 0.5.1\nllama_index version: 0.12.28\nWhich packages was the bug in?\nPython Core (autogen-core)\nAutoGen library version.\nPython 0.5.1\nOther library version.\nNo response\nModel used\ngpt-4o\nModel provider\nAzure OpenAI\nOther model provider\nNo response\nPython version\n3.12\n.NET version\nNone\nOperating system\nMacOS", "created_at": "2025-04-10", "closed_at": "2025-04-10", "labels": ["needs-triage"], "State": "closed", "Author": "zwei25"}
{"issue_number": 6258, "issue_title": "Gemini as model client with autogen 0.5", "issue_body": "What happened?\nDescribe the bug\nI am referring to https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html#gemini-experimental but the documentation seems incomplete\nTo Reproduce\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nmodel_client = OpenAIChatCompletionClient(\n...     model=\"gemini-2.0-flash-lite\",\n...     api_key='<>',\n... )\n\n..._model_info.py\", line 314, in get_info\n    raise ValueError(\"model_info is required when model name is not a valid OpenAI model\")\nValueError: model_info is required when model name is not a valid OpenAI model\n\nHow do I get it to work with Gemini.\nI am using v0.5.1\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython 0.5.1\nOther library version.\nNo response\nModel used\ngemini-2.0-flash-lite\nModel provider\nGoogle Gemini\nOther model provider\nNo response\nPython version\n3.12\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-04-09", "closed_at": "2025-04-10", "labels": [], "State": "closed", "Author": "sonnylaskar"}
{"issue_number": 6255, "issue_title": "Facing Issue while working with Groq openAI compatible endpoint", "issue_body": "What happened?\nwhen trying structured output with groq OpenAI compatible apis getting error:\nTraceback (most recent call last):\n  File \"/Users/shubhamgilada/Developer/github/autogen_test/test_qroq.py\", line 46, in <module>\n    asyncio.run(_runner())\n  File \"/Users/shubhamgilada/miniconda/lib/python3.10/asyncio/runners.py\", line 44, in run\n    return loop.run_until_complete(main)\n  File \"/Users/shubhamgilada/miniconda/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\n    return future.result()\n  File \"/Users/shubhamgilada/Developer/github/autogen_test/test_qroq.py\", line 36, in _runner\n    response = await model_client.create(messages=messages)\n  File \"/Users/shubhamgilada/miniconda/lib/python3.10/site-packages/autogen_ext/models/openai/_openai_client.py\", line 622, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n  File \"/Users/shubhamgilada/miniconda/lib/python3.10/site-packages/openai/resources/beta/chat/completions.py\", line 437, in parse\n    return await self._post(\n  File \"/Users/shubhamgilada/miniconda/lib/python3.10/site-packages/openai/_base_client.py\", line 1767, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n  File \"/Users/shubhamgilada/miniconda/lib/python3.10/site-packages/openai/_base_client.py\", line 1461, in request\n    return await self._request(\n  File \"/Users/shubhamgilada/miniconda/lib/python3.10/site-packages/openai/_base_client.py\", line 1562, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}\n\n\ncode for reproducing:\nimport os\nimport asyncio\nfrom autogen_core.models import UserMessage\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom typing import Literal\nfrom pydantic import BaseModel\nfrom autogen_core.models import ModelFamily\n\napi_key = os.environ.get(\"GROQ_API_KEY\")\n\n\nclass AgentResponse(BaseModel):\n    thoughts: str\n    response: Literal[\"happy\", \"sad\", \"neutral\"]\n\n\nmodel_client = OpenAIChatCompletionClient(\n    base_url=\"https://api.groq.com/openai/v1\",\n    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n    api_key=api_key,\n    response_format=AgentResponse,\n    model_info={\n        \"vision\": False,\n        \"function_calling\": True,\n        \"json_output\": True,\n        \"structured_output\": True,\n        \"family\": ModelFamily.UNKNOWN,\n    },\n)\nmessages = [\n    UserMessage(content=\"I am happy.\", source=\"user\"),\n]\n\n\nasync def _runner():\n    response = await model_client.create(messages=messages)\n    assert isinstance(response.content, str)\n    parsed_response = AgentResponse.model_validate_json(response.content)\n    print(parsed_response.thoughts)\n    print(parsed_response.response)\n\n    # Close the connection to the model client.\n    await model_client.close()\n\n\nasyncio.run(_runner())\n\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython 0.5.1\nOther library version.\nNo response\nModel used\nmeta-llama/llama-4-scout-17b-16e-instruct\nModel provider\nOther (please specify below)\nOther model provider\ngroq\nPython version\n3.10\n.NET version\nNone\nOperating system\nMacOS", "created_at": "2025-04-09", "closed_at": null, "labels": ["needs-triage"], "State": "open", "Author": "gilada-shubham"}
{"issue_number": 6244, "issue_title": "Autogen dotnet installation guide contains reference to non-existent nuget package (Agents)", "issue_body": "What is the doc issue?\nDescribe the issue\nWhen you open the installation guide for AutoGen .NET, there's an instruction to install an Agents package.\ndotnet add package Microsoft.AutoGen.Agents --version 0.4.0-dev-1\nThis package does not exist on NuGet. I checked the release tags and saw that AutoGen dotnet is up to 0.4.0-dev-3, but there's also no existing package under that version.\nWhat do you want to see in the doc?\nRemoval of that line, some kind of indication that this package is no longer available, or a \"coming soon\" message. Depends on the situation.\nLink to the doc page, if applicable\nhttps://microsoft.github.io/autogen/dotnet/dev/core/installation.html", "created_at": "2025-04-08", "closed_at": null, "labels": ["documentation", "needs-triage"], "State": "open", "Author": "robertoost"}
{"issue_number": 6241, "issue_title": "ImportError: cannot import name 'IncludeEnum' from 'chromadb.api.types'", "issue_body": "What happened?\nI am trying to run the code provided in the notebook : https://github.com/microsoft/autogen/blob/22301614478469e74df410626eec03cca535b864/python/packages/autogen-core/docs/src/user-guide/agentchat-user-guide/memory.ipynb\nimport os\nfrom pathlib import Path\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.memory.chromadb import ChromaDBVectorMemory, PersistentChromaDBVectorMemoryConfig\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nGetting the below Error:\nImportError                               Traceback (most recent call last)\nCell In[1], line 6\n      4 from autogen_agentchat.agents import AssistantAgent\n      5 from autogen_agentchat.ui import Console\n----> 6 from autogen_ext.memory.chromadb import ChromaDBVectorMemory, PersistentChromaDBVectorMemoryConfig\n      7 from autogen_ext.models.openai import OpenAIChatCompletionClient\n      9 # Initialize vector memory\n\nFile ~/anaconda3/envs/autogen4/lib/python3.12/site-packages/autogen_ext/memory/chromadb.py:12\n     10 from chromadb.api import ClientAPI\n     11 from chromadb.api.models.Collection import Collection\n---> 12 from chromadb.api.types import Document, IncludeEnum, Metadata\n     13 from pydantic import BaseModel, Field\n     14 from typing_extensions import Self\n\nImportError: cannot import name 'IncludeEnum' from 'chromadb.api.types' (.. .. /anaconda3/envs/autogen4/lib/python3.12/site-packages/chromadb/api/types.py)\n\nWhich packages was the bug in?\nPython Ext (autogen-ext)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\n3.12\n.NET version\nNone\nOperating system\nUbuntu", "created_at": "2025-04-08", "closed_at": "2025-04-10", "labels": [], "State": "closed", "Author": "amd-srijaroy"}
{"issue_number": 6240, "issue_title": "Class AzureAISearchTool without an implementation for abstract method '_get_embedding'", "issue_body": "What happened?\nDescribe the bug\nTraceback (most recent call last):\nFile \"/Users/chenghaolu/workspace/cloud-lab-python/rag_demo_v1.py\", line 112, in \nvector_search = AzureAISearchTool.create_vector_search(\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/Users/chenghaolu/workspace/cloud-lab-python/.venv/lib/python3.12/site-packages/autogen_ext/tools/azure/_ai_search.py\", line 997, in create_vector_search\nreturn cls(\n^^^^\nTypeError: Can't instantiate abstract class AzureAISearchTool without an implementation for abstract method '_get_embedding'\nTo Reproduce\ncreate a index in azure aiseach\nupload document\nrun this code\nvector_search = AzureAISearchTool.create_vector_search(\n    name=\"vector_search\",\n    endpoint=\"\",\n    index_name=\"demo\",\n    credential=AzureKeyCredential(\"\"),\n    vector_fields=[\"embedding\"],\n    select_fields=[\"title\", \"content\", \"url\"],\n    top=5,\n)\n\nrag_assistant = AssistantAgent(\n    name=\"rag_assistant\", model_client=model_client, tools=[vector_search]\n)\n\nasyncio.run(rag_assistant.run(task=\"how to run apiserver locally?\", cancellation_token=None))\nExpected behavior\nMethod should be implemented\nScreenshots\nNone\nAdditional context\nNone\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython 0.5.1\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nAzure OpenAI\nOther model provider\nNo response\nPython version\n3.12\n.NET version\nNone\nOperating system\nMacOS", "created_at": "2025-04-08", "closed_at": "2025-04-09", "labels": ["needs-triage"], "State": "closed", "Author": "toughnoah"}
{"issue_number": 6235, "issue_title": "Magentic-One agents assume Linux shell on Windows \u2014 subprocess commands fail", "issue_body": "What happened?\nDescribe the bug\nWhen running Magentic-One (magentic-one-cli) on a Windows system, terminal-related agents (such as ComputerTerminal or CodeExecutorAgent) assume a Unix-like shell environment and attempt to run sh-style commands (like pip install, etc.) as if on Linux. As a result, shell subprocess calls consistently fail with: FileNotFoundError: [WinError 2] The system cannot find the file specified \nThis appears to be because shell script execution does not conditionally account for Windows environments. There is no fallback to PowerShell or CMD-style execution, nor any detection of the current OS.\nTo Reproduce\n\nInstall and configure magentic-one-cli on a Windows machine\nUse a prompt such as:\nm1 \"Convert all images in this folder to .png: C:\\Users\\Dell AI User\\Pictures\\Screenshots\"\nObserve that the orchestrator attempts to execute a Python-based image conversion script\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 user \u2500\u256e\n\u2502 Convert all images in this folder to .png: C:\\Users\\Dell AI User\\Pictures\\Screenshots                                                                                                                                                                                                                                                                                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 MagenticOneOrchestrator \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 We are working to address the following user request:                                                                                                                                                                                                                                                                                                                         \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 Convert all images in this folder to .png: C:\\Users\\Dell AI User\\Pictures\\Screenshots                                                                                                                                                                                                                                                                                         \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 To answer this request we have assembled the following team:                                                                                                                                                                                                                                                                                                                  \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 FileSurfer: An agent that can handle local files.                                                                                                                                                                                                                                                                                                                             \u2502\n\u2502 WebSurfer: A helpful assistant with access to a web browser. Ask them to perform web searches, open pages, and interact with content (e.g., clicking links, scrolling the viewport, filling in form fields, etc.). It can also summarize the entire page, or answer questions based on the content of the page. It can also be asked to sleep and wait for pages to load, in  \u2502\n\u2502 cases where the page seems not yet fully loaded.                                                                                                                                                                                                                                                                                                                              \u2502\n\u2502 Coder: A helpful and general-purpose AI assistant that has strong language skills, Python skills, and Linux command line skills.                                                                                                                                                                                                                                              \u2502\n\u2502 ComputerTerminal: A computer terminal that performs no other action than running Python scripts (provided to it quoted in ```python code blocks), or sh shell scripts (provided to it quoted in ```sh code blocks).                                                                                                                                                           \u2502\n\u2502 User: A human user                                                                                                                                                                                                                                                                                                                                                            \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 Here is an initial fact sheet to consider:                                                                                                                                                                                                                                                                                                                                    \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 1. GIVEN OR VERIFIED FACTS                                                                                                                                                                                                                                                                                                                                                    \u2502\n\u2502 - The folder path provided is: C:\\Users\\Dell AI User\\Pictures\\Screenshots                                                                                                                                                                                                                                                                                                     \u2502\n\u2502 - The task is to convert all images within this folder to the .png format.                                                                                                                                                                                                                                                                                                    \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 2. FACTS TO LOOK UP                                                                                                                                                                                                                                                                                                                                                           \u2502\n\u2502 - Specific file formats supported in the directory (to confirm they are not already .png).                                                                                                                                                                                                                                                                                    \u2502\n\u2502 - Available image conversion tools or libraries compatible with Windows OS that can handle batch processing.                                                                                                                                                                                                                                                                  \u2502\n\u2502 - File size and number of files in the target directory, if needed for planning.                                                                                                                                                                                                                                                                                              \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 3. FACTS TO DERIVE                                                                                                                                                                                                                                                                                                                                                            \u2502\n\u2502 - The number of images to be converted by counting all non-PNG image files within the specified folder.                                                                                                                                                                                                                                                                       \u2502\n\u2502 - Potential file naming conventions for output files (if not explicitly stated).                                                                                                                                                                                                                                                                                              \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 4. EDUCATED GUESSES                                                                                                                                                                                                                                                                                                                                                           \u2502\n\u2502 - Assuming that there are multiple image formats in the directory other than .png, and that these can be batch converted using a script or software tool.                                                                                                                                                                                                                     \u2502\n\u2502 - The conversion process might require additional steps such as handling metadata, compression settings, or color depth if these specifications are not detailed in the request.                                                                                                                                                                                              \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 Here is the plan to follow as best as possible:                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 - **Coder**: Write a Python script that uses libraries such as `Pillow` or `OpenCV` to convert images from various formats to `.png`. The script will iterate through the specified directory and handle each image file accordingly.                                                                                                                                         \u2502\n\u2502 - **ComputerTerminal**: Run the Python script provided by Coder, using a ```python code block``` to execute it in the terminal.                                                                                                                                                                                                                                               \u2502\n\u2502 - **FileSurfer**: Ensure that the target folder `C:\\Users\\Dell AI User\\Pictures\\Screenshots` is accessible and that all files within can be read and written.                                                                                                                                                                                                                 \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 **Short Bullet-point Plan:**                                                                                                                                                                                                                                                                                                                                                  \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 - Coder writes a Python script for image conversion.                                                                                                                                                                                                                                                                                                                          \u2502\n\u2502 - ComputerTerminal runs the provided Python script to convert images in the specified directory.                                                                                                                                                                                                                                                                              \u2502\n\u2502 - FileSurfer confirms accessibility of the folder and handles any file permissions issues if needed.                                                                                                                                                                                                                                                                          \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 MagenticOneOrchestrator \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Write a Python script that uses libraries such as Pillow or OpenCV to convert all images in the folder C:\\Users\\Dell AI User\\Pictures\\Screenshots to .png format.                                                                                                                                                                                                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Coder \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Sure, let's start by writing a Python script using the `Pillow` library to convert all images in the specified directory to `.png` format. Before running the script, we need to ensure that Pillow is installed on the system.                                                                                                                                               \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 First, we'll check if Pillow is installed and install it if necessary. Then, we will write the conversion script.                                                                                                                                                                                                                                                             \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 Here's how we can do it:                                                                                                                                                                                                                                                                                                                                                      \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 1. Check if Pillow is installed.                                                                                                                                                                                                                                                                                                                                              \u2502\n\u2502 2. Write a Python script to convert all images in the directory to `.png`.                                                                                                                                                                                                                                                                                                    \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 Let's start by checking if Pillow is installed and installing it if needed.                                                                                                                                                                                                                                                                                                   \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 ```sh                                                                                                                                                                                                                                                                                                                                                                         \u2502\n\u2502 pip install pillow                                                                                                                                                                                                                                                                                                                                                            \u2502\n\u2502 ```                                                                                                                                                                                                                                                                                                                                                                           \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 Now, let's write the Python script to perform the conversion:                                                                                                                                                                                                                                                                                                                 \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 ```python                                                                                                                                                                                                                                                                                                                                                                     \u2502\n\u2502 import os                                                                                                                                                                                                                                                                                                                                                                     \u2502\n\u2502 from PIL import Image                                                                                                                                                                                                                                                                                                                                                         \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 # Directory containing the images                                                                                                                                                                                                                                                                                                                                             \u2502\n\u2502 directory = r'C:\\Users\\Dell AI User\\Pictures\\Screenshots'                                                                                                                                                                                                                                                                                                                     \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 # Iterate over all files in the directory                                                                                                                                                                                                                                                                                                                                     \u2502\n\u2502 for filename in os.listdir(directory):                                                                                                                                                                                                                                                                                                                                        \u2502\n\u2502     # Construct full file path                                                                                                                                                                                                                                                                                                                                                \u2502\n\u2502     filepath = os.path.join(directory, filename)                                                                                                                                                                                                                                                                                                                              \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502     # Check if it's a file and not a directory                                                                                                                                                                                                                                                                                                                                \u2502\n\u2502     if os.path.isfile(filepath):                                                                                                                                                                                                                                                                                                                                              \u2502\n\u2502         try:                                                                                                                                                                                                                                                                                                                                                                  \u2502\n\u2502             # Open an image file                                                                                                                                                                                                                                                                                                                                              \u2502\n\u2502             with Image.open(filepath) as img:                                                                                                                                                                                                                                                                                                                                 \u2502\n\u2502                 # Define the new filename with .png extension                                                                                                                                                                                                                                                                                                                 \u2502\n\u2502                 base_filename = os.path.splitext(filename)[0]                                                                                                                                                                                                                                                                                                                 \u2502\n\u2502                 new_filepath = os.path.join(directory, f\"{base_filename}.png\")                                                                                                                                                                                                                                                                                                \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502                 # Save the image in PNG format                                                                                                                                                                                                                                                                                                                                \u2502\n\u2502                 img.save(new_filepath, 'PNG')                                                                                                                                                                                                                                                                                                                                 \u2502\n\u2502                 print(f\"Converted {filename} to {new_filepath}\")                                                                                                                                                                                                                                                                                                              \u2502\n\u2502         except Exception as e:                                                                                                                                                                                                                                                                                                                                                \u2502\n\u2502             print(f\"Error converting {filename}: {e}\")                                                                                                                                                                                                                                                                                                                        \u2502\n\u2502 ```                                                                                                                                                                                                                                                                                                                                                                           \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 This script will iterate through all files in the specified directory, attempt to open them as images using Pillow, and save them with a `.png` extension. It also handles exceptions that might occur during the conversion process.                                                                                                                                         \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 Please run this Python script on your system by copying it into a ```python code block``` for execution.                                                                                                                                                                                                                                                                      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 MagenticOneOrchestrator \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Run the following Python script to convert all images in the directory C:\\Users\\Dell AI User\\Pictures\\Screenshots to .png format:                                                                                                                                                                                                                                             \u2502\n\u2502                                                                                                                                                                                                                                                                                                                                                                               \u2502\n\u2502 ```python                                                                                                                                                                                                                                                                                                                                                                     \u2502\n\u2502 import os                                                                                                                                                                                                                                                                                                                                                                     \u2502\n\u2502 from PIL import Image                                                                                                                                                                                                                                                                                                                                                         \u2502\n\u2502 directory = r'C:\\\\Users\\\\Dell AI User\\\\Pictures\\\\Screenshots'                                                                                                                                                                                                                                                                                                                 \u2502\n\u2502 for filename in os.listdir(directory):                                                                                                                                                                                                                                                                                                                                        \u2502\n\u2502     filepath = os.path.join(directory, filename)                                                                                                                                                                                                                                                                                                                              \u2502\n\u2502     if os.path.isfile(filepath):                                                                                                                                                                                                                                                                                                                                              \u2502\n\u2502         try:                                                                                                                                                                                                                                                                                                                                                                  \u2502\n\u2502             with Image.open(filepath) as img:                                                                                                                                                                                                                                                                                                                                 \u2502\n\u2502                 base_filename = os.path.splitext(filename)[0]                                                                                                                                                                                                                                                                                                                 \u2502\n\u2502                 new_filepath = os.path.join(directory, f\"{base_filename}.png\")                                                                                                                                                                                                                                                                                                \u2502\n\u2502                 img.save(new_filepath, 'PNG')                                                                                                                                                                                                                                                                                                                                 \u2502\n\u2502                 print(f'Successfully converted {filename} to PNG.')                                                                                                                                                                                                                                                                                                           \u2502\n\u2502         except Exception as e:                                                                                                                                                                                                                                                                                                                                                \u2502\n\u2502             print(f'Failed to convert {filename}: {e}')                                                                                                                                                                                                                                                                                                                       \u2502\n\u2502 ```                                                                                                                                                                                                                                                                                                                                                                           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nError processing publish message for ComputerTerminal_affc2839-bf41-4a82-ae36-0fb30f3302b2/affc2839-bf41-4a82-ae36-0fb30f3302b2\nTraceback (most recent call last):\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 533, in _on_message\n    return await agent.on_message(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 113, in on_message\n    return await self.on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 67, in on_message_impl\n    return await super().on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 485, in on_message_impl\n    return await h(self, message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 268, in wrapper\n    return_value = await func(self, message, ctx)  # type: ignore\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 79, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_agentchat\\agents\\_base_chat_agent.py\", line 101, in on_messages_stream\n    response = await self.on_messages(messages, cancellation_token)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_agentchat\\agents\\_code_executor_agent.py\", line 135, in on_messages\n    result = await self._code_executor.execute_code_blocks(code_blocks, cancellation_token=cancellation_token)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_ext\\code_executors\\local\\__init__.py\", line 323, in execute_code_blocks\n    return await self._execute_code_dont_check_setup(code_blocks, cancellation_token)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_ext\\code_executors\\local\\__init__.py\", line 424, in _execute_code_dont_check_setup\n    proc = await task\n           ^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\asyncio\\subprocess.py\", line 223, in create_subprocess_exec\n    transport, protocol = await loop.subprocess_exec(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\asyncio\\base_events.py\", line 1708, in subprocess_exec\n    transport = await self._make_subprocess_transport(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\asyncio\\windows_events.py\", line 399, in _make_subprocess_transport\n    transp = _WindowsSubprocessTransport(self, protocol, args, shell,\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\asyncio\\base_subprocess.py\", line 36, in __init__\n    self._start(args=args, shell=shell, stdin=stdin, stdout=stdout,\n  File \"C:\\Users\\Dell AI User\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\asyncio\\windows_events.py\", line 929, in _start\n    self._proc = windows_utils.Popen(\n                 ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\asyncio\\windows_utils.py\", line 153, in __init__\n    super().__init__(args, stdin=stdin_rfd, stdout=stdout_wfd,\n  File \"C:\\Users\\Dell AI User\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\subprocess.py\", line 1026, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"C:\\Users\\Dell AI User\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\subprocess.py\", line 1538, in _execute_child\n    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [WinError 2] The system cannot find the file specified\nError processing publish message for FileSurfer_affc2839-bf41-4a82-ae36-0fb30f3302b2/affc2839-bf41-4a82-ae36-0fb30f3302b2\nTraceback (most recent call last):\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 533, in _on_message\n    return await agent.on_message(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 113, in on_message\n    return await self.on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 72, in on_message_impl\n    return await super().on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 486, in on_message_impl\n    return await self.on_unhandled_message(message, ctx)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 133, in on_unhandled_message\n    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\nValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\nError processing publish message for WebSurfer_affc2839-bf41-4a82-ae36-0fb30f3302b2/affc2839-bf41-4a82-ae36-0fb30f3302b2\nTraceback (most recent call last):\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 533, in _on_message\n    return await agent.on_message(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 113, in on_message\n    return await self.on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 72, in on_message_impl\n    return await super().on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 486, in on_message_impl\n    return await self.on_unhandled_message(message, ctx)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 133, in on_unhandled_message\n    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\nValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\nError processing publish message for Coder_affc2839-bf41-4a82-ae36-0fb30f3302b2/affc2839-bf41-4a82-ae36-0fb30f3302b2\nTraceback (most recent call last):\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 533, in _on_message\n    return await agent.on_message(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 113, in on_message\n    return await self.on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 72, in on_message_impl\n    return await super().on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 486, in on_message_impl\n    return await self.on_unhandled_message(message, ctx)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 133, in on_unhandled_message\n    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\nValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\nError processing publish message for User_affc2839-bf41-4a82-ae36-0fb30f3302b2/affc2839-bf41-4a82-ae36-0fb30f3302b2\nTraceback (most recent call last):\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 533, in _on_message\n    return await agent.on_message(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 113, in on_message\n    return await self.on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 72, in on_message_impl\n    return await super().on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 486, in on_message_impl\n    return await self.on_unhandled_message(message, ctx)  # type: ignore\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 133, in on_unhandled_message\n    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\nValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\nTraceback (most recent call last):\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen\\python\\packages\\magentic-one-cli\\src\\magentic_one_cli\\_m1.py\", line 145, in <module>\n    main()\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen\\python\\packages\\magentic-one-cli\\src\\magentic_one_cli\\_m1.py\", line 142, in main\n    asyncio.run(run_task(task, not args.no_hil, args.rich, args.executor))\n  File \"C:\\Users\\Dell AI User\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\asyncio\\runners.py\", line 190, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\asyncio\\runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\asyncio\\base_events.py\", line 654, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen\\python\\packages\\magentic-one-cli\\src\\magentic_one_cli\\_m1.py\", line 135, in run_task\n    await RichConsole(m1.run_stream(task=task), output_stats=False, user_input_manager=input_manager)\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_ext\\ui\\_rich_console.py\", line 136, in RichConsole\n    async for message in stream:\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat.py\", line 503, in run_stream\n    raise RuntimeError(str(message.error))\nRuntimeError: FileNotFoundError: [WinError 2] The system cannot find the file specified\nTraceback:\nTraceback (most recent call last):\n\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 79, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_agentchat\\agents\\_base_chat_agent.py\", line 101, in on_messages_stream\n    response = await self.on_messages(messages, cancellation_token)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_agentchat\\agents\\_code_executor_agent.py\", line 135, in on_messages\n    result = await self._code_executor.execute_code_blocks(code_blocks, cancellation_token=cancellation_token)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_ext\\code_executors\\local\\__init__.py\", line 323, in execute_code_blocks\n    return await self._execute_code_dont_check_setup(code_blocks, cancellation_token)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\Dell AI User\\Documents\\autogen\\autogen-v0.5.1\\Lib\\site-packages\\autogen_ext\\code_executors\\local\\__init__.py\", line 424, in _execute_code_dont_check_setup\n    proc = await task\n           ^^^^^^^^^^\n\n  File \"C:\\Users\\Dell AI User\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\asyncio\\subprocess.py\", line 223, in create_subprocess_exec\n    transport, protocol = await loop.subprocess_exec(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\Dell AI User\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\asyncio\\base_events.py\", line 1708, in subprocess_exec\n    transport = await self._make_subprocess_transport(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\Dell AI User\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\asyncio\\windows_events.py\", line 399, in _make_subprocess_transport\n    transp = _WindowsSubprocessTransport(self, protocol, args, shell,\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\Dell AI User\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\asyncio\\base_subprocess.py\", line 36, in __init__\n    self._start(args=args, shell=shell, stdin=stdin, stdout=stdout,\n\n  File \"C:\\Users\\Dell AI User\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\asyncio\\windows_events.py\", line 929, in _start\n    self._proc = windows_utils.Popen(\n                 ^^^^^^^^^^^^^^^^^^^^\n\n  File \"C:\\Users\\Dell AI User\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\asyncio\\windows_utils.py\", line 153, in __init__\n    super().__init__(args, stdin=stdin_rfd, stdout=stdout_wfd,\n\n  File \"C:\\Users\\Dell AI User\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\subprocess.py\", line 1026, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n\n  File \"C:\\Users\\Dell AI User\\AppData\\Roaming\\uv\\python\\cpython-3.11.11-windows-x86_64-none\\Lib\\subprocess.py\", line 1538, in _execute_child\n    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n\nExpected behavior\nTerminal agents should detect the platform (sys.platform, os.name, or platform.system()), and:\n\nUse powershell.exe or cmd.exe on Windows\nUse sh or bash only on Unix-like systems\n\nPython subprocesses should not default to shell=True without specifying a valid executable on Windows.\nSuggested Fix\nAdd OS detection in the code executor and update these prompts:\nCoder: A helpful and general-purpose AI assistant that has strong language skills, Python skills, and Linux command line skills.\nComputerTerminal: A computer terminal that performs no other action than running Python scripts (provided to it quoted in python code blocks), or sh shell scripts (provided to it quoted in sh code blocks).\nWhich packages was the bug in?\nPython Core (autogen-core), Python AgentChat (autogen-agentchat>=0.4.0), Python Extensions (autogen-ext), Magentic One CLI (magentic-one-cli)\nAutoGen library version.\nPython 0.5.1\nOther library version.\nNo response\nModel used\nqwen2.5-coder:32b-instruct-q2_K\nModel provider\nOllama\nOther model provider\nNo response\nPython version\n3.11\n.NET version\nNone\nOperating system\nWindows", "created_at": "2025-04-07", "closed_at": null, "labels": ["needs-triage"], "State": "open", "Author": "zytoh0"}
{"issue_number": 6234, "issue_title": "FileSurfer in Magentic-One no longer able to find or list files after 0.5.1 update. Keeps returning FileNotFoundError", "issue_body": "What happened?\nDescribe the bug\nAfter upgrading to python-v0.5.1, FileSurfer can no longer locate or read local files that exist. Attempting to access a simple test folder causes the agent to return FileNotFoundError, despite the folder being valid and files existing and accessible via Explorer or standard Python libraries.\n0.4.9 works\n\nAfter 0.5.1 update\n\nTo Reproduce\nSteps to reproduce:\n\nCreate a folder locally at C:\\Users\\User\\Documents\\test\nPlace any test file inside (e.g., example.txt)\nRun the following Magentic-One CLI command: m1 \"Ask FileSurfer to find out the contents of this folder: C:\\Users\\User\\Documents\\test\"\nOutput:\n\n---------- FileSurfer ----------\nPath: C:\\Users\\User\\Documents\\test\nTitle: FileNotFoundError\nViewport position: Showing page 1 of 1.\n=======================\n# FileNotFoundError\n\nFile not found: C:\\Users\\User\\Documents\\test\n\nExpected behavior\nList of files in the directory\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython 0.5.1\nOther library version.\nNo response\nModel used\nqwen2.5-coder:32b-instruct-q2_K\nModel provider\nOllama\nOther model provider\nNo response\nPython version\n3.11\n.NET version\nNone\nOperating system\nWindows", "created_at": "2025-04-07", "closed_at": null, "labels": ["needs-triage"], "State": "open", "Author": "zytoh0"}
{"issue_number": 6232, "issue_title": "Github MCP tools Doesn't work for write operations with Autogen", "issue_body": "What happened?\nIssue: MCP Server Not Working with Autogen\nDescription:\nI have been trying to use the MCP server with autogen. I have setup this project and also tried using the latest autogen release for using the tools in the Github server with autogen\nBut it seems that the server is not working correctly when attempting to use it with autogen only for write operations. It seems to work perfectly fine for the tools performing read operations like (list_issues, get_file_content), but throws an error for tools like (push_tool, create_repository, fork_repository).\nError\n[FunctionExecutionResult(content='Error: unhandled errors in a TaskGroup (1 sub-exception)', call_id='call_m9AeqiLyBeaCtLdRH4X0OExg')]\n---------- mcp_tools_agent ----------\nError: unhandled errors in a TaskGroup (1 sub-exception)\nThe error message I receive is frequent and repeats the same error each time. I have verified that the configuration is set correctly and my Github token has all the required permissions.\nConfiguration:\ngithub_mcp_server = StdioServerParams(\ncommand=npx_command,\nargs=[\n\"-y\",\n\"@modelcontextprotocol/server-github\",\n\"--token=***********\",\n\"--debug\"\n],\n)\nI have also given the github token in the .env file as well\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\ngpt-4o-mini\nModel provider\nOpenAI\nOther model provider\nNo response\nPython version\n3.11\n.NET version\nNone\nOperating system\nUbuntu", "created_at": "2025-04-07", "closed_at": "2025-04-17", "labels": ["needs-triage"], "State": "closed", "Author": "Sujith-Srinivas"}
{"issue_number": 6227, "issue_title": "Add `MessageStore` base class abstraction for storing message thread in teams", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nAdd a simple storage abstraction for message_thread, and TTL policy could be a parameter for an implementation of this storage abstraction.", "created_at": "2025-04-06", "closed_at": null, "labels": ["help wanted", "proj-agentchat"], "State": "open", "Author": "ekzhu"}
{"issue_number": 6210, "issue_title": "Update GraphRAG version", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nGraphRAG made some API changes since 1.2.0. We need to catch up with the latest version.\nAlso update the autogen version in the sample: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_graphrag/requirements.txt\nRelated: #6201", "created_at": "2025-04-04", "closed_at": null, "labels": ["help wanted", "proj-extensions"], "State": "open", "Author": "ekzhu"}
{"issue_number": 6207, "issue_title": "Self-debugging in `CodeExecutionAgent`", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nFollow up to #6098, add auto-debugging loop to CodeExecutionAgent so it automatically tries to regenerate code when there is error.", "created_at": "2025-04-04", "closed_at": "2025-04-22", "labels": ["code-execution", "proj-agentchat"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 6206, "issue_title": "AutoGen Studio is **broken** due to a missing message class (LLMCallEventMessage) (v0.5+)", "issue_body": "What happened?\nDescribe the bug\nAutoGen Studio currently fails to run any agent that emits LLMCallEventMessage, because that class is abstract and not implemented.\nThis causes the following runtime error:\nTypeError: Can\u2019t instantiate abstract class LLMCallEventMessage with abstract methods to_model_message, to_model_text, to_text\nThis was reported by a user on Discord:\n\n\u0b9aravana\u0ba9\u0bcd \u2014 6:40 PM\n\u201ci updated agentchat and agentcore and tried running the config from agentstudio and it is now not running the agent and is throwing error ...\u201d\n\nTo Reproduce\nSteps:\n\nUse AgentStudio (v0.4.x from PyPI or v0.5.x from Githun repo too)\nAdd an agent that emits LLMCallEventMessage - Any agent\nRun\n\u2192 Error will occur immediately, breaking the execution.\n\nExpected behavior\nAgent should run without error. LLMCallEventMessage is meant for logging, not model input, so it should be safely handled.\nFix\nFix is proposed in PR: #6204\nThis defines the class with to_model_message() marked as NotImplementedError.\nAdditional context\n\nThis issue blocks AgentStudio for many use cases using latest AgentCore.\nQuick merge & release would help users avoid this broken state.\n\nScreen shot\n\nWhich packages was the bug in?\nAutoGen Studio (autogensudio)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-04-04", "closed_at": "2025-04-04", "labels": ["needs-triage"], "State": "closed", "Author": "SongChiYoung"}
{"issue_number": 6202, "issue_title": "Calling tools through MCP or Autogen's Function Tool timeout setting", "issue_body": "What happened?\nDescribe the bug\nI have a python function tool that calls an image generation api which takes around 20-30 seconds to generate an image. When I implement this function with FunctionTool the agent works ok and waits for the response\nWhen I import this tool with autogen's mcp extension the function has a 5 second timeout and fails to get the response ( the request is sent to the image generation api though)\nTo Reproduce\nMCP tool code\nimport json\nimport os\n\nimport httpx\nimport PIL\nimport requests\nfrom dotenv import load_dotenv\nfrom mcp.server.fastmcp import FastMCP\n\nload_dotenv()\n\nmcp = FastMCP(\"Google Image Generation\")\n\nIMAGEN_URL = os.getenv(\"IMAGEN_URL\")\n@mcp.tool(\n    name=\"generate_images\",\n    description=\"Generates an image with Google Pixel 8 smartphone\",\n)\nasync def google_generate_image(\n    prompt: str,\n    seed: int = -1,\n    num_images: int = 1,\n    negative_prompt: str = \"\",\n) -> PIL.Image:\n    \"\"\"Generates an image with based on a text prompt.\n    It takes a text prompt as input and returns a url representing the generated image.\n\n    Args:\n        prompt: A string containing the text prompt that describes the desired image. This prompt is used\n            as the basis for the image generation process.  The more descriptive the prompt, the more\n            likely the generated image will match the desired output.\n         seed: An integer representing the seed value for the random number generator.  If a seed value is provided,\n             the generated image will be deterministic and reproducible.  If no seed value is provided, the image\n             generation process will be non-deterministic\n         num_images: An integer representing the number of images to generate based on the prompt.  The function\n             will generate multiple images based on the prompt, with each image representing a different interpretation\n             of the prompt.\n         negative_prompt: A string containing a negative prompt that describes the undesired aspects of the image.\n\n    Returns:\n        A signed url of the generated image.\n    \"\"\"\n    headers = {\n        \"accept\": \"application/json\",\n        \"X-API-Key\": os.getenv(\"API_KEY\", \"\"),\n        \"Content-Type\": \"application/json\",\n    }\n    data = {\n        \"seed\": seed,\n        \"prompt\": prompt,\n        \"negative_prompt\": negative_prompt,\n        \"num_images_per_prompt\": num_images,\n    }\n\n    try:\n        response = requests.post(\n            IMAGEN_URL,\n            headers=headers,\n            json=data,\n            timeout=240,\n        )  # Use json= for data\n        # print(response)\n        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n\n        # Assuming the API returns a JSON response with a list of image data (URLs)\n        response_json = response.json()\n\n        # Check if the response contains a 'generated_image_list' key\n        if \"generated_image_list\" in response_json:\n            # Assuming 'generated_image_list' contains a list of URLs\n            return response_json[\"generated_image_list\"][0]\n        else:\n            print(\"Error: 'generated_image_list' key not found in the API response.\")\n            # Print the full response\n            print(f\"Response content: {response.text}\")\n            return \"Error: 'generated_image_list' key not found in the API response\"\n\n    except httpx.HTTPStatusError as e:\n        print(f\"HTTP error: {e}\")\n        print(f\"Response content: {e.response.text}\")\n        return f\"HTTP error: {e}\"\n    except httpx.RequestError as e:\n        print(f\"Request error: {e}\")\n        return None\n    except json.JSONDecodeError as e:\n        print(f\"JSON decode error: {e}\")\n        try:\n            print(\n                f\"Response content: {response.text if response else 'No response'}\",\n            )\n        except NameError:\n            print(\"No response to decode.\")\n        return f\"JSON decode error: {e}\"\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n        return f\"An unexpected error occurred: {e}\"\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"stdio\")\nAgent code\nimport os\nfrom pathlib import Path\n\n\nfrom autogen_ext.tools.mcp import mcp_server_tools\nfrom autogen_ext.tools.mcp import StdioServerParams\n\n\nimport src.tools.mcp.google.compliance_check  # noqa: F401\nimport src.tools.mcp.google.image_generation  # noqa: F401\nimport asyncio\nimport os\n\n\nfrom autogen_agentchat.agents import UserProxyAgent, AssistantAgent\n\nfrom autogen_agentchat.ui import Console\n\nfrom autogen_core import CancellationToken\n\n\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def imagen_agent():\n\n    imagen_server = StdioServerParams(\n        command=\"uv\",\n        args=[\n            \"--directory\",\n            f\"{Path(src.tools.mcp.google.image_generation.__file__).parent}/\",\n            \"run\",\n            f\"{Path(src.tools.mcp.google.image_generation.__file__).name}\",\n        ],\n    )\n\n    imagen_tools = await mcp_server_tools(imagen_server)\n\n    # Create an agent that can use the fetch tool.\n    model_client = OpenAIChatCompletionClient(\n        model=\"gpt-4o\",\n        api_key=os.getenv(\"OPENAI_API_KEY\"),\n    )\n    image_generation_agent = AssistantAgent(\n        name=\"image_generation_agent\",\n        model_client=model_client,\n        tools=imagen_tools,\n        system_message=\"You  are an image generation AI model\",\n        reflect_on_tool_use=True,\n    )\n    return image_generation_agent\nasync def main():\n    agent = await imagen_agent()\n    await Console(\n        agent.run_stream(\n            task=\"Generate an image of a cat\",\n            cancellation_token=CancellationToken(),\n        ),\n    )\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main()) \n\nResponse\n---------- image_generation_agent ----------\n[FunctionCall(id='call_KRrSPst82BWJXLxx8PD5bwAx', arguments='{\"prompt\":\"a cat\",\"num_images\":1}', name='generate_images')]\n[04/04/25 11:30:52] INFO     Processing request of type            server.py:534\n                             CallToolRequest                                    \n[04/04/25 11:30:57] INFO     Publishing    _single_threaded_agent_runtime.py:352\n                             message of                                         \n                             type                                               \n                             GroupChatMess                                      \n                             age to all                                         \n                             subscribers:                                       \n                             {'message':                                        \n                             ToolCallExecu                                      \n                             tionEvent(sou                                      \n                             rce='image_ge                                      \n                             neration_agen                                      \n                             t',                                                \n                             models_usage=                                      \n                             None,                                              \n                             metadata={},                                       \n                             content=[Func                                      \n                             tionExecution                                      \n                             Result(conten                                      \n                             t='Error:                                          \n                             Timed out                                          \n                             while waiting                                      \n                             for response                                       \n                             to                                                 \n                             ClientRequest                                      \n                             . Waited                                           \n                             0:00:05                                            \n                             seconds.\\n',                                       \n                             name='generat                                      \n                             e_images',                                         \n                             call_id='call                                      \n                             _KRrSPst82BWJ                                      \n                             XLxx8PD5bwAx'                                      \n                             ,                                                  \n                             is_error=True                                      \n                             )],                                                \n                             type='ToolCal                                      \n                             lExecutionEve                                      \n                             nt')}                                              \n                    INFO     {\"payload\":   _single_threaded_agent_runtime.py:357\n                             \"{\\\"message\\\"                                      \n                             :{\\\"source\\\":                                      \n                             \\\"image_gener                                      \n                             ation_agent\\\"                                      \n                             ,\\\"models_usa                                      \n                             ge\\\":null,\\\"m                                      \n                             etadata\\\":{},                                      \n                             \\\"content\\\":[                                      \n                             {\\\"content\\\":                                      \n                             \\\"Error:                                           \n                             Timed out                                          \n                             while waiting                                      \n                             for response                                       \n                             to                                                 \n                             ClientRequest                                      \n                             . Waited                                           \n                             0:00:05                                            \n                             seconds.\\\\n\\\"                                      \n                             ,\\\"name\\\":\\\"g                                      \n                             enerate_image                                      \n                             s\\\",\\\"call_id                                      \n                             \\\":\\\"call_KRr                                      \n                             SPst82BWJXLxx                                      \n                             8PD5bwAx\\\",\\\"                                      \n                             is_error\\\":tr                                      \n                             ue}],\\\"type\\\"                                      \n                             :\\\"ToolCallEx                                      \n                             ecutionEvent\\                                      \n                             \"}}\",                                              \n                             \"sender\":                                          \n                             \"image_genera                                      \n                             tion_agent_0a                                      \n                             a1a216-ea8e-4                                      \n                             4f2-a6d9-b020                                      \n                             b58dfab5/0aa1                                      \n                             a216-ea8e-44f                                      \n                             2-a6d9-b020b5                                      \n                             8dfab5\",                                           \n                             \"receiver\":                                        \n                             \"output_topic                                      \n                             _0aa1a216-ea8                                      \n                             e-44f2-a6d9-b                                      \n                             020b58dfab5/0                                      \n                             aa1a216-ea8e-                                      \n                             44f2-a6d9-b02                                      \n                             0b58dfab5\",                                        \n                             \"kind\":                                            \n                             \"MessageKind.                                      \n                             PUBLISH\",                                          \n                             \"delivery_sta                                      \n                             ge\":                                               \n                             \"DeliveryStag                                      \n                             e.SEND\",                                           \n                             \"type\":                                            \n                             \"Message\"}           \n\n[FunctionExecutionResult(content='Error: Timed out while waiting for response to ClientRequest. Waited 0:00:05 seconds.\\n', name='generate_images', call_id='call_cUdHaKIeC29gcD4lASzAT0Gr', is_error=True)]\nIs there a way to control the timeout when using mcp tools with autogen's extension ?\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0), Python Extensions (autogen-ext)\nAutoGen library version.\nPython 0.4.9\nOther library version.\nNo response\nModel used\ngpt-4o\nModel provider\nOpenAI\nOther model provider\nNo response\nPython version\n3.12\n.NET version\nNone\nOperating system\nMacOS", "created_at": "2025-04-04", "closed_at": "2025-04-07", "labels": ["needs-triage"], "State": "closed", "Author": "iliasprc"}
{"issue_number": 6201, "issue_title": "run Autogen and GraphRAG sample, report error: ModuleNotFoundError: No module named 'graphrag.config.config_file_loader'", "issue_body": "What happened?\nDescribe the bug\nI want to run the sample followed: https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_graphrag/README.md,\nafter completed the setup,\nwhen run the command: python app.py\nencounter the error:\n(autogen) C:\\Users\\xxx\\autogen\\autogen\\python\\samples\\agentchat_graphrag>python app.py\nTraceback (most recent call last):\n  File \"C:\\Users\\xxx\\autogen\\autogen\\python\\samples\\agentchat_graphrag\\app.py\", line 10, in <module>\n    from autogen_ext.tools.graphrag import (\n  File \"C:\\Users\\xxx\\AppData\\Local\\miniconda3\\envs\\autogen\\Lib\\site-packages\\autogen_ext\\tools\\graphrag\\__init__.py\", line 9, in <module>\n    from ._global_search import GlobalSearchTool, GlobalSearchToolArgs, GlobalSearchToolReturn\n  File \"C:\\Users\\xxx\\AppData\\Local\\miniconda3\\envs\\autogen\\Lib\\site-packages\\autogen_ext\\tools\\graphrag\\_global_search.py\", line 8, in <module>\n    from graphrag.config.config_file_loader import load_config_from_file\nModuleNotFoundError: No module named 'graphrag.config.config_file_loader'\n\nTo Reproduce\n\nconda create -n test python=3.12 -y\nconda activate test\nin dir autogen\\python\\samples\\agentchat_graphrag, pip install -r requirements.txt\nrun the setup steps\npython app.py\n\nautogen\\python\\samples\\agentchat_graphrag>conda list graphrag\npackages in environment at C:\\Users\\xxx\\AppData\\Local\\miniconda3\\envs\\test:\n\nName                    Version                   Build  Channel\ngraphrag                  2.1.0                    pypi_0    pypi\nExpected behavior\nA clear and concise description of what you expected to happen.\n\nScreenshots\nIf applicable, add screenshots to help explain your problem.\nAdditional context\nAdd any other context about the problem here.\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0), Python Extensions (autogen-ext)\nAutoGen library version.\nPython 0.4.9\nOther library version.\nautogen-ext  0.4.9.3;  graphrag  2.1.0\nModel used\ngpt-4o-2024-08-06\nModel provider\nAzure OpenAI\nOther model provider\nNo response\nPython version\n3.12\n.NET version\nNone\nOperating system\nWindows", "created_at": "2025-04-04", "closed_at": null, "labels": ["help wanted", "proj-extensions"], "State": "open", "Author": "cyberflying"}
{"issue_number": 6198, "issue_title": "`McpToolAdapter` Connects to the MCP Server in a Stateless Mode", "issue_body": "Bug Description\nI create an AssistantAgent that can use tools from the Playwright MCP server. Then, I wrap the AssistantAgent into a RoundRobinGroupChat team to enable consecutive tool invocations. However, I find that the AssistantAgent can only successfully invoke the browser_navigate tool from the Playwright MCP server, but other tool invocations (e.g., browser_save_as_pdf) return execution errors. It seems that the bug stems from the following implementation: \n\n\nautogen/python/packages/autogen-ext/src/autogen_ext/tools/mcp/_base.py\n\n\n        Lines 65 to 66\n      in\n      b62b12e\n\n\n\n\n\n\n async with create_mcp_server_session(self._server_params) as session: \n\n\n\n await session.initialize() \n\n\n\n\n At each time the AssistantAgent invokes a MCP tool, the McpToolAdapter connects to the MCP server from scratch without maintaining previous connections.\nTo Reproduce\nI use the following code to build an AssistantAgent and a RoundRobinGroupChat team.\nimport asyncio\nfrom autogen_ext.tools.mcp import StdioServerParams, mcp_server_tools\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom generation_security.autogen.web_surfer import load_model_client\nimport argparse\nfrom autogen_agentchat.ui import Console\n\n\nasync def main() -> None:\n    model_client=OpenAIChatCompletionClient(model=\"gpt-4\")\n    server_params = StdioServerParams(\n        command=\"npx\",\n        args=[\"@playwright/mcp@latest\", \"--headless\", \"--executable-path\", \"/home/user/.cache/ms-playwright/chromium-1161/chrome-linux/chrome\"],\n    )\n    server_params.read_timeout_seconds = 120\n    tools = await mcp_server_tools(server_params)\n\n    agent = AssistantAgent(\n        name=\"WebSurfer\",\n        model_client=model_client,\n        tools=tools,\n        reflect_on_tool_use=False,\n    )\n    task_content = \"Save the website https://nesa.zju.edu.cn/ as a PDF.\"\n    team = RoundRobinGroupChat([agent], termination_condition=TextMentionTermination(\"TERMINATE\"), max_turns=10)\n    await Console(team.run_stream(task=task_content))\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\nRunning the above code, I get the following results. The browser_save_as_pdf tool returns an error indicating the variable this._page created by the browser_navigate tool is missing. This means that each tool invocation by the AssistantAgent reinitializes the MCP server, making the variable this._page not consistently maintained in consecutive tool invocations.\n---------- user ----------\nSave the website https://nesa.zju.edu.cn/ as a PDF.\n---------- WebSurfer ----------\n[FunctionCall(id='call_8b5f31fc16db46daa2e863', arguments='{\"url\": \"https://nesa.zju.edu.cn/\"}', name='browser_navigate')]\n---------- WebSurfer ----------\n[FunctionExecutionResult(content='[TextContent(type=\\'text\\', text=\\'\\\\n- Page URL: https://nesa.zju.edu.cn/\\\\n- Page Title: Index - NESA - Network System Security & Privacy Lab\\\\n- Page Snapshot\\\\n```yaml\\\\n- link [ref=s1e4]:\\\\n    - /url: index.html\\\\n- list [ref=s1e6]:\\\\n    - listitem [ref=s1e7]:\\\\n        - link \"home\" [ref=s1e8]:\\\\n            - /url: index.html\\\\n    - listitem [ref=s1e9]:\\\\n        - link \"news\" [ref=s1e10]:\\\\n            - /url: webpage/news.html\\\\n    - listitem [ref=s1e11]:\\\\n        - link \"Research\" [ref=s1e12]:\\\\n            - /url: webpage/research.html\\\\n    - listitem [ref=s1e13]:\\\\n        - link \"publication\" [ref=s1e14]:\\\\n            - /url: webpage/pubyear.html\\\\n    - listitem [ref=s1e15]:\\\\n        - link \"Software\" [ref=s1e16]:\\\\n            - /url: webpage/software.html\\\\n    - listitem [ref=s1e17]:\\\\n        - link \"contact\" [ref=s1e18]:\\\\n            - /url: webpage/contact.html\\\\n- heading \"Welcome to NESA\uff01\" [level=1] [ref=s1e22]\\\\n- paragraph [ref=s1e23]: \"The Network SystEm Security & PrivAcy (NESA) Research\\\\n    Lab, directed by Dr. Shouling Ji, works at the intersection of AI, network\\\\n    systems, data analytics, and information security fields. The lab seeks to\\\\n    develop theories, techniques, and systems to enable a more secure and\\\\n    efficient network infrastructure, with computer systems that are more\\\\n    accountable and less vulnerable to attacks and abuse. Our current research\\\\n    focuses on four areas: (1) Data-Driven Security, (2) AI and Security, (3)\\\\n    Software and System Security, and (4) Big Data Mining and Analysis.\"\\\\n- heading \"News\" [level=2] [ref=s1e32]\\\\n- paragraph [ref=s1e33]:\\\\n    - text: \"03/30/2025 - Our paper \\\\\\\\\"Beyond Static Pattern Matching? Rethinking\\\\n        Automatic Cryptographic API Misuse Detection in the Era of LLMs\\\\\\\\\" was\\\\n        accepted by ISSTA 2025. Congrats to Yifan! 03/26/2025 - Our paper\\\\n        \\\\\\\\\"Privacy-preserving Triangle Counting in Directed Graphs\\\\\\\\\" was accepted\\\\n        by ICDE 2025. Congrats to Ziyao! 03/12/2025 - Our paper \\\\\\\\\"TextDefense:\\\\n        Adversarial Text Detection based on Word Importance Score Dispersion\\\\\\\\\"\\\\n        was accepted by IEEE Transactions on Dependable and Secure Computing\\\\n        (TDSC). Congrats to Lujia!\"\\\\n    - link \"More ......\" [ref=s1e40]:\\\\n        - /url: ./webpage/news.html\\\\n- heading \"Announcement\" [level=2] [ref=s1e43]\\\\n- paragraph [ref=s1e44]: NESA Lab looks for highly motivated undergrads, grads,\\\\n    and postdocs to join the group. If you are interested in our research,\\\\n    please come to visit our lab and/or send us your resume.\\\\n- list [ref=s1e46]:\\\\n    - listitem [ref=s1e47]:\\\\n        - heading \"Data-Driven Security\" [level=3] [ref=s1e48]\\\\n        - paragraph [ref=s1e50]: Cyber Threat/Crime Mining and Analysis; Fraud Detection\\\\n            and Analysis; Medical Application/Data Security and Privacy;\\\\n            Password Security; and CAPTCHA Security.\\\\n        - link \"read more\" [ref=s1e51]:\\\\n            - /url: webpage/research.html\\\\n    - listitem [ref=s1e53]:\\\\n        - heading \"AI Security\" [level=3] [ref=s1e54]\\\\n        - paragraph [ref=s1e56]: Adversarial Learning; Machine Learning Security and\\\\n            Privacy; AI-aided Vulnerability Mining; Smart Fuzzing; and Secure\\\\n            Multi-Party Computing and Learning.\\\\n        - link \"read more\" [ref=s1e57]:\\\\n            - /url: webpage/research.html\\\\n    - listitem [ref=s1e59]:\\\\n        - heading \"Privacy\" [level=3] [ref=s1e60]\\\\n        - paragraph [ref=s1e62]: Graph Privacy; Anonymization and De-anonymization;\\\\n            Differential Privacy; and Web Privacy..\\\\n        - link \"read more\" [ref=s1e63]:\\\\n            - /url: webpage/research.html\\\\n    - listitem [ref=s1e65]:\\\\n        - heading \"Big Data Analysis\" [level=3] [ref=s1e66]\\\\n        - paragraph [ref=s1e68]: Social Network Computing and Analytics and Graph and\\\\n            Network Embedding.\\\\n        - link \"read more\" [ref=s1e69]:\\\\n            - /url: webpage/research.html\\\\n- paragraph [ref=s1e73]: Copyright - NESA All rights reserved.\\\\n```\\\\n\\', annotations=None)]', name='browser_navigate', call_id='call_8b5f31fc16db46daa2e863', is_error=False)]\n---------- WebSurfer ----------\n[TextContent(type='text', text='\\n- Page URL: https://nesa.zju.edu.cn/\\n- Page Title: Index - NESA - Network System Security & Privacy Lab\\n- Page Snapshot\\n```yaml\\n- link [ref=s1e4]:\\n    - /url: index.html\\n- list [ref=s1e6]:\\n    - listitem [ref=s1e7]:\\n        - link \"home\" [ref=s1e8]:\\n            - /url: index.html\\n    - listitem [ref=s1e9]:\\n        - link \"news\" [ref=s1e10]:\\n            - /url: webpage/news.html\\n    - listitem [ref=s1e11]:\\n        - link \"Research\" [ref=s1e12]:\\n            - /url: webpage/research.html\\n    - listitem [ref=s1e13]:\\n        - link \"publication\" [ref=s1e14]:\\n            - /url: webpage/pubyear.html\\n    - listitem [ref=s1e15]:\\n        - link \"Software\" [ref=s1e16]:\\n            - /url: webpage/software.html\\n    - listitem [ref=s1e17]:\\n        - link \"contact\" [ref=s1e18]:\\n            - /url: webpage/contact.html\\n- heading \"Welcome to NESA\uff01\" [level=1] [ref=s1e22]\\n- paragraph [ref=s1e23]: \"The Network SystEm Security & PrivAcy (NESA) Research\\n    Lab, directed by Dr. Shouling Ji, works at the intersection of AI, network\\n    systems, data analytics, and information security fields. The lab seeks to\\n    develop theories, techniques, and systems to enable a more secure and\\n    efficient network infrastructure, with computer systems that are more\\n    accountable and less vulnerable to attacks and abuse. Our current research\\n    focuses on four areas: (1) Data-Driven Security, (2) AI and Security, (3)\\n    Software and System Security, and (4) Big Data Mining and Analysis.\"\\n- heading \"News\" [level=2] [ref=s1e32]\\n- paragraph [ref=s1e33]:\\n    - text: \"03/30/2025 - Our paper \\\\\"Beyond Static Pattern Matching? Rethinking\\n        Automatic Cryptographic API Misuse Detection in the Era of LLMs\\\\\" was\\n        accepted by ISSTA 2025. Congrats to Yifan! 03/26/2025 - Our paper\\n        \\\\\"Privacy-preserving Triangle Counting in Directed Graphs\\\\\" was accepted\\n        by ICDE 2025. Congrats to Ziyao! 03/12/2025 - Our paper \\\\\"TextDefense:\\n        Adversarial Text Detection based on Word Importance Score Dispersion\\\\\"\\n        was accepted by IEEE Transactions on Dependable and Secure Computing\\n        (TDSC). Congrats to Lujia!\"\\n    - link \"More ......\" [ref=s1e40]:\\n        - /url: ./webpage/news.html\\n- heading \"Announcement\" [level=2] [ref=s1e43]\\n- paragraph [ref=s1e44]: NESA Lab looks for highly motivated undergrads, grads,\\n    and postdocs to join the group. If you are interested in our research,\\n    please come to visit our lab and/or send us your resume.\\n- list [ref=s1e46]:\\n    - listitem [ref=s1e47]:\\n        - heading \"Data-Driven Security\" [level=3] [ref=s1e48]\\n        - paragraph [ref=s1e50]: Cyber Threat/Crime Mining and Analysis; Fraud Detection\\n            and Analysis; Medical Application/Data Security and Privacy;\\n            Password Security; and CAPTCHA Security.\\n        - link \"read more\" [ref=s1e51]:\\n            - /url: webpage/research.html\\n    - listitem [ref=s1e53]:\\n        - heading \"AI Security\" [level=3] [ref=s1e54]\\n        - paragraph [ref=s1e56]: Adversarial Learning; Machine Learning Security and\\n            Privacy; AI-aided Vulnerability Mining; Smart Fuzzing; and Secure\\n            Multi-Party Computing and Learning.\\n        - link \"read more\" [ref=s1e57]:\\n            - /url: webpage/research.html\\n    - listitem [ref=s1e59]:\\n        - heading \"Privacy\" [level=3] [ref=s1e60]\\n        - paragraph [ref=s1e62]: Graph Privacy; Anonymization and De-anonymization;\\n            Differential Privacy; and Web Privacy..\\n        - link \"read more\" [ref=s1e63]:\\n            - /url: webpage/research.html\\n    - listitem [ref=s1e65]:\\n        - heading \"Big Data Analysis\" [level=3] [ref=s1e66]\\n        - paragraph [ref=s1e68]: Social Network Computing and Analytics and Graph and\\n            Network Embedding.\\n        - link \"read more\" [ref=s1e69]:\\n            - /url: webpage/research.html\\n- paragraph [ref=s1e73]: Copyright - NESA All rights reserved.\\n```\\n', annotations=None)]\n---------- WebSurfer ----------\n[FunctionCall(id='call_d932494bde474f37ba6162', arguments='{}', name='browser_save_as_pdf')]\n---------- WebSurfer ----------\n[FunctionExecutionResult(content=\"Error: MCP tool execution failed: [TextContent(type='text', text='Error: Navigate to a location to create a page', annotations=None)]\\n\", name='browser_save_as_pdf', call_id='call_d932494bde474f37ba6162', is_error=True)]\n---------- WebSurfer ----------\nError: MCP tool execution failed: [TextContent(type='text', text='Error: Navigate to a location to create a page', annotations=None)]\n\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\n0.4.9\nModel used\nGPT-4\nPython version\nPython 3.12\nOperating system\nUbuntu 20.04 LTS", "created_at": "2025-04-04", "closed_at": "2025-04-17", "labels": ["help wanted", "proj-extensions"], "State": "closed", "Author": "Raytsang123"}
{"issue_number": 6189, "issue_title": "Failed to instantiate component: model_info is required when model name is not a valid OpenAI model", "issue_body": "What happened?\nIssue: Failed to instantiate component: model_info is required when model name is not a valid OpenAI model\nPlease Read the following before submitting an issue.\n\n Have you read the docs? (Python AgentChat User Guide and Tutorial, Python Core API User Guide, Python API Doc)\n Have you searched for related issues?\n Are you familiar with GitHub Markdown Syntax?\n\n\nDescribe the bug\nWhen attempting to configure an agent or client using a model name that is not a standard OpenAI model identifier (e.g., an Azure OpenAI deployment name, a local model identifier, or a model from another provider), AutoGen raises a ValueError with the message: Failed to instantiate component: model_info is required when model name is not a valid OpenAI model.\nThis occurs because the configuration lacks the necessary model_info dictionary structure required for AutoGen to correctly handle models that aren't directly mapped to standard OpenAI names like gpt-4 or gpt-3.5-turbo.\nTo Reproduce\nHere is a minimal example demonstrating the scenario using a hypothetical Azure OpenAI configuration where the deployment name is incorrectly placed directly in the model field without using model_info:\nimport autogen\nimport os\nimport logging\nimport sys\n\n# Example configuration attempting to use a non-standard name directly\n# Replace placeholders with your actual (or dummy) values for testing structure\nconfig_list_invalid = [\n    {\n        # Using a deployment name or custom name directly\n        \"model\": \"my-custom-deployment-or-model-name\",\n        \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\", \"YOUR_API_KEY\"),\n        \"api_type\": \"azure\",\n        \"base_url\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"YOUR_ENDPOINT\"),\n        \"api_version\": \"2024-02-15-preview\", # Or your specific API version\n    }\n]\n\nllm_config_invalid = {\n    \"config_list\": config_list_invalid,\n    \"cache_seed\": 42, # Optional: for caching\n}\n\ntry:\n    # Attempting to instantiate an agent with the invalid configuration\n    assistant = autogen.AssistantAgent(\n        name=\"assistant_invalid\",\n        llm_config=llm_config_invalid,\n        system_message=\"You are a helpful assistant.\"\n    )\n    print(\"Agent instantiated successfully (This should not happen with the invalid config).\")\n\nexcept ValueError as e:\n    logging.basicConfig(stream=sys.stderr, level=logging.ERROR)\n    logging.error(f\"Caught expected ValueError: {e}\")\n    # You would typically see a stack trace here in a real run\n    print(\"\\n--- Stack Trace Snippet (Illustrative) ---\")\n    print(f\"\"\"\nTraceback (most recent call last):\n  ... (omitted frames) ...\n  File \".../autogen/oai/client.py\", line XXX, in __init__\n    raise ValueError(\"Failed to instantiate component: model_info is required when model name is not a valid OpenAI model\")\nValueError: Failed to instantiate component: model_info is required when model name is not a valid OpenAI model\n    \"\"\")\nexcept Exception as e:\n    print(f\"Caught unexpected exception: {e}\")\n\n### Which packages was the bug in?\n\nAutoGen Studio (autogensudio)\n\n### AutoGen library version.\n\nOther (please specify)\n\n### Other library version.\n\n0.4.2\n\n### Model used\n\ndeepseek-v3\n\n### Model provider\n\nDeepSeek (Hosted)\n\n### Other model provider\n\n_No response_\n\n### Python version\n\nNone\n\n### .NET version\n\nNone\n\n### Operating system\n\nNone", "created_at": "2025-04-03", "closed_at": "2025-04-04", "labels": ["needs-triage"], "State": "closed", "Author": "xsser"}
{"issue_number": 6187, "issue_title": "Getting ValueError: Task list cannot be empty, while using SocietyOfMindAgent and SelectorGroupChat combination", "issue_body": "What happened?\nDescribe the bug\nWe are trying to create a multi team (Supervisor) pattern, in which a supervisor should orchestrate and communicate between multiple teams to complete the job. For that We have used combination of MagenticOneOrchestrator, SoceityOfMindAgent and SelectorGroupChat, code is attached below. We see the teams are initialized dynamically and the supervisor is channeling the user query to the relevant team. When the job is done we see below exception.\nPlease suggest what is wrong.\nError processing publish message for GeneralAssistantSupervisor_2ae9d9d0-41f3-4916-9f91-4c36856a1242/2ae9d9d0-41f3-4916-9f91-4c36856a1242\nTraceback (most recent call last):\n  File \"/home/azureuser/clientdemo/magenticone/.venv/lib/python3.12/site-packages/autogen_core/_single_threaded_agent_runtime.py\", line 510, in _on_message\n    return await agent.on_message(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/clientdemo/magenticone/.venv/lib/python3.12/site-packages/autogen_core/_base_agent.py\", line 113, in on_message\n    return await self.on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/clientdemo/magenticone/.venv/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 67, i\nn on_message_impl\n    return await super().on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/clientdemo/magenticone/.venv/lib/python3.12/site-packages/autogen_core/_routed_agent.py\", line 485, in on_message_impl\n    return await h(self, message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/clientdemo/magenticone/.venv/lib/python3.12/site-packages/autogen_core/_routed_agent.py\", line 268, in wrapper\n    return_value = await func(self, message, ctx)  # type: ignore\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/azureuser/clientdemo/magenticone/.venv/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 69, in h\nandle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n  File \"/home/azureuser/clientdemo/magenticone/.venv/lib/python3.12/site-packages/autogen_agentchat/agents/_society_of_mind_agent.py\", line 148, in on_message\ns_stream\n    async for inner_msg in self._team.run_stream(task=task, cancellation_token=cancellation_token):\n  File \"/home/azureuser/clientdemo/magenticone/.venv/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py\", line 400, in run_s\ntream\n    raise ValueError(\"Task list cannot be empty.\")\nValueError: Task list cannot be empty.\n\nTo Reproduce\nUse the code below.\nExpected behavior\nRequired task should be shared between agent and MagenticOneOrchestrator, SoceityOfMindAgent and SelectorGroupChat the there should not be exception\nScreenshots\nNA\nAdditional context\nimport json\nimport logging\nimport os\nfrom typing import Any, Awaitable, Callable, Optional\n\nimport aiofiles\nimport yaml\nfrom autogen_agentchat.agents import AssistantAgent, UserProxyAgent, SocietyOfMindAgent\nfrom autogen_agentchat.base import TaskResult\nfrom autogen_agentchat.conditions import TextMentionTermination\nfrom autogen_agentchat.messages import TextMessage, UserInputRequestedEvent\nfrom autogen_agentchat.teams import MagenticOneGroupChat, SelectorGroupChat\nfrom autogen_core import CancellationToken\nfrom fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import FileResponse\nfrom fastapi.staticfiles import StaticFiles\nfrom autogen_ext.models.openai import AzureOpenAIChatCompletionClient\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom tools.onboarding_offboarding_tools import *\nfrom tools.incident_mitigation_tools import *\nfrom tools.wawa_tools import *\n\nlogger = logging.getLogger(__name__)\n\napp = FastAPI()\n\n# Load team configurations from external JSON file\nCONFIG_FILE = \"teams_config.json\"\n\ndef load_team_config():\n    if os.path.exists(CONFIG_FILE):\n        with open(CONFIG_FILE, \"r\") as file:\n            return json.load(file)\n    return []\n\nTEAM_CONFIG = load_team_config()\n\ndef get_team_config(request: str):\n    for team in TEAM_CONFIG:\n        if team.get(\"teamName\", \"\").lower() == request.lower():\n            return team  # \u2705 Ensure it returns the correct team config\n    return {}\n\n# Add CORS middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # Allows all origins\n    allow_credentials=True,\n    allow_methods=[\"*\"],  # Allows all methods\n    allow_headers=[\"*\"],  # Allows all headers\n)\n\nmodel_config_path = \"model_config.yaml\"\nstate_path = \"team_state.json\"\nhistory_path = \"team_history.json\"\n\n# Serve static files\napp.mount(\"/static\", StaticFiles(directory=\".\"), name=\"static\")\n\nclass ConnectionManager:\n    def __init__(self):\n        #self.active_connections: list[WebSocket] = []\n        self.active_connections: dict(str, WebSocket) = {}\n\n    async def connect(self, websocket: WebSocket, clientId):\n        await websocket.accept()\n        #self.active_connections.append(websocket)\n        self.active_connections[clientId] = websocket\n\n    def disconnect(self, websocket: WebSocket):\n        self.active_connections.remove(websocket)\n\n    async def send_admin_message(self, message: str):\n        websocket = self.active_connections.get(\"admin\")\n        await websocket.send_json(message)\n\n    async def send_personal_message(self, message: str, websocket: WebSocket):\n        await websocket.send_json(message)\n        \n    async def broadcast(self, message: str):\n        for connection in self.active_connections:\n            await connection.send_json(message)\n\n\nmanager = ConnectionManager()\n\ntext_termination = TextMentionTermination(\"TERMINATE\")\nmax_msg_termination = MaxMessageTermination(max_messages=100)\ncombined_termination = max_msg_termination | text_termination\nmodel_client = AzureOpenAIChatCompletionClient(\n    model=\"gpt-4o\", temperature=0.0, top_p=0.0, frequency_penalty=0.0, presence_penalty=0.0\n)\n\n@app.get(\"/\")\nasync def root():\n    \"\"\"Serve the chat interface HTML file.\"\"\"\n    return FileResponse(\"app_user.html\")\n\nasync def load_agents_from_config(teamType: str) -> list[AssistantAgent]:\n    \"\"\"Load agent configurations from JSON files.\"\"\"\n\n    team_config = get_team_config(teamType)\n    if not team_config:\n        print(f\"\u274c No team config found for '{teamType}'\")\n        return []\n    \n    agents_config_file = team_config.get(\"config\", \"general_agents_config.json\")  # Get correct config file\n    print(f\"\ud83d\udcc2 Loading agent config file: {agents_config_file}\")\n\n    if not os.path.exists(agents_config_file):\n        print(f\"\u274c File '{agents_config_file}' does not exist!\")\n        return []\n    \n    try:\n        async with aiofiles.open(agents_config_file, \"r\") as file:\n            file_content = await file.read()\n            print(\"\ud83d\udcc4 Loaded File Content (Preview):\", file_content[:200])  # Print first 200 chars for debugging\n            agents_data = json.loads(file_content)\n    except json.JSONDecodeError as e:\n        print(f\"\u274c JSON Error in '{agents_config_file}': {e}\")\n        return []\n    except Exception as e:\n        print(f\"\u274c Unexpected error while reading '{agents_config_file}': {e}\")\n        return []\n    \n    agents = []\n    for agent in agents_data.get(\"agents\", []):\n        tool_functions = [globals().get(tool) for tool in agent.get(\"tools\", []) if globals().get(tool)]\n        \n        if len(tool_functions) != len(agent.get(\"tools\", [])):\n            missing_tools = [tool for tool in agent.get(\"tools\", []) if tool not in globals()]\n            logger.warning(f\"\u26a0\ufe0f Missing tools: {missing_tools} for agent {agent['name']}\")\n\n        agents.append(\n            AssistantAgent(\n                name=agent[\"name\"], \n                model_client=model_client,\n                tools=tool_functions,\n                system_message=agent[\"system_message\"],\n                description=agent[\"description\"],\n                reflect_on_tool_use=True, \n                model_client_stream=False,\n            )\n        )\n    \n    print(f\"\u2705 Successfully loaded {len(agents)} agents from '{agents_config_file}'\")\n    return agents\n\n\nasync def get_team(user_input_func: Callable[[str, Optional[CancellationToken]], Awaitable[str]], request: str) -> MagenticOneGroupChat:\n    \"\"\"Retrieve or create a team based on request type.\"\"\"\n    user_proxy = UserProxyAgent(name=\"User\", input_func=user_input_func)\n    team_config = get_team_config(request)\n    print(\"::::team_config\", team_config)\n    team_type = team_config.get(\"teamName\", \"GeneralAssistantSupervisor\")\n    print(\"::::team_type\", team_type)\n    agent_list = await load_agents_from_config(team_type)\n    agent_list.append(user_proxy)\n    \"\"\"\n    Args:\n        participants (List[ChatAgent]): The participants in the group chat.\n        model_client (ChatCompletionClient): The model client used for generating responses.\n        termination_condition (TerminationCondition, optional): The termination condition for the group chat. Defaults to None.\n            Without a termination condition, the group chat will run based on the orchestrator logic or until the maximum number of turns is reached.\n        max_turns (int, optional): The maximum number of turns in the group chat before stopping. Defaults to 20.\n        max_stalls (int, optional): The maximum number of stalls allowed before re-planning. Defaults to 3.\n        final_answer_prompt (str, optional): The LLM prompt used to generate the final answer or response from the team's transcript. A default (sensible for GPT-4o class models) is provided.\n    \"\"\"\n    team = MagenticOneGroupChat(\n        agent_list, \n        model_client=model_client,\n        termination_condition=combined_termination,\n        max_turns=100,\n        max_stalls=30\n        )\n    \n    # Load state from file.\n    \"\"\"if not os.path.exists(state_path):\n        return team\n    async with aiofiles.open(state_path, \"r\") as file:\n        state = json.loads(await file.read())\n    await team.load_state(state)\"\"\"\n    return team\n\nasync def get_history() -> list[dict[str, Any]]:\n    \"\"\"Get chat history from file.\"\"\"\n    if not os.path.exists(history_path):\n        return []\n    async with aiofiles.open(history_path, \"r\") as file:\n        return json.loads(await file.read())\n\n@app.get(\"/history\")\nasync def history() -> list[dict[str, Any]]:\n    \"\"\"Fetch chat history from storage.\"\"\"\n    try:\n        # return await get_history()\n        return []\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e)) from e\n\n@app.websocket(\"/ws/chat/{client_id}\")\nasync def chat(websocket: WebSocket,client_id: str):\n    \"\"\"WebSocket handler for real-time chat.\"\"\"\n    global model_client\n    await manager.connect(websocket, client_id)\n\n    # User input function used by the team.\n    async def _user_input(prompt: str, cancellation_token: CancellationToken | None) -> str:\n        data = await websocket.receive_json()\n        return TextMessage.model_validate(data).content\n    \n    try:\n        while True:\n            try:\n                # Get user message.\n                data = await websocket.receive_json()\n                request = TextMessage.model_validate(data)\n                print(\"TEAM_CONFIG:\", TEAM_CONFIG)\n\n                teams = {}\n\n                for team in TEAM_CONFIG:\n                    print(\"Current team:\", team)  # Debugging\n                    print(\"Type:\", type(team)) \n                    if isinstance(team, dict) and \"teamName\" in team:\n                        teams[team[\"teamName\"]] = await get_team(_user_input, team[\"teamName\"])\n                    else:\n                        print(f\"Invalid team format: {team}\")  # Debugging line\n\n                agents = {}\n\n                for team_name, team_instance in teams.items():\n                    if isinstance(team_instance, MagenticOneGroupChat):  # Ensure it's a valid team instance\n                        agents[team_name] = SocietyOfMindAgent(team_name, team=team_instance, model_client=model_client, description=\"An agent that uses an inner team of agents to generate responses. Strictly - Reponse should be in 20 - 30 words only.\")\n                    else:\n                        print(f\"Skipping invalid team instance for {team_name}\")  # Debugging line\n                \n\n                selector_prompt = (\n                    \"Select the most suitable agent to perform the next task. {roles}\\n\\n\"\n                    \"Current conversation context: {history}\\n\\n\"\n                    \"- Prioritize expertise.\\n\"\n                    \"- Planner agent assigns tasks before execution.\\n\"\n                    \"- General tasks: GeneralAssistantSupervisor.\\n\"\n                    \"- Pump issues: FuelStationSupervisor.\\n\"\n                    \"- Coordinate across teams when necessary.\\n\"\n                    \"- Select only one agent at a time.\"\n                )\n                \n                orchestrator = SelectorGroupChat(\n                    list(agents.values()), \n                    model_client=model_client, \n                    termination_condition=combined_termination, \n                    selector_prompt=selector_prompt,\n                    allow_repeated_speaker=True\n                )\n                \n\n                history = await get_history()\n                async for message in orchestrator.run_stream(task=request):\n                    if isinstance(message, TaskResult):\n                        continue\n                    await websocket.send_json(message.model_dump())\n                    await manager.send_admin_message(message.model_dump())\n                    if not isinstance(message, UserInputRequestedEvent):\n                        history.append(message.model_dump())\n                \"\"\"\n                # Save team state to file.\n                async with aiofiles.open(state_path, \"w\") as file:\n                    state = await team.save_state()\n                    await file.write(json.dumps(state))\n\n                # Save chat history to file.\n                async with aiofiles.open(history_path, \"w\") as file:\n                    await file.write(json.dumps(history))\n                \"\"\"  \n            except Exception as e:\n                logger.error(f\"Unexpected error: {str(e)}\")\n                await websocket.send_json({\"type\": \"error\", \"content\": f\"Unexpected error: {str(e)}\", \"source\": \"system\"})\n    except WebSocketDisconnect:\n        logger.info(\"Client disconnected\")\n    except Exception as e:\n        logger.error(f\"Unexpected error: {str(e)}\")\n        await websocket.send_json({\"type\": \"error\", \"content\": f\"Unexpected error: {str(e)}\", \"source\": \"system\"})\n\n# Example usage\nif __name__ == \"__main__\":\n    import uvicorn\n\n    # uvicorn.run(app, host=\"0.0.0.0\", port=8002, timeout_keep_alive=1200.0, ws_ping_interval=3000.0, ws_ping_timeout=3000.0)\n    uvicorn.run(\n        app,\n        host=\"0.0.0.0\",\n        port=8003,\n        log_level=\"debug\",  # Same as '--log-level debug'\n        timeout_keep_alive=600,  # Keeps idle HTTP connections for 10 minutes\n        ws_ping_interval=300,  # Sends a WebSocket ping every 300 seconds\n        ws_ping_timeout=300  # Waits 5 minutes for a pong before closing\n    )\n\n\nSample team config file\n{\n  \"agents\": [\n    {\n      \"name\": \"GeneralAssistantAgent\",\n      \"system_message\": \"You are a helpful assistant.\",\n\t    \"description\": \"You are a helpful assistant.\",\n      \"tools\": []\n    },\n    {\n      \"name\": \"WeatherAgent\",\n      \"system_message\": \"You are a helpful assistant to answer weather queries.\",\n\t    \"description\": \"You are a helpful assistant to answer weather queries.\",\n      \"tools\": [\"get_weather\"]\n    }\n  ]\n}\n\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython 0.4.7\nOther library version.\nNo response\nModel used\ngpt-4o\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-04-03", "closed_at": "2025-04-04", "labels": ["needs-triage"], "State": "closed", "Author": "karun19"}
{"issue_number": 6185, "issue_title": "The support for MCP is a bit poor. We may have to consider using other frameworks.", "issue_body": "What happened?\nThe call of MCP is very unstable, and the multi-agent collaboration is also a bit poor. When using MCP, it feels like Autogen doesn't know what it should do, especially when it involves multiple MCPs.\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-04-03", "closed_at": "2025-04-17", "labels": ["needs-triage"], "State": "closed", "Author": "bidada-bcr"}
{"issue_number": 6182, "issue_title": "Add `StreamProcessor` class for handling message streams produced by `run_stream` and `on_messages_stream`.", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nConsider this design:\nprocessor = StreamProcessor()\nprocessor.add_handler(TextMessage, ...) # register a handler for `TextMessage` type.\nprocessor.add_handler(HandoffMessage, ...) # register ad handler for the `HandoffMessage` type\n\nstream = team.run_stream()\nresult = processor(stream)\nWe can then rewrite Console to subclass StreamProcessor to provide handlers for all the built-in message types and default handler for unknown message types.\nNow you can customize Console to override handling of specific message types.", "created_at": "2025-04-02", "closed_at": null, "labels": ["help wanted", "proj-agentchat"], "State": "open", "Author": "ekzhu"}
{"issue_number": 6178, "issue_title": "OpenAIChatCompletionClient does not allow to use azure_ad_token_provider as an authentication option", "issue_body": "What happened?\nDescribe the bug\nOpenAIChatCompletionClient does not allow to use token provider as its authentication method. The only option is using api_key, which is not fit for my application.\nI confirmed that the same token provider works without any issue when I used it for AzureOpenAI in azure-identity\nTo Reproduce\nSystem information:\nOS: Windows 11\nPython version: Python 3.13.2\nautogen-agentchat==0.4.9.3\nautogen-core==0.4.9.3\nautogen-ext==0.4.9.3\nopenai==1.70.0\nazure-identity==1.21.0\nModified Web Browsing Agent Team example at the main github page: https://github.com/microsoft/autogen?tab=readme-ov-file#web-browsing-agent-team\nMy edit is only for the client side:\n    endpoint = [my endpoint]\n    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n    model_client = OpenAIChatCompletionClient(\n                                            model=\"gpt-4o-mini\",\n                                            azure_deployment=\"gpt-4o-mini\",\n                                            azure_ad_token_provider=token_provider,\n                                            azure_endpoint=endpoint,\n                                            api_version=\"2025-03-01-preview\"\n                                            )\nExpected behavior\nThe client init part should succeed.\nHowever, I got below error:\n  File \"E:\\Work\\python-projects\\AutoGen\\.venv\\Lib\\site-packages\\openai\\_client.py\", line 345, in __init__\n    raise OpenAIError(\n        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n    )\nI looked at the code part on _openai_client.py. It turns out that it route the init request to AsyncOpenAI (in openai), which does not support token based authentication.\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n            raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nAdditional context\nThe behavior is not aligned with the documentation. https://microsoft.github.io/autogen/dev//reference/python/autogen_ext.models.openai.html#autogen_ext.models.openai.AzureOpenAIChatCompletionClient\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\ngpt-4o-mini\nModel provider\nAzure OpenAI\nOther model provider\nNo response\nPython version\n3.13\n.NET version\nNone\nOperating system\nWindows", "created_at": "2025-04-02", "closed_at": "2025-04-02", "labels": ["needs-triage"], "State": "closed", "Author": "hyeoungho"}
{"issue_number": 6174, "issue_title": "Python FastAPI to get input via userproxy", "issue_body": "What happened?\nDescribe the bug\nI am writing an API using Fast API, that initialize the Selector Group Chat of assistants and user proxy agents.\nThrough streaming, I am able to stream the conversation however during user proxy's turn where it expects user's response is expecting the input in the python console where the app is being run. It is kind of going into a blocked state and to resume, I need to provide user input to the user proxy agent from the python console. As soon as I do that, it resumes the team chat and complete streaming the whole interaction. There is no direct way of carrying user proxy input via same streaming.\nExpected behavior\nThere could be a way to enable interaction of the user input through the streaming API itself.\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\n3.11\n.NET version\nNone\nOperating system\nWindows", "created_at": "2025-04-02", "closed_at": "2025-04-02", "labels": ["needs-triage"], "State": "closed", "Author": "avinashmihani"}
{"issue_number": 6172, "issue_title": "the allow_repeated_speaker function in the selector", "issue_body": "What is the doc issue?\nRegarding the allow_repeated_speaker function in the selector, I would like to ask why it is disabled by default during the design. Will this avoid any bugs?\nLink to the doc page, if applicable\nhttps://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/selector-group-chat.html", "created_at": "2025-04-02", "closed_at": "2025-04-02", "labels": ["documentation", "needs-triage"], "State": "closed", "Author": "CalvinHMX"}
{"issue_number": 6167, "issue_title": "Anthropic model final assistant content cannot end with trailing whitespace error \u2014 needs quick fix & structural discussion", "issue_body": "What happened?\nDescribe the bug\nWhen using Anthropic models (e.g., claude-3-haiku-20240307), a 400 error is raised if the final message is an AssistantMessage and its content ends with whitespace (e.g., a space \" \" or \"Say : \"):\nError code: 400 - {\n  'error': {\n    'code': 'invalid_request_error',\n    'message': 'messages: final assistant content cannot end with trailing whitespace',\n    ...\n  }\n}\n\nThis is a vendor-specific constraint and currently not handled in the AutoGen message stream.\nTo Reproduce\nUse the following code (or similar message sequences):\nmessages = [\n    UserMessage(content=\"hello\", source=\"user\"),\n    UserMessage(content=\"say world \", source=\"user\"),\n    AssistantMessage(content=\"Say : \", source=\"assistant\"),  # \u2190 ends with whitespace\n]\n\n# AnthropicChatCompletionClient(model=\"claude-3-haiku-20240307\") will raise error\n\u2705 Reordering the message so that AssistantMessage is not the final entry avoids the error:\nmessages = [\n    UserMessage(content=\"hello\", source=\"user\"),\n    AssistantMessage(content=\"Say : \", source=\"assistant\"),\n    UserMessage(content=\"say world \", source=\"user\"),\n]\nNote: This issue does not occur on OpenAI or Gemini clients.\nExpected behavior\nAutoGen should either:\n\nsanitize the final AssistantMessage for Anthropic models (e.g., via rstrip()), or\nprovide a structural hook to manage message-level stream postprocessing.\n\nRight now, this inconsistency can cause hard-to-debug 400 errors during inference.\nAdditional context\nThis issue has already caused silent failures during testing, and is similar in pattern to a previous Anthropic-specific bug involving multi-system message handling (see: #6118).\nThese types of stream-level quirks are increasingly common across vendors.\nWhile a quick .rstrip() patch is trivial, this bug signals a growing need for a message stream\u2013level transformer.\n\n\ud83d\udd27 Short-term plan (Ad-hoc fix)\nA minimal PR will be opened that simply checks:\nif is_anthropic and isinstance(messages[-1], AssistantMessage):\n    messages[-1].content = messages[-1].content.rstrip()\nThis will resolve the issue for now.\n\n\ud83e\udde9 Long-term structural suggestion\nIf more stream-related issues like this arise, it may be better to introduce a MessageStreamTransformer module, analogous to the existing TransformerMap, but operating on message sequences.\nThis will allow centralized, per-model stream-level fixes like:\n\nAnthropic trailing whitespace issue (this one)\nSystem message concat (#6118)\nGemini-specific role behavior\nFuture multimodal format requirements\n\nProposed location: autogen-ext/models/ or autogen-ext/, to avoid SDK-specific duplication. Cause, many case, we need to duplication same logic for other SDK (in #6118 has that too)\n\"Let me know if the maintainers are open to such a structure \u2014 I\u2019d be happy to contribute it if needed.\"\n\nRelated\n\n#6118 (multi-system-message concat logic)\n#6063 (modular transformer structure)\n\n\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\n\"claude-3-haiku-20240307\", and maybe whole of anthropic model\nModel provider\nAnthropic\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-04-01", "closed_at": "2025-04-02", "labels": ["needs-triage"], "State": "closed", "Author": "SongChiYoung"}
{"issue_number": 6165, "issue_title": "the installation instruction for autogen-studio had a missing step", "issue_body": "What is the doc issue?\nDescribe the issue\nFollowing the guide instructions the user will execute the commands in the wrong directory.\nI get this error:\nERROR: file:///C:/ws/py/autogen/autogen does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\n\nWhat do you want to see in the doc?\nThe right directory where I have to execute the commands.\nScreenshots\nAdditional context\nLink to the doc page, if applicable\nhttps://microsoft.github.io/autogen/stable//user-guide/autogenstudio-user-guide/installation.html#a-install-from-source-manually", "created_at": "2025-04-01", "closed_at": "2025-04-01", "labels": ["documentation", "proj-studio"], "State": "closed", "Author": "dicaeffe"}
{"issue_number": 6163, "issue_title": "Support a wider range of termination conditions in AGS", "issue_body": "AGS UI currently supports text message term and max message term.\nIn reality, AgentChat supports much more.\nThis PR is to track work on ensuring parity across agentchat and ags UI\ncc @ekzhu", "created_at": "2025-04-01", "closed_at": null, "labels": ["proj-studio"], "State": "open", "Author": "victordibia"}
{"issue_number": 6161, "issue_title": "Option to emit group chat manager messages in AgentChat", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nAdd an option emit_team_events to BaseGroupChat to emit events from group chat manager through run_stream.\nFor example, SpeakerSelectedEvent from group chat speaker selection.\nRelated, could be addressed together: #5127", "created_at": "2025-04-01", "closed_at": "2025-04-16", "labels": ["help wanted", "proj-agentchat"], "State": "closed", "Author": "ekzhu"}
{"issue_number": 6155, "issue_title": "from autogen_ext.models.ollama import OllamaChatCompletionClient init", "issue_body": "What happened?\n success\uff0c\nbut use OllamaChatCompletionClient fail\n\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-31", "closed_at": null, "labels": ["needs-triage"], "State": "open", "Author": "xsw1006931693"}
{"issue_number": 6152, "issue_title": "A mcp_server_tools error with StdioServerParams by using excel-mcp-server", "issue_body": "What happened?\nDescribe the bug\nI got\n\njson_schema_to_pydantic.exceptions.TypeError: Array type must specify 'items' schema\n\nand\n\nValueError: I/O operation on closed pipe\n\nwhen using mcp_server_tools()\nTo Reproduce\nSteps to reproduce the behavior. Please include code and outputs such as stacktrace.\nI pulled the @negokaz/excel-mcp-server and try to build an agent for test.\nnpm install -g @negokaz/excel-mcp-server\nFirst I build in VSCode with Cline for testing, and it works with this quest:\n\nCreate a excel file called test_with_autogen.xlsx with some content, and save in Desktop folder\n\nMy os is Windows 11, so my configuration is\n{\n    \"mcpServers\": {\n        \"excel\": {\n            \"command\": \"cmd\",\n            \"args\": [\"/c\", \"npx\", \"--yes\", \"@negokaz/excel-mcp-server\"],\n            \"env\": {\n                \"EXCEL_MCP_PAGING_CELLS_LIMIT\": \"4000\"\n            }\n        }\n    }\n}\n\nSecond, when I build the same excel-mcp-server in Autogen with MCP, the code is\nimport asyncio\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.tools.mcp import StdioServerParams, mcp_server_tools\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core import CancellationToken\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_agentchat.ui import Console\n\nasync def main_excel() -> None:\n    server_params = StdioServerParams(\n        command=\"cmd\",\n        args=[\"/c\", \"npx\", \"--yes\", \"@negokaz/excel-mcp-server\"],\n        env={\n            \"EXCEL_MCP_PAGING_CELLS_LIMIT\": \"4000\"\n            }\n    )\n\n    # Get all available tools from the server\n    tools = await mcp_server_tools(server_params)\n\n    # Create an agent that can use all the tools\n    model_client_OpenAI = \\\n        OpenAIChatCompletionClient(model=\"gpt-3.5-turbo\",base_url=\"https://xxxxxxx\",api_key=\"xxxxxxxx\")\n\n    agent = AssistantAgent(\n        name=\"excel\",\n        model_client=model_client_OpenAI,\n        tools=tools,  # type: ignore\n    )\n\n\n    # The agent can now use any of the filesystem tools\n    await agent.run(task=\"Create a excel file called test_with_autogen.xlsx with some content, and save in Desktop folder\", cancellation_token=CancellationToken())\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main_excel())\n\nIt got the error:\n\nTraceback (most recent call last):\nFile \"E:\\Work\\AI\\APEXBOT\\autogen_mcp_1.py\", line 71, in \nasyncio.run(main_excel())\nFile \"C:\\Users\\Omar\\anaconda3\\Lib\\asyncio\\runners.py\", line 194, in run\nreturn runner.run(main)\n^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\Omar\\anaconda3\\Lib\\asyncio\\runners.py\", line 118, in run\nreturn self.loop.run_until_complete(task)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\Omar\\anaconda3\\Lib\\asyncio\\base_events.py\", line 687, in run_until_complete\nreturn future.result()\n^^^^^^^^^^^^^^^\nFile \"E:\\Work\\AI\\APEXBOT\\autogen_mcp_1.py\", line 53, in main_excel\ntools = await mcp_server_tools(server_params)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\Omar\\anaconda3\\Lib\\site-packages\\autogen_ext\\tools\\mcp_factory.py\", line 139, in mcp_server_tools\nreturn [StdioMcpToolAdapter(server_params=server_params, tool=tool) for tool in tools.tools]\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\Omar\\anaconda3\\Lib\\site-packages\\autogen_ext\\tools\\mcp_stdio.py\", line 48, in init\nsuper().init(server_params=server_params, tool=tool)\nFile \"C:\\Users\\Omar\\anaconda3\\Lib\\site-packages\\autogen_ext\\tools\\mcp_base.py\", line 38, in init\ninput_model = create_model(tool.inputSchema)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\Omar\\anaconda3\\Lib\\site-packages\\json_schema_to_pydantic_init.py\", line 42, in create_model\nreturn builder.create_pydantic_model(schema, root_schema)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\Omar\\anaconda3\\Lib\\site-packages\\json_schema_to_pydantic\\model_builder.py\", line 62, in create_pydantic_model\nfield_type = self._get_field_type(field_schema, root_schema)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\Omar\\anaconda3\\Lib\\site-packages\\json_schema_to_pydantic\\model_builder.py\", line 98, in _get_field_type\nreturn self.type_resolver.resolve_type(field_schema, root_schema)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\Omar\\anaconda3\\Lib\\site-packages\\json_schema_to_pydantic\\resolvers.py\", line 52, in resolve_type\nraise TypeError(\"Array type must specify 'items' schema\")\njson_schema_to_pydantic.exceptions.TypeError: Array type must specify 'items' schema\nException ignored in: <function BaseSubprocessTransport.del at 0x000002353D51DDA0>\nTraceback (most recent call last):\nFile \"C:\\Users\\Omar\\anaconda3\\Lib\\asyncio\\base_subprocess.py\", line 126, in del\nself.close()\nFile \"C:\\Users\\Omar\\anaconda3\\Lib\\asyncio\\base_subprocess.py\", line 104, in close\nproto.pipe.close()\nFile \"C:\\Users\\Omar\\anaconda3\\Lib\\asyncio\\proactor_events.py\", line 109, in close\nself._loop.call_soon(self._call_connection_lost, None)\nFile \"C:\\Users\\Omar\\anaconda3\\Lib\\asyncio\\base_events.py\", line 795, in call_soon\nself._check_closed()\nFile \"C:\\Users\\Omar\\anaconda3\\Lib\\asyncio\\base_events.py\", line 541, in _check_closed\nraise RuntimeError('Event loop is closed')\nRuntimeError: Event loop is closed\nException ignored in: <function _ProactorBasePipeTransport.del at 0x000002353D51F560>\nTraceback (most recent call last):\nFile \"C:\\Users\\Omar\\anaconda3\\Lib\\asyncio\\proactor_events.py\", line 116, in del\n_warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n^^^^^^^^\nFile \"C:\\Users\\Omar\\anaconda3\\Lib\\asyncio\\proactor_events.py\", line 80, in repr\ninfo.append(f'fd={self._sock.fileno()}')\n^^^^^^^^^^^^^^^^^^^\nFile \"C:\\Users\\Omar\\anaconda3\\Lib\\asyncio\\windows_utils.py\", line 102, in fileno\nraise ValueError(\"I/O operation on closed pipe\")\nValueError: I/O operation on closed pipe\n\nExpected behavior\nI think it has to do the same work like \"VSCode + Cline\", but it got errors.\nAdditional context\nI'd build the @modelcontextprotocol\\server-filesystem in Autogen, and it works. So I just want to build more MCP application for testing, like excel, but I met some problem.\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython 0.4.7\nOther library version.\nNo response\nModel used\ngpt-3.5-turbo\nModel provider\nOpenAI\nOther model provider\nNo response\nPython version\n3.12\n.NET version\nNone\nOperating system\nWindows", "created_at": "2025-03-31", "closed_at": "2025-04-07", "labels": ["needs-triage"], "State": "closed", "Author": "Omarchen-Good"}
{"issue_number": 6151, "issue_title": "Mistral support through `OpenAIChatCompletionClient`.", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nMistral AI models can be added to the list here: https://github.com/microsoft/autogen/blob/fbdd89b46bf3883efe8b0e0838370b9d74561ee7/python/packages/autogen-ext/src/autogen_ext/models/openai/_model_info.py\nAnd the base_url should automatically be set if the model matches a Mistral AI model. API key should be detected from the environment. Use the support for Gemini model as a reference.\nPR should address: #6147", "created_at": "2025-03-31", "closed_at": null, "labels": ["help wanted", "proj-extensions"], "State": "open", "Author": "ekzhu"}
{"issue_number": 6150, "issue_title": "Adding a `output_task_messages` to `run_stream` methods to avoid messages in `task` are included.", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nHelp SocietyOfMindAgent to avoid fragile counting-based skipping of task messages. Also, it makes it easier to nest agent teams within teams.", "created_at": "2025-03-31", "closed_at": null, "labels": ["help wanted", "proj-agentchat"], "State": "open", "Author": "ekzhu"}
{"issue_number": 6147, "issue_title": "Update of @langchain/mistralai necessary for using Mistral API in Autogen Studio", "issue_body": "What happened?\nDescribe the bug\nWhen using Autogen Studio with the Mistral API, the generated request payload includes an extra field (\"name\") inside the user object. The Mistral API only accepts \"role\" and \"content\" for user messages, so including the \"name\" field causes a HTTP 422 error (\"Extra inputs are not permitted\"). This issue causes all API calls to fail unless the extra field is removed.\nTo Reproduce\n\nConfigure Autogen Studio with the following provider settings:\n{\n  \"provider\": \"autogen_ext.models.openai.OpenAIChatCompletionClient\",\n  \"component_type\": \"model\",\n  \"version\": 1,\n  \"component_version\": 1,\n  \"description\": \"Chat completion client for OpenAI hosted models.\",\n  \"label\": \"OpenAIChatCompletionClient\",\n  \"config\": {\n    \"model\": \"mistral-small-latest\",\n    \"model_info\": {\n      \"family\": \"mistral\",\n      \"model_name\": \"mistral-small-latest\",\n      \"vision\": false,\n      \"function_calling\": true,\n      \"json_output\": false\n    },\n    \"api_key\": \"YOUR_API_KEY\",\n    \"api_type\": \"mistral\",\n    \"base_url\": \"https://api.mistral.ai/v1/\"\n  }\n}\n\nTrigger a chat completion request. The generated payload contains an object similar to:\n{\n  \"role\": \"user\",\n  \"content\": \"Your prompt here\",\n  \"name\": \"user\"\n}\n\nThe Mistral API responds with a HTTP 422 error:\n{\n  \"detail\": [\n    {\n      \"type\": \"extra_forbidden\",\n      \"loc\": [\"body\", \"messages\", 0, \"user\", \"name\"],\n      \"msg\": \"Extra inputs are not permitted\",\n      \"input\": \"user\"\n    }\n  ]\n}\n\n\n\nExpected behavior\nThe API request payload should only include the \"role\" and \"content\" keys for the user object. With the correct payload structure, the Mistral API call should succeed without returning a 422 error.\nAdditional context\nThis issue has been addressed in the underlying library @langchain/mistralai, which in version 0.0.19 removes the extra \"name\" field from the request. Updating to version 0.0.19 resolves the problem.\nSince Autogen Studio currently uses an older version, please update the dependency to @langchain/mistralai version 0.0.19 (or later) to ensure compatibility with the stricter validation rules of the Mistral API.\nFeedback or further questions are welcome.\nWhich packages was the bug in?\nAutoGen Studio (autogensudio)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nAll mistral models\nModel provider\nMistral AI\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-30", "closed_at": null, "labels": ["needs-triage"], "State": "open", "Author": "AlexKnowsIt"}
{"issue_number": 6145, "issue_title": "Model client streaming from the selector of SelectorGroupChat", "issue_body": "Feature Request\nWe can enable streaming for SelectoGroupChat's built-in selector by introducing an option in SelectorGroupChat, e.g., model_client_stream so the model client will be used in streaming model. It will use create_stream rather than create.\nAs the next step, we can enable streaming of orchestration events through run_stream so the streaming output will be visible from consumer of run_stream. Issue here: #6161\n--- Below is the original bug report ---\nWhat happened?\nDescribe the bug\nSome llm models only support stream  = True. The assistant agent supports this very well by setting model_client_stream = True. But the OpenAIChatCompletionClient does not allow to pass stream = True to it. Therefore, it's not very possible to use llm models which only supports stream = True.\nTo Reproduce\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'code': 'invalid_parameter_error', 'param': None, 'message': 'This model only support stream mode, please enable the stream parameter to access the model. ', 'type': 'invalid_request_error'}, 'id': \n\nExpected behavior\nA clear and concise description of what you expected to happen.\nScreenshots\nIf applicable, add screenshots to help explain your problem.\nAdditional context\nAdd any other context about the problem here.\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-29", "closed_at": "2025-04-21", "labels": ["help wanted"], "State": "closed", "Author": "yingjiewei"}
{"issue_number": 6140, "issue_title": "worker runtime does not emit exceptions from main loop", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nThe worker runtime currently eats all exceptions that are thrown in the main loop. We should mirror the SingleThreadedAgentRuntime by having a parameter that will control the behavior", "created_at": "2025-03-28", "closed_at": null, "labels": [], "State": "open", "Author": "peterychang"}
{"issue_number": 6136, "issue_title": "Token Streaming for agents does not work with Tool calls", "issue_body": "What happened?\nDescribe the bug\nToken level streaming (enabled by setting model_client_stream = True) does not seem to work when instantiating an AssistantAgent with tools. The feature works as expected when no tools are specified.\nTo Reproduce\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.messages import TextMessage\nfrom autogen_core import CancellationToken\nfrom autogen_core.tools import FunctionTool\n\n\n# Define a tool using a Python function.\nasync def web_search_func(query: str) -> str:\n    \"\"\"Find information on the web\"\"\"\n    return \"AutoGen is a programming framework for building multi-agent applications.\"\n\n\n# This step is automatically performed inside the AssistantAgent if the tool is a Python function.\nweb_search_function_tool = FunctionTool(web_search_func, description=\"Find information on the web\")\n\n\nstreaming_assistant_with_tools = AssistantAgent(\n    name=\"assistant\",\n    model_client=gpt4o_client,\n    system_message=\"You are a helpful assistant.\",\n    tools = [web_search_function_tool],\n    model_client_stream=True,  # Enable streaming tokens.\n)\n\nasync for message in streaming_assistant_with_tools.on_messages_stream(  # type: ignore\n    [TextMessage(content=\"Name two cities in South America.Use tools to solve tasks\", source=\"user\")],\n    cancellation_token=CancellationToken(),\n):\n    print(message)\nExpected behavior\nI would expect the on_messages_stream or run_stream methods to stream the ModelClientStreamingChunk objects. However adding tools to the assistant agent seems to force the agent to stream each complete message.\nScreenshots\nStreaming without tools:\n\nStreaming with tools:\n\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\n0.4.8.1\nModel used\ngpt-4o-2023-07-01-preview\nModel provider\nAzure OpenAI\nOther model provider\nNo response\nPython version\n3.11\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-27", "closed_at": "2025-03-31", "labels": ["needs-triage"], "State": "closed", "Author": "c3-harshav"}
{"issue_number": 6133, "issue_title": "Missing content key causes error while using AuzerOpenAI", "issue_body": "What happened?\nDescribe the bug\ncontent key is missing while using AzureOpenAI model\nTo Reproduce\n\nuse AzureOpenAI model and Assistant\nAdd a tool for Assistant Agent\nChat with the agent\nCheck the generated request body: {'detail': [{'type': 'missing', 'loc': ['2024-02-01', 'messages', 3, 'assistant', 'content'], 'msg': 'Field required', 'input': {'tool_calls': [{'id': 'call_uaxTKJMcR0Do0n0oszQkJ0vz', 'function': {'arguments': '{}', 'name': 'transfer_to_user'}, 'type': 'function'}], 'role': 'assistant', 'name': 'manager_agent'}\n\n# Define a tool using a Python function.\nasync def web_search_func(query: str) -> str:\n    \"\"\"Find information on the web\"\"\"\n    return \"AutoGen is a programming framework for building multi-agent applications.\"\n\nhandoff_termination = HandoffTermination(target=\"user\")\n# Define a termination condition that checks for a specific text mention.\ntext_termination = TextMentionTermination(\"TERMINATE\")\n\n\ndef create_team() -> RoundRobinGroupChat:\n    assistant = AssistantAgent(\"assistant\",handoffs=[\"user\"], model_client=model_client, tools=[web_search_func],\n                               system_message=\"If you cannot complete the task, transfer to user. Otherwise, when finished, respond with 'TERMINATE'.\")\n    return RoundRobinGroupChat([assistant], termination_condition=handoff_termination | text_termination)\nExpected behavior\ncontent key should be added\nAdditional context\nI have checked the  issue #75 and #78. It seems to be fixed already. However I just met this error again\nI am using apiversion 2024-02-01,  GPT-4O-2024-08-06 deployment\nhttps://raw.githubusercontent.com/Azure/azure-rest-api-specs/refs/heads/main/specification/cognitiveservices/data-plane/AzureOpenAI/inference/preview/2024-12-01-preview/inference.json\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython 0.4.7\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nAzure OpenAI\nOther model provider\nNo response\nPython version\n3.12\n.NET version\nNone\nOperating system\nMacOS", "created_at": "2025-03-27", "closed_at": "2025-03-28", "labels": ["needs-triage"], "State": "closed", "Author": "toughnoah"}
{"issue_number": 6123, "issue_title": "Re-evaluating Internal Message Handling in SocietyOfMindAgent (v0.4+)", "issue_body": "What happened?\n\nold message : First start with SocietyOfMindAgent leaking inner monologues to outter team\nDescribe the bug\nWhen using a SocietyOfMindAgent inside a GroupChat, messages from the inner team (e.g., inner agents like agent1, agent2) are exposed to the outer GroupChat stream, rather than being contained within the internal reasoning. This leads to unexpected messages being surfaced to outer-level agents and logic, breaking isolation and causing potential routing/termination issues.\nTo Reproduce\nRun the following minimal example:\nimport asyncio\nfrom autogen_agentchat.agents import AssistantAgent, SocietyOfMindAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import MaxMessageTermination\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_agentchat.ui import Console\n\nasync def main():\n    model_client = OpenAIChatCompletionClient(model=\"gpt-4o\")\n\n    agent1 = AssistantAgent(\"agent1\", model_client=model_client, system_message=\"You are a writer.\")\n    agent2 = AssistantAgent(\"agent2\", model_client=model_client, system_message=\"You are a critic.\")\n\n    inner_team = RoundRobinGroupChat(\n        participants=[agent1, agent2],\n        termination_condition=MaxMessageTermination(2)\n    )\n\n    society_agent = SocietyOfMindAgent(\"society\", team=inner_team, model_client=model_client)\n\n    outer_agent = AssistantAgent(\"translator\", model_client=model_client, system_message=\"Translate to Spanish.\")\n\n    team = RoundRobinGroupChat(\n        participants=[society_agent, outer_agent],\n        termination_condition=MaxMessageTermination(2)\n    )\n\n    await Console(team.run_stream(task=\"Write a short story.\"))\n\nasyncio.run(main())\n\ud83d\udca5 Expected Console output shows all intermediate messages from the inner team:\nuser: Write a short story.\nagent1: Once upon a time...\nagent2: Needs more drama.\nsociety: Here's the revised version.\ntranslator: Aqu\u00ed est\u00e1 la historia.\n\n\ud83d\udca5 Real  Console output shows all intermediate messages from the inner team (Termination is do not work expected...):\nuser: Write a short story.\nagent1: Once upon a time... \nsociety: Here's the revised version.\n\nBut expected behavior:\nuser: Write a short story.\n>>> (inner, it could be do not showing) agent1: Once upon a time...\n>>> (inner, it could be do not showing) agent2: Needs more drama.\nsociety: Here's the revised version.\ntranslator: Aqu\u00ed est\u00e1 la historia.\n\nExpected behavior\nThe SocietyOfMindAgent should:\n\n\u2705 Internally run its team and return a single Response\n\u2705 Prevent inner messages from leaking into the outer GroupChat stream\n\u2699\ufe0f Optionally log intermediate messages to Console, but not expose them as ChatMessages to the rest of the system\n\n\nAdditional context\nThis behavior breaks message encapsulation, which is particularly problematic when using nested teams.\nThese changes would make SocietyOfMindAgent much more robust and suitable for nested orchestration scenarios.\n\nQuestion\nIs this behavior (inner messages leaking into the outer GroupChat) intended?\nOr is this something that might be considered a design oversight or bug?\nHappy to propose a patch once the expected behavior is clarified. \ud83d\ude4c\n\nI was able to investigate the core behavior of SocietyOfMindAgent more deeply. Since the original design intentions were not fully clear from the documentation alone, I compared the current implementation to the earlier version from v0.2.\nThrough this comparison, I confirmed that the current behavior introduces four functional regressions that did not exist before. Additionally, I identified four more architectural concerns introduced in recent versions.\nI've summarized them in the table below for review. I would really appreciate any feedback or validation on this analysis.\n\n\ud83d\udd0d SocietyOfMindAgent: Design Issues and Historical Comparison (v0.2 vs v0.4+)\n\u2705 P1\u2013P4 Regression Issue Table (Updated with Fixes in PR #6142)\n\n\n\nID\nDescription\nCurrent v0.4+ Issue\nResolution in PR #6142\nWas it a problem in v0.2?\nNotes\n\n\n\n\nP1\ninner_messages leaks into outer team termination evaluation\nResponse.inner_messages is appended to the outer team's _message_thread, affecting termination conditions. Violates encapsulation.\n\u2705 inner_messages is excluded from _message_thread, avoiding contamination of outer termination logic.\n\u274c No\nStructural boundary is now enforced\n\n\nP2\nInner team does not execute when outer message history is empty\nIn chained executions, if no new outer message exists, no task is created and the inner team is skipped entirely\n\u2705 Detects absence of new outer message and reuses the previous task, passing it via a handoff message. This ensures the inner team always receives a valid task to execute\n\u274c No\nThe issue was silent task omission, not summary failure. Summary succeeds as a downstream effect\n\n\nP3\nSummary LLM prompt is built from external input only\nPrompt is constructed using external message history, ignoring internal reasoning\n\u2705 Prompt construction now uses final_response.inner_messages, restoring internal reasoning as the source of summarization\n\u274c No\nMatches v0.2 internal monologue behavior\n\n\nP4\nExternal input is included in summary prompt (possibly incorrectly)\nOuter messages are used in the final LLM summarization prompt\n\u2705 Resolved via the same fix as P3; outer messages are no longer used for summary\n\u274c No\nRedundant with P3, now fully addressed\n\n\n\n\n\n\nID\nDescription\nCurrent v0.4+ Issue\nSuggested Fix\nWas it a problem in v0.2?\nNotes\n\n\n\n\nE1\nFragile count <= len(task) logic in stream parsing\nSkips a fixed number of messages assuming they are tasks. Breaks with team structure changes.\nUse explicit criteria like source == \"user\" to filter task messages\n\u274c No\nv0.2 had no streaming/yield logic\n\n\nE2\nStreaming chunks (e.g. ModelClientStreamingChunkEvent) ambiguity\nSome events are streamed but not stored \u2014 unclear if this is intentional\nAdd comments to clarify intent. Maintain current behavior.\n\u274c No\nv0.2 had no streaming structure. Keep current but document clearly.\n\n\nE3\nAmbiguous task/message boundary\nOuter tasks and inner messages are mixed conceptually\nClarify roles using message types or consistent tagging (e.g. source)\n\u274c No\nv0.2 handled outer input as \"User\" consistently. Just verify that continues to be true.\n\n\nE4\nreset() may not run if exception occurs\nIf run_stream() fails mid-execution, reset() is skipped \u2192 potential team state corruption\nWrap reset() inside a finally block for guaranteed cleanup\n\u26a0\ufe0f Partially\nSame logic in v0.2; lacks finally, so it's not always guaranteed either\n\n\n\n\nError?? Need to more information\n\n\n\nID\nDescription\nCurrent v0.4+ Issue\nSuggested Fix\nWas it a problem in v0.2?\nNotes\n\n\n\n\nP5\nreset() on inner team affects outer team state\nSocietyOfMindAgent calls await self._team.reset(), which resets shared team instances, unintentionally clearing the outer team's state\nEnsure inner team is a separate instance (e.g., deep copy), or isolate reset() behavior to avoid cross-team interference\nDoNot Check\nDoNot Check however It's Error. Q.E.D.\n\n\n\n\n\n\nMaybe... it's okay. Model context remember their context, so it's right behavior\n\n\n\n\nIf there are no objections to my conclusions, I would like to open a DRAFT PR to begin addressing these issues.\nSince this agent is critical for a production use case I'm working on, I\u2019m highly motivated to contribute toward improving its reliability.\nLooking forward to your feedback\u2014thank you!\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-26", "closed_at": "2025-04-04", "labels": ["needs-triage"], "State": "closed", "Author": "SongChiYoung"}
{"issue_number": 6117, "issue_title": "Gemini could not understand multiple system_message when work with openai SDK", "issue_body": "What happened?\nDescribe the bug\nThis BUG is related by #6116.\nWhen anthropic and openAI with openAI SDK case, Multiple system messages are working.\nHowever, Gemini case, it could not understand multiple system message, Just understand last SystemMessage.\nTo Reproduce\nHere is test code.\nclients =  [\n    anthropic_client,\n    openai_client,\n    anthropic_openai_client,\n    gemini_openai_client,\n]\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_core.models import (\n    SystemMessage,\n    UserMessage,\n)\n\n\nmessages = [\n    SystemMessage(content=\"When you say anything Start with 'FOO'\"),\n    SystemMessage(content=\"When you say anything End with 'BAR'\"),\n    UserMessage(content=\"Just say '.'\", source=\"user\"),\n]\n\nfor client in clients:\n    client = client()\n    print(f'=====[{client.model_info.get(\"family\")}]=====')\n    # try:\n    text = asyncio.run(client.create(messages=messages))\n\n    print(text.content)\n    # except Exception as e:\n    #     print(\"ERROR\", e, end=\" \")\n    #     print(f\"Error with {client.model_info.get('family')}\")\n    print(\"=====================================\")\nOutput(When fix #6116 haha)\n=====[claude-3.7-sonnet]=====\nFOO.BAR\n=====================================\n=====[gpt-4o]=====\nFOO. BAR\n=====================================\n=====[claude-3.7-sonnet]=====\nFOO.BAR\n=====================================\n=====[gemini-2.0-flash]=====\n.BAR\n\n=====================================\n\nYou can see it. Gemini only says BAR.\nMaybe.. it's just Gemini could not understand my system message?\nLet's test it.\nmessages = [\n#     SystemMessage(content=\"When you say anything Start with 'FOO'\"),\n#     SystemMessage(content=\"When you say anything End with 'BAR'\"),\n    SystemMessage(content=\"When you say anything Start with 'FOO'\\nWhen you say anything End with 'BAR'\"),\n    UserMessage(content=\"Just say '.'\", source=\"user\"),\n]\n\nfor client in clients:\n    client = client()\n    print(f'=====[{client.model_info.get(\"family\")}]=====')\n    # try:\n    text = asyncio.run(client.create(messages=messages))\n\n    print(text.content)\n    # except Exception as e:\n    #     print(\"ERROR\", e, end=\" \")\n    #     print(f\"Error with {client.model_info.get('family')}\")\n    print(\"=====================================\")\nOutput(When fix #6116 haha)\n=====[claude-3.7-sonnet]=====\nFOO.BAR\n=====================================\n=====[gpt-4o]=====\nFOO.BAR\n=====================================\n=====[claude-3.7-sonnet]=====\nFOO.BAR\n=====================================\n=====[gemini-2.0-flash]=====\nFOO.BAR\n\n=====================================\n\nNah~\nWe could check the gemini's openSDK aware codes has bug in there.\nExpected behavior\nI do not check the what code make that issue now.\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0), Python Extensions (autogen-ext)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nOpenAI SDK\nModel used\ngemini-2.0-flash\nModel provider\nGoogle Gemini\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-26", "closed_at": "2025-03-28", "labels": ["needs-triage"], "State": "closed", "Author": "SongChiYoung"}
{"issue_number": 6116, "issue_title": "Could not send Multiple system message at autogen-ext/.../anthropic/_anthropic_client.py", "issue_body": "What happened?\nDescribe the bug\nAnthropic sdk aware code(autogen-ext/../model/anthropic) could not work with multiple system message.\nHowever it could take it.\nTo Reproduce\n\nClinets *\n\nclients =  [\n    anthropic_client,\n    openai_client,\n    anthropic_openai_client,\n    gemini_openai_client,\n]\nmessages = [\n    SystemMessage(content=\"Must say foo too\"),\n    SystemMessage(content=\"Must say bar too\"),\n    UserMessage(content=\"tell\", source=\"user\"),\n]\n\nfor client in clients:\n    try:\n        text = asyncio.run(client.create(messages=messages))\n        print(text.content, client.model_info.get(\"family\"))\n    except Exception as e:\n        print(\"ERROR\", e, end=\" \")\n        print(f\"Error with {client.model_info.get('family')}\")\nthe output is that.\nERROR Multiple system messages are not supported Error with claude-3.7-sonnet\nI'm here to help! What would you like to know or talk about today? Foo and bar, perhaps? gpt-4o\nI need to say both \"foo\" and \"bar\" because those are the instructions from the prompt. So:\n\nfoo\nbar claude-3.7-sonnet\nBar too!\n gemini-2.0-flash\n\nwe could know that, JUST ANTHROPIC is could not work withmultiple system message.\nSo, I was change to _anthropic_client.py like it.\n        for message in messages:\n            if isinstance(message, SystemMessage):\n                if system_message is not None:\n                    pass\n                    # raise ValueError(\"Multiple system messages are not supported\")\n                system_message = to_anthropic_type(message)\n            else:\n                anthropic_message = to_anthropic_type(message)\n                if isinstance(anthropic_message, list):\n                    anthropic_messages.extend(anthropic_message)\n                elif isinstance(anthropic_message, str):\n                    msg = MessageParam(\n                        role=\"user\" if isinstance(message, UserMessage) else \"assistant\", content=anthropic_message\n                    )\n                    anthropic_messages.append(msg)\n                else:\n                    anthropic_messages.append(anthropic_message)\n??? WOW IT WORKS COLLECT!\n(I do not know about It works, why?, cause, Anthropic SDK case system_message is change at last system message, just delete that raise)\nI need to say both \"foo\" and \"bar\" in my response, so here they are: foo bar.\n\nIs there something specific you'd like me to tell you about? I'm happy to help with information, explanations, or other assistance. claude-3.7-sonnet\nfoo bar gpt-4o\nI understand you're asking me to tell you something, but I need a bit more guidance on what specifically you'd like me to tell you about. Could you please provide more details about what information you're looking for? I'd be happy to help once I understand your request better. claude-3.7-sonnet\nBar too.\n gemini-2.0-flash\n\nand they say \"foo\" and \"bar\" too. So we could know it.\n-> multiple system message is working now.\nExpected behavior\nMerge contents each other when multiple system message\nCould I know about any information of why adding it?\n-> SocietyOfMindAgent was makes two system message, so now Anthropic could not using that Agent.\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-26", "closed_at": "2025-03-28", "labels": ["needs-triage"], "State": "closed", "Author": "SongChiYoung"}
{"issue_number": 6096, "issue_title": "Autogen MCP : Error: unhandled errors in a TaskGroup (1 sub-exception) using Autogen and gpt-4o.", "issue_body": "What happened?\nDescribe the bug\nI faced a bug when I tried to use Autogen with the Shopify MCP server. The model calls the tool correctly, including the correct query.  It is working fine when tried using langchain and MCP Inspector.\nTo Reproduce\nimport asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.tools.mcp import StdioServerParams, mcp_server_tools\nfrom autogen_agentchat.ui import Console\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\nasync def main() -> None:\n# Get the fetch tool from mcp-server-fetch.\nshopify_mcp_server = StdioServerParams(command=\"node\", args=[\"shopify-mcp-server/build/index.js\"],env={'SHOPIFY_ACCESS_TOKEN':#shopify_access_token ,'MYSHOPIFY_DOMAIN':#shopify_domain})\ntools = await mcp_server_tools(shopify_mcp_server)\n# Create an agent that can use the fetch tool.\nmodel_client = OpenAIChatCompletionClient(model=\"gpt-4o\",api_key=#api_key)\nagent = AssistantAgent(name=\"shopify_assistant\", model_client=model_client, tools=tools, reflect_on_tool_use=True)  # type: ignore\n# Let the agent fetch the content of a URL and summarize it.\nawait Console (agent.run_stream(task=\"show order #1001\"))\n# print(result.messages[-1].content)\n\nasyncio.run(main())\nCurrent Behavior\nWhen I asked to show an order of order number 100 it is showing this output:\nShopify MCP Server running on stdio\n---------- user ----------\nshow order 100\n---------- shopify_assistant ----------\n[FunctionCall(id='call_2gEpQguofxe0Z5cLkLYNg3yk', arguments='{\"query\":\"name:100\",\"first\":1}', name='get-orders')]\nShopify MCP Server running on stdio\n---------- shopify_assistant ----------\n[FunctionExecutionResult(content='Error: unhandled errors in a TaskGroup (1 sub-exception)', name='get-orders', call_id='call_2gEpQguofxe0Z5cLkLYNg3yk', is_error=True)]\n---------- shopify_assistant ----------\nIt seems there was an error processing your request to fetch order 100. Could you please verify the order number or check if there are any connectivity issues? Let me know if you would like me to try again or assist further.\nWhen I give the orderId instead or order_number the model is fetching the correct result. But for most of the queries the same issue is happening.\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\ngpt-4o\nModel provider\nOpenAI\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nWindows", "created_at": "2025-03-25", "closed_at": "2025-03-27", "labels": ["needs-triage"], "State": "closed", "Author": "appuk45"}
{"issue_number": 6095, "issue_title": "Anthropic client should support 'thinking' argument", "issue_body": "What happened?\nTo take full advantage of the latest Anthropic model, Claude 3.7 Sonnet, the Anthropic client should support the thinking parameter as documented here.\nMaybe this should count as a feature request, but it's so trivial I wasn't sure it qualified.\nI think literally all that needs to be done is to add 'thinking' to this line:\n        # Optional parameters\n        for param in [\"top_p\", \"top_k\", \"stop_sequences\", \"metadata\"]:\nAnd probably add it as a constructor parameter as well.\nI have another question, though. Currently, almost nothing seems to be using the extra_create_args parameter. What if I want different AssistantAgents to use different arguments (including temperature and others). It seems that if I want this, I need to construct different clients for each set of arguments. But that feels a little odd. Is it by design?\nI'd like to go a step further and allow an AssistantAgent's parameters to change over its lifetime. I have agents which, depending on the circumstances, may want more predictable or more creative output. So I would like the ability to change the temperature and top-p dynamically. It would seem logical to me to do this by calling a method on the agent (possibly triggered by a tool, either invoked by the agent itself or a coordinating agent). Again, is there a design reason for not allowing this, or has the use case just not come up often enough?\nI'm happy to submit pull requests to add the necessary functionality (both the thinking parameter and the other changes discussed above), I just wanted to check that I'm not going down the wrong path.\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nclaude-3-7-sonnet-20250219\nModel provider\nAnthropic\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-25", "closed_at": null, "labels": ["needs-triage"], "State": "open", "Author": "shatteredLuigi"}
{"issue_number": 6093, "issue_title": "[Bug]: AutoGen Studio - Unclickable New Team, Session buttons", "issue_body": "What happened?\nNew Team and New Session buttons are not clickable in AutoGen Studio UI.\nSteps To Reproduce\n\nrun AutoGen Studio\n\nautogenstudio ui --port 8081\n\nopen in a browser\nclick those buttons\nyou can see that those buttons are not clickable.\n\nExpected behavior\nNo response\nScreenshots\n\n\nAdditional context\n\nAn application directory path is as follows:\nC:\\Users\\share0\\.autogenstudio\n\nWhich packages was the bug in?\nAutoGen Studio (autogensudio)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\n3.12\n.NET version\nNone\nOperating system\nWindows", "created_at": "2025-03-25", "closed_at": "2025-03-25", "labels": ["proj-studio"], "State": "closed", "Author": "Jung-YongHan"}
{"issue_number": 6090, "issue_title": "Autogen website accessibility issues (low severity)", "issue_body": "What happened?\nNote: Some of these issues may be the duplicates of the same underlying problem. If you are unable to reproduce an issue, please mark the issue as fixed and leave a comment to that effect!\n\n (9)\tClose button is not defined inside the 'hamburger' dialog. Problem Image\n (12)\tNo underline or change in color is appearing, when keyboard focus lands on links control. Problem Video -- @AndreaTang123\n (15)\tNo underline is provided on left navigation controls. Problem Image\n (16)\tScreen reader is announcing the information 'Navigation' twice when focus lands on left navigation control 'Installation'. Problem Video\n (17)\tScreen reader is announcing the information 'list' twice when focus lands on link control 'Trace Logger Name'. Problem Video\n (18)\tMultiple <H1> is defined on the page. Problem Video\n (21)\tUnderline/ Chevron is not defined for links present in \u2018On This Page\u2019 section. Problem Image\n (23)\tScreen reader is announcing unwanted position information '1 of 1' when focus lands on Collapse button. Problem Video\n (25)\tBack to Top control is not accessible with keyboard. Problem Video\n (28)\tAt 320*256 settings, Horizontal scrollbar appears on the page. Problem Image\n (30)\tIncorrect role is defined for the 'Venv' and 'Conda' controls. Problem Video\n (32)\tIn browse mode, screen reader is focus moving to hidden after 'Discover and create your own' link control. Problem Video\n (37)\tScreen reader not announcing out of list information. Problem Video\n (38)\tScreen reader announcing unnecessary information as document when focus lands on copy control. Problem Image\n (39)\tScreen reader announcing blank after 'Discover community projects' text. Problem Image\n (42)\tIn browse mode, screen reader focus order is not logical on tabs controls. Problem Video\n\nWhich packages was the bug in?\nPython Core (autogen-core)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-24", "closed_at": null, "labels": ["documentation", "help wanted"], "State": "open", "Author": "peterychang"}
{"issue_number": 6089, "issue_title": "Unable to view outputs of tool based agents in tracing tools like LangFuse, OpenLit and Microsoft Tracing", "issue_body": "What happened?\nDescribe the bug\n\nWe are unable to view outputs of tool based agents in Tracing tools like LangFuse, OpenLit and Microsoft Tracing.\nWe are encountering an error and unable to export traces when using the code provided in #5992\n\nTo Reproduce\n\nCode for reproducing Bug 1\n\nimport os\nimport base64\nimport openlit\nimport asyncio\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\nfrom opentelemetry import trace\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom autogen_agentchat.messages import AgentEvent, ChatMessage\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import AzureOpenAIChatCompletionClient\nfrom autogen_core import SingleThreadedAgentRuntime\nfrom azure.identity import DefaultAzureCredential, get_bearer_token_provider\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nos.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"...\"\nos.environ[\"LANGFUSE_SECRET_KEY\"] = \"...\"\nos.environ[\"LANGFUSE_HOST\"] = \"...\"\nLANGFUSE_AUTH = base64.b64encode(\n    f\"{os.environ.get('LANGFUSE_PUBLIC_KEY')}:{os.environ.get('LANGFUSE_SECRET_KEY')}\".encode()\n).decode()\nos.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = os.environ.get(\"LANGFUSE_HOST\") + \"/api/public/otel\"\nos.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Basic {LANGFUSE_AUTH}\"\n \ntrace_provider = TracerProvider()\ntrace_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))\ntrace.set_tracer_provider(trace_provider)\ntracer = trace.get_tracer(__name__)\nopenlit.init(tracer=tracer, disable_batch=True, disable_metrics=True)\n\nasync def Moral() -> str:\n    return \"No morals to this story\"\n\nasync def main():\n    API_KEY = os.getenv(\"api_key\")\n    Model_Name = os.getenv(\"model-name\")\n    Api_Version = os.getenv(\"api-version\")\n    Azure_Endpoint = os.getenv(\"azure_endpoint\")\n    Deployment_Name = os.getenv(\"deployment-name\")\n\n    az_model_client = AzureOpenAIChatCompletionClient(\n        azure_deployment=Deployment_Name,\n        model=Model_Name,\n        api_version=Api_Version,\n        azure_endpoint=Azure_Endpoint,\n        api_key=API_KEY\n    )\n    planning_agent = AssistantAgent(\n    \"PlanningAgent\",\n    description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n    model_client=az_model_client,\n    system_message=\"\"\"\n        You are a planning agent.\n        Your job is to break down complex tasks into smaller, manageable subtasks.\n        Your team members are:\n            Story_writer: Writes story and make corrections.\n            Story_reviewer: Checks if the story is for kids and Provides constructive feedback on Kids stories to add a positive impactful ending. It doesn't write the story, only provide feedback and improvements.\n            Story_moral: Finally, adds the moral to the story.\n\n        You only plan and delegate tasks - you do not execute them yourself. You can engage team members multiple times so that a perfect story is provided.\n\n        When assigning tasks, use this format:\n        1. <agent> : <task>\n\n        After all tasks are complete, summarize the findings and end with \"TERMINATE\".\n        \"\"\",\n    )\n\n    # Create the Writer agent.\n    Story_writer = AssistantAgent(\n        \"Story_writer\",\n        model_client=az_model_client,\n        system_message=\"You are a helpful AI assistant which write the story. Keep the story short.\"\n    )\n\n    # Create the Reviewer agent.\n    Story_reviewer = AssistantAgent(\n        \"Story_reviewer\",\n        model_client=az_model_client,\n        system_message=\"You are a helpful AI assistant which checks if the story is for kids and provides constructive feedback on Kids stories to have a postive impactful ending\",\n    )\n\n    # Story Moral Agent.\n    Story_moral = AssistantAgent(\n        \"Story_moral\",\n        model_client=az_model_client,\n        system_message=\"\"\"You are a helpful AI assistant which add the moral of the story using the 'Moral' tool, it has to be written by a seperation ' ========moral of the story==========='\"\"\",\n        tools=[Moral],\n        reflect_on_tool_use=True\n    )\n\n    text_mention_termination = TextMentionTermination(\"TERMINATE\")\n    max_messages_termination = MaxMessageTermination(max_messages=10)\n    termination = text_mention_termination | max_messages_termination\n\n    runtime = SingleThreadedAgentRuntime(tracer_provider=trace_provider)\n\n    with tracer.start_as_current_span(\"Sample-Trace\") as span:\n        runtime.start()\n        team = SelectorGroupChat(\n            [planning_agent, Story_writer, Story_reviewer, Story_moral],\n            model_client=az_model_client,\n            termination_condition=termination,\n            runtime = runtime\n        )\n        span.set_attribute(\"langfuse.user.id\", \"user-A\")\n        span.set_attribute(\"langfuse.session.id\", \"28\")\n        span.set_attribute(\"langfuse.tags\", [\"semantic-kernel\", \"demo-9\"])\n        span.set_attribute(\"langfuse.prompt.name\", \"test-9\")\n        await Console(\n            team.run_stream(task=\"write a story on rocket crash\")\n        )\nawait runtime.close()\nawait model_client.close()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\nCode for reproducing Bug 2\n\nimport os\nimport base64\nimport asyncio\nimport openlit\nfrom dotenv import load_dotenv\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import SimpleSpanProcessor\nfrom opentelemetry.sdk.resources import Resource\nfrom autogen_core import SingleThreadedAgentRuntime\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom autogen_agentchat.teams import SelectorGroupChat\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n\nload_dotenv()\nos.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"...\"\nos.environ[\"LANGFUSE_SECRET_KEY\"] = \"...\" \nLANGFUSE_AUTH = base64.b64encode(\n    f\"{os.environ.get('LANGFUSE_PUBLIC_KEY')}:{os.environ.get('LANGFUSE_SECRET_KEY')}\".encode()\n).decode()\n\ndef configure_oltp_tracing():\n    langfuse_exporter = OTLPSpanExporter(endpoint=\"...\", headers={\"Authorization\" : f\"Basic {LANGFUSE_AUTH}\"})\n    tracer_provider = TracerProvider(resource=Resource({\"service.name\" : \"autogen-test-agentchat\"}))\n    span_processor = SimpleSpanProcessor(langfuse_exporter)\n    tracer_provider.add_span_processor(span_processor)\n    trace.set_tracer_provider(tracer_provider)\n    return tracer_provider\n\ndef search_web_tool(query: str) -> str:\n    if \"2006-2007\" in query:\n        return \"\"\"Here are the total points scored by Miami Heat players in the 2006-2007 season:\n        Udonis Haslem: 844 points\n        Dwayne Wade: 1397 points\n        James Posey: 550 points\n        ...\n        \"\"\"\n    elif \"2007-2008\" in query:\n        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214.\"\n    elif \"2008-2009\" in query:\n        return \"The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398.\"\n    return \"No data found.\"\n\ndef percentage_change_tool(start: float, end: float) -> float:\n    return ((end - start) / start) * 100\n\nasync def main():\n    API_KEY = os.getenv(\"api_key\")\n    Model_Name = os.getenv(\"model-name\")\n    Deployment_Name = os.getenv(\"deployment-name\")\n    Api_Version = os.getenv(\"api-version\")\n    Azure_Endpoint = os.getenv(\"azure_endpoint\")\n    model_client = AzureOpenAIChatCompletionClient(\n        azure_deployment=Deployment_Name,\n        model=Model_Name,\n        api_version=Api_Version,\n        azure_endpoint=Azure_Endpoint,\n        api_key=API_KEY\n    )\n\n    planning_agent = AssistantAgent(\n        \"PlanningAgent\",\n        description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n        model_client=model_client,\n        system_message=\"\"\"\n        You are a planning agent.\n        Your job is to break down complex tasks into smaller, manageable subtasks.\n        Your team members are:\n            WebSearchAgent: Searches for information\n            DataAnalystAgent: Performs calculations\n\n        You only plan and delegate tasks - you do not execute them yourself.\n\n        When assigning tasks, use this format:\n        1. <agent> : <task>\n\n        After all tasks are complete, summarize the findings and end with \"TERMINATE\".\n        \"\"\",\n    )\n\n    web_search_agent = AssistantAgent(\n        \"WebSearchAgent\",\n        description=\"An agent for searching information on the web.\",\n        tools=[search_web_tool],\n        model_client=model_client,\n        system_message=\"\"\"\n        You are a web search agent.\n        Your only tool is search_tool - use it to find information.\n        You make only one search call at a time.\n        Once you have the results, you never do calculations based on them.\n        \"\"\",\n    )\n\n    data_analyst_agent = AssistantAgent(\n        \"DataAnalystAgent\",\n        description=\"An agent for performing calculations.\",\n        model_client=model_client,\n        tools=[percentage_change_tool],\n        system_message=\"\"\"\n        You are a data analyst.\n        Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.\n        If you have not seen the data, ask for it.\n        \"\"\",\n    )\n\n    text_mention_termination = TextMentionTermination(\"TERMINATE\")\n    max_messages_termination = MaxMessageTermination(max_messages=25)\n    termination = text_mention_termination | max_messages_termination\n\n    selector_prompt = \"\"\"Select an agent to perform task.\n\n    {roles}\n\n    Current conversation context:\n    {history}\n\n    Read the above conversation, then select an agent from {participants} to perform the next task.\n    Make sure the planner agent has assigned tasks before other agents start working.\n    Only select one agent.\n    \"\"\"\n\n    task = \"Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\"\n    tracer_provider = configure_oltp_tracing()\n    openlit.init(tracer=tracer_provider, disable_batch=True, disable_metrics=True)\n    runtime = SingleThreadedAgentRuntime(tracer_provider=tracer_provider)\n    tracer = trace.get_tracer(\"autogen-test-agentchat\")\n    with tracer.start_as_current_span(\"runtime\") as span:\n        runtime.start()\n        team = SelectorGroupChat(\n            [planning_agent, web_search_agent, data_analyst_agent],\n            model_client=model_client,\n            termination_condition=termination,\n            selector_prompt=selector_prompt,\n            allow_repeated_speaker=True,  # Allow an agent to speak multiple turns in a row.\n            runtime=runtime,\n        )\n        span.set_attribute(\"langfuse.user.id\", \"user-A\")\n        span.set_attribute(\"langfuse.session.id\", \"26\")\n        span.set_attribute(\"langfuse.tags\", [\"semantic-kernel\", \"demo-9\"])\n        span.set_attribute(\"langfuse.prompt.name\", \"test-9\")\n        await Console(team.run_stream(task=task))\n    \n    await runtime.close()\n    await model_client.close()\n\nasyncio.run(main())\n\nScreenshots\n\nScreenshot for bug 1\n\n\n\nScreenshot for bug 2\n\n\nTags\n#5995\n@victordibia\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.0)\nAutoGen library version.\n0.4.9.2\nOther library version.\nNo response\nModel used\ngpt-4o\nModel provider\nAzure OpenAI\nOther model provider\nNo response\nPython version\n3.11\n.NET version\nNone\nOperating system\nWindows", "created_at": "2025-03-24", "closed_at": null, "labels": ["needs-triage"], "State": "open", "Author": "Arjun-Prakash10"}
{"issue_number": 6088, "issue_title": "Session State Leaking Between Different Browser Sessions in AutoGen", "issue_body": "What happened?\nSession State Leaking Between Different Browser Sessions in AutoGen\nDescription\nI've discovered an issue where conversation state is being shared between different browser sessions despite having unique session IDs and unique agent names. When a user connects to my application from a second browser, the second session's agent has access to messages from the first session, creating a privacy concern.\nObserved Behavior\nWhen two different browser sessions connect to the same WebSocket endpoint, they are assigned different session IDs and I create agents with different names for each session. However, the second session's state contains messages from the first session, indicating state leakage between sessions.\nHere are the saved states showing the problem:\nSession 1 State:\n{\n  \"type\": \"TeamState\",\n  \"version\": \"1.0.0\",\n  \"agent_states\": {\n    \"group_chat_manager/6b5cf9d6-3d5a-48c5-b09d-775510e7401c\": {\n      \"type\": \"RoundRobinManagerState\",\n      \"version\": \"1.0.0\",\n      \"message_thread\": [\n        {\n          \"source\": \"user\",\n          \"models_usage\": null,\n          \"metadata\": {},\n          \"content\": \"Hi\",\n          \"type\": \"TextMessage\"\n        },\n        {\n          \"source\": \"AgentSowWriter_s0e2fefd4\",\n          \"models_usage\": {\n            \"prompt_tokens\": 964,\n            \"completion_tokens\": 26\n          },\n          \"metadata\": {},\n          \"content\": \"Hello! How can I assist you today in writing a Due Diligence Scope of Work (SOW) document?\",\n          \"type\": \"TextMessage\"\n        }\n      ],\n      \"current_turn\": 0,\n      \"next_speaker_index\": 0\n    },\n    \"collect_output_messages/6b5cf9d6-3d5a-48c5-b09d-775510e7401c\": {},\n    \"AgentSowWriter_s0e2fefd4/6b5cf9d6-3d5a-48c5-b09d-775510e7401c\": {\n      \"type\": \"ChatAgentContainerState\",\n      \"version\": \"1.0.0\",\n      \"agent_state\": {\n        \"type\": \"AssistantAgentState\",\n        \"version\": \"1.0.0\",\n        \"llm_context\": {\n          \"messages\": [\n            {\n              \"content\": \"Hi\",\n              \"source\": \"user\",\n              \"type\": \"UserMessage\"\n            },\n            {\n              \"content\": \"Hello! How can I assist you today in writing a Due Diligence Scope of Work (SOW) document?\",\n              \"thought\": null,\n              \"source\": \"AgentSowWriter_s0e2fefd4\",\n              \"type\": \"AssistantMessage\"\n            }\n          ]\n        }\n      },\n      \"message_buffer\": []\n    }\n  },\n  \"team_id\": \"6b5cf9d6-3d5a-48c5-b09d-775510e7401c\"\n}\nSession 2 State:\n{\n  \"type\": \"TeamState\",\n  \"version\": \"1.0.0\",\n  \"agent_states\": {\n    \"group_chat_manager/fdd33bfd-345f-494d-a73b-99c257bd3a60\": {\n      \"type\": \"RoundRobinManagerState\",\n      \"version\": \"1.0.0\",\n      \"message_thread\": [\n        {\n          \"source\": \"user\",\n          \"models_usage\": null,\n          \"metadata\": {},\n          \"content\": \"Hi\",\n          \"type\": \"TextMessage\"\n        },\n        {\n          \"source\": \"AgentSowWriter_s00010d6e\",\n          \"models_usage\": {\n            \"prompt_tokens\": 1010,\n            \"completion_tokens\": 25\n          },\n          \"metadata\": {},\n          \"content\": \"Hello again! How can I assist you with the Due Diligence Scope of Work (SOW) document?\",\n          \"type\": \"TextMessage\"\n        }\n      ],\n      \"current_turn\": 0,\n      \"next_speaker_index\": 0\n    },\n    \"collect_output_messages/fdd33bfd-345f-494d-a73b-99c257bd3a60\": {},\n    \"AgentSowWriter_s00010d6e/fdd33bfd-345f-494d-a73b-99c257bd3a60\": {\n      \"type\": \"ChatAgentContainerState\",\n      \"version\": \"1.0.0\",\n      \"agent_state\": {\n        \"type\": \"AssistantAgentState\",\n        \"version\": \"1.0.0\",\n        \"llm_context\": {\n          \"messages\": [\n            {\n              \"content\": \"Hi\",\n              \"source\": \"user\",\n              \"type\": \"UserMessage\"\n            },\n            {\n              \"content\": \"Hello! How can I assist you today in writing a Due Diligence Scope of Work (SOW) document?\",\n              \"thought\": null,\n              \"source\": \"AgentSowWriter_s0e2fefd4\",\n              \"type\": \"AssistantMessage\"\n            },\n            {\n              \"content\": \"Hi\",\n              \"source\": \"user\",\n              \"type\": \"UserMessage\"\n            },\n            {\n              \"content\": \"Hello again! How can I assist you with the Due Diligence Scope of Work (SOW) document?\",\n              \"thought\": null,\n              \"source\": \"AgentSowWriter_s00010d6e\",\n              \"type\": \"AssistantMessage\"\n            }\n          ]\n        }\n      },\n      \"message_buffer\": []\n    }\n  },\n  \"team_id\": \"fdd33bfd-345f-494d-a73b-99c257bd3a60\"\n}\nNotice that in Session 2's llm_context, it contains messages from both AgentSowWriter_s0e2fefd4 (Session 1) and AgentSowWriter_s00010d6e (Session 2).\nExpected Behavior\nEach browser session should have its own isolated state. Session 2 should only contain messages from its own conversation, not messages from Session 1.\nCode\nHere's the relevant code for creating separate teams for each session:\nasync def get_team(assistant: str, session_id: str) -> RoundRobinGroupChat:\n    # Create a model client\n    az_model_client = AzureOpenAIChatCompletionClient(\n        model=os.getenv(\"AZURE_MODEL_NAME\"),\n        api_version=os.getenv(\"AZURE_API_VERSION\"),\n        azure_endpoint=os.getenv(\"AZURE_ENDPOINT_URL\"),\n        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n    )\n\n    # Each session has a unique ID\n    import hashlib\n    session_hash = hashlib.md5(session_id.encode()).hexdigest()[:8]\n    unique_agent_name = f\"{assistant}_s{session_hash}\"\n\n    if assistant not in assistant_configs:\n        raise ValueError(f\"Invalid assistant name: {assistant}\")\n\n    config = assistant_configs[assistant]\n    agent = AssistantAgent(name=unique_agent_name, model_client=az_model_client, **config)\n\n    termination_condition = TextMessageTermination(unique_agent_name)\n\n    team = RoundRobinGroupChat(\n        [agent],\n        termination_condition=termination_condition,\n    )\n\n    # Get session-specific paths\n    paths = get_session_path(session_id, assistant)\n    state_path = paths[\"state_path\"]\n\n    # Load state if it exists\n    if os.path.exists(state_path):\n        async with aiofiles.open(state_path, \"r\") as file:\n            state = json.loads(await file.read())\n        await team.load_state(state)\n\n    return team\nSteps to Reproduce\n\nSet up a WebSocket server as in the code provided\nConnect from Browser 1, send a message, and get a response\nConnect from Browser 2 (using a different session)\nSend a message from Browser 2\nCheck the state for Browser 2's session - it will contain messages from Browser 1's session\n\nImpact\nThis is a critical issue for multi-user applications as it creates a privacy breach where users could potentially see messages from other users' conversations.\nAttempted Workarounds\nI've tried:\n\nCreating agents with unique names for each session\nStoring state in session-specific files\n\nNone of these approaches have successfully isolated the state between sessions.\nAny help would be greatly appreciated.\nWhich packages was the bug in?\nPython AgentChat (autogen-agentchat>=0.4.8)\nAutoGen library version.\nOther (please specify)\nOther library version.\n0.4.8\nModel used\ngpt-4o-mini\nModel provider\nAzure OpenAI\nOther model provider\nNo response\nPython version\n3.11\n.NET version\nNone\nOperating system\nWindows", "created_at": "2025-03-24", "closed_at": "2025-03-26", "labels": ["needs-triage"], "State": "closed", "Author": "divyansh-23906"}
{"issue_number": 6085, "issue_title": "Get current message thread from a group chat team.", "issue_body": "Confirmation\n\n I confirm that I am a maintainer and so can use this template. If I am not, I understand this issue will be closed and I will be asked to use a different template.\n\nIssue body\nFor BaseGroupChat, support a public method for getting all the messages happened so far via an async send_message to the group chat manager agent. This will require a new GroupChatGetThread RPC event type, and handled by the BaseGroupChatManager.", "created_at": "2025-03-24", "closed_at": null, "labels": ["good first issue", "help wanted", "proj-agentchat"], "State": "open", "Author": "ekzhu"}
{"issue_number": 6084, "issue_title": "When Call MultimodalWebSurfer after using tool, error occurred", "issue_body": "What happened?\nDescribe the bug\nWhen using MultimodalWebSurfer after a tool call, the agent crashes due to an unhandled message type in on_messages_stream.\nTo Reproduce\nfrom autogen_ext.agents.web_surfer import MultimodalWebSurfer\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.teams import RoundRobinGroupChat\nfrom autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\nfrom autogen_agentchat.ui import Console\n\n\ntermination = TextMentionTermination(\"TERMINATE\")\n\ndef test():\n    return \"Hello, World!\"\n\nmulti_modal_web_surfer = MultimodalWebSurfer(\n    name=\"web_surfer\",\n    description=\"A web surfer agent that can surf the web for information.\",\n    model_client=anthropic_client,\n    # system_message=\"You are a web surfer agent and your task is to surf the web for information. Return `TERMINATE` once the information is found.\",\n)\n\ntest_agent = AssistantAgent(\n    name=\"test_agent\",\n    description=\"A test agent that returns a string.\",\n    model_client=anthropic_client,\n    system_message=\"You are a test agent that run test function.\",\n    tools = [test],\n)\n\nteam = RoundRobinGroupChat(\n    participants=[test_agent, multi_modal_web_surfer],\n    termination_condition=termination,\n)\n\nasync def team_run():\n    await Console(\n        team.run_stream(\n            task=\"try test function\"\n        )\n    )\n\nasyncio.run(team_run())\nObserved Behavior The agent throws:\n/Users/cysong/Documents/MOTOV_\u110b\u1165\u11b8\u1106\u116e/codes/TIPS/ai-agent/autogen/python/packages/autogen-ext/src/autogen_ext/models/openai/_openai_client.py:397: UserWarning: Missing required field 'structured_output' in ModelInfo. This field will be required in a future version of AutoGen.\n  validate_model_info(self._model_info)\n---------- user ----------\ntry test function\n---------- test_agent ----------\nI'll help you run the test function. Let me do that for you now.\n---------- test_agent ----------\nError processing publish message for web_surfer_d5a78cc6-f002-46ef-9eae-ab0ccfe113a9/d5a78cc6-f002-46ef-9eae-ab0ccfe113a9\nTraceback (most recent call last):\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_single_threaded_agent_runtime.py\", line 533, in _on_message\n    return await agent.on_message(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_base_agent.py\", line 113, in on_message\n    return await self.on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 67, in on_message_impl\n    return await super().on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_routed_agent.py\", line 485, in on_message_impl\n    return await h(self, message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_routed_agent.py\", line 268, in wrapper\n    return_value = await func(self, message, ctx)  # type: ignore\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 69, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-ext/src/autogen_ext/agents/web_surfer/_multimodal_web_surfer.py\", line 438, in on_messages_stream\n    raise ValueError(f\"Unexpected message in MultiModalWebSurfer: {chat_message}\")\nValueError: Unexpected message in MultiModalWebSurfer: source='test_agent' models_usage=None metadata={} content='Hello, World!' type='ToolCallSummaryMessage'\n[FunctionCall(id='toolu_01PgBqKGtjgWJ4ib6x2jx5T5', arguments='{}', name='test')]\n---------- test_agent ----------\n[FunctionExecutionResult(content='Hello, World!', name='test', call_id='toolu_01PgBqKGtjgWJ4ib6x2jx5T5', is_error=False)]\n---------- test_agent ----------\nHello, World!\nTraceback (most recent call last):\n  File \"/Users/cysong/Documents/ai-agent/test-agent/my-app/errortest.py\", line 72, in <module>\n    asyncio.run(team_run())\n  File \"/Users/cysong/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/asyncio/runners.py\", line 190, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/test-agent/my-app/errortest.py\", line 66, in team_run\n    await Console(\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-agentchat/src/autogen_agentchat/ui/_console.py\", line 117, in Console\n    async for message in stream:\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_base_group_chat.py\", line 482, in run_stream\n    await shutdown_task\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_base_group_chat.py\", line 426, in stop_runtime\n    await self._runtime.stop_when_idle()\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_single_threaded_agent_runtime.py\", line 769, in stop_when_idle\n    await self._run_context.stop_when_idle()\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_single_threaded_agent_runtime.py\", line 120, in stop_when_idle\n    await self._run_task\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_single_threaded_agent_runtime.py\", line 109, in _run\n    await self._runtime._process_next()  # type: ignore\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_single_threaded_agent_runtime.py\", line 604, in _process_next\n    raise e from None\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_single_threaded_agent_runtime.py\", line 551, in _process_publish\n    await asyncio.gather(*responses)\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_single_threaded_agent_runtime.py\", line 546, in _on_message\n    raise e\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_single_threaded_agent_runtime.py\", line 533, in _on_message\n    return await agent.on_message(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_base_agent.py\", line 113, in on_message\n    return await self.on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_sequential_routed_agent.py\", line 67, in on_message_impl\n    return await super().on_message_impl(message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_routed_agent.py\", line 485, in on_message_impl\n    return await h(self, message, ctx)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-core/src/autogen_core/_routed_agent.py\", line 268, in wrapper\n    return_value = await func(self, message, ctx)  # type: ignore\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-agentchat/src/autogen_agentchat/teams/_group_chat/_chat_agent_container.py\", line 69, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n  File \"/Users/cysong/Documents/ai-agent/autogen/python/packages/autogen-ext/src/autogen_ext/agents/web_surfer/_multimodal_web_surfer.py\", line 438, in on_messages_stream\n    raise ValueError(f\"Unexpected message in MultiModalWebSurfer: {chat_message}\")\nValueError: Unexpected message in MultiModalWebSurfer: source='test_agent' models_usage=None metadata={} content='Hello, World!' type='ToolCallSummaryMessage'\n\nExpected behavior\nRoot Cause In _multimodal_web_surfer.py:\n    async def on_messages_stream(\n        self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken\n    ) -> AsyncGenerator[AgentEvent | ChatMessage | Response, None]:\n        for chat_message in messages:\n            if isinstance(chat_message, TextMessage | MultiModalMessage):\n                self._chat_history.append(UserMessage(content=chat_message.content, source=chat_message.source))\n            else:\n                raise ValueError(f\"Unexpected message in MultiModalWebSurfer: {chat_message}\")\nCurrently, only TextMessage and MultiModalMessage are allowed. However, in team-based workflows (especially after tool calls), messages like ToolCallSummaryMessage, FunctionExecutionResultMessage may also be streamed.\nExpected Behavior MultimodalWebSurfer should either:\nGracefully skip unsupported message types, or\nHandle/append fallback content from those messages (e.g., str(chat_message.content))\nSuggestion Would it make sense to generalize this logic to handle all ChatMessage types, or at least skip the unrecognized ones with a warning?\nAlso, could you clarify the rationale for limiting _chat_history to only TextMessage and MultiModalMessage? Would broader support break assumptions elsewhere?\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nAnthropic\nOther model provider\nNo response\nPython version\n3.11\n.NET version\nNone\nOperating system\nMacOS", "created_at": "2025-03-24", "closed_at": "2025-03-26", "labels": ["needs-triage"], "State": "closed", "Author": "SongChiYoung"}
{"issue_number": 6083, "issue_title": "Anthorpic models have error when send empty content", "issue_body": "What happened?\nDescribe the bug\nWhen send empty content to anthropic model, it will be response error.\nIt's same error with #5762\nTo Reproduce\nclients =  [\n    anthropic_client,\n    openai_client,\n    anthropic_openai_client,\n    gemini_openai_client,\n]\n\nmessages = [\n    UserMessage(content=\"\", source=\"user\"),\n]\n\nfor client in clients:\n    try:\n        text = asyncio.run(client.create(messages=messages))\n        # print(text)\n    except Exception as e:\n        print(e)\n        print(f\"Error with {client.model_info.get(\"family\")}\")\nresult\nError code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'messages.0: all messages must have non-empty content except for the optional final assistant message'}}\nError with claude-3.7-sonnet\nError code: 400 - {'error': {'code': 'invalid_request_error', 'message': 'messages.0: all messages must have non-empty content except for the optional final assistant message', 'type': 'invalid_request_error', 'param': None}}\nError with claude-3.7-sonnet\n\nWhen send empty content, anthropic response error.\nExpected behavior\nWhen request to anthropic model, change empty to whitespace(\" \")\nSame error as OpenAI SDK and Anthropic SDK too\nAdditional context\nHow about fix this error with that PR #6063\nWhich packages was the bug in?\nPython Extensions (autogen-ext)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nNo response\nModel provider\nAnthropic\nOther model provider\nNo response\nPython version\n3.11\n.NET version\nNone\nOperating system\nMacOS", "created_at": "2025-03-24", "closed_at": "2025-03-31", "labels": ["needs-triage"], "State": "closed", "Author": "SongChiYoung"}
{"issue_number": 6081, "issue_title": "Autogen when tried for Gemini model", "issue_body": "What happened?\nDescribe the bug\nI am trying to use Gemini model from vertex AI in autogen\nUsed the documentation (Use Autogen with Gemini via VertexAI)\nTo Reproduce\nCreated a project in vertex ai, used that model in Config in autogen.\nCode used:\nconfig=[{\n\"model\": \"gemini-1.5-flash-002\",\n\"api_type\": \"google\",\n\"project_id\": PROJECT,\n\"location\": \"us-central1\"\n}\n]\nImageAgent= MultimodalConversableAgent(\"assitant\", llm_config={\"config_list\": config}, max_consecutive_auto_reply =1)\nuser_proxy = UserProxyAgent(\"userproxy\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=0)\nuser_proxy.initiate_chat(ImageAgent,\nmessage=\"\"\"Describe the image clearly\"\n. \"\"\")\nExpected behavior\nExpected the response but getting\nResourceExhausted: 429 received Metadata size exceeded hard limit (value length 637301 vs. 16384).\nIt seems like grpc error.\nI followed documentation code thoroughly but still facing issues\nWhich packages was the bug in?\nPython Core (autogen-core)\nAutoGen library version.\nPython dev (main branch)\nOther library version.\nNo response\nModel used\nGemini flash 1.5\nModel provider\nNone\nOther model provider\nNo response\nPython version\nNone\n.NET version\nNone\nOperating system\nNone", "created_at": "2025-03-24", "closed_at": null, "labels": ["0.2", "needs-triage"], "State": "open", "Author": "purvaja2015"}
