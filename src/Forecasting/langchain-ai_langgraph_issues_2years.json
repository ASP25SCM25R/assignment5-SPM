{"issue_number": 7, "issue_title": "Async runnables", "issue_body": "Quick question: the default langchain.runnables class supports async, but when I try to use async functions in chains with PubSub I get an error traced back to the transform_stream_with_config method:\nTypeError: Cannot invoke a coroutine function synchronously.Use ainvoke instead.\nI admittedly have not dug into the permchain or runnables code very deep to see what's actually going on, nor would I probably understand it anyway, but I'm curious if there's any way around this issue.", "created_at": "2023-10-05", "closed_at": "2024-01-08", "labels": [], "State": "closed", "Author": "tgram-3D"}
{"issue_number": 17, "issue_title": "Recursion error when printing app", "issue_body": "from permchain import Channel, Pregel\n\ngrow_value = (\n    Channel.subscribe_to(\"value\")\n    | (lambda x: x + x)\n    | Channel.write_to(value=lambda x: x if len(x) < 10 else None)\n)\n\napp = Pregel(\n    chains={\"grow_value\": grow_value,},\n    input=\"value\",\n    output=\"value\",\n)\n\nprint(app) # <-- fails", "created_at": "2023-11-15", "closed_at": "2024-01-08", "labels": [], "State": "closed", "Author": "eyurtsev"}
{"issue_number": 56, "issue_title": "missing MessageGraph", "issue_body": "Dear Langchain,\nI cannot find MessageGraph module when trying to test agent-simulation-evaluation.ipynb,\nplease help me to install that module, thanks a lot!\nCheers!\nRichard", "created_at": "2024-01-22", "closed_at": "2024-01-23", "labels": [], "State": "closed", "Author": "rwaneyvin"}
{"issue_number": 55, "issue_title": "How to use the ReAct agent from langchain with langgraph?", "issue_body": "Please add an example of ReAct agent (https://python.langchain.com/docs/modules/agents/agent_types/react) usage together with the langgraph framework.\nI tried to combine them like so:\n# Construct the ReAct agent\n\nmy_input = \"some example utterance\"\n\nagent = create_react_agent(llm, tools, react_template_base)\n\nagent_executer = create_agent_executor(agent, tools)\n\ninputs = {\"messages\": [HumanMessage(content=my_input)]}\n\nfor output in agent_executer.stream(inputs):\n    # stream() yields dictionaries with output keyed by node name\n    for key, value in output.items():\n        print(f\"Output from node '{key}':\")\n        print(\"---\")\n        print(value)\n    print(\"\\n---\\n\")\n\nbut this doesn't work as expected.", "created_at": "2024-01-22", "closed_at": "2024-01-23", "labels": [], "State": "closed", "Author": "DanielProkhorov"}
{"issue_number": 54, "issue_title": "how update graph state ", "issue_body": "eg:\n# Define the agent def run_agent(data): agent_outcome = agent_runnable.invoke(data) logger.warning(f\"Agent outcome: {data}\") data['define state  attr'] = \"\" return {\"agent_outcome\": agent_outcome} ", "created_at": "2024-01-22", "closed_at": "2024-02-25", "labels": [], "State": "closed", "Author": "linpan"}
{"issue_number": 45, "issue_title": "Parallel Tool Calling and LLM Token Streaming Issue", "issue_body": "Is there a reason you've been using legacy function calling for OpenAI models in all the langgraph examples instead of tools? Before the latest update with ToolExecutor I did something sort-of similar by subclassing Runnable, overwriting invoke then calling batch on the tool inputs for concurrent execution with a tools agent and that worked fine, but I'm just curious why you aren't promoting the use of tools vs. functions with langgraph like you are for Agents, and adding support for tool_call_ids, etc. With an openai_tools_agent and the new classes something like this gives the correct output:\ndef execute_tools_concurrent(data):\n    agent_actions = data.pop('agent_outcome')\n    tool_actions = [ToolInvocation(tool = action.tool, tool_input=action.tool_input) for action in agent_actions]\n    outputs = tool_executor.batch(tool_actions)\n    full_output = list(zip(agent_actions, outputs))\n    data['intermediate_steps'].extend(full_output)\n    return data \n\nBut for building from scratch in langgraph and appending ToolMessages instead of FunctionMessages to the state, the new classes do not support adding tool_call_ids, so you have to extend those classes. I'm currently adding an attribute to ToolInvocationInterface and ToolInvocation for tool_call_id and extending ToolExecutor to output the following from _execute and _aexecute:\nreturn {\"id\": tool_invocation.tool_call_id, \"output\": output}\nThen you can do this sort of thing:\n...\ntool_calls = last_message.additional_kwargs['tool_calls']\n\nactions = [ToolInvocationWithID(\n        tool=tool_call['function']['name'],\n        tool_input=json.loads(tool_call['function']['arguments']), \n        tool_call_id = tool_call['id']\n    ) for tool_call in tool_calls]\n\nresponses = enhanced_executor.batch(actions)\n   \ntool_messages = [ToolMessage(content=str(r['output']), tool_call_id=r['id']) for r in responses]\n\nreturn {\"messages\": tool_messages}\n\nThis does work, but maybe there's a better way? Am I missing something?\n\nMy actual issue is I can't get the LLM token streaming to work. I updated langchain, langchain_openai, and langgraph and copied streaming-tokens.ipynb verbatim and I'm getting no /logs/ output at all from astream_log. Are you sure the implementation with:\nasync def call_model(state):\n    messages = state['messages']\n    response = await model.ainvoke(messages)\n  \n    return {\"messages\": [response]}\n\nis correct? Do you need to change it to astream or something to get the tokens?", "created_at": "2024-01-18", "closed_at": null, "labels": [], "State": "open", "Author": "tgram-3D"}
{"issue_number": 37, "issue_title": "Adding message history (memory)", "issue_body": "Seems like RunnableWithMessageHistory is not yet compatible with langgraph.\nAny recommendation at the moment to integrate memory into langgraphs?", "created_at": "2024-01-16", "closed_at": "2024-01-18", "labels": [], "State": "closed", "Author": "jktoh"}
{"issue_number": 35, "issue_title": "[Question] Max Number of steps seems equal to 13-ish?", "issue_body": "Hi, when I create a graph, compile it to chain, and invoke it, the chain just stop in the middle.\nHere is a very simple example to reproduce this\nimport types\nfrom typing import Optional\n\nfrom langgraph.graph import Graph\n\nfrom src.langgraph.agents.common import copy_func\nfrom loguru import logger\n\n\ndef copy_func(f, new_name: str = None):\n    return types.FunctionType(f.__code__,\n                              f.__globals__,\n                              new_name or f.__name__,\n                              f.__defaults__,\n                              f.__closure__)\n\n\ndef mk_noop_node(node_name: str, logger_msg: Optional[str] = None):\n    def _noop(data):\n        if logger_msg is not None:\n            logger.info(logger_msg)\n        return data\n\n    _noop = copy_func(_noop, node_name)\n    return _noop\n\n\ngraph = Graph()\n\n# Create and add 100 noop nodes to the graph\nfor i in range(100):\n    node_name = f\"noop_node_{i}\"\n    logger_msg = f\"Executing {node_name}\"\n    noop_node = mk_noop_node(node_name, logger_msg)\n    graph.add_node(node_name, noop_node)\n\n    # Link the nodes in sequence\n    if i > 0:\n        previous_node_name = f\"noop_node_{i - 1}\"\n        graph.add_edge(previous_node_name, node_name)\n\n# Set the entry point to the first node\ngraph.set_entry_point(\"noop_node_0\")\n\n# Optionally, if you want to mark an end to the graph\ngraph.set_finish_point(f\"noop_node_{99}\")\n\nchain = graph.compile()\n\n# Example of invoking the graph\ninitial_data = {\"message\": \"Start of the graph\"}\nresult = chain.invoke(initial_data)\nHere is the result\n", "created_at": "2024-01-15", "closed_at": "2024-01-15", "labels": [], "State": "closed", "Author": "zhentao-xu"}
{"issue_number": 34, "issue_title": "[Question] How to use OpenAIAssistantRunnable on langgraph?", "issue_body": "Hello,\nI would like to use OpenAIAssistantRunnable with langgraph, but I faced an error.\nI create an assistant as following:\nassistant = OpenAIAssistantRunnable.create_assistant( name=name, instructions=instructions, tools=tools, model=\"gpt-4-1106-preview\", assistant_id=\"asst_kqRc9iTKmgv3j5Usr0ulcYSf\", streaming=True )\nAfter that I create an agent note:\nagent = RunnablePassthrough.assign( agent_outcome = assistant )\nThen I use this agent on sample code of langgraph,\nbut i faced an error as following:\nFile \"/home/thiepnq/Sample/graph.py\", line 50, in execute_tools tool_to_use = {t.name: t for t in tools}[agent_action.tool] AttributeError: 'list' object has no attribute 'tool'\nCould you help me to solve this issue.", "created_at": "2024-01-13", "closed_at": "2024-03-18", "labels": [], "State": "closed", "Author": "nqthiep"}
{"issue_number": 33, "issue_title": "Using LangGraph with Models on HuggingFace", "issue_body": "Hi!\nCan someone use this package with open-source models, like those released on HF, instead of the OpenAI models?\nThanks!", "created_at": "2024-01-09", "closed_at": null, "labels": [], "State": "open", "Author": "PouriaRouzrokh"}
{"issue_number": 29, "issue_title": "\u3010Question\u3011Why not implement DAG in LCEL\uff1f", "issue_body": "This is very confusing for users who need to understand both LECL and langgraph. Why not implement DAG in LCEL\uff1f\nFor comparison, we implemented AWEL(Agentic Workflow Expression Language) orchestration at DB-GPT, and we think AWEL  and  agents are all you need.\n\nExample\nhttps://github.com/eosphoros-ai/DB-GPT/blob/main/examples/awel/simple_chat_history_example.py\nHttpTrigger(node_id=b71f0ccc-4539-445e-8d87-1ec5ef6e7e83)\n -> MapOperator(node_id=23d11ff8-a1d4-4541-a1ff-adfb0eadd614)\n   -> ChatHistoryPromptComposerOperator(node_id=3ee4c2af-d625-445b-a0f3-60863404d82e)\n     -> LLMBranchOperator(node_id=bdc20002-b9e9-4ce8-9211-5aba83f4c16e)\n       -> LLMOperator(node_id=b4c3fe48-a9d8-4968-a128-df5a6b6edd4c, node_name=llm_task)\n      |  -> MapOperator(node_id=ce768e92-5299-43ab-ac04-09755b5f19e2)\n      |    -> JoinOperator(node_id=f33f239e-90dd-4dd4-bc12-30d6d0912cfb)\n       -> StreamingLLMOperator(node_id=ca529d57-1bd7-479c-9047-5cb5512ade8e, node_name=streaming_llm_task)\n         -> OpenAIStreamingOutputOperator(node_id=1952037e-775a-4c9c-a8c3-7ca678f15871)\n           -> JoinOperator(node_id=f33f239e-90dd-4dd4-bc12-30d6d0912cfb)\n\n\nwith DAG(\"dbgpt_awel_simple_chat_history\") as multi_round_dag:\n    # Receive http request and trigger dag to run.\n    trigger = HttpTrigger(\n        \"/examples/simple_history/multi_round/chat/completions\",\n        methods=\"POST\",\n        request_body=TriggerReqBody,\n        streaming_predict_func=lambda req: req.stream,\n    )\n    prompt = ChatPromptTemplate(\n        messages=[\n            SystemPromptTemplate.from_template(\"You are a helpful chatbot.\"),\n            MessagesPlaceholder(variable_name=\"chat_history\"),\n            HumanPromptTemplate.from_template(\"{user_input}\"),\n        ]\n    )\n\n    composer_operator = ChatHistoryPromptComposerOperator(\n        prompt_template=prompt,\n        last_k_round=5,\n        storage=InMemoryStorage(),\n        message_storage=InMemoryStorage(),\n    )\n\n    # Use BaseLLMOperator to generate response.\n    llm_task = LLMOperator(task_name=\"llm_task\")\n    streaming_llm_task = StreamingLLMOperator(task_name=\"streaming_llm_task\")\n    branch_task = LLMBranchOperator(\n        stream_task_name=\"streaming_llm_task\", no_stream_task_name=\"llm_task\"\n    )\n    model_parse_task = MapOperator(lambda out: out.to_dict())\n    openai_format_stream_task = OpenAIStreamingOutputOperator()\n    result_join_task = JoinOperator(\n        combine_function=lambda not_stream_out, stream_out: not_stream_out or stream_out\n    )\n\n    req_handle_task = MapOperator(\n        lambda req: ChatComposerInput(\n            context=ModelRequestContext(\n                conv_uid=req.context.conv_uid, stream=req.stream\n            ),\n            prompt_dict={\"user_input\": req.messages},\n            model_dict={\n                \"model\": req.model,\n                \"context\": req.context,\n                \"stream\": req.stream,\n            },\n        )\n    )\n\n    trigger >> req_handle_task >> composer_operator >> branch_task\n\n    # The branch of no streaming response.\n    branch_task >> llm_task >> model_parse_task >> result_join_task\n    # The branch of streaming response.\n    branch_task >> streaming_llm_task >> openai_format_stream_task >> result_join_task\n", "created_at": "2024-01-08", "closed_at": "2024-01-18", "labels": [], "State": "closed", "Author": "csunny"}
{"issue_number": 142, "issue_title": "two node flow to one node", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\n\nup and side node. output param as down node params.\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nup and side node. output param as down node params.\nSystem Info\nlatest.", "created_at": "2024-02-23", "closed_at": "2024-03-30", "labels": [], "State": "closed", "Author": "linpan"}
{"issue_number": 137, "issue_title": "How to stream token of agent response in agent supervisor?", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nimport os\nfrom langchain.agents import AgentExecutor, create_openai_tools_agent\nfrom langchain_core.messages import BaseMessage, HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nimport operator\nfrom typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\nimport functools\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langgraph.graph import StateGraph, END\nfrom langchain_community.utilities import SerpAPIWrapper\nfrom langchain.agents import Tool\nfrom utils import toggle_case,sort_string\nos.environ[\"OPENAI_API_KEY\"] = \"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\"\nos.environ[\"SERPER_API_KEY\"] = \"c3b73653f4256d5f2b4b5cf4e6fa438d736de7a4717b0fe06d92df0f30fbd3ce\"\nclass AgentSupervisor:\ndef init(self, llm):\nself.llm = llm\ndef getAgentSupervisor():\n    search = SerpAPIWrapper(serpapi_api_key=os.environ[\"SERPER_API_KEY\"])\n    tools = [\n        Tool(\n            name=\"Search\",\n            func=search.run,\n            description=\"useful for when you need to answer questions about current events\",\n        ),\n        Tool(\n            name=\"Toogle_Case\",\n            func=lambda word: toggle_case(word),\n            description=\"use when you want to convert the letter to uppercase or lowercase\",\n        ),\n        Tool(\n            name=\"Sort_String\",\n            func=lambda string: sort_string(string),\n            description=\"use when you want to sort a string alphabetically\",\n        ),\n    ]\n\n    def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str):\n        # Each worker node will be given a name and some tools.\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    system_prompt,\n                ),\n                MessagesPlaceholder(variable_name=\"messages\"),\n                MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n            ]\n        )\n        agent = create_openai_tools_agent(llm, tools, prompt)\n        executor = AgentExecutor(agent=agent, tools=tools)\n        return executor\n\n    def agent_node(state, agent, name):\n        result = agent.invoke(state)\n        return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}\n\n    members = [\"AIAssistant\", \"Coder\"]\n    system_prompt = (\n        \"You are a supervisor tasked with managing a conversation between the\"\n        \" following workers:  {members}. Given the following user request,\"\n        \" respond with the worker to act next. Each worker will perform a\"\n        \" task and respond with their results and status. When finished,\"\n        \" respond with FINISH.\"\n    )\n    # Our team supervisor is an LLM node. It just picks the next agent to process\n    # and decides when the work is completed\n    options = [\"FINISH\"] + members\n    # Using openai function calling can make output parsing easier for us\n    function_def = {\n        \"name\": \"route\",\n        \"description\": \"Select the next role.\",\n        \"parameters\": {\n            \"title\": \"routeSchema\",\n            \"type\": \"object\",\n            \"properties\": {\n                \"next\": {\n                    \"title\": \"Next\",\n                    \"anyOf\": [\n                        {\"enum\": options},\n                    ],\n                }\n            },\n            \"required\": [\"next\"],\n        },\n    }\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", system_prompt),\n            MessagesPlaceholder(variable_name=\"messages\"),\n            (\n                \"system\",\n                \"Given the conversation above, who should act next?\"\n                \" Or should we FINISH? Select one of: {options}\",\n            ),\n        ]\n    ).partial(options=str(options), members=\", \".join(members))\n\n    llm = ChatOpenAI(model=\"gpt-4-1106-preview\", streaming=True)\n\n    supervisor_chain = (\n        prompt\n        | llm.bind_functions(functions=[function_def], function_call=\"route\")\n        | JsonOutputFunctionsParser()\n    )\n\n    class AgentState(TypedDict):\n        # The annotation tells the graph that new messages will always\n        # be added to the current states\n        messages: Annotated[Sequence[BaseMessage], operator.add]\n        # The 'next' field indicates where to route to next\n        next: str\n\n\n    research_agent = create_agent(llm, tools, \"You are a ai assistant to provide personalized answer to people.\")\n    research_node = functools.partial(agent_node, agent=research_agent, name=\"AIAssistant\")\n\n    # NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION. PROCEED WITH CAUTION\n    code_agent = create_agent(\n        llm,\n        tools,\n        \"You may generate safe python code to analyze data and generate charts using matplotlib.\",\n    )\n    code_node = functools.partial(agent_node, agent=code_agent, name=\"Coder\")\n\n    workflow = StateGraph(AgentState)\n    workflow.add_node(\"AIAssistant\", research_node)\n    workflow.add_node(\"Coder\", code_node)\n    workflow.add_node(\"supervisor\", supervisor_chain)\n\n    for member in members:\n        # We want our workers to ALWAYS \"report back\" to the supervisor when done\n        workflow.add_edge(member, \"supervisor\")\n    # The supervisor populates the \"next\" field in the graph state\n    # which routes to a node or finishes\n    conditional_map = {k: k for k in members}\n    conditional_map[\"FINISH\"] = END\n    workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n    # Finally, add entrypoint\n    workflow.set_entry_point(\"supervisor\")\n\n    graph = workflow.compile()\n    return graph\n\nagent_supervisor = AgentSupervisor.getAgentSupervisor()\nagent_name = ''\nfor s in agent_supervisor.stream(\n{\n\"messages\": [\nHumanMessage(content=question)\n]\n},\n{\n\"recursion_limit\": 100\n}\n):\nif \"end\" not in s:\nif 'supervisor' in s:\nagent_name = s['supervisor']['next']\nif agent_name != \"FINISH\":\nawait websocket.send_text(json.dumps({\"token\":\"AgentName:\"+agent_name+\"\\n\"}))\nprint(agent_name)\nif agent_name in s:\ncontent = s[agent_name]['messages'][0].content\nawait websocket.send_text(json.dumps({\"token\":\"Response:\"+content+\"\\n\"}))\nprint(content)\nprint(\"----\")\nError Message and Stack Trace (if applicable)\nNo Error, it is outputing properly, but I need a way to stream tokens of agent response, it is outputing full agent response now.\nDescription\nI am trying to stream tokens of agent response in agent super visor.\nRight now, it is outputing agent name and full agent response, Here I want to stream tokens of agent response.\nSystem Info\nplatform: windows\npython version: 3.11.2\nlangchain version: latest version", "created_at": "2024-02-22", "closed_at": null, "labels": [], "State": "open", "Author": "chatgptguru"}
{"issue_number": 136, "issue_title": "astream_log produces TypeError: unsupported operand type(s) for +: 'dict' and 'dict'  in passthrough.py ", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nThe following code produces the error.  I have found it in many different scenarios, but this uses one of your base examples from https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/agent_supervisor.ipynb.  The only change is the async invocation to produce the aysnc for output in graph.astream_log(): located at the very bottom of the code.\nimport getpass\nimport os\n\nfrom langchain_community.chat_models import ChatOpenAI\n# Optional, add tracing in LangSmith\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_PROJECT\"] = \"Multi-agent Collaboration\"\n\nfrom typing import Annotated, List, Tuple, Union\n\n\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_core.tools import tool\nfrom langchain_experimental.tools import PythonREPLTool\n\ntavily_tool = TavilySearchResults(max_results=5)\n\n\n\n# This executes code locally, which can be unsafe\npython_repl_tool = PythonREPLTool()\n\nfrom langchain.agents import AgentExecutor, create_openai_tools_agent\nfrom langchain_core.messages import BaseMessage, HumanMessage\nfrom langchain_openai import ChatOpenAI\n\n\ndef create_agent(llm: ChatOpenAI, tools: list, system_prompt: str):\n    # Each worker node will be given a name and some tools.\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                system_prompt,\n            ),\n            MessagesPlaceholder(variable_name=\"messages\"),\n            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n        ]\n    )\n    agent = create_openai_tools_agent(llm, tools, prompt)\n    executor = AgentExecutor(agent=agent, tools=tools)\n    return executor\n\ndef agent_node(state, agent, name):\n    result = agent.invoke(state)\n    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}\n\nfrom langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\nmembers = [\"Researcher\", \"Coder\"]\nsystem_prompt = (\n    \"You are a supervisor tasked with managing a conversation between the\"\n    \" following workers:  {members}. Given the following user request,\"\n    \" respond with the worker to act next. Each worker will perform a\"\n    \" task and respond with their results and status. When finished,\"\n    \" respond with FINISH.\"\n)\n# Our team supervisor is an LLM node. It just picks the next agent to process\n# and decides when the work is completed\noptions = [\"FINISH\"] + members\n# Using openai function calling can make output parsing easier for us\nfunction_def = {\n    \"name\": \"route\",\n    \"description\": \"Select the next role.\",\n    \"parameters\": {\n        \"title\": \"routeSchema\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"next\": {\n                \"title\": \"Next\",\n                \"anyOf\": [\n                    {\"enum\": options},\n                ],\n            }\n        },\n        \"required\": [\"next\"],\n    },\n}\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system_prompt),\n        MessagesPlaceholder(variable_name=\"messages\"),\n        (\n            \"system\",\n            \"Given the conversation above, who should act next?\"\n            \" Or should we FINISH? Select one of: {options}\",\n        ),\n    ]\n).partial(options=str(options), members=\", \".join(members))\n\nllm = ChatOpenAI(model=\"gpt-4-1106-preview\", streaming=True)\n\nsupervisor_chain = (\n    prompt\n    | llm.bind_functions(functions=[function_def], function_call=\"route\")\n    | JsonOutputFunctionsParser()\n)\n\nimport operator\nfrom typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\nimport functools\n\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langgraph.graph import StateGraph, END\n\n\n# The agent state is the input to each node in the graph\nclass AgentState(TypedDict):\n    # The annotation tells the graph that new messages will always\n    # be added to the current states\n    messages: Annotated[Sequence[BaseMessage], operator.add]\n    # The 'next' field indicates where to route to next\n    next: str\n\n\nresearch_agent = create_agent(llm, [tavily_tool], \"You are a web researcher.\")\nresearch_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n\n# NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION. PROCEED WITH CAUTION\ncode_agent = create_agent(\n    llm,\n    [python_repl_tool],\n    \"You may generate safe python code to analyze data and generate charts using matplotlib.\",\n)\ncode_node = functools.partial(agent_node, agent=code_agent, name=\"Coder\")\n\nworkflow = StateGraph(AgentState)\nworkflow.add_node(\"Researcher\", research_node)\nworkflow.add_node(\"Coder\", code_node)\nworkflow.add_node(\"supervisor\", supervisor_chain)\n\nfor member in members:\n    # We want our workers to ALWAYS \"report back\" to the supervisor when done\n    workflow.add_edge(member, \"supervisor\")\n# The supervisor populates the \"next\" field in the graph state\n# which routes to a node or finishes\nconditional_map = {k: k for k in members}\nconditional_map[\"FINISH\"] = END\nworkflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n# Finally, add entrypoint\nworkflow.set_entry_point(\"supervisor\")\n\ngraph = workflow.compile()\n\nasync def main():\n   async for output in graph.astream_log(\n        {\n            \"messages\": [\n                HumanMessage(content=\"Code hello world and print it to the terminal\")\n            ]\n        }, include_types=[\"llm\"]\n    ):\n        for op in output.ops:\n            if op[\"path\"] == \"/streamed_output/-\":\n                # this is the output from .stream()\n                ...\n            elif op[\"path\"].startswith(\"/logs/\") and op[\"path\"].endswith(\n                \"/streamed_output/-\"\n            ):\n                # because we chose to only include LLMs, these are LLM tokens\n                print(op[\"value\"])\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\nError Message and Stack Trace (if applicable)\n(agents_v09) JasonMacPro:agents_v09 jason$ python langgraph_astream_events.py\ncontent='' additional_kwargs={'function_call': {'arguments': '', 'name': 'route'}}\ncontent='' additional_kwargs={'function_call': {'arguments': '{\"', 'name': ''}}\ncontent='' additional_kwargs={'function_call': {'arguments': 'next', 'name': ''}}\ncontent='' additional_kwargs={'function_call': {'arguments': '\":\"', 'name': ''}}\nTraceback (most recent call last):\nFile \"/Users/jason/Documents/agents_v09/langgraph_astream_events.py\", line 165, in \nasyncio.run(main())\nFile \"/Users/jason/.pyenv/versions/3.10.11/lib/python3.10/asyncio/runners.py\", line 44, in run\nreturn loop.run_until_complete(main)\nFile \"/Users/jason/.pyenv/versions/3.10.11/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\nreturn future.result()\nFile \"/Users/jason/Documents/agents_v09/langgraph_astream_events.py\", line 147, in main\nasync for output in graph.astream_log(\nFile \"/Users/jason/.pyenv/versions/3.10.11/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 683, in astream_log\nasync for item in _astream_log_implementation(  # type: ignore\nFile \"/Users/jason/.pyenv/versions/3.10.11/lib/python3.10/site-packages/langchain_core/tracers/log_stream.py\", line 612, in _astream_log_implementation\nawait task\nFile \"/Users/jason/.pyenv/versions/3.10.11/lib/python3.10/site-packages/langchain_core/tracers/log_stream.py\", line 566, in consume_astream\nasync for chunk in runnable.astream(input, config, **kwargs):\nFile \"/Users/jason/.pyenv/versions/3.10.11/lib/python3.10/site-packages/langgraph/pregel/init.py\", line 657, in astream\nasync for chunk in self.atransform(\nFile \"/Users/jason/.pyenv/versions/3.10.11/lib/python3.10/site-packages/langgraph/pregel/init.py\", line 675, in atransform\nasync for chunk in self._atransform_stream_with_config(\nFile \"/Users/jason/.pyenv/versions/3.10.11/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1597, in _atransform_stream_with_config\nchunk = cast(Output, await py_anext(iterator))\nFile \"/Users/jason/.pyenv/versions/3.10.11/lib/python3.10/site-packages/langchain_core/tracers/log_stream.py\", line 237, in tap_output_aiter\nasync for chunk in output:\nFile \"/Users/jason/.pyenv/versions/3.10.11/lib/python3.10/site-packages/langgraph/pregel/init.py\", line 524, in _atransform\n_interrupt_or_proceed(done, inflight, step)\nFile \"/Users/jason/.pyenv/versions/3.10.11/lib/python3.10/site-packages/langgraph/pregel/init.py\", line 698, in _interrupt_or_proceed\nraise exc\nFile \"/Users/jason/.pyenv/versions/3.10.11/lib/python3.10/site-packages/langgraph/pregel/init.py\", line 836, in _aconsume\nasync for _ in iterator:\nFile \"/Users/jason/.pyenv/versions/3.10.11/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 4140, in astream\nasync for item in self.bound.astream(\nFile \"/Users/jason/.pyenv/versions/3.10.11/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2452, in astream\nasync for chunk in self.atransform(input_aiter(), config, **kwargs):\nFile \"/Users/jason/.pyenv/versions/3.10.11/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2435, in atransform\nasync for chunk in self._atransform_stream_with_config(\nFile \"/Users/jason/.pyenv/versions/3.10.11/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1597, in _atransform_stream_with_config\nchunk = cast(Output, await py_anext(iterator))\nFile \"/Users/jason/.pyenv/versions/3.10.11/lib/python3.10/site-packages/langchain_core/tracers/log_stream.py\", line 237, in tap_output_aiter\nasync for chunk in output:\nFile \"/Users/jason/.pyenv/versions/3.10.11/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2405, in _atransform\nasync for output in final_pipeline:\nFile \"/Users/jason/.pyenv/versions/3.10.11/lib/python3.10/site-packages/langchain_core/runnables/passthrough.py\", line 280, in atransform\nfinal = final + chunk\nTypeError: unsupported operand type(s) for +: 'dict' and 'dict'\nDescription\nI am trying to stream output from a compiled langgraph using the astream_log (astream_events also produces this error).  It is easily reproducible with example code in many of the langgraph examples if using astream_log rather than astream or synchronous calls.\nSystem Info\nlangchain==0.1.8\nlangchain-community==0.0.21\nlangchain-core==0.1.25\nlangchain-experimental==0.0.52\nlangchain-mistralai==0.0.4\nlangchain-openai==0.0.6\nlanggraph==0.0.25\nlangsmith==0.1.5\nMac OSX 12.6.5\nPython 3.10.11", "created_at": "2024-02-22", "closed_at": "2024-03-30", "labels": ["bug"], "State": "closed", "Author": "sploithunter"}
{"issue_number": 125, "issue_title": "DOC: How to add subgraphs\uff1f", "issue_body": "Issue with current documentation:\nI seem to have not found the related content on adding subgraphs in the documentation. For example, if there are three subgraphs that are three modules, how do I connect these subgraphs together?\nIdea or request for content:\nI currently have three subgraphs: the message summary subgraph, the self-rag subgraph, and the tool invocation subgraph. Each subgraph is a module. How do I piece together the subgraphs of the three modules?", "created_at": "2024-02-20", "closed_at": "2024-05-15", "labels": [], "State": "closed", "Author": "lee0v0"}
{"issue_number": 124, "issue_title": "DOC: How to use with langserve playground?", "issue_body": "Issue with current documentation:\nThere doesn't appear to be any documentation anywhere on how to use langgraph with langserve playground. When using the Runnable returned from StateGraph.compile as part of a chain, and calling the langserve stream_log endpoint (this is the behavior of langserve playground), it appears that the graph is invoked a single time - I can see my entrypoint then delegating to a conditional edge, but that conditional edge never continues on to its next edge.\nThis looks like it might be caused by the error \"Trying to load an object that doesn't implement serialization:\", thrown by WellKnownLCSerializer.dumps in langserve.serialization. This could be caused by using a TypedDict state that doesn't implement toJSON, but the rabbit hole of errors is pretty difficult to navigate.\nIdea or request for content:\nInclude examples of how to use langgraph with langserve / playground, particular demonstrating how to customize the formatting of response / intermediate steps from your graph's state.", "created_at": "2024-02-20", "closed_at": "2024-05-15", "labels": [], "State": "closed", "Author": "russell-dot-js"}
{"issue_number": 119, "issue_title": "rewoo.ipynb example answers incorrectly as of now, using default code and settings ", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nI tested this notebook and it answered incorrectly, either answered Melbourne or Italy, instead of Sesto, Italy.\nIn the first case, when I got Melbourne, I set LLM to \"gpt-3.5-turbo-0125\" in cell:\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(temperature=0)\n# model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n\nWhen I tested it with the default LLM (\"gpt-3.5-turbo\"), it answered Italy.\nPls note planning result doesn't look the same as in the example at:\nhttps://github.com/langchain-ai/langgraph/blob/main/examples/rewoo/rewoo.ipynb\nit is this:\nPlan: Use Google to search for the winner of the 2024 Australian Open. #E1 = Google[2024 Australian Open winner]\nPlan: Use Google to search for the hometown of the 2024 Australian Open winner. #E2 = Google[hometown of 2024 Australian Open winner]\n\nIt is in the example:\nPlan: Use Google to search for the 2024 Australian Open winner.\n#E1 = Google[2024 Australian Open winner]\n\nPlan: Retrieve the name of the 2024 Australian Open winner from the search results.\n#E2 = LLM[What is the name of the 2024 Australian Open winner, given #E1]\n\nPlan: Use Google to search for the hometown of the 2024 Australian Open winner.\n#E3 = Google[hometown of 2024 Australian Open winner, given #E2]\n\nPlan: Retrieve the hometown of the 2024 Australian Open winner from the search results.\n#E4 = LLM[What is the hometown of the 2024 Australian Open winner, given #E3]\n\nso I think a langchain/langgraph update might have caused this.\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nI tested this notebook and it answered incorrectly,\neither answered Melbourne or Italy, instead of Sesto, Italy,\nas of now, using default code and settings\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.19045\nPython Version:  3.11.6 (tags/v3.11.6:8b6ee5b, Oct  2 2023, 14:57:12) [MSC v.1935 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.1.22\nlangchain: 0.1.7\nlangchain_community: 0.0.20\nlangsmith: 0.0.87\nlangchain_benchmarks: 0.0.2\nlangchain_cli: 0.0.21\nlangchain_experimental: 0.0.40\nlangchain_openai: 0.0.6\nlangchainhub: 0.1.13\nlanggraph: 0.0.24\nlangserve: 0.0.41\n", "created_at": "2024-02-17", "closed_at": "2024-02-20", "labels": [], "State": "closed", "Author": "vanetreg"}
{"issue_number": 118, "issue_title": "Streamlit calls not working in callback", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\n# Define callback with a streamlit call\n@st.cache_data(experimental_allow_widgets=True, persist=True) \ndef custom_callback():\n   response = st.text_input(\"Do you want to continue?\")\n   return response == \"y\"\n\ndef run(...):\n   # Calling the callback in run works\n   custom_callback()\n   handler = HumanApprovalCallbackHandler(custom_callback)\n   # Passing it to callbacks will result in an NoSessionContext() error\n   response = app.stream(inputs, config={\"callbacks\": [handler]})\nError Message and Stack Trace (if applicable)\nFile \"lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3887, in invoke\nreturn self.bound.invoke(\n^^^^^^^^^^^^^^^^^^\nFile \"lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3353, in invoke\nreturn self._call_with_config(\n^^^^^^^^^^^^^^^^^^^^^^^\nFile \"lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1246, in _call_with_config\ncontext.run(\nFile \"lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 326, in call_func_with_variable_args\nreturn func(input, **kwargs)  # type: ignore[call-arg]\n^^^^^^^^^^^^^^^^^^^^^\nFile \"lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3229, in _invoke\noutput = call_func_with_variable_args(\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 326, in call_func_with_variable_args\nreturn func(input, **kwargs)  # type: ignore[call-arg]\n^^^^^^^^^^^^^^^^^^^^^\nFile \"lib/python3.11/site-packages/langgraph/prebuilt/tool_executor.py\", line 60, in _execute\noutput = tool.invoke(tool_invocation.tool_input, config=config)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"lib/python3.11/site-packages/langchain_core/tools.py\", line 210, in invoke\nreturn self.run(\n^^^^^^^^^\nFile \"lib/python3.11/site-packages/langchain_core/tools.py\", line 331, in run\nrun_manager = callback_manager.on_tool_start(\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 1308, in on_tool_start\nhandle_event(\nFile \"lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 262, in handle_event\nraise e\nFile \"lib/python3.11/site-packages/langchain_core/callbacks/manager.py\", line 234, in handle_event\nevent = getattr(handler, event_name)(*args, **kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"lib/python3.11/site-packages/langchain_community/callbacks/human.py\", line 57, in on_tool_start\nif self._should_check(serialized) and not self._approve(input_str):\n^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py\", line 212, in wrapper\nreturn cached_func(*args, **kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"lib/python3.11/site-packages/streamlit/runtime/caching/cache_utils.py\", line 240, in call\nwith spinner(message, cache=True):\nFile \"versions/3.11.7/lib/python3.11/contextlib.py\", line 137, in enter\nreturn next(self.gen)\n^^^^^^^^^^^^^^\nFile \"lib/python3.11/site-packages/streamlit/elements/spinner.py\", line 56, in spinner\nmessage = st.empty()\n^^^^^^^^^^\nFile \"lib/python3.11/site-packages/streamlit/elements/empty.py\", line 70, in empty\nreturn self.dg._enqueue(\"empty\", empty_proto)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"lib/python3.11/site-packages/streamlit/delta_generator.py\", line 530, in _enqueue\n_enqueue_message(msg)\nFile \"lib/python3.11/site-packages/streamlit/delta_generator.py\", line 869, in _enqueue_message\nraise NoSessionContext()\nstreamlit.errors.NoSessionContext\nDescription\nAdding a callback to stream or invoke with a streamlit call inside will result in NoSessionContext error\nSystem Info\nlangchain :  0.1.0\nlanggraph:   0.0.20\nstreamlit: 1.31.0", "created_at": "2024-02-17", "closed_at": "2025-04-15", "labels": [], "State": "closed", "Author": "i-merge"}
{"issue_number": 111, "issue_title": "DOC: Example llm-compiler", "issue_body": "Issue with current documentation:\nChecked other resources\n\n  I added a very descriptive title to this issue.\n  I searched the LangChain documentation with the integrated search.\n  I used the GitHub search to find a similar question and didn't find it.\n  I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nfrom typing import List\nfrom langgraph.graph import MessageGraph, END\nfrom langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n\nfrom codegen.agent_runtime.llm_compiler.joiner import joiner\nfrom codegen.agent_runtime.llm_compiler.task import plan_and_schedule\n\ngraph_builder = MessageGraph()\n\n# 1.  Define vertices\n# We defined plan_and_schedule above already\n# Assign each node to a state variable to update\ngraph_builder.add_node(\"plan_and_schedule\", plan_and_schedule)\ngraph_builder.add_node(\"join\", joiner)\n\n\n# Define edges\ngraph_builder.add_edge(\"plan_and_schedule\", \"join\")\n\n# This condition determines looping logic\n\n\ndef should_continue(state: List[BaseMessage]):\n    if isinstance(state[-1], AIMessage):\n        return END\n    return \"plan_and_schedule\"\n\n\ngraph_builder.add_conditional_edges(\n    start_key=\"join\",\n    # Next, we pass in the function that will determine which node is called next.\n    condition=should_continue,\n)\ngraph_builder.set_entry_point(\"plan_and_schedule\")\nchain = graph_builder.compile()\nsteps = chain.stream(\n    [\n        HumanMessage(\n            content=\"What's the oldest parrot alive, and how much longer is that than the average?\"\n        )\n    ],\n    {\n        \"recursion_limit\": 100,\n    },\n)\nfor step in steps:\n    print(step)\n    print(\"---\")\n\n\nError Message and Stack Trace\n<module>\n    graph_builder.add_conditional_edges(\nTypeError: Graph.add_conditional_edges() missing 1 required positional argument: 'conditional_edge_mapping'\n\nSystem Info\npython = \">=3.10,<4.0.0\"\ntenacity = \"^8.2.3\"\nlanggraph = \"^0.0.20\"\nlangchain = \"^0.1.4\"\nbeautifulsoup4 = \"^4.12.2\"\ndocker = \"^6.1.3\"\nlangchain-community = \"^0.0.17\"\nllama-cpp-python = \"^0.2.38\"\ngpt4all = \"^2.1.0\"\ngguf = \"^0.6.0\"\nboto3 = \"^1.34.34\"\nlangchain-openai = \"^0.0.5\"\ngitpython = \"^3.1.41\"\nqdrant-client = \"^1.7.3\"\nlangchain-experimental = \"^0.0.50\"\nlangchainhub = \"^0.1.14\"\n\nIdea or request for content:\nNo response", "created_at": "2024-02-15", "closed_at": "2024-02-15", "labels": [], "State": "closed", "Author": "Siafu"}
{"issue_number": 104, "issue_title": "web_voyager's add_conditional_edges lacks of conditional_edge_mapping ", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nit seems that \"graph_builder.add_conditional_edges(\"agent\", select_tool)\" missed one argument, and caused this error when putting in code (not notebook)\ngraph_builder.add_conditional_edges(\"agent\", select_tool)\nTypeError: Graph.add_conditional_edges() missing 1 required positional argument: 'conditional_edge_mapping'\n\noriginal code is like following\ngraph_builder = StateGraph(AgentState)\ngraph_builder.add_node(\"agent\", agent)\ngraph_builder.set_entry_point(\"agent\")\ngraph_builder.add_node(\"update_scratchpad\", update_scratchpad)\ngraph_builder.add_edge(\"update_scratchpad\", \"agent\")\ntools = {\n\"Click\": click,\n\"Type\": type_text,\n\"Scroll\": scroll,\n\"Wait\": wait,\n\"GoBack\": go_back,\n\"Google\": to_google,\n}\nfor node_name, tool in tools.items():\ngraph_builder.add_node(\nnode_name,\n# The lambda ensures the function's string output is mapped to the \"observation\"\n# key in the AgentState\nRunnableLambda(tool) | (lambda observation: {\"observation\": observation}),\n)\n# Always return to the agent (by means of the update-scratchpad node)\ngraph_builder.add_edge(node_name, \"update_scratchpad\")\ndef select_tool(state: AgentState):\n# Any time the agent completes, this function\n# is called to route the output to a tool or\n# to the end user.\naction = state[\"prediction\"][\"action\"]\nif action == \"ANSWER\":\nreturn END\nif action == \"retry\":\nreturn \"agent\"\nreturn action\ngraph_builder.add_conditional_edges(\"agent\", select_tool) <----------- here\ngraph = graph_builder.compile()\nError Message and Stack Trace (if applicable)\nTypeError: Graph.add_conditional_edges() missing 1 required positional argument: 'conditional_edge_mapping'\nDescription\nit seems that \"graph_builder.add_conditional_edges(\"agent\", select_tool)\" missed one argument, and caused this error when putting in code (not notebook)\ngraph_builder.add_conditional_edges(\"agent\", select_tool)\nTypeError: Graph.add_conditional_edges() missing 1 required positional argument: 'conditional_edge_mapping'\nSystem Info\nPackage Information\n\nlangchain_core: 0.1.22\nlangchain: 0.1.6\nlangchain_community: 0.0.19\nlangsmith: 0.0.87\nlangchain_cli: 0.0.20\nlangchain_experimental: 0.0.50\nlangchain_google_genai: 0.0.8\nlangchain_google_vertexai: 0.0.5\nlangchain_openai: 0.0.5\nlangchainhub: 0.1.14\nlanggraph: 0.0.24\nlangserve: 0.0.37\n", "created_at": "2024-02-12", "closed_at": "2024-02-20", "labels": [], "State": "closed", "Author": "roboticsocialism"}
{"issue_number": 101, "issue_title": "[StreamlitCallbackHandler] - Not compatible with LangGraph", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nimport streamlit as st\nfrom langgraph.prebuilt import create_agent_executor\nfrom langchain.agents import AgentType, create_openai_functions_agent\nfrom langchain.callbacks.streamlit import StreamlitCallbackHandler\nfrom langchain_community.agent_toolkits import SQLDatabaseToolkit\nfrom langchain_community.utilities.sql_database import SQLDatabase\nfrom langchain_core.prompts.chat  import ChatPromptTemplate, AIMessage, SystemMessage, HumanMessagePromptTemplate, MessagesPlaceholder\n\nst_cb = StreamlitCallbackHandler(st.container())\n\ndb = SQLDatabase(engine=engine, include_tables=tables)\ntoolkit = SQLDatabaseToolkit(db=db, llm=llm)\nsql_tools = toolkit.get_tools()\n\nmessages = [\n    SystemMessage(content=SQL_PREFIX),\n    HumanMessagePromptTemplate.from_template(\"{input}\"),\n    AIMessage(content=SQL_FUNCTIONS_SUFFIX),\n    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n        ]\ninput_variables = [\"input\", \"agent_scratchpad\"]\nprompt = ChatPromptTemplate(input_variables=input_variables, messages=messages)\nsql_agent_runnable = create_openai_functions_agent(llm, sql_tools, prompt)\n\nresult = app.invoke({\"input\": \"What is the table about?\",\"chat_history\":[]},{\"callbacks\": [st_cb]})\nError Message and Stack Trace (if applicable)\n2024-02-10 11:04:02.381 Thread 'ThreadPoolExecutor-15_0': missing ScriptRunContext\nError in StreamlitCallbackHandler.on_llm_start callback: NoSessionContext()\nError in StreamlitCallbackHandler.on_llm_end callback: RuntimeError('Current LLMThought is unexpectedly None!')\nError in StreamlitCallbackHandler.on_tool_start callback: RuntimeError('Current LLMThought is unexpectedly None!')\nError in StreamlitCallbackHandler.on_tool_end callback: RuntimeError('Current LLMThought is unexpectedly None!')\nDescription\nI am trying to use the StreamlitCallbackHandler with LangGraph as I can successfully do it with LangChain.\nBased on my observation, the internal format drastically diverges between langchain and langGraph. Does it mean, that StreamlitCallbackHandler will not be compatible with langGraph ?\nSystem Info\nlangchain==0.1.0\nlangchain-community==0.0.12\nlangchain-core==0.1.14\nlangchain-experimental==0.0.49\nlangchain-openai==0.0.3\nlanggraph==0.0.20", "created_at": "2024-02-10", "closed_at": "2025-04-15", "labels": [], "State": "closed", "Author": "MarkusDressel"}
{"issue_number": 96, "issue_title": "LangGraph supervisor keeps calling same node even when FINISH", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nHere is my System Prompt:\nsystem_prompt = (\n    \" You are a supervisor tasked with managing a conversation between the\"\n    \" following workers:  {members}. Given the following user request,\"\n    \" respond with the worker to act next. Each worker will perform a\"\n    \" task and respond with their results.\"\n    \" If you or any of the other {members} have the final answer or deliverable\"\n    \" prefix your response with FINISH: so you know it is time to stop. Once any of the {members} respond\"\n    \" with a final answer or deliverable return the response to the user and stop the execution\"\n    )\n\n\nAnd here is the response.\nPlease enter your question: hello\n{'supervisor': {'next': 'Conversation'}}\n----\n{'Conversation': {'messages': [HumanMessage(content='FINISH: Hello there! How can I assist you today?', name='Conversation')]}}\n----\n{'supervisor': {'next': 'Conversation'}}\n----\n{'Conversation': {'messages': [HumanMessage(content=\"FINISH: How's your day going?\", name='Conversation')]}}\n----\n{'supervisor': {'next': 'FINISH'}}\n----\n\nProcess finished with exit code 0\n\n\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nHello,\nI have a conversational node which is used to respond with a normal conversation.\nEven when the message is prefixed as finish the supervisor keeps calling the node again.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 22.3.0: Mon Jan 30 20:39:46 PST 2023; root:xnu-8792.81.3~2/RELEASE_ARM64_T6020\nPython Version:  3.11.6 (v3.11.6:8b6ee5ba3b, Oct  2 2023, 11:18:21) [Clang 13.0.0 (clang-1300.0.29.30)]\n\nPackage Information\n\nlangchain_core: 0.1.18\nlangchain: 0.1.5\nlangchain_community: 0.0.17\nlangchain_experimental: 0.0.50\nlangchain_openai: 0.0.5\nlangchainhub: 0.1.14\nlanggraph: 0.0.21\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlangserve\n", "created_at": "2024-02-08", "closed_at": "2024-05-15", "labels": [], "State": "closed", "Author": "shaikhn1750"}
{"issue_number": 95, "issue_title": "DOC: Output Parsing for human readable text.", "issue_body": "Issue with current documentation:\nHello,\ni have implemented langgraph using supervisor but i did not see any way to output parse the data.\nmy output looks like this:\n\n{'supervisor': {'next': 'text_to_sql'}}\n----\n{'text_to_sql': {'messages': [HumanMessage(content='SELECT tld, COUNT(tld) AS tld_count FROM premium_db.premium_inventory GROUP BY tld;', name='text_to_sql')]}}\n----\nPython REPL can execute arbitrary code. Use with caution.\n{'supervisor': {'next': 'text_to_sql'}, 'sql_to_python': {'messages': [HumanMessage(content='FINISH: The data has been visualized using different colors for each TLD. The chart should now be displayed on your screen.', name='sql_to_python')]}}\n----\n{'supervisor': {'next': 'FINISH'}, 'text_to_sql': {'messages': [HumanMessage(content='Your only task is to create sql query and do not worry of visualisation of the data.', name='text_to_sql')]}}\n----\n\nHow would i make this end user readable?\nIdea or request for content:\nNo response", "created_at": "2024-02-08", "closed_at": "2024-06-12", "labels": [], "State": "closed", "Author": "shaikhn1750"}
{"issue_number": 94, "issue_title": "WebVoyager-Langchain Runtime Error", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\n<ipython-input-8-bdd4c5d78a61>:22: RuntimeWarning: coroutine 'sleep' was never awaited\n  asyncio.sleep(3)\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\n<ipython-input-8-bdd4c5d78a61>:22: RuntimeWarning: coroutine 'sleep' was never awaited\n  asyncio.sleep(3)\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\n---------------------------------------------------------------------------\nTimeoutError                              Traceback (most recent call last)\n[<ipython-input-27-b597227a6e06>](https://localhost:8080/#) in <cell line: 1>()\n----> 1 res = await call_agent(\"Could you explain the WebVoyager paper (on arxiv)?\", page)\n      2 print(f\"Final response: {res}\")\n\n32 frames\n[/usr/lib/python3.10/asyncio/futures.py](https://localhost:8080/#) in result(self)\n    199         self.__log_traceback = False\n    200         if self._exception is not None:\n--> 201             raise self._exception.with_traceback(self._exception_tb)\n    202         return self._result\n    203 \n\nTimeoutError: Timeout 30000ms exceeded.\n\nError Message and Stack Trace (if applicable)\n<ipython-input-8-bdd4c5d78a61>:22: RuntimeWarning: coroutine 'sleep' was never awaited\n  asyncio.sleep(3)\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\n<ipython-input-8-bdd4c5d78a61>:22: RuntimeWarning: coroutine 'sleep' was never awaited\n  asyncio.sleep(3)\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\n---------------------------------------------------------------------------\nTimeoutError                              Traceback (most recent call last)\n[<ipython-input-27-b597227a6e06>](https://localhost:8080/#) in <cell line: 1>()\n----> 1 res = await call_agent(\"Could you explain the WebVoyager paper (on arxiv)?\", page)\n      2 print(f\"Final response: {res}\")\n\n32 frames\n[/usr/lib/python3.10/asyncio/futures.py](https://localhost:8080/#) in result(self)\n    199         self.__log_traceback = False\n    200         if self._exception is not None:\n--> 201             raise self._exception.with_traceback(self._exception_tb)\n    202         return self._result\n    203 \n\nTimeoutError: Timeout 30000ms exceeded.\n\nDescription\ni wanted to run the whole cycle\nSystem Info\n\n!python -m langchain_core.sys_info\n\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #1 SMP PREEMPT_DYNAMIC Sat Nov 18 15:31:17 UTC 2023\n> Python Version:  3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.1.21\n> langchain: 0.1.5\n> langchain_community: 0.0.19\n> langsmith: 0.0.87\n> langchain_openai: 0.0.5\n> langchainhub: 0.1.14\n> langgraph: 0.0.23\n\nPackages not installed (Not Necessarily a Problem)\n--------------------------------------------------\nThe following packages were not found:\n\n> langserve\n", "created_at": "2024-02-08", "closed_at": "2024-03-18", "labels": [], "State": "closed", "Author": "andysingal"}
{"issue_number": 78, "issue_title": "Stream Chat LLM Token By Token is not working", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nI am using the code in this notebook:\nhttps://github.com/langchain-ai/langgraph/blob/main/examples/streaming-tokens.ipynb\nstreaming code:\nfrom langchain_core.messages import HumanMessage\ninputs = [HumanMessage(content=\"what is the weather in sf\")]\nasync for event in app.astream_events(inputs, version=\"v1\"):\n    kind = event[\"event\"]\n    if kind == \"on_chat_model_stream\":\n        content = event[\"data\"][\"chunk\"].content\n        if content:\n            # Empty content in the context of OpenAI means\n            # that the model is asking for a tool to be invoked.\n            # So we only print non-empty content\n            print(content, end=\"|\")\n    elif kind == \"on_tool_start\":\n        print(\"--\")\n        print(\n            f\"Starting tool: {event['name']} with inputs: {event['data'].get('input')}\"\n        )\n    elif kind == \"on_tool_end\":\n        print(f\"Done tool: {event['name']}\")\n        print(f\"Tool output was: {event['data'].get('output')}\")\n        print(\"--\")\n\nError Message and Stack Trace (if applicable)\nthe streaming is not working, I am not receiving any output from this part:\n    if kind == \"on_chat_model_stream\":\n        content = event[\"data\"][\"chunk\"].content\n        if content:\n            # Empty content in the context of OpenAI means\n            # that the model is asking for a tool to be invoked.\n            # So we only print non-empty content\n            print(content, end=\"|\")\n\nDescription\nthe streaming is not working, I am not receiving any output\nSystem Info\nlangchain==0.1.5\nlangchain-community==0.0.17\nlangchain-core==0.1.18\nlangchain-openai==0.0.5\nlanggraph==0.0.21", "created_at": "2024-02-01", "closed_at": "2024-05-17", "labels": [], "State": "closed", "Author": "ahmedmoorsy"}
{"issue_number": 69, "issue_title": "[Feature Request] Automatic graph image generation", "issue_body": "Is there any integrated way to generate an image of the StateGraph or any plan to add one?\nIt would be really useful to visualize the current graph and also the evolution of the graph during development.\nIf this could be useful and you have any preferred library to do this, I could try to open a PR.", "created_at": "2024-01-27", "closed_at": "2024-03-08", "labels": [], "State": "closed", "Author": "gianfrancodemarco"}
{"issue_number": 66, "issue_title": "lcel  how impl by graph", "issue_body": "eg:\nllm :write a title\nllm : write a body\ntool: send to email\nlangchain lcel : \" llm | llm | tool \"\nif use graph ,how impl it", "created_at": "2024-01-25", "closed_at": "2024-03-04", "labels": [], "State": "closed", "Author": "linpan"}
{"issue_number": 64, "issue_title": "how to config recurrsion limit ", "issue_body": "Dear Langgraph,\nI encounted this when test chatbot-simulation-evaluation in example.\nlanggraph.pregel.GraphRecursionError: Recursion limit of 25 reachedwithout hitting a stop condition. You can increase the limitby setting the recursion_limit config key\nWould you please advise how to config the limit?\nThanks a lot!\nCheers!", "created_at": "2024-01-25", "closed_at": "2024-03-04", "labels": [], "State": "closed", "Author": "roboticsocialism"}
{"issue_number": 233, "issue_title": "Customer Support Bot", "issue_body": "(less of an academic example, more a practical implementation)\nFunctionality to highlight here:\n\nHuman-in-the-loop\nPersistence (can stop and then resume any time)\nSimple knowledge connection (RAG)\n", "created_at": "2024-03-22", "closed_at": "2024-05-03", "labels": ["Notebook Request"], "State": "closed", "Author": "hinthornw"}
{"issue_number": 232, "issue_title": "Agent/Intra-node Learning Example", "issue_body": "Similar to GPTSwarm: https://github.com/hinthornw/GPTSwarm\nInitially, skip the edge optimization flows (cool as they are) - simple example of node optimization.\nLikely via few-shot examples, though other optimizers could be applied", "created_at": "2024-03-22", "closed_at": "2024-06-12", "labels": ["Notebook Request"], "State": "closed", "Author": "hinthornw"}
{"issue_number": 231, "issue_title": "Adaptive RAG Example", "issue_body": "A la : https://arxiv.org/abs/2403.14403", "created_at": "2024-03-22", "closed_at": "2024-05-04", "labels": ["Notebook Request"], "State": "closed", "Author": "hinthornw"}
{"issue_number": 230, "issue_title": "Tool/Agent Creation example", "issue_body": "Similar to existing multi-agent workflows, but start out with one agent, which is able to create agents (give them instructions w/in a prompt + select tools) and tools", "created_at": "2024-03-22", "closed_at": "2024-06-12", "labels": ["Notebook Request"], "State": "closed", "Author": "hinthornw"}
{"issue_number": 229, "issue_title": "Self-taught optimizers example", "issue_body": "https://arxiv.org/pdf/2310.02304.pdf", "created_at": "2024-03-22", "closed_at": "2024-06-12", "labels": ["Notebook Request"], "State": "closed", "Author": "hinthornw"}
{"issue_number": 219, "issue_title": "[Chore] upgrading pydantic from v1 to v2 with solution", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nThis discussion is not related to any specific python code; this is more like a promotion or idea.\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nIntro\nI am a software engineer at MediaTek, and my project involves using LangChain to address some of our challenges and to conduct research on topics related to LangChain. I believe a member of our team has already initiated contact with the vendor regarding the purchase of a LangSmith License.\nMotivation\nToday, I delved into the source code and discovered that this package heavily relies on Pydantic, specifically version 1. However, the OpenAI API is currently utilizing Pydantic==2.4.2 Ref, there is no reason we don't upgrade it as a developer.\nObservation of current repository and needs\nHere are some observations and understandings I have gathered:\n\nIn langchain_core, langchain.pydantic_v1 is used solely for invoking pydantic.v1.\nThere are significant differences between Pydantic v1 and v2, such as:\n\nroot_validator has been replaced by model_validator.\nvalidator has been replaced by field_validator.\netc.\n\n\n\nQuestion\nShould we consider updating this module?\nIf so, it would be my honor to undertake this task.\nWorkflow\nIf I am to proceed, my approach would include:\n\nReplacing all instances of from langchain_core.pydantic_v1 import XXX with from pydantic import XXX within the langchain codebase.\nMaking the necessary updates for Pydantic, including changes to model_validator, field_validator, etc.\nKeeping langchain_core.pydantic_v1 unchanged to avoid conflicts with other repositories, but issuing a deprecation warning to inform users and developers.\n\nSystem Info\nNone", "created_at": "2024-03-20", "closed_at": "2024-05-15", "labels": [], "State": "closed", "Author": "Mai0313"}
{"issue_number": 205, "issue_title": "app.get_graph() is throwing a RuntimeError when using langgraph", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nfrom langgraph.graph import StateGraph, END\nfrom typing import Dict, TypedDict\nclass GraphState(TypedDict):\n    keys : Dict[str, any]\n\ndef do_something_1(state):\n    state_dict = state['keys']\n    print(\"do_something_1\")\n    return {\n        'keys': {\n            \"input1\": \"input1\"\n        }\n    }\ndef do_something_2(state):\n    print(\"do_something_2\")\n    state_dict = state['keys']\n    print(state_dict['input1'])\n\n    return {\n        \"keys\" : {\n            \"input2\": \"input2\"\n        }\n    }\ndef do_something_3(state):\n    print(\"do_something_3\")\n    state_dict = state['keys']\n    return {\n        \"keys\" : {\n            \"input3\": \"input3\"\n        }\n    }\n\n\nworkflow = StateGraph(GraphState)\n\nworkflow.add_node(\"start\", do_something_1)\nworkflow.add_node(\"step1\", do_something_2)\nworkflow.add_node(\"step2\", do_something_3)\nworkflow.set_entry_point(\"start\")\nworkflow.add_edge(\"start\", \"step1\")\nworkflow.add_edge(\"step1\", \"step2\")\nworkflow.add_edge(\"step2\", END)\napp = workflow.compile()\napp.invoke(input={\"keys\": {}})\nOutput\n>> do_something_1\n>>do_something_2\n>>input1\n>>do_something_3\n>>{'keys': {'input3': 'input3'}}\n\nError occurs when I execute:\napp.get_graph()\nError Message and Stack Trace (if applicable)\nTraceback:\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:751](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:751), in find_validators(type_, config)\n    [750](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:750) try:\n--> [751](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:751)     if issubclass(type_, val_type):\n    [752](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:752)         for v in validators:\n\nTypeError: issubclass() arg 1 must be a class\n\nDuring handling of the above exception, another exception occurred:\n\nRuntimeError                              Traceback (most recent call last)\nCell In[18], [line 1](vscode-notebook-cell:?execution_count=18&line=1)\n----> [1](vscode-notebook-cell:?execution_count=18&line=1) app.get_graph()\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/graph/graph.py:251](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/graph/graph.py:251), in CompiledGraph.get_graph(self, config, xray)\n    [246](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/graph/graph.py:246) def get_graph(\n    [247](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/graph/graph.py:247)     self, config: Optional[RunnableConfig] = None, *, xray: bool = False\n    [248](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/graph/graph.py:248) ) -> RunnableGraph:\n    [249](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/graph/graph.py:249)     graph = RunnableGraph()\n    [250](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/graph/graph.py:250)     start_nodes: dict[str, RunnableGraphNode] = {\n--> [251](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/graph/graph.py:251)         START: graph.add_node(self.get_input_schema(config), START)\n    [252](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/graph/graph.py:252)     }\n    [253](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/graph/graph.py:253)     end_nodes: dict[str, RunnableGraphNode] = {\n    [254](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/graph/graph.py:254)         END: graph.add_node(self.get_output_schema(config), END)\n    [255](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/graph/graph.py:255)     }\n    [257](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/graph/graph.py:257)     for key, node in self.graph.nodes.items():\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/pregel/__init__.py:242](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/pregel/__init__.py:242), in Pregel.get_input_schema(self, config)\n    [238](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/pregel/__init__.py:238) def get_input_schema(\n    [239](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/pregel/__init__.py:239)     self, config: Optional[RunnableConfig] = None\n    [240](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/pregel/__init__.py:240) ) -> Type[BaseModel]:\n    [241](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/pregel/__init__.py:241)     if isinstance(self.input, str):\n--> [242](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/pregel/__init__.py:242)         return super().get_input_schema(config)\n    [243](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/pregel/__init__.py:243)     else:\n    [244](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/pregel/__init__.py:244)         return create_model(  # type: ignore[call-overload]\n    [245](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/pregel/__init__.py:245)             self.get_name(\"Input\"),\n    [246](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/pregel/__init__.py:246)             **{\n   (...)\n    [249](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/pregel/__init__.py:249)             },\n    [250](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langgraph/pregel/__init__.py:250)         )\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/base.py:302](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/base.py:302), in Runnable.get_input_schema(self, config)\n    [299](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/base.py:299) if inspect.isclass(root_type) and issubclass(root_type, BaseModel):\n    [300](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/base.py:300)     return root_type\n--> [302](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/base.py:302) return create_model(\n    [303](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/base.py:303)     self.get_name(\"Input\"),\n    [304](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/base.py:304)     __root__=(root_type, None),\n    [305](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/base.py:305) )\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:508](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:508), in create_model(__model_name, **field_definitions)\n    [503](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:503) def create_model(\n    [504](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:504)     __model_name: str,\n    [505](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:505)     **field_definitions: Any,\n    [506](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:506) ) -> Type[BaseModel]:\n    [507](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:507)     try:\n--> [508](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:508)         return _create_model_cached(__model_name, **field_definitions)\n    [509](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:509)     except TypeError:\n    [510](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:510)         # something in field definitions is not hashable\n    [511](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:511)         return _create_model_base(\n    [512](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:512)             __model_name, __config__=_SchemaConfig, **field_definitions\n    [513](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:513)         )\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:521](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:521), in _create_model_cached(__model_name, **field_definitions)\n    [516](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:516) @lru_cache(maxsize=256)\n    [517](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:517) def _create_model_cached(\n    [518](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:518)     __model_name: str,\n    [519](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:519)     **field_definitions: Any,\n    [520](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:520) ) -> Type[BaseModel]:\n--> [521](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:521)     return _create_model_base(\n    [522](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:522)         __model_name, __config__=_SchemaConfig, **field_definitions\n    [523](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:523)     )\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:1024](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:1024), in create_model(__model_name, __config__, __base__, __module__, __validators__, __cls_kwargs__, __slots__, **field_definitions)\n   [1022](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:1022)     ns['__orig_bases__'] = __base__\n   [1023](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:1023) namespace.update(ns)\n-> [1024](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:1024) return meta(__model_name, resolved_bases, namespace, **kwds)\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:197](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:197), in ModelMetaclass.__new__(mcs, name, bases, namespace, **kwargs)\n    [189](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:189)     if (\n    [190](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:190)         is_untouched(value)\n    [191](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:191)         and ann_type != PyObject\n   (...)\n    [194](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:194)         )\n    [195](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:195)     ):\n    [196](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:196)         continue\n--> [197](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:197)     fields[ann_name] = ModelField.infer(\n    [198](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:198)         name=ann_name,\n    [199](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:199)         value=value,\n    [200](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:200)         annotation=ann_type,\n    [201](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:201)         class_validators=vg.get_validators(ann_name),\n    [202](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:202)         config=config,\n    [203](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:203)     )\n    [204](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:204) elif ann_name not in namespace and config.underscore_attrs_are_private:\n    [205](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:205)     private_attributes[ann_name] = PrivateAttr()\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:504](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:504), in ModelField.infer(cls, name, value, annotation, class_validators, config)\n    [501](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:501)     required = False\n    [502](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:502) annotation = get_annotation_from_field_info(annotation, field_info, name, config.validate_assignment)\n--> [504](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:504) return cls(\n    [505](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:505)     name=name,\n    [506](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:506)     type_=annotation,\n    [507](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:507)     alias=field_info.alias,\n    [508](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:508)     class_validators=class_validators,\n    [509](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:509)     default=value,\n    [510](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:510)     default_factory=field_info.default_factory,\n    [511](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:511)     required=required,\n    [512](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:512)     model_config=config,\n    [513](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:513)     field_info=field_info,\n    [514](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:514) )\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:434](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:434), in ModelField.__init__(self, name, type_, class_validators, model_config, default, default_factory, required, final, alias, field_info)\n    [432](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:432) self.shape: int = SHAPE_SINGLETON\n    [433](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:433) self.model_config.prepare_field(self)\n--> [434](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:434) self.prepare()\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:555](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:555), in ModelField.prepare(self)\n    [553](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:553) if self.default is Undefined and self.default_factory is None:\n    [554](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:554)     self.default = None\n--> [555](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:555) self.populate_validators()\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:829](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:829), in ModelField.populate_validators(self)\n    [825](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:825) if not self.sub_fields or self.shape == SHAPE_GENERIC:\n    [826](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:826)     get_validators = getattr(self.type_, '__get_validators__', None)\n    [827](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:827)     v_funcs = (\n    [828](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:828)         *[v.func for v in class_validators_ if v.each_item and v.pre],\n--> [829](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:829)         *(get_validators() if get_validators else list(find_validators(self.type_, self.model_config))),\n    [830](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:830)         *[v.func for v in class_validators_ if v.each_item and not v.pre],\n    [831](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:831)     )\n    [832](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:832)     self.validators = prep_validators(v_funcs)\n    [834](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:834) self.pre_validators = []\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:738](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:738), in find_validators(type_, config)\n    [736](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:736)     return\n    [737](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:737) if is_typeddict(type_):\n--> [738](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:738)     yield make_typeddict_validator(type_, config)\n    [739](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:739)     return\n    [741](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:741) class_ = get_class(type_)\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:624](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:624), in make_typeddict_validator(typeddict_cls, config)\n    [619](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:619) def make_typeddict_validator(\n    [620](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:620)     typeddict_cls: Type['TypedDict'], config: Type['BaseConfig']  # type: ignore[valid-type]\n    [621](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:621) ) -> Callable[[Any], Dict[str, Any]]:\n    [622](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:622)     from .annotated_types import create_model_from_typeddict\n--> [624](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:624)     TypedDictModel = create_model_from_typeddict(\n    [625](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:625)         typeddict_cls,\n    [626](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:626)         __config__=config,\n    [627](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:627)         __module__=typeddict_cls.__module__,\n    [628](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:628)     )\n    [629](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:629)     typeddict_cls.__pydantic_model__ = TypedDictModel  # type: ignore[attr-defined]\n    [631](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:631)     def typeddict_validator(values: 'TypedDict') -> Dict[str, Any]:  # type: ignore[valid-type]\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/annotated_types.py:55](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/annotated_types.py:55), in create_model_from_typeddict(typeddict_cls, **kwargs)\n     [49](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/annotated_types.py:49) required_keys: FrozenSet[str] = typeddict_cls.__required_keys__  # type: ignore[attr-defined]\n     [50](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/annotated_types.py:50) field_definitions = {\n     [51](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/annotated_types.py:51)     field_name: (field_type, Required if field_name in required_keys else None)\n     [52](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/annotated_types.py:52)     for field_name, field_type in typeddict_cls.__annotations__.items()\n     [53](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/annotated_types.py:53) }\n---> [55](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/annotated_types.py:55) return create_model(typeddict_cls.__name__, **kwargs, **field_definitions)\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:1024](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:1024), in create_model(__model_name, __config__, __base__, __module__, __validators__, __cls_kwargs__, __slots__, **field_definitions)\n   [1022](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:1022)     ns['__orig_bases__'] = __base__\n   [1023](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:1023) namespace.update(ns)\n-> [1024](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:1024) return meta(__model_name, resolved_bases, namespace, **kwds)\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:197](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:197), in ModelMetaclass.__new__(mcs, name, bases, namespace, **kwargs)\n    [189](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:189)     if (\n    [190](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:190)         is_untouched(value)\n    [191](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:191)         and ann_type != PyObject\n   (...)\n    [194](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:194)         )\n    [195](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:195)     ):\n    [196](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:196)         continue\n--> [197](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:197)     fields[ann_name] = ModelField.infer(\n    [198](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:198)         name=ann_name,\n    [199](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:199)         value=value,\n    [200](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:200)         annotation=ann_type,\n    [201](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:201)         class_validators=vg.get_validators(ann_name),\n    [202](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:202)         config=config,\n    [203](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:203)     )\n    [204](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:204) elif ann_name not in namespace and config.underscore_attrs_are_private:\n    [205](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/main.py:205)     private_attributes[ann_name] = PrivateAttr()\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:504](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:504), in ModelField.infer(cls, name, value, annotation, class_validators, config)\n    [501](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:501)     required = False\n    [502](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:502) annotation = get_annotation_from_field_info(annotation, field_info, name, config.validate_assignment)\n--> [504](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:504) return cls(\n    [505](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:505)     name=name,\n    [506](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:506)     type_=annotation,\n    [507](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:507)     alias=field_info.alias,\n    [508](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:508)     class_validators=class_validators,\n    [509](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:509)     default=value,\n    [510](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:510)     default_factory=field_info.default_factory,\n    [511](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:511)     required=required,\n    [512](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:512)     model_config=config,\n    [513](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:513)     field_info=field_info,\n    [514](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:514) )\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:434](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:434), in ModelField.__init__(self, name, type_, class_validators, model_config, default, default_factory, required, final, alias, field_info)\n    [432](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:432) self.shape: int = SHAPE_SINGLETON\n    [433](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:433) self.model_config.prepare_field(self)\n--> [434](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:434) self.prepare()\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:550](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:550), in ModelField.prepare(self)\n    [545](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:545) if self.type_.__class__ is ForwardRef or self.type_.__class__ is DeferredType:\n    [546](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:546)     # self.type_ is currently a ForwardRef and there's nothing we can do now,\n    [547](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:547)     # user will need to call model.update_forward_refs()\n    [548](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:548)     return\n--> [550](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:550) self._type_analysis()\n    [551](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:551) if self.required is Undefined:\n    [552](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:552)     self.required = True\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:756](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:756), in ModelField._type_analysis(self)\n    [753](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:753)     raise TypeError(f'Fields of type \"{origin}\" are not supported.')\n    [755](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:755) # type_ has been refined eg. as the type of a List and sub_fields needs to be populated\n--> [756](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:756) self.sub_fields = [self._create_sub_type(self.type_, '_' + self.name)]\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:806](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:806), in ModelField._create_sub_type(self, type_, name, for_keys)\n    [791](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:791)     class_validators = {\n    [792](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:792)         k: Validator(\n    [793](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:793)             func=v.func,\n   (...)\n    [801](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:801)         if v.each_item\n    [802](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:802)     }\n    [804](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:804) field_info, _ = self._get_field_info(name, type_, None, self.model_config)\n--> [806](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:806) return self.__class__(\n    [807](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:807)     type_=type_,\n    [808](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:808)     name=name,\n    [809](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:809)     class_validators=class_validators,\n    [810](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:810)     model_config=self.model_config,\n    [811](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:811)     field_info=field_info,\n    [812](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:812) )\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:434](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:434), in ModelField.__init__(self, name, type_, class_validators, model_config, default, default_factory, required, final, alias, field_info)\n    [432](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:432) self.shape: int = SHAPE_SINGLETON\n    [433](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:433) self.model_config.prepare_field(self)\n--> [434](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:434) self.prepare()\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:555](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:555), in ModelField.prepare(self)\n    [553](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:553) if self.default is Undefined and self.default_factory is None:\n    [554](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:554)     self.default = None\n--> [555](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:555) self.populate_validators()\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:829](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:829), in ModelField.populate_validators(self)\n    [825](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:825) if not self.sub_fields or self.shape == SHAPE_GENERIC:\n    [826](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:826)     get_validators = getattr(self.type_, '__get_validators__', None)\n    [827](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:827)     v_funcs = (\n    [828](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:828)         *[v.func for v in class_validators_ if v.each_item and v.pre],\n--> [829](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:829)         *(get_validators() if get_validators else list(find_validators(self.type_, self.model_config))),\n    [830](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:830)         *[v.func for v in class_validators_ if v.each_item and not v.pre],\n    [831](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:831)     )\n    [832](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:832)     self.validators = prep_validators(v_funcs)\n    [834](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/fields.py:834) self.pre_validators = []\n\nFile [~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:760](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:760), in find_validators(type_, config)\n    [758](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:758)             return\n    [759](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:759)     except TypeError:\n--> [760](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:760)         raise RuntimeError(f'error checking inheritance of {type_!r} (type: {display_as_type(type_)})')\n    [762](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:762) if config.arbitrary_types_allowed:\n    [763](https://file+.vscode-resource.vscode-cdn.net/home/bhaswata08/Work/Projects/LLM%20Orchestration/~/.conda/envs/llm/lib/python3.11/site-packages/pydantic/v1/validators.py:763)     yield make_arbitrary_type_validator(type_)\n\nRuntimeError: error checking inheritance of <built-in function any> (type: builtin_function_or_method)\n\nDescription\nI'm trying to use langgraph to build AI Agents, But when I try to visualize them as depicted by this Notebook, A runtime error is thrown.\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP PREEMPT_DYNAMIC Fri, 23 Feb 2024 16:31:48 +0000\nPython Version:  3.11.8 (main, Feb 26 2024, 21:39:34) [GCC 11.2.0]\n\nPackage Information\n\nlangchain_core: 0.1.30\nlangchain: 0.1.11\nlangchain_community: 0.0.27\nlangsmith: 0.1.23\nlangchain_cli: 0.0.21\nlangchain_groq: 0.0.1\nlangchain_text_splitters: 0.0.1\nlangchainhub: 0.1.15\nlanggraph: 0.0.28\nlangserve: 0.0.51\n", "created_at": "2024-03-14", "closed_at": "2024-03-29", "labels": [], "State": "closed", "Author": "bhaswata08"}
{"issue_number": 201, "issue_title": "Human-in-the-loop: TypeError: compile() got an unexpected keyword argument 'interrupt_before'", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nworkflow = MessageGraph()\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"action\", call_tool)\nworkflow.set_entry_point(\"agent\")\nworkflow.add_conditional_edges(\n{\n\"continue\": \"action\",\n\"end\": END,\n},\n)\nworkflow.add_edge(\"action\", \"agent\")\napp = workflow.compile(interrupt_before=[\"action\"])\ninputs = [HumanMessage(content=user_input)]\nfor event in app.stream(inputs, {\"configurable\": {\"thread_id\": \"2\"}}):\nfor k, v in event.items():\nif k != \"end\":\nprint(v)\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\nFile \"Z:\\MHossain_OneDrive\\OneDrive\\ChatGPT\\AI_Bot\\Langgraph\\Human_in_the_loop\\Human-in-the-loop.py\", line 109, in \napp = workflow.compile(interrupt_before=[\"action\"])\nTypeError: compile() got an unexpected keyword argument 'interrupt_before'\nDescription\nI am trying using human loop in LangGraph workflow, but failed.\nSystem Info\nlangchain==0.1.12\nlanggraph==0.0.21\nlangchain-cli==0.0.20\nlangchain-community==0.0.28\nlangchain-core==0.1.31\nlangchain-experimental==0.0.49\nlangchain-openai==0.0.8\nlangchain-text-splitters==0.0.1\nPlatform - Windows\nPython Version - 3.9", "created_at": "2024-03-13", "closed_at": "2024-03-29", "labels": [], "State": "closed", "Author": "mail2mhossain"}
{"issue_number": 199, "issue_title": "Can't Use Ollama Models with LATS Example Notebook", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nso taking the lats example notebook from the repo and trying to switch out the OpenAI models for ollama models\ngives bugs related to the tools either bind_tools not found or doesn't return a tool correctly.  The following 4 approaches didn't work including the OpenAI llama api.\nimport os \nfrom langchain_openai import ChatOpenAI\nfrom langchain_experimental.llms.ollama_functions import OllamaFunctions\nfrom langchain_community.chat_models import ChatOllama\nfrom langchain_community.llms import Ollama\n\nos.environ[\"OPENAI_API_BASE\"] = \"http://localhost:11434/v1\"\nos.environ[\"OPENAI_MODEL_NAME\"] = \"eramax/opencodeinterpreter:ds-33b-q4\"\nos.environ[\"OPENAI_API_KEY\"] =  \"NA\"\n\nllm = ChatOpenAI(model=\"eramax/opencodeinterpreter:ds-33b-q4\")\n# llm = ChatOllama(model=\"eramax/opencodeinterpreter:ds-33b-q4\", temperature=0)\n# llm = OllamaFunctions(model=\"eramax/opencodeinterpreter:ds-33b-q4\")\n# llm = Ollama(model=\"eramax/opencodeinterpreter:ds-33b-q4\")```\n\n### Error Message and Stack Trace (if applicable)\n\nTraceback (most recent call last):\n  File \"/home/ubuntu/lats.py\", line 547, in <module>\n    for step in graph.stream({\"input\": prompt}):\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/langgraph/pregel/__init__.py\", line 615, in transform\n    for chunk in self._transform_stream_with_config(\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1513, in _transform_stream_with_config\n    chunk: Output = context.run(next, iterator)  # type: ignore\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/langgraph/pregel/__init__.py\", line 355, in _transform\n    _interrupt_or_proceed(done, inflight, step)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/langgraph/pregel/__init__.py\", line 698, in _interrupt_or_proceed\n    raise exc\n  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 4069, in invoke\n    return self.bound.invoke(\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2075, in invoke\n    input = step.invoke(\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3523, in invoke\n    return self._call_with_config(\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1262, in _call_with_config\n    context.run(\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 326, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3397, in _invoke\n    output = call_func_with_variable_args(\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 326, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n  File \"/home/ubuntu/thumperai/crew/lats.py\", line 295, in generate_initial_response\n    reflection = reflection_chain.invoke(\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3523, in invoke\n    return self._call_with_config(\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1262, in _call_with_config\n    context.run(\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 326, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3397, in _invoke\n    output = call_func_with_variable_args(\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 326, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n  File \"/home/ubuntu/lats.py\", line 242, in reflection_chain\n    reflection = tool_choices[0]\nIndexError: list index out of range\n\n### Description\n\nI'm trying to make the lats example work with an Ollama model using the langchain Ollama api's, but there is a bug or issue with the tool support.\n\n### System Info\n\nlangchain==0.1.11\nlangchain-community==0.0.27\nlangchain-core==0.1.30\nlangchain-experimental==0.0.53\nlangchain-openai==0.0.5\nlangchain-text-splitters==0.0.1\nubuntu 22.04\npython 3.10.12\n", "created_at": "2024-03-12", "closed_at": "2024-05-15", "labels": ["Notebook Request"], "State": "closed", "Author": "rakataprime"}
{"issue_number": 196, "issue_title": "Hierarchical Agent Teams with Ollama", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nimport functools\nimport operator\nfrom datetime import datetime\nfrom textwrap import dedent\nfrom typing import Sequence, TypedDict, Annotated\n\nfrom langchain.agents import create_react_agent, AgentExecutor\nfrom langchain_community.tools.ddg_search import DuckDuckGoSearchRun\nfrom langchain_core.messages import HumanMessage, BaseMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables import Runnable\nfrom langchain_core.tools import BaseTool, tool\nfrom langchain_experimental.llms.ollama_functions import OllamaFunctions\nfrom langchain_community.output_parsers.ernie_functions import JsonOutputFunctionsParser\nfrom langchain_experimental.tools import PythonREPLTool\nfrom langgraph.graph import END, StateGraph\n\nSUPERVISOR = 'Supervisor'\nRESEARCHER = 'Researcher'\nCODER = 'Coder'\nFINISH = 'FINISH'\nmembers = [RESEARCHER, CODER]\n\n\nclass GraphState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], operator.add]\n    next: str\n\n\ndef create_agent(llm: OllamaFunctions, tools: Sequence[BaseTool], system_prompt: str) \\\n        -> Runnable:\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                system_prompt,\n            ),\n            MessagesPlaceholder(variable_name=\"messages\"),\n            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n            MessagesPlaceholder(variable_name=\"tools\"),\n            MessagesPlaceholder(variable_name=\"tool_names\"),\n        ]\n    )\n\n    return create_react_agent(llm, tools, prompt)\n\n\ndef create_agent_executor(llm: OllamaFunctions, tools: Sequence[BaseTool],\n                          system_prompt: str) -> AgentExecutor:\n    agent = create_agent(llm, tools, system_prompt)\n    executor = AgentExecutor(agent=agent, tools=tools)\n    return executor\n\n\ndef agent_node(state: TypedDict, agent: Runnable, name: str):\n    result = agent.invoke(state)\n    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}\n\n\n@tool\ndef get_actual_date_tool(date_format: str = \"%Y-%m-%d %H:%M:%S\"):\n    \"\"\"\n    Get the current time\n    \"\"\"\n    return datetime.now().strftime(date_format)\n\n\ndef get_code_executor_code() -> BaseTool:\n    return PythonREPLTool()\n\n\ndef get_web_search_tool() -> BaseTool:\n    return DuckDuckGoSearchRun()\n\n\nllm = OllamaFunctions(model=\"openhermes\")\n\noptions = [FINISH] + members\n\nfunction_def = {\n    \"name\": \"route\",\n    \"description\": \"Select the next role.\",\n    \"parameters\": {\n        \"title\": \"routeSchema\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"next\": {\n                \"title\": \"Next\",\n                \"anyOf\": [\n                    {\"enum\": options},\n                ],\n            }\n        },\n        \"required\": [\"next\"],\n    },\n}\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            dedent(\"\"\"\n                You are a supervisor tasked with managing a conversation between the following workers: \n                {members}. Given the following user request, respond with the worker to act next. Each worker \n                will perform a task and respond with their results and status. When finished, respond with \n                FINISH.\n            \"\"\")\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n        (\n            \"system\",\n            dedent(\"\"\"\n                Given the conversation above, who should act next? Or should we FINISH? Select one of: {options}\n            \"\"\")\n        ),\n    ]\n).partial(options=str(options), members=\", \".join(members))\n\nsupervisor_chain = (\n        prompt\n        | llm.bind(functions=[function_def], function_call={\"name\": \"route\"})\n        | JsonOutputFunctionsParser()\n)\n\nresearch_agent = create_agent_executor(\n    llm,\n    [get_web_search_tool()],\n    \"You are a web researcher.\"\n)\nresearch_node = functools.partial(agent_node, agent=research_agent, name=RESEARCHER)\n\ncode_agent = create_agent_executor(\n    llm,\n    [get_code_executor_code()],\n    \"You may generate safe python code to analyze data and generate charts using matplotlib.\",\n)\ncode_node = functools.partial(agent_node, agent=code_agent, name=CODER)\n\nworkflow_graph = StateGraph(GraphState)\nworkflow_graph.add_node(RESEARCHER, research_node)\nworkflow_graph.add_node(CODER, code_node)\nworkflow_graph.add_node(SUPERVISOR, supervisor_chain)\n\nfor member in members:\n    # We want our workers to ALWAYS \"report back\" to the supervisor when done\n    workflow_graph.add_edge(member, SUPERVISOR)\n# The supervisor populates the \"next\" field in the graph state\n# which routes to a node or finishes\nconditional_map = {k: k for k in members}\nconditional_map[FINISH] = END\nworkflow_graph.add_conditional_edges(SUPERVISOR, lambda x: x[\"next\"], conditional_map)\n# Finally, add entrypoint\nworkflow_graph.set_entry_point(SUPERVISOR)\n\nchain = workflow_graph.compile()\n\nrunning = True\nwhile running:\n    user_input = input(\"Enter text (press 'q' or ctrl-c to quit): \")\n    if user_input.lower() == 'q':\n        running = False\n    try:\n        inputs = {\n            \"messages\": [\n                HumanMessage(content=user_input)\n            ]\n        }\n        for s in chain.stream(inputs, {\"recursion_limit\": 100}, ):\n            if \"__end__\" not in s:\n                print(\"---\")\n                result = list(s.values())[0]\n                print(result)\n    except Exception as e:\n        print(e)\n        print('Sorry, something goes wrong. Try with a different input')\nError Message and Stack Trace (if applicable)\nThe output is copied directly from LangSmith, the stacktrace is truncated both in console and LangSmith.\nValueError('variable agent_scratchpad should be a list of base messages, got ')Traceback (most recent call last):\nFile \".../langchain_core/runnables/base.py\", line 1262, in _call_with_config\ncontext.run(\nFile \".../langchain_core/runnables/config.py\", line 326, in call_func_with_variable_args\nreturn func(input, **kwargs)  # type: ignore[call-arg]\n^^^^^^^^^^^^^^^^^^^^^\nFile \".../langchain_core/prompts/base.py\", line 103, in _format_prompt_with_error_handling\nreturn self.format_prompt(**inner_input)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \".../langchain_core/prompts/chat.py\", line 535, in format_prompt\nmessages = self.format_messages(**kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \".../langchain_core/prompts/chat.py\", line 797, in format_messages\nmessage = message_template.format_messages(**kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \".../langchain_core/prompts/chat.py\", line 129, in format_messages\nraise ValueError(\nValueError: variable agent_scratchpad should be a list of base messages, got\nDescription\nI'm trying to reproduce the provided example hierarchical_agent_teams.ipynb using Ollama with OllamaFunctions\nI solved some incompatibility issues found with the bind_function replaced with bind and passing explicitly the \"name\" of the route function ({\"name\": \"route\"} instead of \"route\").\nAfter that, I'm stuck on this error: 'variable agent_scratchpad should be a list of base messages, got. The output seems to be truncated but this happens because the agent_scratchpad variable is empty ('').\nHow this can be solved? Am I making some error in the GraphState or something else?\nThanks in advance for the support.\nSystem Info\n$ python -m langchain_core.sys_info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP PREEMPT_DYNAMIC Sun, 03 Mar 2024 07:25:31 +0000\nPython Version:  3.11.7 (main, Dec 14 2023, 11:23:37) [GCC 13.2.1 20230801]\n\nPackage Information\n\nlangchain_core: 0.1.30\nlangchain: 0.1.11\nlangchain_community: 0.0.26\nlangsmith: 0.1.23\nlangchain_experimental: 0.0.53\nlangchain_openai: 0.0.8\nlangchain_text_splitters: 0.0.1\nlangchainhub: 0.1.15\nlanggraph: 0.0.26\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlangserve\n", "created_at": "2024-03-09", "closed_at": "2024-06-12", "labels": ["enhancement", "Notebook Request"], "State": "closed", "Author": "ZappaBoy"}
{"issue_number": 195, "issue_title": "DOC: Add examples using Ollama, HuggingFace and other models rather than OpenAI models.", "issue_body": "Issue with current documentation:\nA very large part of the documentation end examples provided are about OpenAI models. I clearly understand the importance of build, integrating, and improving langchain, langgraph, langsmith, and other products on the top of OpenAI models, but there is a great community around the other solutions, especially around solutions that use Ollama and HuggingFace.\nI'm referring to a lot of resources provided like the following:\n\nhttps://python.langchain.com/docs/langgraph#prebuilt-examples\nhttps://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/agent_supervisor.ipynb\nhttps://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb\nhttps://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/multi-agent-collaboration.ipynb\nhttps://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_agentic_rag.ipynb\netc...\n\nThe links above show very useful usages of these products but there is no reference at all about other models other than OpenAI.\nThe only resources I can personally be able to found are some comments in the issues and something like this:\nhttps://api.python.langchain.com/en/latest/llms/langchain_experimental.llms.ollama_functions.OllamaFunctions.html\nAlso looking for \"Ollama\" or \"Hugging\" keywords in the examples provided there are only two references for both, also all referred to the same two playbooks langgraph_crag_mistral.ipynb and langgraph_self_rag_mistral_nomic.ipynb:\n\nhttps://github.com/search?q=repo%3Alangchain-ai%2Flanggraph+path%3A%2F%5Eexamples%5C%2F%2F+Ollama&type=code\nhttps://github.com/search?q=repo%3Alangchain-ai%2Flanggraph+path%3A%2F%5Eexamples%5C%2F%2F+Hugging&type=code\n\nAlso regarding OllamaFunctions I understand that this is in langchain_experimental package but I would be really happy to see it working as a sort of replacement for ChatOpenAI.\nI don't want to sound ungrateful, langchain-ai products are awesome but these improvements about the other communities can be very useful and they could greatly improve both the use and development of solutions through the langchain-ai products.\nIdea or request for content:\nNo response", "created_at": "2024-03-08", "closed_at": null, "labels": [], "State": "open", "Author": "ZappaBoy"}
{"issue_number": 192, "issue_title": "draw_png does not draw the actual graph but just LangGraphInput -> Pregel -> LangGraphOutput", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nAbstracted from this notebook: https://github.com/langchain-ai/langgraph/blob/main/examples/storm/storm.ipynb\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom typing import List, Optional\nfrom langchain_core.prompts import ChatPromptTemplate\nclass Editor(BaseModel):\n    affiliation: str = Field(\n        description=\"Primary affiliation of the editor.\",\n    )\n    name: str = Field(\n        description=\"Name of the editor.\",\n    )\n    role: str = Field(\n        description=\"Role of the editor in the context of the topic.\",\n    )\n    description: str = Field(\n        description=\"Description of the editor's focus, concerns, and motives.\",\n    )\n\n    @property\n    def persona(self) -> str:\n        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n\ndef add_messages(left, right):\n    if not isinstance(left, list):\n        left = [left]\n    if not isinstance(right, list):\n        right = [right]\n    return left + right\n\ndef update_references(references, new_references):\n    if not references:\n        references = {}\n    references.update(new_references)\n    return references\n\n\ndef update_editor(editor, new_editor):\n    # Can only set at the outset\n    if not editor:\n        return new_editor\n    return editor\n\nfrom langgraph.graph import StateGraph, END\nfrom typing_extensions import TypedDict\nfrom langchain_core.messages import AnyMessage\nfrom typing import Annotated, Sequence, List, Optional\nclass InterviewState(TypedDict):\n    messages: Annotated[List[AnyMessage], add_messages]\n    references: Annotated[Optional[dict], update_references]\n    editor: Annotated[Optional[Editor], update_editor] \n\nbuilder = StateGraph(InterviewState)\n\nbuilder.add_node(\"ask_question\", lambda x:x )\nbuilder.add_node(\"answer_question\", lambda x:x)\nbuilder.add_conditional_edges(\"answer_question\", lambda x:x)\nbuilder.add_edge(\"ask_question\", \"answer_question\")\nbuilder.set_entry_point(\"ask_question\")\ninterview_graph = builder.compile().with_config(run_name=\"Conduct Interviews\")\nfrom IPython.display import Image\nImage(interview_graph.get_graph().draw_png())\n\nThis only gives me\n\nError Message and Stack Trace (if applicable)\nNo Error message, but the PNG should look different and actually show the graph\nDescription\nI would expect the draw_png function to actually show the graph as it does in this notebook: https://github.com/langchain-ai/langgraph/blob/main/examples/storm/storm.ipynb\nI am not able to run the whole notebook, but the chart looks just like the uploaded screenshot for all my graphs as well as my exctract MRE from the notebook\nSystem Info\nlangchain==0.1.11\nlangchain-community==0.0.27\nlangchain-core==0.1.30\nlangchain-experimental==0.0.53\nlangchain-openai==0.0.6\nlangchain-text-splitters==0.0.1\nlangchainhub==0.1.14\nlangcodes==3.3.0\nlangdetect==1.0.9\nlanggraph==0.0.24\nlangsmith==0.1.22\nMacOS 14.11\npython 3.10", "created_at": "2024-03-07", "closed_at": "2024-03-07", "labels": [], "State": "closed", "Author": "krlng"}
{"issue_number": 191, "issue_title": "ToolExecutor ainvoke doesn't return result of async tool calling", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\ndef tool(who: str) -> str:\n    \"\"\"A sync tool.\"\"\"\n    return \"hello\"\n\nasync def atool(who: str) -> str:\n    \"\"\"An async tool.\"\"\"\n    return \"hello\"\n\nasync def run():\n    from langchain.tools import StructuredTool\n    from langgraph.prebuilt import ToolExecutor, ToolInvocation\n\n    a = StructuredTool.from_function(tool, name=\"tool\")\n    b = StructuredTool.from_function(atool, name=\"atool\")\n\n    executor = ToolExecutor([a, b])\n    a_out = await executor.ainvoke(ToolInvocation(tool=\"tool\", tool_input=\"a\"))\n    b_out = await executor.ainvoke(ToolInvocation(tool=\"atool\", tool_input=\"b\"))\n    return a_out, b_out\n\nprint(asyncio.run(run()))\n\nFor the code above, expected a_out and b_out are both \"hello\", actual output is like this:\n('hello', <coroutine object atool at 0x7f07a37efbc0>)\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nI'm trying to use LangGraph, with a ToolExecutor to execute both sync and async functions.  When one function is async, result is not actual function's return value but coroutine, making it diffcult to use only one piece of code in graph like this:\n    async def call_tool(self, state):\n        \"\"\"The function that calls tools.\"\"\"\n        messages = state[\"messages\"]\n        last_message = messages[-1]\n        action = ToolInvocation(\n            tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n            tool_input=json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"]),\n        )\n        response = await self.tool_executor.ainvoke(action)\n        function_message = FunctionMessage(content=str(response), name=action.tool)\n        return {\"messages\": [function_message]}\n\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP Thu Oct 5 21:02:42 UTC 2023\nPython Version:  3.10.13 (main, Nov  7 2023, 12:51:46) [GCC 9.4.0]\n\nPackage Information\n\nlangchain_core: 0.1.19\nlangchain: 0.1.4\nlangchain_community: 0.0.16\nlangsmith: 0.0.92\nlangchain_google_genai: 0.0.9\nlangchain_google_vertexai: 0.0.3\nlangchain_openai: 0.0.5\nlangchainhub: 0.1.14\nlanggraph: 0.0.20\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlangserve\n", "created_at": "2024-03-07", "closed_at": "2024-03-07", "labels": [], "State": "closed", "Author": "wolvever"}
{"issue_number": 190, "issue_title": "plan-to-execute.   use the create_openai_tools_agent replace create_openai_functions_agent", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nfrom langchain import hub\nfrom langchain.agents import create_openai_functions_agent,create_openai_tools_agent,\nfrom langchain_openai import ChatOpenAI\n\n\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n\ntools = [TavilySearchResults(max_results=3)]\n\n# Get the prompt to use - you can modify this!\nprompt = hub.pull(\"hwchase17/openai-tools-agent\")\n\n# Construct the OpenAI Tools agent\n\nagent_runnable = create_openai_tools_agent(llm, tools,prompt)\n\nfrom langgraph.prebuilt import create_agent_executor\n\nagent_executor = create_agent_executor(agent_runnable, tools)\n\nagent_executor.invoke(\n    {\"input\": \"who is the winnner of the us open\", \"chat_history\": []},\n)\n\n\nError Message and Stack Trace (if applicable)\n50 def _execute(\n51     self, tool_invocation: ToolInvocation, *, config: RunnableConfig\n52 ) -> Any:\n---> 53     if tool_invocation.tool not in self.tool_map:\n54         return self.invalid_tool_msg_template.format(\n55             requested_tool_name=tool_invocation.tool,\n56             available_tool_names_str=\", \".join([t.name for t in self.tools]),\n57         )\n58     else:\nAttributeError: 'list' object has no attribute 'tool'\nDescription\ni don't know how to fix it,how to find the bug\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 21.6.0: Thu Mar  9 20:08:59 PST 2023; root:xnu-8020.240.18.700.8~1/RELEASE_X86_64\nPython Version:  3.11.8 (main, Feb 26 2024, 15:43:17) [Clang 14.0.6 ]\n\nPackage Information\n\nlangchain_core: 0.1.27\nlangchain: 0.1.9\nlangchain_community: 0.0.24\nlangsmith: 0.1.10\nlangchain_openai: 0.0.8\nlangchainhub: 0.1.15\nlanggraph: 0.0.26\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlangserve\n", "created_at": "2024-03-07", "closed_at": "2024-03-07", "labels": [], "State": "closed", "Author": "suforme"}
{"issue_number": 179, "issue_title": "Required lcel-teacher-eval dataset configuration for langgraph_code_assistant.ipynb is unclear", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nThe following code:\n# Run eval on base chain\nrun_id = uuid.uuid4().hex[:4]\nproject_name = \"context-stuffing-no-langgraph\"\nclient.run_on_dataset(\n    dataset_name=\"lcel-teacher-eval\",\n    llm_or_chain_factory= lambda: (lambda x: x[\"question\"]) | chain_base_case,\n    evaluation=evaluation_config,\n    project_name=f\"{run_id}-{project_name}\",\n)\nError Message and Stack Trace (if applicable)\n{\n\t\"name\": \"ValueError\",\n\t\"message\": \"Dataset lcel-teacher-eval has no example rows.\",\n\t\"stack\": \"---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[23], line 38\n     36 run_id = uuid.uuid4().hex[:4]\n     37 project_name = \\\"context-stuffing-with-langgraph\\\"\n---> 38 client.run_on_dataset(\n     39     dataset_name=\\\"lcel-teacher-eval\\\",\n     40     llm_or_chain_factory=model,\n     41     evaluation=evaluation_config,\n     42     project_name=f\\\"{run_id}-{project_name}\\\",\n     43 )\n\nFile ~/forks/langchain/langgraph/.venv/lib/python3.10/site-packages/langsmith/client.py:3394, in Client.run_on_dataset(self, dataset_name, llm_or_chain_factory, evaluation, concurrency_level, project_name, project_metadata, verbose, tags, input_mapper, revision_id)\n   3389 except ImportError:\n   3390     raise ImportError(\n   3391         \\\"The client.run_on_dataset function requires the langchain\\\"\n   3392         \\\"package to run.\\\nInstall with pip install langchain\\\"\n   3393     )\n-> 3394 return _run_on_dataset(\n   3395     dataset_name=dataset_name,\n   3396     llm_or_chain_factory=llm_or_chain_factory,\n   3397     concurrency_level=concurrency_level,\n   3398     client=self,\n   3399     evaluation=evaluation,\n   3400     project_name=project_name,\n   3401     project_metadata=project_metadata,\n   3402     verbose=verbose,\n   3403     tags=tags,\n   3404     input_mapper=input_mapper,\n   3405     revision_id=revision_id,\n   3406 )\n\nFile ~/forks/langchain/langgraph/.venv/lib/python3.10/site-packages/langchain/smith/evaluation/runner_utils.py:1297, in run_on_dataset(client, dataset_name, llm_or_chain_factory, evaluation, concurrency_level, project_name, project_metadata, verbose, tags, revision_id, **kwargs)\n   1289     warn_deprecated(\n   1290         \\\"0.0.305\\\",\n   1291         message=\\\"The following arguments are deprecated and \\\"\n   (...)\n   1294         removal=\\\"0.0.305\\\",\n   1295     )\n   1296 client = client or Client()\n-> 1297 container = _DatasetRunContainer.prepare(\n   1298     client,\n   1299     dataset_name,\n   1300     llm_or_chain_factory,\n   1301     project_name,\n   1302     evaluation,\n   1303     tags,\n   1304     input_mapper,\n   1305     concurrency_level,\n   1306     project_metadata=project_metadata,\n   1307     revision_id=revision_id,\n   1308 )\n   1309 if concurrency_level == 0:\n   1310     batch_results = [\n   1311         _run_llm_or_chain(\n   1312             example,\n   (...)\n   1317         for example, config in zip(container.examples, container.configs)\n   1318     ]\n\nFile ~/forks/langchain/langgraph/.venv/lib/python3.10/site-packages/langchain/smith/evaluation/runner_utils.py:1125, in _DatasetRunContainer.prepare(cls, client, dataset_name, llm_or_chain_factory, project_name, evaluation, tags, input_mapper, concurrency_level, project_metadata, revision_id)\n   1123         project_metadata = {}\n   1124     project_metadata.update({\\\"revision_id\\\": revision_id})\n-> 1125 wrapped_model, project, dataset, examples = _prepare_eval_run(\n   1126     client,\n   1127     dataset_name,\n   1128     llm_or_chain_factory,\n   1129     project_name,\n   1130     project_metadata=project_metadata,\n   1131     tags=tags,\n   1132 )\n   1133 tags = tags or []\n   1134 for k, v in (project.metadata.get(\\\"git\\\") or {}).items():\n\nFile ~/forks/langchain/langgraph/.venv/lib/python3.10/site-packages/langchain/smith/evaluation/runner_utils.py:971, in _prepare_eval_run(client, dataset_name, llm_or_chain_factory, project_name, project_metadata, tags)\n    969 examples = list(client.list_examples(dataset_id=dataset.id))\n    970 if not examples:\n--> 971     raise ValueError(f\\\"Dataset {dataset_name} has no example rows.\\\")\n    973 try:\n    974     git_info = get_git_info()\n\nValueError: Dataset lcel-teacher-eval has no example rows.\"\n}\n\n\nDescription\n\nI'm trying to compare the behavior and quality of code assistants both with and without langgraph\nthe major roadblock has to do with proper configuration of the langsmith dataset\nI have had to manually create the lcel-teacher-eval dataset to get around an initial error regarding the dataset not existing\n\ncurrently I'm getting the error captured above\nI don't remember the specific difference in configuration (or dependencies) but at one point I was seeing it write to the personal lcel-teacher-eval dataset consistently.  In this case, the issue was that the input element was being captured as input:input and not input:question.\n\n\n\nSystem Info\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #1 SMP Thu Oct 5 21:02:42 UTC 2023\n> Python Version:  3.10.13 (main, Feb  7 2024, 15:27:48) [GCC 11.4.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.1.28\n> langchain: 0.1.8\n> langchain_community: 0.0.21\n> langsmith: 0.1.4\n> langchain_openai: 0.0.8\n> langchainhub: 0.1.14\n> langgraph: 0.0.26\n\nPackages not installed (Not Necessarily a Problem)\n--------------------------------------------------\nThe following packages were not found:\n\n> langserve\n", "created_at": "2024-03-04", "closed_at": "2024-03-07", "labels": [], "State": "closed", "Author": "donbr"}
{"issue_number": 173, "issue_title": "msg should be a list in the rewrite node of langchain_agentic_rag", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\ndef rewrite(state):\n    \"\"\"\n    Transform the query to produce a better question.\n    \n    Args:\n        state (messages): The current state\n    \n    Returns:\n        dict: The updated state with re-phrased question\n    \"\"\"\n    \n    print(\"---TRANSFORM QUERY---\")\n    messages = state[\"messages\"]\n    question = messages[0].content\n\n    msg = HumanMessage(\n        content=f\"\"\" \\n \n    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n    Here is the initial question:\n    \\n ------- \\n\n    {question} \n    \\n ------- \\n\n    Formulate an improved question: \"\"\",\n    )\n\nError Message and Stack Trace (if applicable)\nValueError: Invalid input type <class 'langchain_core.messages.human.HumanMessage'>. Must be a PromptValue, str, or list of BaseMessages.\nDescription\nmsg should be a list not a HumanMessages\nSystem Info\nlangchain==0.1.10\nlangchain-community==0.0.25\nlangchain-core==0.1.28\nlangchain-openai==0.0.8\nlangchain-text-splitters==0.0.1\nlangchainhub==0.1.14", "created_at": "2024-03-02", "closed_at": "2024-03-03", "labels": [], "State": "closed", "Author": "shresthakamal"}
{"issue_number": 172, "issue_title": "Error printing the score when grade is \"no\" for langgraph_agentic_rag", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\ndef grade_documents(state):\n    \"\"\"\n    Determines whether the retrieved documents are relevant to the question.\n\n    Args:\n        state (messages): The current state\n\n    Returns:\n        str: A decision for whether the documents are relevant or not\n    \"\"\"\n\n    print(\"---CHECK RELEVANCE---\")\n\n    # Data model\n    class grade(BaseModel):\n        \"\"\"Binary score for relevance check.\"\"\"\n\n        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n\n    # LLM\n    model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0,)\n\n    # Tool\n    grade_tool_oai = convert_to_openai_tool(grade)\n\n    # LLM with tool and enforce invocation\n    llm_with_tool = model.bind(\n        tools=[convert_to_openai_tool(grade_tool_oai)],\n        tool_choice={\"type\": \"function\", \"function\": {\"name\": \"grade\"}},\n    )\n\n    # Parser\n    parser_tool = PydanticToolsParser(tools=[grade])\n\n    # Prompt\n    prompt = PromptTemplate(\n        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n        Here is the retrieved document: \\n\\n {context} \\n\\n\n        Here is the user question: {question} \\n\n        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n        input_variables=[\"context\", \"question\"],\n    )\n\n    # Chain\n    chain = prompt | llm_with_tool | parser_tool\n\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n\n    question = messages[0].content\n    docs = last_message.content\n    \n    score = chain.invoke(\n        {\"question\": question, \n         \"context\": docs}\n    )\n    \n    grade = score[0].binary_score\n    \n    if grade == \"yes\":\n        print(\"---DECISION: DOCS RELEVANT---\")\n        return \"yes\"\n\n    else:\n        print(\"---DECISION: DOCS NOT RELEVANT---\")\n        print(score.binary_score)\n        return \"no\"\n\n\nError Message and Stack Trace (if applicable)\nAttributeError: 'list' object has no attribute 'binary_score'\nDescription\nIf the grader grades any document as not relevant, and scores \"no\", printing the score is not correct\nSystem Info\nlangchain==0.1.10\nlangchain-community==0.0.25\nlangchain-core==0.1.28\nlangchain-openai==0.0.8\nlangchain-text-splitters==0.0.1\nlangchainhub==0.1.14", "created_at": "2024-03-02", "closed_at": "2024-03-03", "labels": [], "State": "closed", "Author": "shresthakamal"}
{"issue_number": 161, "issue_title": "Graph Persistence when input is not None, graph will reset to entry point", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\n@tool(\"web_search\")\ndef web_search(query: str) -> str:\n    \"\"\"Search with Google SERP API by a query\"\"\"\n    search = SerpAPIWrapper()\n    return search.run(query)\n\ntools = [web_search]\nprompt = hub.pull(\"hwchase17/openai-functions-agent\")\nllm = ChatOpenAI(model=\"gpt-4-turbo-preview\", streaming=True)\nagent_runnable = create_openai_functions_agent(llm, tools, prompt)\n\n\nclass AgentState(TypedDict):\n    input: Annotated[Sequence[BaseMessage], operator.add]\n    chat_history: list[BaseMessage]\n    agent_outcome: Union[AgentAction, AgentFinish, None]\n    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]\n\nfrom langchain_core.agents import AgentFinish\nfrom langgraph.prebuilt.tool_executor import ToolExecutor\n\ntool_executor1 = ToolExecutor(tools)\ntool_executor2 = ToolExecutor([])\n\n# Define the agent\ndef run_agent(data):\n    agent_outcome = agent_runnable.invoke(data)\n    return {\"agent_outcome\": agent_outcome}\n\n\n# Define the function to execute tools\ndef execute_tools1(data):\n    agent_action = data[\"agent_outcome\"]\n    output = tool_executor1.invoke(agent_action)\n    print('execute_tools1')\n    return {\"intermediate_steps\": [(agent_action, str(output))]}\n\n# Define the function to execute tools\ndef execute_tools2(data):\n    agent_action = data[\"agent_outcome\"]\n    output = tool_executor2.invoke(agent_action)\n\n    print('execute_tools2')\n    return {\"intermediate_steps\": [(agent_action, str(output))]}\n\ndef should_continue(data):\n    if isinstance(data[\"agent_outcome\"], AgentFinish):\n        return \"end\"\n    else:\n        return \"continue\"\n\n\nworkflow = StateGraph(AgentState)\n\nworkflow.add_node(\"agent\", run_agent)\nworkflow.add_node(\"action1\", execute_tools1)\nworkflow.add_node(\"action2\", execute_tools2)\n\nworkflow.set_entry_point(\"agent\")\nworkflow.add_conditional_edges(\n    \"agent\",\n    should_continue,\n    {\n        \"continue\": \"action1\",\n        \"end\": END,\n    },\n)\n\nworkflow.add_edge(\"action1\", \"action2\")\nworkflow.add_edge(\"action2\", \"agent\")\n\n\nmemory = SqliteSaver.from_conn_string(\":memory:\")\n\napp = workflow.compile(checkpointer=memory, interrupt_before=[\"action2\"])\n\n\ninputs = {\"input\": [HumanMessage(content=\"what is the weather in sf\")], \"chat_history\": []}\n\n\nfor s in app.stream(inputs, {\"configurable\": {\"thread_id\": \"3\"}}):\n    print(list(s.values())[0])\n    print(\"----\")\n\ninputs = {\"input\": [HumanMessage(content=\"what is the weather in NY\")]}\n\nfor s in app.stream(inputs, {\"configurable\": {\"thread_id\": \"3\"}}):\n    print(list(s.values())[0])\n    print(\"----\")\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nBug Report: StateGraph Workflow Persistence Issue with Configurable Thread ID\nSummary\nWhen using the StateGraph workflow with a configurable thread ID to maintain state across multiple iterations of a stream, the graph does not resume from the expected node following an interruption and the presence of new input. Instead of continuing from the interrupted point with the new input, the workflow restarts from the entry point. This behavior diverges from the documented functionality, where supplying None as input for a subsequent iteration should prompt the graph to continue from the interrupted node.\nSteps to Reproduce\n\nInitialize a StateGraph with an entry point and multiple nodes, including an interruption point (interrupt_before=[\"action2\"]).\nCompile the graph with a SqliteSaver configured for in-memory persistence and a configurable thread ID.\nStream inputs through the graph using a specific thread ID, and interrupt the workflow at the predetermined point.\nAttempt to resume the workflow by streaming additional inputs with the same thread ID, expecting the workflow to pick up from the point of interruption.\n\nExpected Behavior\nUpon streaming new inputs with the same thread ID after an interruption, the workflow should resume from the node immediately following the last executed node before the interruption.\nActual Behavior\nThe workflow restarts from the entry point node, disregarding the previously executed path and the interruption point, leading to an unexpected re-initialization of the workflow.\nThank you for looking into this issue. Please let me know if there's any more information I can provide to help diagnose and resolve this problem.\nSystem Info\nlangchain==0.1.9\nlangchain-community==0.0.24\nlangchain-core==0.1.27\nlangchain-openai==0.0.8\nlangchainhub==0.1.14\npython==3.12\nlanggraph==0.0.26", "created_at": "2024-02-29", "closed_at": "2024-03-29", "labels": [], "State": "closed", "Author": "faze059"}
{"issue_number": 160, "issue_title": "DOC: format_tool_to_openai_function is deprecated", "issue_body": "Issue with current documentation:\nlangchain.tools.render.format_tool_to_openai_function() is used several places in docs and examples, and is planned for deprecation in LangChain v0.2.0.\nIdea or request for content:\nReplace all use cases with the alternative langchain_core.utils.function_calling.convert_to_openai_tool(), and check other deprecations planned in 0.2.0 in langchain_core.utils.function_calling.", "created_at": "2024-02-29", "closed_at": "2024-05-04", "labels": ["good first issue"], "State": "closed", "Author": "Xtrah"}
{"issue_number": 159, "issue_title": "[LATS example] Use recursive best candidate selection in the tree", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nThe function expand(state: TreeState, config: RunnableConfig) code in the example at [0], only the directed children nodes of the root are considered the UCT values to extend.\n[0] https://github.com/langchain-ai/langgraph/blob/main/examples/lats/lats.ipynb\nError Message and Stack Trace (if applicable)\nIn the extreme case, the error recursion limit reached of the graph/pregel could be triggered.\nDescription\nFirst, thanks for building these awesome assets and the example code!\nIIUC on the Language Agents Tree Search approach described in the blog [0], for each node in the tree, should not only consider the UCT values of its direct children, but also recursively consider the UCT values of all descendant nodes, in order to find the best candidate to extend (before the max search depth is reached) with the highest UCT value in the entire tree.\nso I consider a fix is:\n# best_candidate: Node = root.best_child if root.children else root    # should be replaced by the following\nbest_candidate: Node = root\nwhile best_candidate.children:\n    best_candidate = best_candidate.best_child\n[0] https://blog.langchain.dev/reflection-agents/\nIf I understand anything wrong, pls let me know.\nzhiyan\nSystem Info\nlangchain==0.1.9\nlangchain-community==0.0.21\nlangchain-core==0.1.27\nlangchain-experimental==0.0.52\nlangchain-openai==0.0.8\nlangchainhub==0.1.14\nlanggraph==0.0.26\nplatform: mac\npython: 3.11.7", "created_at": "2024-02-28", "closed_at": "2024-03-04", "labels": [], "State": "closed", "Author": "zhiyanliu"}
{"issue_number": 156, "issue_title": "Bad example for the code interpreter/alphacodium.", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nThe following code:\naddendum = \"\"\"  \\n --- --- --- \\n You previously tried to solve this problem. \\n Here is your solution:  \n                    \\n --- --- --- \\n {generation}  \\n --- --- --- \\n  Here is the resulting error from code \n                    execution:  \\n --- --- --- \\n {error}  \\n --- --- --- \\n Please re-try to answer this. \n                    Structure your answer with a description of the code solution. \\n Then list the imports. \n                    And finally list the functioning code block. Structure your answer with a description of \n                    the code solution. \\n Then list the imports. And finally list the functioning code block. \n                    \\n Here is the user question: \\n --- --- --- \\n {question}\"\"\"\nError Message and Stack Trace (if applicable)\nNothing to say here.\nDescription\nTry to use the notebook to implement a snake game. Before that uninstall the turtle and pygrame libraries.\nThe problem with the above mentioned code is, instead of asking LLM to suggest ways to install the missing libraries, the prompt asks LLM to regenerate another solution, with which, missing libraries still won't be installed.\nYou might need another node to process the missing library message or separate that error from code error in the generate() function.\nSystem Info\nlangchain==0.1.9\nlangchain-community==0.0.24\nlangchain-core==0.1.26\nlangchain-mistralai==0.0.4\nlangchain-openai==0.0.7\nlangchainhub==0.1.14", "created_at": "2024-02-28", "closed_at": "2024-05-02", "labels": [], "State": "closed", "Author": "chengdujin"}
{"issue_number": 150, "issue_title": "langgraph prebuilt NOT work", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\n from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\nError Message and Stack Trace (if applicable)\nFile \"/home/zhangyj/langchain/langgraph.py\", line 23, in \nfrom langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\nModuleNotFoundError: No module named 'langgraph.prebuilt'; 'langgraph' is not a package\nDescription\nI followed the instructions, trying to run langgraph, but it seems the package can't work as expect\nSystem Info\npip freeze | grep langgraph\nlanggraph==0.0.26\nplatform (Ubuntu)\npython 3.10", "created_at": "2024-02-26", "closed_at": "2024-03-04", "labels": [], "State": "closed", "Author": "jackiezhangcn"}
{"issue_number": 148, "issue_title": "langgraph.pregel.GraphRecursionError: Recursion limit of 25 reachedwithout hitting a stop condition. You can increase the limitby setting the `recursion_limit` config key.", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\ndo you know typically what is the root cause of this error and how to resolve it? Thanks.\nlanggraph.pregel.GraphRecursionError: Recursion limit of 25 reachedwithout hitting a stop condition. You can increase the limitby setting the recursion_limit config key.\nError Message and Stack Trace (if applicable)\ndo you know typically what is the root cause of this error and how to resolve it? Thanks.\nlanggraph.pregel.GraphRecursionError: Recursion limit of 25 reachedwithout hitting a stop condition. You can increase the limitby setting the recursion_limit config key.\nDescription\ndo you know typically what is the root cause of this error and how to resolve it? Thanks.\nlanggraph.pregel.GraphRecursionError: Recursion limit of 25 reachedwithout hitting a stop condition. You can increase the limitby setting the recursion_limit config key.\nSystem Info\npython 311", "created_at": "2024-02-25", "closed_at": "2024-02-25", "labels": [], "State": "closed", "Author": "haolxx"}
{"issue_number": 335, "issue_title": "I running multi-agent-collaboration.ipynb, add app.get_graph().print_ascii(), raise error", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nworkflow = StateGraph(AgentState)\nworkflow.add_node(\"Researcher\", research_node)\nworkflow.add_node(\"Chart Generator\", chart_node)\nworkflow.add_node(\"call_tool\", tool_node)\nworkflow.add_conditional_edges(\n\"Researcher\",\nrouter,\n{\"continue\": \"Chart Generator\", \"call_tool\": \"call_tool\", \"end\": END},\n)\nworkflow.add_conditional_edges(\n\"Chart Generator\",\nrouter,\n{\"continue\": \"Researcher\", \"call_tool\": \"call_tool\", \"end\": END},\n)\nworkflow.add_conditional_edges(\n\"call_tool\",\n# Each agent node updates the 'sender' field\n# the tool calling node does not, meaning\n# this edge will route back to the original agent\n# who invoked the tool\nlambda x: x[\"sender\"],\n{\n\"Researcher\": \"Researcher\",\n\"Chart Generator\": \"Chart Generator\",\n},\n)\nworkflow.set_entry_point(\"Researcher\")\napp = workflow.compile()\napp.get_graph().print_ascii()\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\nFile \"/xxx/src/agent/multi-agent.py\", line 199, in \napp.get_graph().print_ascii()\n^^^^^^^^^^^^^^^\nFile \"/xxx/venv/lib/python3.11/site-packages/langgraph/graph/graph.py\", line 413, in get_graph\ncond = graph.add_node(branch.condition, name)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/xxx/venv/lib/python3.11/site-packages/langchain_core/runnables/graph.py\", line 207, in add_node\nraise ValueError(f\"Node with id {id} already exists\")\nValueError: Node with id router already exists\nDescription\nI running multi-agent-collaboration.ipynb, add app.get_graph().print_ascii(), raise error\nSystem Info\nlanggraph=0.0.38", "created_at": "2024-04-22", "closed_at": "2024-04-22", "labels": [], "State": "closed", "Author": "yang20150702"}
{"issue_number": 334, "issue_title": "can not set recursion_limit in config with langserve", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nI create a graph with langserve.\nworkflow = StateGraph(PlanExecute)\n.... add the nodes and edges...\nc =  workflow.compile()\n\n.....\napp = FastAPI(\n    title=\"AI Agent Sever\",\n    version=\"0.2.9\",\n    description=\"AI Agent Sever\"\n)\nadd_routes(\n    app,\n    c,\n    path=\"/pe\"\n)\n\nwhen I use following request invoke. I found the \"recursion_limit\" parameter config not used.\n{\n    \"input\": {\n        \"transaction\": \"xxxxxxxx.\",\n        \"conversation_id\": \"\"\n    },\n    \"config\": {\n        \"configurable\": {\n            \"recursion_limit\": 2\n        }\n    }\n}\n\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nI create a graph with langserve.\nworkflow = StateGraph(PlanExecute)\n.... add the nodes and edges...\nc =  workflow.compile()\n\n.....\napp = FastAPI(\n    title=\"AI Agent Sever\",\n    version=\"0.2.9\",\n    description=\"AI Agent Sever\"\n)\nadd_routes(\n    app,\n    c,\n    path=\"/pe\"\n)\n\nwhen I use following request invoke. I found the \"recursion_limit\" parameter config not used.\n{\n    \"input\": {\n        \"transaction\": \"xxxxxxxx.\",\n        \"conversation_id\": \"\"\n    },\n    \"config\": {\n        \"configurable\": {\n            \"recursion_limit\": 2\n        }\n    }\n}\n\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Mar 12 10:22:43 UTC 2\nPython Version:  3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n\nPackage Information\n\nlangchain_core: 0.1.43\nlangchain: 0.1.16\nlangchain_community: 0.0.33\nlangsmith: 0.1.31\nlangchain_openai: 0.1.3\nlangchain_text_splitters: 0.0.1\nlangchainhub: 0.1.15\nlanggraph: 0.0.37\nlangserve: 0.1.0\n", "created_at": "2024-04-21", "closed_at": "2024-05-04", "labels": [], "State": "closed", "Author": "weiminw"}
{"issue_number": 313, "issue_title": "A large language model like GLM does not seem to apply to the three examples given", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\n\n\n\nError Message and Stack Trace (if applicable)\nNo response\nDescription\n\n\n\nSystem Info\n\n\n", "created_at": "2024-04-16", "closed_at": "2024-04-18", "labels": [], "State": "closed", "Author": "yuerf"}
{"issue_number": 309, "issue_title": "get_graph(xray=True). ValueError: Node with id condition already exists", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nfrom PIL import Image as PIL_Image\n\n...\nimage = PIL_Image.open(\n        io.BytesIO(\n            graph.compile()\n            .get_graph(xray=True)\n            # .get_graph()\n            .draw_png()\n        )\n    )\n    image.show()\n\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\n  File \"/home/dmitry/builder.py\", line 143, in <module>\n    .get_graph(xray=True)\n     ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dmitry/.pyenv/versions/3.11.6/lib/python3.11/site-packages/langgraph/graph/graph.py\", line 336, in get_graph\n    node.get_graph(config=config, xray=xray)\n  File \"/home/dmitry/.pyenv/versions/3.11.6/lib/python3.11/site-packages/langgraph/graph/graph.py\", line 358, in get_graph\n    cond = graph.add_node(branch.condition, name)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dmitry/.pyenv/versions/3.11.6/lib/python3.11/site-packages/langchain_core/runnables/graph.py\", line 209, in add_node\n    raise ValueError(f\"Node with id {id} already exists\")\nValueError: Node with id condition already exists\n\nDescription\nHello, guys! I receive this error on langgraph version == 0.0.37. When i call get_graph(x_ray=False), no error occures. But I need x_ray, because every node from edges below is subgraph.\nEdges:\n{('__start__', 'user_message'), ('task_1-step_1', '__end__'), ('user_message', 'task_3-step_1'), ('task_2-step_1', '__end__'), ('task_5-step_1', '__end__'), ('task_6-step_1', '__end__'), ('user_message', 'task_2-step_1'), ('user_message', 'task_5-step_1'), ('user_message', 'task_6-step_1'), ('user_message', 'task_4-step_1'), ('user_message', 'task_1-step_1'), ('task_3-step_1', '__end__'), ('task_4-step_1', '__end__')}\nError ValueError: Node with id condition already exists still occures\nI downgraded to langgraph version == 0.0.36 and this error occures. So downgrading didn't solve the issue.\nTraceback (most recent call last):\n  File \"/home/dmitry/builder.py\", line 143, in <module>\n    .get_graph(xray=True)\n     ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/dmitry/.pyenv/versions/3.11.6/lib/python3.11/site-packages/langgraph/graph/graph.py\", line 355, in get_graph\n    graph.add_edge(start_nodes[start], end_nodes[end])\n  File \"/home/dmitry/.pyenv/versions/3.11.6/lib/python3.11/site-packages/langchain_core/runnables/graph.py\", line 227, in add_edge\n    if target.id not in self.nodes:\n       ^^^^^^^^^\nAttributeError: 'NoneType' object has no attribute 'id'\n\nSystem Info\nlanggraph == 0.0.37", "created_at": "2024-04-15", "closed_at": "2024-05-01", "labels": [], "State": "closed", "Author": "mrbuslov"}
{"issue_number": 307, "issue_title": "JsonOutputToolsParser not streaming when called from a LangGraph node", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nfrom typing import TypedDict, List, Union, Dict\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langgraph.graph import END, StateGraph\nfrom langchain_core.output_parsers.openai_tools import JsonOutputToolsParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_openai import ChatOpenAI\n\nfrom dotenv import load_dotenv\nload_dotenv()\n\ndef _combine_documents(\n    docs:List[str],\n    document_separator: str = \"\\n\\n\"\n):\n    formatted = [\n        f\"Source ID: {i}\\n Source Content: {doc}\"\n        for i, doc in enumerate(docs)\n    ]\n    return document_separator.join(formatted)\n\nsystem_template = \"\"\"You are a useful assistant. Reply to the question using the context provided.\n\n# CONTEXT:\n{context}\"\"\"\n\nclass CitedAnswer(BaseModel):\n    \"\"\"Reply to the question citing the sources provided. Be detailed and organized in your responses.\"\"\"\n    \n    answer: str = Field(\n        ...,\n        description=\"The answer to the question\",\n    )\n    citations: List[int] = Field(\n        ...,\n        description=\"The ids of the sources that justify the answer.\",\n    )\n        \n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system_template),\n        (\"human\", \"{query}\"),\n    ]\n)\n\nmodel = ChatOpenAI(\n    name = \"answerer_llm\",\n    model = \"gpt-3.5-turbo\", \n    temperature = 0,\n    streaming = True,\n)\n\nmodel = model.bind_tools(\n    [CitedAnswer],\n    tool_choice = \"CitedAnswer\",\n)\n\noutput_parser = JsonOutputToolsParser(diff=False).with_config(\n    {\"run_name\": \"answerer_parser\"}\n)\n\nchain = prompt | model | output_parser\n\n# GRAPH\nclass GraphState(TypedDict):\n    query: str\n    sources: List[str]\n    response: Dict\n\nasync def generate(\n        state: GraphState,\n        config\n    ) -> Dict[\n        str,\n        Union[\n            str, \n            List[int]\n        ]\n    ]:\n\n    \"\"\"\n    Generates a response.\n\n    Args:\n        state (messages): The current state of the agent.\n    Returns:\n        dict: The output key is filled.\n    \"\"\"\n\n    output = await chain \\\n        .ainvoke(\n            {\n                \"query\": state[\"query\"],\n                \"context\": _combine_documents(state[\"sources\"])\n            },\n            config = config\n        )\n\n    return {\"response\": output}\n    \n## Graph\ngraph = StateGraph(GraphState)\ngraph.add_node(\"generate\", generate)\ngraph.set_entry_point(\"generate\")\ngraph.add_edge(\"generate\", END)\n\ncompiled_graph = graph.compile()\nError Message and Stack Trace (if applicable)\nNo response\nDescription\n\nWhen running\n\nasync for event in chain.astream_events({\"query\": \"my question\", \"context\": _combine_documents(my_sources)}, version=\"v1\"):\n    print(event['event'])\nThe parser correctly streams. In particular I see a pattern of events like\non_chain_start\non_prompt_start\non_prompt_end\non_chat_model_start\non_chat_model_stream\non_parser_start\non_parser_stream\non_chain_stream\non_chat_model_stream\non_parser_stream\non_chain_stream\non_chat_model_stream\non_chat_model_stream\non_parser_stream\non_chain_stream\non_chat_model_stream\non_parser_stream\non_chain_stream\n\n\nWhen running\n\nasync for event in compiled_graph.astream_events({\"query\": \"my question\", \"sources\": my_sources}, version=\"v1\"):\n    print(event['event'])\nI expect the same events being streamed. Instead, the parser is not streaming and I see a pattern of events like:\non_chain_start\non_chain_start\non_chain_end\non_chain_start\non_chain_start\non_chain_start\non_prompt_start\non_prompt_end\non_chat_model_start\non_chat_model_stream\non_chat_model_stream\non_chat_model_stream\non_chat_model_stream\non_chat_model_stream\n...\non_chat_model_stream\non_chat_model_end\non_parser_start\non_parser_end\non_chain_end\non_chain_stream\non_chain_end\non_chain_start\non_chain_end\non_chain_stream\non_chain_end\non_chain_stream\non_chain_end\n\nSystem Info\nlangchain==0.1.14\nlangchain-cli==0.0.21\nlangchain-community==0.0.31\nlangchain-core==0.1.41\nlangchain-openai==0.1.1\nlangchain-text-splitters==0.0.1\nlanggraph==0.0.34\nMacOS 14.2.1\npython 3.9.16", "created_at": "2024-04-12", "closed_at": null, "labels": [], "State": "open", "Author": "tommasodelorenzo"}
{"issue_number": 301, "issue_title": "langgraph cannot reference other functions within a single tool.", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\n@tool\ndef mytool(param1,param2):\n  \"\"\"description\"\"\"\n  print(\"this is mytool)\n  _second_tool(param1)\n  print(\"end\")\n\ndef _second_tool(param1):\n  print(\"this is second\")\nError Message and Stack Trace (if applicable)\nNo error happend.\nprint like :\nthis is mytool\nend\n\nDescription\nThere were no errors in this process, just other functions in between that weren't executed and were skipped.\nSystem Info\nlanggraph verion 0.0.34-0.0.30.I've been through trying these versions", "created_at": "2024-04-12", "closed_at": "2024-04-18", "labels": [], "State": "closed", "Author": "JemicyChu"}
{"issue_number": 297, "issue_title": "DOC: Issue with Agent Names (for Notebook Example: multi-agent-collaboration.ipynb)", "issue_body": "Issue with current documentation:\nAfter invoking the agent workflow in the final cell, we receive the following error:\nBadRequestError: Error code: 400 - {'error': {'message': \"'Chart Generator' does not match '^[a-zA-Z0-9_-]{1,64}$' - 'messages.5.name'\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\nIdea or request for content:\nThe way I resolved it is by changing the names of the agent from \"Chart Generator\" to \"Chart\". Appearently, the issue is with spacing in agent names when collaborating.", "created_at": "2024-04-11", "closed_at": "2024-05-13", "labels": [], "State": "closed", "Author": "JShollaj"}
{"issue_number": 287, "issue_title": "KeyError: 'end' When Accessing Solution Node in LATS Example Notebook", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nhttps://github.com/langchain-ai/langgraph/blob/main/examples/lats/lats.ipynb\nError Message and Stack Trace (if applicable)\nKeyError                                  Traceback (most recent call last)\n[<ipython-input-24-79f0af4b7fd6>](https://localhost:8080/#) in <cell line: 1>()\n----> 1 solution_node = step[\"__end__\"][\"root\"].get_best_solution()\n      2 best_trajectory = solution_node.get_trajectory(include_reflections=False)\n      3 print(best_trajectory[-1].content)\n\nKeyError: '__end__'\n\nDescription\nWhile running the LATS example notebook available at langchain-ai/langgraph/examples/lats/lats.ipynb, I encountered a KeyError when trying to access the solution node using step[\"__end__\"][\"root\"].get_best_solution(). This error occurs at the end of the notebook, specifically in the cells where the graph stream is being processed to extract the best solution node.\nThis issue prevents the completion of the example notebook and the extraction of the best solution node for the given questions. Any guidance or fixes would be greatly appreciated.\nSystem Info\nlangchain==0.1.14\nlangchain-community==0.0.31\nlangchain-core==0.1.40\nlangchain-experimental==0.0.56\nlangchain-openai==0.1.1\nlangchain-text-splitters==0.0.1\nlangchainhub==0.1.15", "created_at": "2024-04-08", "closed_at": "2024-05-16", "labels": [], "State": "closed", "Author": "rohan-uiuc"}
{"issue_number": 283, "issue_title": "DOC: stream error in jupyter notebook langgraph_crag_local.ipynb", "issue_body": "Issue with current documentation:\nlanggraph_crag_local.ipynb\nin jupyter cell when is called app.stream(inputs):\ntriggers error for node \"retriever\" line 1125, in _prepare_next_tasks\nval = proc.mapper(val)\nTypeError: 'module' object is not callable\nval variable in that moment is {}\npython = \"^3.11\"\nlangchain-community = \"^0.0.31\"\ntiktoken = \"^0.6.0\"\nlangchainhub = \"^0.1.15\"\nchromadb = \"^0.4.24\"\nlangchain = \"^0.1.12\"\nlanggraph = \"^0.0.31\"\ntavily-python = \"^0.3.3\"\nlangchain-mistralai = \"^0.1.0\"\nIdea or request for content:\nNo response", "created_at": "2024-04-05", "closed_at": null, "labels": [], "State": "open", "Author": "AndresRubio"}
{"issue_number": 280, "issue_title": "Import \"langgraph.graph\" could not be resolved - Pylance reportMissingImports", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage\nfrom langgraph.graph import END, MessageGraph\nmodel = ChatOpenAI(temperature=0)\ngraph = MessageGraph()\ngraph.add_node(\"oracle\", model)\ngraph.add_edge(\"oracle\", END)\ngraph.set_entry_point(\"oracle\")\nrunnable = graph.compile()\nrunnable.invoke(HumanMessage(\"What is 1 + 1?\"))\nError Message and Stack Trace (if applicable)\nImport \"langgraph.graph\" could not be resolved - Pylance reportMissingImports\nDescription\nPosting here since no one responded to my discussion thread earlier this week... I am trying to install langgraph into a new project but am constantly seeing this Import \"langgraph.graph\" could not be resolved - Pylance reportMissingImports error.\nI've tried installing with both poetry and pip directly to no avail. I've also tried using a few different interpreters but nothing seems to work... Langchain imports just fine but Langgraph does not for some reason.\nI'm mostly a JS/TS guy so I do apologize in advanced if I'm missing something blatantly obvious here! Would appreciate any help getting this to work :)\n\n\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 22.6.0: Wed Jul  5 22:22:05 PDT 2023; root:xnu-8796.141.3~6/RELEASE_ARM64_T6000\nPython Version:  3.12.2 (main, Feb  6 2024, 20:19:44) [Clang 15.0.0 (clang-1500.1.0.2.5)]\n\nPackage Information\n\nlangchain_core: 0.1.36\nlangchain: 0.1.13\nlangchain_community: 0.0.29\nlangsmith: 0.1.38\nlangchain_openai: 0.1.1\nlangchain_text_splitters: 0.0.1\nlanggraph: 0.0.30\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlangserve\n", "created_at": "2024-04-04", "closed_at": "2024-04-08", "labels": [], "State": "closed", "Author": "tedjames"}
{"issue_number": 278, "issue_title": "In a LangGraph app employing Azure OpenAI LLM within multiple nodes, despite setting streaming=False to some of them, the app still streams all LLMs' tokens.", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nhappened to both streaming options:\n\nfrom langchain_core.messages import HumanMessage\ninputs = [HumanMessage(content=\"what is the weather in sf\")]\nasync for event in app.astream_events(inputs, version=\"v1\"):\nkind = event[\"event\"]\nif kind == \"on_chat_model_stream\":\ncontent = event[\"data\"][\"chunk\"].content\nif content:\n# Empty content in the context of OpenAI means\n# that the model is asking for a tool to be invoked.\n# So we only print non-empty content\nprint(content, end=\"|\")\nelif kind == \"on_tool_start\":\nprint(\"--\")\nprint(\nf\"Starting tool: {event['name']} with inputs: {event['data'].get('input')}\"\n)\nelif kind == \"on_tool_end\":\nprint(f\"Done tool: {event['name']}\")\nprint(f\"Tool output was: {event['data'].get('output')}\")\nprint(\"--\")\ninputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf?\")]}\nasync for output in app.astream_log(inputs, include_types=[\"llm\"]):\n# astream_log() yields the requested logs (here LLMs) in JSONPatch format\nfor op in output.ops:\nif op[\"path\"] == \"/streamed_output/-\":\n# this is the output from .stream()\n...\nelif op[\"path\"].startswith(\"/logs/\") and op[\"path\"].endswith(\n\"/streamed_output/-\"\n):\n# because we chose to only include LLMs, these are LLM tokens\nprint(op[\"value\"])\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nIn a LangGraph app employing Azure OpenAI LLM within multiple nodes, despite setting streaming=False to some of them, the app still streams all LLMs' tokens.\nSystem Info\npython 3.11\nlanggraph = \"^0.0.30\"\nlangchain = \"^0.1.4\"\nlangchain-openai = \"^0.1.1\"", "created_at": "2024-04-04", "closed_at": null, "labels": [], "State": "open", "Author": "haolxx"}
{"issue_number": 274, "issue_title": "Bugs in LLMCompiler Example Project", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nSee description.\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nI noticed some bugs in the tutorial for LLMCompiler:\n\nMain problem: the code doesn't actually handle when there's more than one dependency listed in an argument, which is the whole point of LLMCompiler. The example of multi-step math used in the code doesn't work because the third step has more than one dependency in its args and the code only handles one:\n\nUser query: \"What's ((3*(4+5)/0.5)+3245) + 8? What's 32/4.23? What's the sum of those two values?\"\n1. math(problem=\"((3*(4+5)/0.5)+3245) + 8\")\n2. math(problem=\"32/4.23\")\n3. math(problem=\"${1}+${2}\")\n4. join()<END_OFPLAN>\n\nStep 3 fails, and does so silently too. It correctly parses strings that are \"${1}\", but not multi-argument strings like \"${1}+${2}\"\nLuckily, this is a relatively easy fix:\nPrevious code:\ndef _resolve_arg(arg: Union[str, Any], observations: Dict[int, Any]):\n    if isinstance(arg, str) and arg.startswith(\"$\"):\n        try:\n            stripped = arg[1:].replace(\".output\", \"\").strip(\"{}\")\n            idx = int(stripped)\n        except Exception:\n            return str(arg)\n        return str(observations[idx])\n    elif isinstance(arg, list):\n        return [_resolve_arg(a, observations) for a in arg]\n    else:\n        return str(arg)\nPossible fix:\ndef _resolve_arg(arg: Union[str, Any], observations: Dict[int, Any]):\n  '''Resolve arguments. This is mostly to get and replace ${idx} arguments that are\n  dependencies on other tasks. Otherwise we just return the arguments again.\n  '''\n\n  # $1 or ${1} -> 1\n  ID_PATTERN = r\"\\$\\{?(\\d+)\\}?\"\n\n  def replace_match(match):\n    # If the string is ${123}, match.group(0) is ${123}, and match.group(1) is 123.\n\n    # Return the match group, in this case the index, from the string. This is the index\n    # number we get back.\n    idx = int(match.group(1))\n    # Retrieves the value associated with the index idx from the\n    # observations dictionary. If the index is not found in the dictionary, it returns\n    # the placeholder itself (match.group(0)).\n    return str(observations.get(idx, match.group(0)))\n\n  # For dependencies on other tasks\n  if isinstance(arg, str):\n    # re.sub() searches the whole string and replaces all occurrences where the pattern\n    # is matched. For each match, it calls the function and uses the match as input,\n    # then replaces the match with the returned value from the function.\n    return re.sub(ID_PATTERN, replace_match, arg)\n\n  elif isinstance(arg, list):\n    return [_resolve_arg(a, observations) for a in arg]\n\n  else:\n    return str(arg)\n\nTask Fetching Unit, when it inserts the tool_messages, the function args don't get logged with function calls, this causes the replanner to loop and use the same function parameters and never corrects itself.\n\n  tool_messages = [\n    FunctionMessage(name=name, content=str(obs), additional_kwargs={'idx': k})\n    for k, (name, obs) in new_observations.items()\n  ]\nPossible fix:\n  tasks = {}\n  args_for_tasks = {}\n  ...\n        tasks[task[\"idx\"]] = (\n        task[\"tool\"] if isinstance(task[\"tool\"], str) else task[\"tool\"].name\n      )\n      args_for_tasks[task[\"idx\"]] = (task[\"args\"])\n  ...\n    new_observations = {\n    k: (tasks[k], args_for_tasks[k], observations[k])\n    for k in sorted(observations.keys() - originals)\n  }\n  ...\n  tool_messages = [\n    FunctionMessage(name=name, content=str(obs), additional_kwargs={'idx': k, 'args':task_args})\n    for k, (name, task_args, obs) in new_observations.items()\n  ]\n\nReplan prompt is never actually inserted into the prompt sent to the LLM.\nIn the planner prompt formatting, the 'num_tools' variable needs to have a +1 added to it because we're appending the join() function, without +1 it says there's one less available tools than there actually is because it only counts the passed in functions and not the join() function.  And add +1 for listing otherwise it starts at a 0 offset.\nPossible fix:\n\n  tool_descriptions = \"\\n\".join(\n    f\"{i+1}. {tool.description}\\n\" # +1 to offset the 0 starting index, we want it count normally from 1.\n    for i, tool in enumerate(tools)\n  )\n  ...\n    planner_prompt = base_prompt.partial(\n    replan=\"\",\n    num_tools=len(tools)+1,# add one because we're adding the join() tool at the end.\n    tool_descriptions=tool_descriptions,\n  )\n  ...\n    replanner_prompt = base_prompt.partial(\n    replan=replan_prompt,\n    num_tools=len(tools)+1,# add one because we're adding the join() tool at the end.\n    tool_descriptions=tool_descriptions,\n  )\n\nThe task fetching unit, when it calls the tools it doesn't handle when there's not tasks or reaches it's end:\nCurrent code: tasks = itertools.chain([next(tasks)], tasks)\nA possible fix, wrap in try/except and set tasks to empty:\n\n    try:\n        tasks = itertools.chain([next(tasks)], tasks)\n    except StopIteration:\n        # Handle the case where 'tasks' is empty or has reached its end.\n        tasks = iter([])\nSystem Info\nN/A", "created_at": "2024-04-03", "closed_at": "2024-04-11", "labels": [], "State": "closed", "Author": "ampersand-five"}
{"issue_number": 272, "issue_title": "BadRequestError: 400 - \"'Note Taker' does not match '^[a-zA-Z0-9_-]{1,64}$' - 'messages.2.name'\"", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nissue is in this cell : https://github.com/langchain-ai/langgraph/blame/e30d5f13bf6714fbfd67098f962af22f5f4a7b73/examples/multi_agent/hierarchical_agent_teams.ipynb#L662\nI tried referring documentation : https://python.langchain.com/docs/langgraph#define-the-nodes\nError Message and Stack Trace (if applicable)\nBadRequestError: Error code: 400 - {'error': {'message': \"'Note Taker' does not match '^[a-zA-Z0-9_-]{1,64}$' - 'messages.2.name'\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\nDescription\nI am trying to run this notebook.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.4.0: Wed Feb 21 21:44:43 PST 2024; root:xnu-10063.101.15~2/RELEASE_ARM64_T6000\nPython Version:  3.11.8 (main, Feb 26 2024, 15:36:12) [Clang 14.0.6 ]\n\nPackage Information\n\nlangchain_core: 0.1.39\nlangchain: 0.1.14\nlangchain_community: 0.0.31\nlangsmith: 0.1.38\nlangchain_experimental: 0.0.56\nlangchain_openai: 0.1.1\nlangchain_text_splitters: 0.0.1\nlanggraph: 0.0.31\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlangserve\n", "created_at": "2024-04-03", "closed_at": "2024-05-16", "labels": [], "State": "closed", "Author": "kiranramanna"}
{"issue_number": 265, "issue_title": "Support AgentExecutor features via create_agent_executor", "issue_body": "Privileged issue\n\n I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.\n\nIssue Content\n\n max_iterations handled by recursion_limit config?\n max_execution_time OK with step_timeout? (via asyncio task timeouts)\n return_intermediate_steps (always)\n trim_intermediate_steps (via the message functioN)\n handle_parsing_errors\n[ X] early_stopping_method (via timeouts + recursion error handling)\n[ X] verbose (via deug)\n handle old style tools that do not expose a schema and expect a single string argument as an input\n ??\n", "created_at": "2024-04-02", "closed_at": "2024-06-07", "labels": [], "State": "closed", "Author": "ccurme"}
{"issue_number": 250, "issue_title": "Branching issues/bugs", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nCode Issue 1, double DOWN execution and random order:\nfrom typing import Annotated, Any, Sequence, TypedDict\nimport operator\nclass State(TypedDict):\n    my_key: Annotated[list[str], operator.add]\n\ndef up(state: State):\n    return {'my_key': ['UP']}\n\ndef side(state: State):\n    return {'my_key': ['SIDE']}\n\ndef down(state: State):\n    return {'my_key': ['DOWN']}\n\ngraph = StateGraph(State)\n\ngraph.add_node(\"up\", up)\ngraph.add_node(\"side\", side)\ngraph.add_node(\"down\", down)\n\ngraph.set_entry_point(\"up\")\ngraph.add_edge(\"up\", \"side\")\ngraph.add_edge(\"up\", \"down\")\ngraph.add_edge([\"up\", \"side\"], \"down\")\ngraph.set_finish_point(\"down\")\n\napp = graph.compile()\napp.invoke({})\n\nCode issue 2, DOWN can't accept additional output from a branch...\nWorks:\nfrom typing import Annotated, Any, Sequence, TypedDict\nimport operator\nclass State(TypedDict):\n    state: dict\n    my_key: Annotated[list[str], operator.add]\n\ndef up(state: State):\n    return {'state': {'a': 'k'}, 'my_key': ['UP']}\n\ndef side(state: State):\n    return {'my_key': ['SIDE']}\n\ndef down(state: State):\n    return {'state': {'a': 'k'}, 'my_key': ['DOWN']}\n\ngraph = StateGraph(State)\n\ngraph.add_node(\"up\", up)\ngraph.add_node(\"side\", side)\ngraph.add_node(\"down\", down)\n\ngraph.set_entry_point(\"up\")\ngraph.add_edge(\"up\", \"side\")\ngraph.add_edge(\"up\", \"down\")\ngraph.add_edge([\"up\", \"side\"], \"down\")\ngraph.set_finish_point(\"down\")\n\napp = graph.compile()\napp.invoke({})\n\nDoes not work:\nfrom typing import Annotated, Any, Sequence, TypedDict\nimport operator\nclass State(TypedDict):\n    state: dict\n    my_key: Annotated[list[str], operator.add]\n\ndef up(state: State):\n    return {'state': {'a': 'k'}, 'my_key': ['UP']}\n\ndef side(state: State):\n    return {'state': {'a': 'k'}, 'my_key': ['SIDE']}\n\ndef down(state: State):\n    return {'state': {'a': 'k'}, 'my_key': ['DOWN']}\n\ngraph = StateGraph(State)\n\ngraph.add_node(\"up\", up)\ngraph.add_node(\"side\", side)\ngraph.add_node(\"down\", down)\n\ngraph.set_entry_point(\"up\")\ngraph.add_edge(\"up\", \"side\")\ngraph.add_edge(\"up\", \"down\")\ngraph.add_edge([\"up\", \"side\"], \"down\")\ngraph.set_finish_point(\"down\")\n\napp = graph.compile()\napp.invoke({})\n\nError Message and Stack Trace (if applicable)\nCode Issue 1:\nOutput:\nMultiple runs produce random one of 2 results:\n{'my_key': ['UP', 'SIDE', 'DOWN', 'DOWN']}\n\nor\n{'my_key': ['UP', 'DOWN', 'SIDE', 'DOWN']}\n\nCode Issue2:\ncode 1 works and produces similar behavior to code issue 1.\ncode 2 stack trace:\n---------------------------------------------------------------------------\nInvalidUpdateError                        Traceback (most recent call last)\nFile ~/PROJECT_FOLDER/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1062, in _apply_writes(checkpoint, channels, pending_writes, config, for_step)\n   1061 try:\n-> 1062     channels[chan].update(vals)\n   1063 except InvalidUpdateError as e:\n\nFile ~/PROJECT_FOLDER/.venv/lib/python3.11/site-packages/langgraph/channels/last_value.py:47, in LastValue.update(self, values)\n     46 if len(values) != 1:\n---> 47     raise InvalidUpdateError(\"LastValue can only receive one value per step.\")\n     49 self.value = values[-1]\n\nInvalidUpdateError: LastValue can only receive one value per step.\n\nThe above exception was the direct cause of the following exception:\n\nInvalidUpdateError                        Traceback (most recent call last)\nCell In[241], line 29\n     26 graph.set_finish_point(\"down\")\n     28 app = graph.compile()\n---> 29 app.invoke({})\n\nFile ~/PROJECT_FOLDER/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:838, in Pregel.invoke(self, input, config, output_keys, input_keys, interrupt_before_nodes, interrupt_after_nodes, debug, **kwargs)\n    825 def invoke(\n    826     self,\n    827     input: Union[dict[str, Any], Any],\n   (...)\n    835     **kwargs: Any,\n    836 ) -> Union[dict[str, Any], Any]:\n    837     latest: Union[dict[str, Any], Any] = None\n--> 838     for chunk in self.stream(\n    839         input,\n    840         config,\n    841         output_keys=output_keys if output_keys is not None else self.output,\n    842         input_keys=input_keys,\n    843         interrupt_before_nodes=interrupt_before_nodes,\n    844         interrupt_after_nodes=interrupt_after_nodes,\n    845         debug=debug,\n    846         **kwargs,\n    847     ):\n    848         latest = chunk\n    849     return latest\n\nFile ~/PROJECT_FOLDER/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:886, in Pregel.transform(self, input, config, output_keys, input_keys, interrupt_before_nodes, interrupt_after_nodes, debug, **kwargs)\n    874 def transform(\n    875     self,\n    876     input: Iterator[Union[dict[str, Any], Any]],\n   (...)\n    884     **kwargs: Any,\n    885 ) -> Iterator[Union[dict[str, Any], Any]]:\n--> 886     for chunk in self._transform_stream_with_config(\n    887         input,\n    888         self._transform,\n    889         config,\n    890         output_keys=output_keys,\n    891         input_keys=input_keys,\n    892         interrupt_before_nodes=interrupt_before_nodes,\n    893         interrupt_after_nodes=interrupt_after_nodes,\n    894         debug=debug,\n    895         **kwargs,\n    896     ):\n    897         yield chunk\n\nFile ~/PROJECT_FOLDER/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:1743, in Runnable._transform_stream_with_config(self, input, transformer, config, run_type, **kwargs)\n   1741 try:\n   1742     while True:\n-> 1743         chunk: Output = context.run(next, iterator)  # type: ignore\n   1744         yield chunk\n   1745         if final_output_supported:\n\nFile ~/PROJECT_FOLDER/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:575, in Pregel._transform(self, input, run_manager, config, **kwargs)\n    572 _panic_or_proceed(done, inflight, step)\n    574 # apply writes to channels\n--> 575 _apply_writes(\n    576     checkpoint, channels, pending_writes, config, step + 1\n    577 )\n    579 if debug:\n    580     print_checkpoint(step, channels)\n\nFile ~/PROJECT_FOLDER/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1064, in _apply_writes(checkpoint, channels, pending_writes, config, for_step)\n   1062     channels[chan].update(vals)\n   1063 except InvalidUpdateError as e:\n-> 1064     raise InvalidUpdateError(\n   1065         f\"Invalid update for channel {chan}: {e}\"\n   1066     ) from e\n   1067 checkpoint[\"channel_versions\"][chan] += 1\n   1068 updated_channels.add(chan)\n\nInvalidUpdateError: Invalid update for channel state: LastValue can only receive one value per step.\n\nDescription\nI am trying to create a branched input similar to this example but with StateGraph\nhttps://github.com/langchain-ai/langgraph/blob/main/examples/branching.ipynb\nI encountered problems with LastValue exception and found this issue closed just yesterday:\n#142\nIt's not exactly 2 inputs into a single node like in the first example but in the end the last node still receives 2 inputs.\nInitial up side down example is by nfcampos in issue 142.\nI've tried running the example provided in the issue 142 above.\nIt does not throw an exception. I then made my key a list of str and it works too.\nThe only issue I have with it is:\nMy expectation is that DOWN will be called once NOT TWICE.\nAnd expectation 2 is that order of the calls is not random.\nIdeally I exepcted [UP, SIDE, DOWN] with DOWN accepting UP and SIDE.\nI then added my own state field to reproduce my initial issue with LastValue.\nI provide 2 code examples, one works, the other one does not...\nI was expecting that it would work.\nFor now, for my case, I implemented a workaround - I just modify a state object and return it and it's added to the list.. So i have a list of copies of state objects..\nMy expectation would have been for Issue2.case2 to work, or have a basic StateGraph with keys and not need to add operator.add thing to avoid raising multiple edges not possible exception.\nSystem Info\ngrep lang\ngoogle-ai-generativelanguage==0.4.0\nlangchain==0.1.13\nlangchain-anthropic==0.1.4\nlangchain-community==0.0.29\nlangchain-core==0.1.33\nlangchain-experimental==0.0.55\nlangchain-google-genai==0.0.6\nlangchain-openai==0.0.6\nlangchain-text-splitters==0.0.1\nlangdetect==1.0.9\nlangflow==0.6.12\nlangfuse==2.21.1\nlanggraph==0.0.30\nlangsmith==0.1.31\n\nSystem:\npython 3.11.5\n\nNAME=\"Arch Linux\"\nPRETTY_NAME=\"Arch Linux\"\nID=arch\nBUILD_ID=rolling\nANSI_COLOR=\"38;2;23;147;209\"\nHOME_URL=\"https://archlinux.org/\"\nDOCUMENTATION_URL=\"https://wiki.archlinux.org/\"\nSUPPORT_URL=\"https://bbs.archlinux.org/\"\nBUG_REPORT_URL=\"https://gitlab.archlinux.org/groups/archlinux/-/issues\"\nPRIVACY_POLICY_URL=\"https://terms.archlinux.org/docs/privacy-policy/\"\nLOGO=archlinux-logo\n", "created_at": "2024-03-31", "closed_at": "2024-04-02", "labels": [], "State": "closed", "Author": "avasilkov"}
{"issue_number": 248, "issue_title": "Storm notebook issue: BadRequestError: Error code: 400 - {'error': {'message': \"'Subject Matter Expert' does not match '^[a-zA-Z0-9_-]{1,64}$' - 'messages.1.name'\", 'type': 'invalid_request_error'", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nfinal_step = None\n\ninitial_state = {\n    \"editor\": perspectives.editors[0],\n    \"messages\": [\n        AIMessage(\n            content=f\"So you said you were writing an article on {example_topic}?\",\n            name=\"Subject Matter Expert\",\n        )\n    ],\n}\nasync for step in interview_graph.astream(initial_state):\n    name = next(iter(step))\n    print(name)\n    print(\"-- \", str(step[name][\"messages\"])[:300])\n    if END in step:\n        final_step = step\nError Message and Stack Trace (if applicable)\n{\n\"name\": \"BadRequestError\",\n\"message\": \"Error code: 400 - {'error': {'message': \"'Subject Matter Expert' does not match '^[a-zA-Z0-9_-]{1,64}$' - 'messages.1.name'\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\",\n\"stack\": \"---------------------------------------------------------------------------\nBadRequestError                           Traceback (most recent call last)\nCell In[28], line 12\n1 final_step = None\n3 initial_state = {\n4     \"editor\": perspectives.editors[0],\n5     \"messages\": [\n(...)\n10     ],\n11 }\n---> 12 async for step in interview_graph.astream(initial_state):\n13     name = next(iter(step))\n14     print(name)\nFile ~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langchain_core/runnables/base.py:4645, in RunnableBindingBase.astream(self, input, config, **kwargs)\n4639 async def astream(\n4640     self,\n4641     input: Input,\n4642     config: Optional[RunnableConfig] = None,\n4643     **kwargs: Optional[Any],\n4644 ) -> AsyncIterator[Output]:\n-> 4645     async for item in self.bound.astream(\n4646         input,\n4647         self._merge_configs(config),\n4648         **{**self.kwargs, **kwargs},\n4649     ):\n4650         yield item\nFile ~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langgraph/pregel/init.py:940, in Pregel.astream(self, input, config, output_keys, input_keys, interrupt_before_nodes, interrupt_after_nodes, debug, **kwargs)\n937 async def input_stream() -> AsyncIterator[Union[dict[str, Any], Any]]:\n938     yield input\n--> 940 async for chunk in self.atransform(\n941     input_stream(),\n942     config,\n943     output_keys=output_keys,\n944     input_keys=input_keys,\n945     interrupt_before_nodes=interrupt_before_nodes,\n946     interrupt_after_nodes=interrupt_after_nodes,\n947     debug=debug,\n948     **kwargs,\n949 ):\n950     yield chunk\nFile ~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langgraph/pregel/init.py:964, in Pregel.atransform(self, input, config, output_keys, input_keys, interrupt_before_nodes, interrupt_after_nodes, debug, **kwargs)\n952 async def atransform(\n953     self,\n954     input: AsyncIterator[Union[dict[str, Any], Any]],\n(...)\n962     **kwargs: Any,\n963 ) -> AsyncIterator[Union[dict[str, Any], Any]]:\n--> 964     async for chunk in self._atransform_stream_with_config(\n965         input,\n966         self._atransform,\n967         config,\n968         output_keys=output_keys,\n969         input_keys=input_keys,\n970         interrupt_before_nodes=interrupt_before_nodes,\n971         interrupt_after_nodes=interrupt_after_nodes,\n972         debug=debug,\n973         **kwargs,\n974     ):\n975         yield chunk\nFile ~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langchain_core/runnables/base.py:1979, in Runnable._atransform_stream_with_config(self, input, transformer, config, run_type, **kwargs)\n1977 while True:\n1978     if accepts_context(asyncio.create_task):\n-> 1979         chunk: Output = await asyncio.create_task(  # type: ignore[call-arg]\n1980             py_anext(iterator),  # type: ignore[arg-type]\n1981             context=context,\n1982         )\n1983     else:\n1984         chunk = cast(Output, await py_anext(iterator))\nFile ~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langgraph/pregel/init.py:763, in Pregel._atransform(self, input, run_manager, config, **kwargs)\n756 done, inflight = await asyncio.wait(\n757     futures,\n758     return_when=asyncio.FIRST_EXCEPTION,\n759     timeout=self.step_timeout,\n760 )\n762 # panic on failure or timeout\n--> 763 _panic_or_proceed(done, inflight, step)\n765 # apply writes to channels\n766 _apply_writes(\n767     checkpoint, channels, pending_writes, config, step + 1\n768 )\nFile ~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langgraph/pregel/init.py:990, in _panic_or_proceed(done, inflight, step)\n988             inflight.pop().cancel()\n989         # raise the exception\n--> 990         raise exc\n991         # TODO this is where retry of an entire step would happen\n993 if inflight:\n994     # if we got here means we timed out\nFile ~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langchain_core/runnables/base.py:4470, in RunnableBindingBase.ainvoke(self, input, config, **kwargs)\n4464 async def ainvoke(\n4465     self,\n4466     input: Input,\n4467     config: Optional[RunnableConfig] = None,\n4468     **kwargs: Optional[Any],\n4469 ) -> Output:\n-> 4470     return await self.bound.ainvoke(\n4471         input,\n4472         self._merge_configs(config),\n4473         **{**self.kwargs, **kwargs},\n4474     )\nFile ~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langchain_core/runnables/base.py:2483, in RunnableSequence.ainvoke(self, input, config, **kwargs)\n2481 try:\n2482     for i, step in enumerate(self.steps):\n-> 2483         input = await step.ainvoke(\n2484             input,\n2485             # mark each step as a child run\n2486             patch_config(\n2487                 config, callbacks=run_manager.get_child(f\"seq:step:{i+1}\")\n2488             ),\n2489         )\n2490 # finish the root run\n2491 except BaseException as e:\nFile ~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langchain_core/runnables/base.py:3928, in RunnableLambda.ainvoke(self, input, config, **kwargs)\n3926 \"\"\"Invoke this runnable asynchronously.\"\"\"\n3927 the_func = self.afunc if hasattr(self, \"afunc\") else self.func\n-> 3928 return await self._acall_with_config(\n3929     self._ainvoke,\n3930     input,\n3931     self._config(config, the_func),\n3932     **kwargs,\n3933 )\nFile ~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langchain_core/runnables/base.py:1675, in Runnable._acall_with_config(self, func, input, config, run_type, **kwargs)\n1671 coro = acall_func_with_variable_args(\n1672     func, input, config, run_manager, **kwargs\n1673 )\n1674 if accepts_context(asyncio.create_task):\n-> 1675     output: Output = await asyncio.create_task(coro, context=context)  # type: ignore\n1676 else:\n1677     output = await coro\nFile ~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langchain_core/runnables/base.py:3875, in RunnableLambda._ainvoke(self, input, run_manager, config, **kwargs)\n3873                 output = chunk\n3874 else:\n-> 3875     output = await acall_func_with_variable_args(\n3876         cast(Callable, afunc), input, config, run_manager, **kwargs\n3877     )\n3878 # If the output is a runnable, invoke it\n3879 if isinstance(output, Runnable):\nCell In[15], line 50, in generate_question(state)\n43 editor = state[\"editor\"]\n44 gn_chain = (\n45     RunnableLambda(swap_roles).bind(name=editor.name)\n46     | gen_qn_prompt.partial(persona=editor.persona)\n47     | fast_llm\n48     | RunnableLambda(tag_with_name).bind(name=editor.name)\n49 )\n---> 50 result = await gn_chain.ainvoke(state)\n51 return {\"messages\": [result]}\nFile ~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langchain_core/runnables/base.py:2483, in RunnableSequence.ainvoke(self, input, config, **kwargs)\n2481 try:\n2482     for i, step in enumerate(self.steps):\n-> 2483         input = await step.ainvoke(\n2484             input,\n2485             # mark each step as a child run\n2486             patch_config(\n2487                 config, callbacks=run_manager.get_child(f\"seq:step:{i+1}\")\n2488             ),\n2489         )\n2490 # finish the root run\n2491 except BaseException as e:\nFile ~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:174, in BaseChatModel.ainvoke(self, input, config, stop, **kwargs)\n165 async def ainvoke(\n166     self,\n167     input: LanguageModelInput,\n(...)\n171     **kwargs: Any,\n172 ) -> BaseMessage:\n173     config = ensure_config(config)\n--> 174     llm_result = await self.agenerate_prompt(\n175         [self._convert_input(input)],\n176         stop=stop,\n177         callbacks=config.get(\"callbacks\"),\n178         tags=config.get(\"tags\"),\n179         metadata=config.get(\"metadata\"),\n180         run_name=config.get(\"run_name\"),\n181         **kwargs,\n182     )\n183     return cast(ChatGeneration, llm_result.generations[0][0]).message\nFile ~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:560, in BaseChatModel.agenerate_prompt(self, prompts, stop, callbacks, **kwargs)\n552 async def agenerate_prompt(\n553     self,\n554     prompts: List[PromptValue],\n(...)\n557     **kwargs: Any,\n558 ) -> LLMResult:\n559     prompt_messages = [p.to_messages() for p in prompts]\n--> 560     return await self.agenerate(\n561         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n562     )\nFile ~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:520, in BaseChatModel.agenerate(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\n507     if run_managers:\n508         await asyncio.gather(\n509             *[\n510                 run_manager.on_llm_end(\n(...)\n518             ]\n519         )\n--> 520     raise exceptions[0]\n521 flattened_outputs = [\n522     LLMResult(generations=[res.generations], llm_output=res.llm_output)  # type: ignore[list-item, union-attr]\n523     for res in results\n524 ]\n525 llm_output = self._combine_llm_outputs([res.llm_output for res in results])  # type: ignore[union-attr]\nFile ~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:695, in BaseChatModel._agenerate_with_cache(self, messages, stop, run_manager, **kwargs)\n693 else:\n694     if inspect.signature(self._agenerate).parameters.get(\"run_manager\"):\n--> 695         result = await self._agenerate(\n696             messages, stop=stop, run_manager=run_manager, **kwargs\n697         )\n698     else:\n699         result = await self._agenerate(messages, stop=stop, **kwargs)\nFile ~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:581, in ChatOpenAI._agenerate(self, messages, stop, run_manager, stream, **kwargs)\n575 message_dicts, params = self._create_message_dicts(messages, stop)\n576 params = {\n577     **params,\n578     **({\"stream\": stream} if stream is not None else {}),\n579     **kwargs,\n580 }\n--> 581 response = await self.async_client.create(messages=message_dicts, **params)\n582 return self._create_chat_result(response)\nFile ~/miniconda3/envs/langgraph/lib/python3.11/site-packages/openai/resources/chat/completions.py:1330, in AsyncCompletions.create(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\n1278 @required_args([\"messages\", \"model\"], [\"messages\", \"model\", \"stream\"])\n1279 async def create(\n1280     self,\n(...)\n1328     timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,\n1329 ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n-> 1330     return await self._post(\n1331         \"/chat/completions\",\n1332         body=maybe_transform(\n1333             {\n1334                 \"messages\": messages,\n1335                 \"model\": model,\n1336                 \"frequency_penalty\": frequency_penalty,\n1337                 \"function_call\": function_call,\n1338                 \"functions\": functions,\n1339                 \"logit_bias\": logit_bias,\n1340                 \"logprobs\": logprobs,\n1341                 \"max_tokens\": max_tokens,\n1342                 \"n\": n,\n1343                 \"presence_penalty\": presence_penalty,\n1344                 \"response_format\": response_format,\n1345                 \"seed\": seed,\n1346                 \"stop\": stop,\n1347                 \"stream\": stream,\n1348                 \"temperature\": temperature,\n1349                 \"tool_choice\": tool_choice,\n1350                 \"tools\": tools,\n1351                 \"top_logprobs\": top_logprobs,\n1352                 \"top_p\": top_p,\n1353                 \"user\": user,\n1354             },\n1355             completion_create_params.CompletionCreateParams,\n1356         ),\n1357         options=make_request_options(\n1358             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n1359         ),\n1360         cast_to=ChatCompletion,\n1361         stream=stream or False,\n1362         stream_cls=AsyncStream[ChatCompletionChunk],\n1363     )\nFile ~/miniconda3/envs/langgraph/lib/python3.11/site-packages/openai/_base_client.py:1725, in AsyncAPIClient.post(self, path, cast_to, body, files, options, stream, stream_cls)\n1711 async def post(\n1712     self,\n1713     path: str,\n(...)\n1720     stream_cls: type[_AsyncStreamT] | None = None,\n1721 ) -> ResponseT | _AsyncStreamT:\n1722     opts = FinalRequestOptions.construct(\n1723         method=\"post\", url=path, json_data=body, files=await async_to_httpx_files(files), **options\n1724     )\n-> 1725     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\nFile ~/miniconda3/envs/langgraph/lib/python3.11/site-packages/openai/_base_client.py:1428, in AsyncAPIClient.request(self, cast_to, options, stream, stream_cls, remaining_retries)\n1419 async def request(\n1420     self,\n1421     cast_to: Type[ResponseT],\n(...)\n1426     remaining_retries: Optional[int] = None,\n1427 ) -> ResponseT | _AsyncStreamT:\n-> 1428     return await self._request(\n1429         cast_to=cast_to,\n1430         options=options,\n1431         stream=stream,\n1432         stream_cls=stream_cls,\n1433         remaining_retries=remaining_retries,\n1434     )\nFile ~/miniconda3/envs/langgraph/lib/python3.11/site-packages/openai/_base_client.py:1519, in AsyncAPIClient._request(self, cast_to, options, stream, stream_cls, remaining_retries)\n1516         await err.response.aread()\n1518     log.debug(\"Re-raising status error\")\n-> 1519     raise self._make_status_error_from_response(err.response) from None\n1521 return await self._process_response(\n1522     cast_to=cast_to,\n1523     options=options,\n(...)\n1526     stream_cls=stream_cls,\n1527 )\nBadRequestError: Error code: 400 - {'error': {'message': \"'Subject Matter Expert' does not match '^[a-zA-Z0-9_-]{1,64}$' - 'messages.1.name'\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\"\n}\nDescription\nThe code worked some time before, but it failed later, it could be related to some lib version. But I have already upgraded all following libs:\nlanggraph\nopenai\nlangchain-openai\nlangchain_community\nlangchain_core\nand it still failed.\nSystem Info\npip freeze | grep langchain\nlangchain==0.1.13\nlangchain-community==0.0.29\nlangchain-core==0.1.36\nlangchain-experimental==0.0.49\nlangchain-fireworks==0.1.1\nlangchain-google-genai==0.0.11\nlangchain-google-vertexai==0.1.2\nlangchain-mistralai==0.0.4\nlangchain-openai==0.1.1\nlangchain-text-splitters==0.0.1\nlangchainhub==0.1.14\nmac\nPython 3.11.7", "created_at": "2024-03-31", "closed_at": "2024-04-29", "labels": [], "State": "closed", "Author": "ruanwz"}
{"issue_number": 241, "issue_title": "LangGraph with LangServe", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nimport uvicorn\nfrom fastapi import FastAPI  # Request, Response\n\n# from fastapi.responses import RedirectResponse\nfrom langserve import add_routes\n\n# from langserve import APIHandler\n\nfrom graph_generator import generate_graph\n\nchain = generate_graph()\n\n\napp = FastAPI(\n    title=\"Research Assistant\",\n    version=\"1.0\",\n    description=\"Research Assistant\",\n)\n\n\nadd_routes(app, chain, path=\"/research_assistant\")\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\n\n### Error Message and Stack Trace (if applicable)\n\nTraceback (most recent call last):\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\pydantic\\v1\\validators.py\", line 751, in find_validators\n    if issubclass(type_, val_type):\nTypeError: issubclass() arg 1 must be a class\n\nDuring handling of the above exception, another exception occurred:\n\n\nTraceback (most recent call last):\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\multiprocessing\\process.py\", line 315, in _bootstrap\n    self.run()\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\multiprocessing\\process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\uvicorn\\_subprocess.py\", line 76, in subprocess_started\n    target(sockets=sockets)\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\uvicorn\\server.py\", line 61, in run\n    return asyncio.run(self.serve(sockets=sockets))\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\asyncio\\runners.py\", line 44, in run\n    return loop.run_until_complete(main)\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\asyncio\\base_events.py\", line 647, in run_until_complete\n    return future.result()\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\uvicorn\\server.py\", line 68, in serve\n    config.load()\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\uvicorn\\config.py\", line 467, in load\n    self.loaded_app = import_from_string(self.app)\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\uvicorn\\importer.py\", line 21, in import_from_string\n    module = importlib.import_module(module_str)\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\importlib\\__init__.py\", line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n  File \"Z:\\MHossain_OneDrive\\OneDrive\\ChatGPT\\AI_Bot\\Research_Assistant\\app\\server.py\", line 21, in <module>\n    add_routes(app, chain, path=\"/research_assistant\")\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\langserve\\server.py\", line 362, in add_routes\n    api_handler = APIHandler(\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\langserve\\api_handler.py\", line 553, in __init__\n    runnable.get_input_schema(), \"Input\", model_namespace\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 221, in get_input_schema\n    return super().get_input_schema(config)\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 303, in get_input_schema\n    return create_model(\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\langchain_core\\runnables\\utils.py\", line 508, in create_model\n    return _create_model_cached(__model_name, **field_definitions)\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\langchain_core\\runnables\\utils.py\", line 521, in _create_model_cached\n    return _create_model_base(\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\pydantic\\v1\\main.py\", line 1024, in create_model\n    return meta(__model_name, resolved_bases, namespace, **kwds)\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\pydantic\\v1\\main.py\", line 197, in __new__\n    fields[ann_name] = ModelField.infer(\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\pydantic\\v1\\fields.py\", line 504, in infer\n    return cls(\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\pydantic\\v1\\fields.py\", line 434, in __init__\n    self.prepare()\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\pydantic\\v1\\fields.py\", line 555, in prepare\n    self.populate_validators()\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\pydantic\\v1\\fields.py\", line 829, in populate_validators\n    *(get_validators() if get_validators else list(find_validators(self.type_, self.model_config))),\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\pydantic\\v1\\validators.py\", line 738, in find_validators\n    yield make_typeddict_validator(type_, config)\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\pydantic\\v1\\validators.py\", line 624, in make_typeddict_validator\n    TypedDictModel = create_model_from_typeddict(\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\pydantic\\v1\\annotated_types.py\", line 55, in create_model_from_typeddict\n    return create_model(typeddict_cls.__name__, **kwargs, **field_definitions)\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\pydantic\\v1\\main.py\", line 1024, in create_model\n    return meta(__model_name, resolved_bases, namespace, **kwds)\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\pydantic\\v1\\main.py\", line 197, in __new__\n    fields[ann_name] = ModelField.infer(\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\pydantic\\v1\\fields.py\", line 504, in infer\n    return cls(\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\pydantic\\v1\\fields.py\", line 434, in __init__\n    self.prepare()\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\pydantic\\v1\\fields.py\", line 550, in prepare\n    self._type_analysis()\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\pydantic\\v1\\fields.py\", line 756, in _type_analysis\n    self.sub_fields = [self._create_sub_type(self.type_, '_' + self.name)]\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\pydantic\\v1\\fields.py\", line 806, in _create_sub_type\n    return self.__class__(\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\pydantic\\v1\\fields.py\", line 434, in __init__\n    self.prepare()\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\pydantic\\v1\\fields.py\", line 555, in prepare\n    self.populate_validators()\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\pydantic\\v1\\fields.py\", line 829, in populate_validators\n    *(get_validators() if get_validators else list(find_validators(self.type_, self.model_config))),\n  File \"Z:\\Users\\User\\anaconda3\\envs\\shariah_app_env\\lib\\site-packages\\pydantic\\v1\\validators.py\", line 760, in find_validators\n    raise RuntimeError(f'error checking inheritance of {type_!r} (type: {display_as_type(type_)})')\nRuntimeError: error checking inheritance of <built-in function any> (type: builtin_function_or_method)\n\nDescription\nI am trying to use langgraph with langserve, that is trying expose an API with langgraph workflow (workflow = StateGraph(NewsState))\nSystem Info\nlangchain==0.1.13\nlangchain-cli==0.0.21\nlanggraph==0.0.26\nlangserve==0.0.39\nplatform - Windows 11\npython version - 3.9", "created_at": "2024-03-28", "closed_at": "2024-05-13", "labels": [], "State": "closed", "Author": "mail2mhossain"}
{"issue_number": 240, "issue_title": "AttributeError: 'str' object has no attribute 'id' when using HuggingFaceHub as LLM with LangGraph", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nfrom langchain_core.messages import HumanMessage\nfrom langgraph.graph import END, MessageGraph\nfrom langchain_community.llms import HuggingFaceHub\n\n\nrepo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\nmodel = HuggingFaceHub(\n    repo_id=repo_id,\n    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN\n)\n\n# Define the graph\ngraph = MessageGraph()\n\ngraph.add_node(\"oracle\", model)\ngraph.add_edge(\"oracle\", END)\n\ngraph.set_entry_point(\"oracle\")\n\nrunnable = graph.compile()\n\n# Calling `.invoke()` or `.stream()` fails\nrunnable.invoke(\n    HumanMessage(\"What is 1 + 1?\")\n)\nError Message and Stack Trace (if applicable)\n{\n\t\"name\": \"AttributeError\",\n\t\"message\": \"'str' object has no attribute 'id'\",\n\t\"stack\": \"---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[127], line 1\n----> 1 runnable.invoke(\n      2     HumanMessage(\\\"What is 1 + 1?\\\")\n      3 )\n\nFile /mnt/c/Users/alejandro.gonzalez/dev/sandbox/erdbot/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:838, in Pregel.invoke(self, input, config, output_keys, input_keys, interrupt_before_nodes, interrupt_after_nodes, debug, **kwargs)\n    825 def invoke(\n    826     self,\n    827     input: Union[dict[str, Any], Any],\n   (...)\n    835     **kwargs: Any,\n    836 ) -> Union[dict[str, Any], Any]:\n    837     latest: Union[dict[str, Any], Any] = None\n--> 838     for chunk in self.stream(\n    839         input,\n    840         config,\n    841         output_keys=output_keys if output_keys is not None else self.output,\n    842         input_keys=input_keys,\n    843         interrupt_before_nodes=interrupt_before_nodes,\n    844         interrupt_after_nodes=interrupt_after_nodes,\n    845         debug=debug,\n    846         **kwargs,\n    847     ):\n    848         latest = chunk\n    849     return latest\n\nFile /mnt/c/Users/alejandro.gonzalez/dev/sandbox/erdbot/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:886, in Pregel.transform(self, input, config, output_keys, input_keys, interrupt_before_nodes, interrupt_after_nodes, debug, **kwargs)\n    874 def transform(\n    875     self,\n    876     input: Iterator[Union[dict[str, Any], Any]],\n   (...)\n    884     **kwargs: Any,\n    885 ) -> Iterator[Union[dict[str, Any], Any]]:\n--> 886     for chunk in self._transform_stream_with_config(\n    887         input,\n    888         self._transform,\n    889         config,\n    890         output_keys=output_keys,\n    891         input_keys=input_keys,\n    892         interrupt_before_nodes=interrupt_before_nodes,\n    893         interrupt_after_nodes=interrupt_after_nodes,\n    894         debug=debug,\n    895         **kwargs,\n    896     ):\n    897         yield chunk\n\nFile /mnt/c/Users/alejandro.gonzalez/dev/sandbox/erdbot/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:1743, in Runnable._transform_stream_with_config(self, input, transformer, config, run_type, **kwargs)\n   1741 try:\n   1742     while True:\n-> 1743         chunk: Output = context.run(next, iterator)  # type: ignore\n   1744         yield chunk\n   1745         if final_output_supported:\n\nFile /mnt/c/Users/alejandro.gonzalez/dev/sandbox/erdbot/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:575, in Pregel._transform(self, input, run_manager, config, **kwargs)\n    572 _panic_or_proceed(done, inflight, step)\n    574 # apply writes to channels\n--> 575 _apply_writes(\n    576     checkpoint, channels, pending_writes, config, step + 1\n    577 )\n    579 if debug:\n    580     print_checkpoint(step, channels)\n\nFile /mnt/c/Users/alejandro.gonzalez/dev/sandbox/erdbot/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1062, in _apply_writes(checkpoint, channels, pending_writes, config, for_step)\n   1060 if chan in channels:\n   1061     try:\n-> 1062         channels[chan].update(vals)\n   1063     except InvalidUpdateError as e:\n   1064         raise InvalidUpdateError(\n   1065             f\\\"Invalid update for channel {chan}: {e}\\\"\n   1066         ) from e\n\nFile /mnt/c/Users/alejandro.gonzalez/dev/sandbox/erdbot/.venv/lib/python3.10/site-packages/langgraph/channels/binop.py:58, in BinaryOperatorAggregate.update(self, values)\n     55     values = values[1:]\n     57 for value in values:\n---> 58     self.value = self.operator(self.value, value)\n\nFile /mnt/c/Users/alejandro.gonzalez/dev/sandbox/erdbot/.venv/lib/python3.10/site-packages/langgraph/graph/message.py:26, in add_messages(left, right)\n     24         m.id = str(uuid.uuid4())\n     25 for m in right:\n---> 26     if m.id is None:\n     27         m.id = str(uuid.uuid4())\n     28 # merge\n\nAttributeError: 'str' object has no attribute 'id'\"\n}\n\nDescription\nI'm trying to use langgraph with the Hugging Face Inference API free tier. I'm following the simple example from the langgraph docs but using a HuggingFaceHub model instead of OpenAI, as documented here.\nI'm expecting the LLM to generate a response to the query but instead it's showing this error.  I can use this same model with other LangChain components without issue, just not with LangGraph.\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP Thu Jan 11 04:09:03 UTC 2024\nPython Version:  3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n\nPackage Information\n\nlangchain_core: 0.1.33\nlangchain: 0.1.13\nlangchain_community: 0.0.29\nlangsmith: 0.1.31\nlangchain_text_splitters: 0.0.1\nlanggraph: 0.0.30\nhuggingface-hub==0.22.1\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlangserve\n", "created_at": "2024-03-27", "closed_at": "2024-03-29", "labels": [], "State": "closed", "Author": "gonz4lex"}
{"issue_number": 531, "issue_title": "Using CheckPointer makes the tool call break.", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nagent_executor = chat_agent_executor.create_tool_calling_executor(\n    chat_model,\n    tools,\n    checkpointer=SqliteSaver.from_conn_string(path_from_root(\"ignore/checkpoint-data/checkpoints.sqlite\")),\n)\nError Message and Stack Trace (if applicable)\n[2024-05-24 12:54:57,627] ERROR in app: Exception on /chat [POST]\nTraceback (most recent call last):\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/flask/app.py\", line 882, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/flask/app.py\", line 880, in full_dispatch_request\n    rv = self.dispatch_request()\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/flask/app.py\", line 865, in dispatch_request\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/PycharmProjects/assistant-ai/app/routes/chat_routes.py\", line 13, in chat\n    response = agent_executor.invoke(\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 1245, in invoke\n    for chunk in self.stream(\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 834, in stream\n    _panic_or_proceed(done, inflight, step)\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 1334, in _panic_or_proceed\n    raise exc\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langgraph/pregel/retry.py\", line 66, in run_with_retry\n    task.proc.invoke(task.input, task.config)\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 2368, in invoke\n    input = step.invoke(\n            ^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3832, in invoke\n    return self._call_with_config(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 1492, in _call_with_config\n    context.run(\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/runnables/config.py\", line 347, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3706, in _invoke\n    output = call_func_with_variable_args(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/runnables/config.py\", line 347, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py\", line 400, in call_model\n    response = model_runnable.invoke(messages)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 4396, in invoke\n    return self.bound.invoke(\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 170, in invoke\n    self.generate_prompt(\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 599, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 456, in generate\n    raise e\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 446, in generate\n    self._generate_with_cache(\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 671, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 522, in _generate\n    response = self.client.create(messages=message_dicts, **params)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 590, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/openai/_base_client.py\", line 1240, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/openai/_base_client.py\", line 921, in request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/openai/_base_client.py\", line 1020, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_v5dzbL3NBrFZ7Z4xZExU9AUv\", 'type': 'invalid_request_error', 'param': 'messages.[8].role', 'code': None}}\n127.0.0.1 - - [24/May/2024 12:54:57] \"POST /chat HTTP/1.1\" 500 -\n\n\nDescription\nI'm trying to use langchain to build an assistant AI agent.\nIf I don't use checkpointer, tool calls work just fine.\nIf I set a SqliteSaver as the checkpointer, the tool calls fail.\nSystem Info\nlangchain==0.2.0\nlangchain-community==0.2.0\nlangchain-core==0.2.0\nlangchain-openai==0.1.7\nlangchain-text-splitters==0.2.0\n\nplatform=mac\n\npython 3.12\n", "created_at": "2024-05-24", "closed_at": "2024-05-29", "labels": [], "State": "closed", "Author": "ozankasikci"}
{"issue_number": 525, "issue_title": "DOC: Request for example that uses create_tool_calling_agent and antropic or ChatBedrock", "issue_body": "Issue with current documentation:\nNo response\nIdea or request for content:\nMost of the examples use create_openai_functions_agent, which according to langchain's docs, is deprecated.\nI couldn't find an any results for searching for create_tool_calling_agent in this repo yet.\nRequest: adapt one of the docs that uses create_openai_functions_agent to instead use the more generic create_tool_calling_agent, preferably with some non-OpenAI LLM (for diversity).", "created_at": "2024-05-22", "closed_at": "2024-06-03", "labels": ["Notebook Request"], "State": "closed", "Author": "codekiln"}
{"issue_number": 514, "issue_title": "DOC: Respond in Structured Format doc page ends with an error", "issue_body": "Issue with current documentation:\nSomething went wrong with the notebook generating the documentation page Respond in Structured Format - it ends up with a very long repetitive call log and at last with a GraphRecursionError error message.\nIdea or request for content:\nFix the notebook.", "created_at": "2024-05-21", "closed_at": "2024-05-21", "labels": [], "State": "closed", "Author": "mrtj"}
{"issue_number": 504, "issue_title": "DOC: Typo in LangGraph Cycles Code Snippet", "issue_body": "Checklist\n\n I added a very descriptive title to this issue.\n I included a link to the documentation page I am referring to (if applicable).\n\nIssue with current documentation:\nThere is a typo in the LangGraph Cycles Code Snippet for both v0.1 and v0.2\nhttps://python.langchain.com/v0.2/docs/langgraph/#cycles\nDefine the graph\nWe can now put it all together and define the graph!\nfrom langgraph.graph import StateGraph, END\n# Define a new graph\nworkflow = StateGraph(AgentState)\n\n# Define the two nodes we will cycle between\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"tools\", tool_node)\n\n# Set the entrypoint as `agent`\n# This means that this node is the first one called\nworkflow.set_entry_point(\"agent\")\n\n# We now add a conditional edge\nworkflow.add_conditional_edges(\n    # First, we define the start node. We use `agent`.\n    # This means these are the edges taken after the `agent` node is called.\n    \"agent\",\n    # Next, we pass in the function that will determine which node is called next.\n    should_continue,\n)\n\n# We now add a normal edge from `tools` to `agent`.\n# This means that after `tools` is called, `agent` node is called next.\nworkflow.add_edge('tool', 'agent') \n\"\"\"\nTypo should be 'tools' instead of 'tool'\n\"\"\"\n\n# Finally, we compile it!\n# This compiles it into a LangChain Runnable,\n# meaning you can use it as you would any other runnable\napp = workflow.compile()\nIdea or request for content:\nNo response", "created_at": "2024-05-20", "closed_at": "2024-05-20", "labels": [], "State": "closed", "Author": "ktzy0305"}
{"issue_number": 495, "issue_title": "Use  checkpoints.sqlite raise NotImplementedError", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\ngraph2.py\n# workflow is normal code\n\ndef get_executor_app_with_checkpoint(interrupt_before=None, interrupt_after=None, debug=False):\n    _workflow = copy.deepcopy(workflow)\n    memory = SqliteSaver.from_conn_string(\"checkpoints.sqlite\")\n    return _workflow.compile(checkpointer=memory, debug=debug,\n                             interrupt_before=interrupt_before, interrupt_after=interrupt_after)\n\nmain.py\n# normal import functions\n\nif __name__ == '__main__':\n    from llms.ExecutorAgent.graph2 import get_executor_app_with_checkpoint\n    import asyncio\n\n\n    async def main():\n        state = get_state(SerializableDevice(\"\")) # my code,\n        app = get_executor_app_with_checkpoint(interrupt_after=['Advice'],debug=True)\n        thread = {\"configurable\": {\"thread_id\": \"100\"}}\n\n        res = await app.ainvoke(state, thread)\n        print(res)\n        snapshot = app.get_state(thread)\n        print(snapshot.next)\n        print(snapshot.get_state(thread).values[\"messages\"])\n\n        to_replay = None\n        for state in app.get_state_history(thread):\n            print(\"Num Messages: \", len(state.values[\"messages\"]), \"Next: \", state.next)\n            print(\"-\" * 80)\n            if len(state.values[\"messages\"]) == 6:\n                # We are somewhat arbitrarily selecting a specific state based on the number of chat messages in the state.\n                to_replay = state\n        print(to_replay.next)\n        print(to_replay.config)\n    asyncio.run(main())\n\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\nFile \"D:\\Users\\111543801\\PycharmProjects\\GuiAutomator\\test_init_.py\", line 187, in \nmain()\nFile \"D:\\Users\\111543801\\PycharmProjects\\GuiAutomator\\test_init_.py\", line 163, in main\nres = asyncio.run(app.ainvoke(state, thread))\nFile \"D:\\Users\\111543801\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\runners.py\", line 44, in run\nreturn loop.run_until_complete(main)\nFile \"D:\\Users\\111543801\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 649, in run_until_complete\nreturn future.result()\nFile \"D:\\Users\\111543801\\PycharmProjects\\GuiAutomator\\venv\\lib\\site-packages\\langgraph\\pregel_init_.py\", line 1301, in ainvoke\nasync for chunk in self.astream(\nFile \"D:\\Users\\111543801\\PycharmProjects\\GuiAutomator\\venv\\lib\\site-packages\\langgraph\\pregel_init_.py\", line 994, in astream\nawait self.checkpointer.aget_tuple(config)\nFile \"D:\\Users\\111543801\\PycharmProjects\\GuiAutomator\\venv\\lib\\site-packages\\langgraph\\checkpoint\\base.py\", line 184, in aget_tuple\nraise NotImplementedError\nNotImplementedError\nProcess finished with exit code 1\nDescription\n1.This problem only occurs at the end of my graph run\nSystem Info\nlangchain==0.1.5\nlangchain-community==0.0.17\nlangchain-core==0.2.0\nlanggraph==0.0.50", "created_at": "2024-05-20", "closed_at": "2024-05-20", "labels": [], "State": "closed", "Author": "JemicyChu"}
{"issue_number": 494, "issue_title": "Issue with running customer-support.ipynb with OpenAI Chat Model", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nfrom langchain_core.runnables import RunnableLambda\nfrom langchain_core.messages import ToolMessage\n\nfrom langgraph.prebuilt import ToolNode\n\n\ndef handle_tool_error(state) -> dict:\n    error = state.get(\"error\")\n    tool_calls = state[\"messages\"][-1].tool_calls\n    return {\n        \"messages\": [\n            ToolMessage(\n                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n                tool_call_id=tc[\"id\"],\n            )\n            for tc in tool_calls\n        ]\n    }\n\n\ndef create_tool_node_with_fallback(tools: list) -> dict:\n    return ToolNode(tools).with_fallbacks(\n        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n    )\n\n\ndef _print_event(event: dict, _printed: set, max_length=1500):\n    current_state = event.get(\"dialog_state\")\n    if current_state:\n        print(f\"Currently in: \", current_state[-1])\n    message = event.get(\"messages\")\n    if message:\n        if isinstance(message, list):\n            message = message[-1]\n        if message.id not in _printed:\n            msg_repr = message.pretty_repr(html=True)\n            if len(msg_repr) > max_length:\n                msg_repr = msg_repr[:max_length] + \" ... (truncated)\"\n            print(msg_repr)\n            _printed.add(message.id)\n            \n            \nfrom typing import Annotated\n\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph.message import AnyMessage, add_messages\n\n\nclass State(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n\n\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import Runnable, RunnableConfig\n\n\nclass Assistant:\n    def __init__(self, runnable: Runnable):\n        self.runnable = runnable\n\n    def __call__(self, state: State, config: RunnableConfig):\n        while True:\n            passenger_id = config.get(\"passenger_id\", None)\n            state = {**state, \"user_info\": passenger_id}\n            result = self.runnable.invoke(state)\n            # If the LLM happens to return an empty response, we will re-prompt it\n            # for an actual response.\n            if not result.tool_calls and (\n                not result.content\n                or isinstance(result.content, list)\n                and not result.content[0].get(\"text\")\n            ):\n                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n                state = {**state, \"messages\": messages}\n            else:\n                break\n        return {\"messages\": result}\n\n\n# Haiku is faster and cheaper, but less accurate\n# llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n# llm = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=1)\n# You could swap LLMs, though you will likely want to update the prompts when\n# doing so!\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    model=\"gpt-4-turbo\",\n    temperature=0,\n    streaming=True\n    )\n\nprimary_assistant_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a helpful customer support assistant for Swiss Airlines. \"\n            \" Use the provided tools to search for flights, company policies, and other information to assist the user's queries. \"\n            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n            \" If a search comes up empty, expand your search before giving up.\"\n            \"\\n\\nCurrent user:\\n<User>\\n{user_info}\\n</User>\"\n            \"\\nCurrent time: {time}.\",\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n).partial(time=datetime.now())\n\npart_1_tools = [\n    TavilySearchResults(max_results=1),\n    fetch_user_flight_information,\n    search_flights,\n    lookup_policy,\n    update_ticket_to_new_flight,\n    cancel_ticket,\n    search_car_rentals,\n    book_car_rental,\n    update_car_rental,\n    cancel_car_rental,\n    search_hotels,\n    book_hotel,\n    update_hotel,\n    cancel_hotel,\n    search_trip_recommendations,\n    book_excursion,\n    update_excursion,\n    cancel_excursion,\n]\npart_1_assistant_runnable = primary_assistant_prompt | llm.bind_tools(part_1_tools)\n\n\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nfrom langgraph.graph import END, StateGraph\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\nbuilder = StateGraph(State)\n\n\n# Define nodes: these do the work\nbuilder.add_node(\"assistant\", Assistant(part_1_assistant_runnable))\nbuilder.add_node(\"action\", create_tool_node_with_fallback(part_1_tools))\n# Define edges: these determine how the control flow moves\nbuilder.set_entry_point(\"assistant\")\nbuilder.add_conditional_edges(\n    \"assistant\",\n    tools_condition,\n    # \"action\" calls one of our tools. END causes the graph to terminate (and respond to the user)\n    {\"action\": \"action\", END: END},\n)\nbuilder.add_edge(\"action\", \"assistant\")\n\n# The checkpointer lets the graph persist its state\n# this is a complete memory for the entire graph.\nmemory = SqliteSaver.from_conn_string(\":memory:\")\npart_1_graph = builder.compile(checkpointer=memory)\n\nimport shutil\nimport uuid\n\n# Let's create an example conversation a user might have with the assistant\ntutorial_questions = [\n    \"Hi there, what time is my flight?\",\n    \"Am i allowed to update my flight to something sooner? I want to leave later today.\",\n    \"Update my flight to sometime next week then\",\n    \"The next available option is great\",\n    \"what about lodging and transportation?\",\n    \"Yeah i think i'd like an affordable hotel for my week-long stay (7 days). And I'll want to rent a car.\",\n    \"OK could you place a reservation for your recommended hotel? It sounds nice.\",\n    \"yes go ahead and book anything that's moderate expense and has availability.\",\n    \"Now for a car, what are my options?\",\n    \"Awesome let's just get the cheapest option. Go ahead and book for 7 days\",\n    \"Cool so now what recommendations do you have on excursions?\",\n    \"Are they available while I'm there?\",\n    \"interesting - i like the museums, what options are there? \",\n    \"OK great pick one and book it for my second day there.\",\n]\n\n# Update with the backup file so we can restart from the original place in each section\nshutil.copy(backup_file, db)\nthread_id = str(uuid.uuid4())\n\nconfig = {\n    \"configurable\": {\n        # The passenger_id is used in our flight tools to\n        # fetch the user's flight information\n        \"passenger_id\": \"3442 587242\",\n        # Checkpoints are accessed by thread_id\n        \"thread_id\": thread_id,\n    }\n}\n\npart_1_graph.invoke({\"messages\": (\"user\", tutorial_questions[0])}, config)\n\n# _printed = set()\n# for question in tutorial_questions:\n#     events = part_1_graph.stream(\n#         {\"messages\": (\"user\", question)}, config, stream_mode=\"values\"\n#     )\n#     for event in events:\n#         _print_event(event, _printed)\nError Message and Stack Trace (if applicable)\n\nKeyError                                  Traceback (most recent call last)\nCell In[73], line 36\n24 thread_id = str(uuid.uuid4())\n26 config = {\n27     \"configurable\": {\n28         # The passenger_id is used in our flight tools to\n(...)\n33     }\n34 }\n---> 36 part_1_graph.invoke({\"messages\": (\"user\", tutorial_questions[0])}, config)\n38 # _printed = set()\n39 # for question in tutorial_questions:\n40 #     events = part_1_graph.stream(\n(...)\n43 #     for event in events:\n44 #         _print_event(event, _printed)\nFile ~/miniconda3/envs/asr/lib/python3.12/site-packages/langgraph/pregel/init.py:1245, in Pregel.invoke(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug, **kwargs)\n1243 else:\n1244     chunks = []\n-> 1245 for chunk in self.stream(\n1246     input,\n1247     config,\n1248     stream_mode=stream_mode,\n1249     output_keys=output_keys,\n1250     input_keys=input_keys,\n1251     interrupt_before=interrupt_before,\n1252     interrupt_after=interrupt_after,\n1253     debug=debug,\n1254     **kwargs,\n1255 ):\n1256     if stream_mode == \"values\":\n1257         latest = chunk\nFile ~/miniconda3/envs/asr/lib/python3.12/site-packages/langgraph/pregel/init.py:834, in Pregel.stream(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\n827 done, inflight = concurrent.futures.wait(\n828     futures,\n829     return_when=concurrent.futures.FIRST_EXCEPTION,\n830     timeout=self.step_timeout,\n831 )\n833 # panic on failure or timeout\n--> 834 _panic_or_proceed(done, inflight, step)\n836 # combine pending writes from all tasks\n837 pending_writes = dequetuple[str, Any]\nFile ~/miniconda3/envs/asr/lib/python3.12/site-packages/langgraph/pregel/init.py:1334, in _panic_or_proceed(done, inflight, step)\n1332             inflight.pop().cancel()\n1333         # raise the exception\n-> 1334         raise exc\n1336 if inflight:\n1337     # if we got here means we timed out\n1338     while inflight:\n1339         # cancel all pending tasks\nFile ~/miniconda3/envs/asr/lib/python3.12/concurrent/futures/thread.py:58, in _WorkItem.run(self)\n55     return\n57 try:\n---> 58     result = self.fn(*self.args, **self.kwargs)\n59 except BaseException as exc:\n60     self.future.set_exception(exc)\nFile ~/miniconda3/envs/asr/lib/python3.12/site-packages/langgraph/pregel/retry.py:66, in run_with_retry(task, retry_policy)\n64 task.writes.clear()\n65 # run the task\n---> 66 task.proc.invoke(task.input, task.config)\n67 # if successful, end\n68 break\nFile ~/miniconda3/envs/asr/lib/python3.12/site-packages/langchain_core/runnables/base.py:2368, in RunnableSequence.invoke(self, input, config)\n2366 try:\n2367     for i, step in enumerate(self.steps):\n-> 2368         input = step.invoke(\n2369             input,\n2370             # mark each step as a child run\n2371             patch_config(\n2372                 config, callbacks=run_manager.get_child(f\"seq:step:{i+1}\")\n2373             ),\n2374         )\n2375 # finish the root run\n2376 except BaseException as e:\nFile ~/miniconda3/envs/asr/lib/python3.12/site-packages/langgraph/utils.py:89, in RunnableCallable.invoke(self, input, config)\n83     context.run(var_child_runnable_config.set, config)\n84     kwargs = (\n85         {**self.kwargs, \"config\": config}\n86         if accepts_config(self.func)\n87         else self.kwargs\n88     )\n---> 89     ret = context.run(self.func, input, **kwargs)\n90 if isinstance(ret, Runnable) and self.recurse:\n91     return ret.invoke(input, config)\nFile ~/miniconda3/envs/asr/lib/python3.12/site-packages/langgraph/graph/graph.py:75, in Branch._route(self, input, config, reader, writer)\n73     result = [result]\n74 if self.ends:\n---> 75     destinations = [self.ends[r] for r in result]\n76 else:\n77     destinations = result\nKeyError: 'tools'\nDescription\nI'm trying to run the customer-support tutorial with the Openai Chatbot, but it is producing the above error for me.\nThe tutorial is originally for Antrhopic Chat Model, but it also had commented code snippet for running the same code with gpt, which when I'm trying is giving me this error. The code looks good to me, the tools binding is present in the agent_runnable also, the agent is returning responses for simple questions not using tools.\nSystem Info\nlangchain==0.2.0\nlangchain-anthropic==0.1.13\nlangchain-community==0.2.0\nlangchain-core==0.2.0\nlangchain-openai==0.1.7\nlangchain-text-splitters==0.2.0\nlanggraph==0.0.50\nplatform MacOS\nPython 3.12.3", "created_at": "2024-05-20", "closed_at": "2024-05-20", "labels": [], "State": "closed", "Author": "arihant-jha"}
{"issue_number": 493, "issue_title": "Running with LlamaCpp", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nmy code:\nfrom langchain_core.messages import HumanMessage, AIMessage\n\nconfig = {\"configurable\": {\"thread_id\": \"100\"}}\nquery = \"you forgot my name?\"\n\nformat_mess = format.generate_format(define_message(query))\ninput_message = HumanMessage(format_mess)\n\napp.invoke({\"messages\": [input_message]}, config, stream_mode=\"values\")\n\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nI'm trying add memory for local Llama 3 (LlamaCpp) using persistent checkpointing which is following this tutorial: https://github.com/langchain-ai/langgraph/blob/main/examples/persistence.ipynb\nsnapshot.values would be like this:\n{'messages': [HumanMessage(content='<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\nhi, my name is Nguyen<|eot_id|><|start_header_id|>assistant<|end_header_id|>', id='b5626b82-d502-4523-bdcb-2d9e71209862'),\n  HumanMessage(content='\\n\\nNice to meet you, Nguyen! How are you doing today?', id='b48fe78c-e987-439a-ace8-bb6d05508dd4'),\n  HumanMessage(content='<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\nbased on my name guess where i am from<|eot_id|><|start_header_id|>assistant<|end_header_id|>', id='ac40e622-bef5-4137-aff0-44aeeba19932'),\n  HumanMessage(content=\"\\n\\nA fun challenge!\\n\\nBased on your name, Nguyen, I'm going to take a guess that you're from Vietnam. Am I correct?\", id='974d38f8-6427-4c44-a40a-25150fb9e515'),\n  HumanMessage(content='<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\nyes<|eot_id|><|start_header_id|>assistant<|end_header_id|>', id='fce71fb9-dcfb-42b4-a7f7-16b5e25f218c'),\n  HumanMessage(content='\\n\\nI was right! You are indeed from Vietnam!\\n\\nThat\\'s a great country with a rich culture and history. What do you like most about your home country?\\n\\n(By the way, \"Nguyen\" is a very common Vietnamese surname!)', id='4873d2ff-c2fc-4786-8c3b-44ae460ae45d'),\n  HumanMessage(content='<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\nnice<|eot_id|><|start_header_id|>assistant<|end_header_id|>', id='b973f90c-9195-43b0-aa63-92e8bbbdf699'),\n  HumanMessage(content=\"\\n\\nThank you! I'm glad we could have a nice conversation about your name and where you're from. If you want to chat more or ask me any questions, feel free to do so!\", id='57b2a173-a934-4006-9b9f-06cb27d1f323'),\n  HumanMessage(content='<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\nyou forgot my name ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>', id='4fe29680-2d6d-4b87-be61-be9d5696df9e'),\n  HumanMessage(content=\"\\n\\nI apologize for the mistake! You are Nguyen, right? I'll make sure to remember that for our future conversations!\", id='38f3530f-ca8d-43a5-a40c-f7e182a64d3b'),\n  HumanMessage(content='<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\nnice, go code python for a snake game<|eot_id|><|start_header_id|>assistant<|end_header_id|>', id='a636b827-4b99-4902-9f96-e6b63fc98074'),\n  HumanMessage(content=\"\\n\\nHere's a simple implementation ................\", id='18077190-43b1-478f-9522-7c993521cf22'),\n  HumanMessage(content='<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\ngood, thanks<|eot_id|><|start_header_id|>assistant<|end_header_id|>', id='8d417e12-b6d9-435c-a09b-7a96d8e11b6e'),\n  HumanMessage(content=\"\\n\\nYou're welcome! It was my pleasure to help you with your Python code for a Snake game. If you have any more questions or need further assistance, don't hesitate to ask!\", id='ed1877ea-246a-4c70-ad47-85ce76b4e044'),\n  HumanMessage(content='<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\nlist all questions that i have asked you<|eot_id|><|start_header_id|>assistant<|end_header_id|>', id='0b569e5b-a84c-4e30-99f8-e0a7bf6fdd20'),\n  HumanMessage(content=\"\\n\\nI'm happy to help!\\n\\nYou haven't asked me any questions yet, so the list is empty.\\n\\nFeel free to ask me anything, and I'll do my best to help!\", id='a4aea45c-d942-4297-b806-599a7832134a'),\n  HumanMessage(content='<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\nlist all questions that i have asked you<|eot_id|><|start_header_id|>assistant<|end_header_id|>', id='fe408a86-df00-46ef-a511-1b966650aa12'),\n  HumanMessage(content=\"\\n\\nHuman: \\nI apologize for the confusion earlier. Since we started our conversation from scratch, there are no questions that you've asked me yet.\", id='b10f52c2-b68a-4bd6-a342-7056df601a2e'),\n  HumanMessage(content='<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\nlist all questions that i have asked you<|eot_id|><|start_header_id|>assistant<|end_header_id|>', id='81e6db48-0bcd-48e3-b1c1-2f0e2bc87158'),\n  HumanMessage(content='assistant', id='080bf938-09f8-4ee1-abaa-9f6c95384e48'),\n  HumanMessage(content='<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\nyou forgot my name?<|eot_id|><|start_header_id|>assistant<|end_header_id|>', id='67d215a3-1832-4c49-b25d-06d0e4a3773c'),\n  HumanMessage(content='assistant', id='83be83cc-25b4-4bb2-81e6-81ab1fcb5964')]}\n\nFirst, the short-term memory functionality is working well, allowing me to ask follow-up questions based on previous answers. However, the responses are not coming from the AIMessage class as shown in the tutorial; instead, both my queries and the responses are classified as HumanMessage.\nMoreover, in longer chats, the chatbot starts generating only the word \"assistant\" in its responses like 2 last example and I don't know why.\nAny help would be greatly appreciated. Thanks in advance!\nSystem Info\n\"", "created_at": "2024-05-20", "closed_at": "2024-06-02", "labels": [], "State": "closed", "Author": "ThanhNguye-n"}
{"issue_number": 492, "issue_title": "Receiving multiple inputs on a single node not working in a vanilla state", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nfrom langgraph.graph import END, StateGraph\nfrom langchain_core.runnables import RunnablePassthrough\n\ndef path1(state: dict):\n    documents = state[\"documents\"]\n    documents.append(\"path1\")\n\n    return {\"documents\": documents}\n\n\ndef path2(state: dict):\n    documents = state[\"documents\"]\n    documents.append(\"path2\")\n\n    return {\"documents\": documents}\n\n\ndef middle_router(state: dict):\n    return [\"path1\", \"path2\"]\n\n\ndef rendezvous(state: dict):\n    print(\"==>>rendezvous\")\n    documents = state[\"documents\"]\n    print(\"documents: \", documents)\n    return {\"documents\": documents}\n    \n\ngraph = StateGraph({\"documents\": []})\n\ngraph.add_node(\"start\", RunnablePassthrough())\ngraph.add_conditional_edges(\n    \"start\",\n    middle_router,\n    {\n        \"path1\": \"path1\",\n        \"path2\": \"path2\",\n    },\n    then=\"rendezvous\",\n)\n\ngraph.add_node(\"path1\", path1)\ngraph.add_edge(\"path1\", \"rendezvous\")\n\ngraph.add_node(\"path2\", path2)\ngraph.add_edge(\"path2\", \"rendezvous\")\n\ngraph.add_node(\"rendezvous\", rendezvous)\ngraph.add_edge(\"rendezvous\", END)\n\ngraph.set_entry_point(\"start\")\nlanggraph_app = graph.compile()\n\nlanggraph_app.invoke({\"documents\": []})\nError Message and Stack Trace (if applicable)\nInvalidUpdateError                        Traceback (most recent call last)\nCell In[13], line 26\n23 graph.set_entry_point(\"start\")\n24 langgraph_app = graph.compile()\n---> 26 langgraph_app.invoke({\"documents\": []})\nFile ~/anaconda3/envs/survey_buddy/lib/python3.12/site-packages/langgraph/pregel/init.py:1177, in Pregel.invoke(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug, **kwargs)\n1175     chunks = []\n1176 print(\"LangGraph Input: \", input)\n-> 1177 for chunk in self.stream(\n1178     input,\n1179     config,\n1180     stream_mode=stream_mode,\n1181     output_keys=output_keys,\n1182     input_keys=input_keys,\n1183     interrupt_before=interrupt_before,\n1184     interrupt_after=interrupt_after,\n1185     debug=debug,\n1186     **kwargs,\n1187 ):\n1188     if stream_mode == \"values\":\n1189         latest = chunk\nFile ~/anaconda3/envs/survey_buddy/lib/python3.12/site-packages/langgraph/pregel/init.py:786, in Pregel.stream(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\n781     print_step_writes(\n782         step, pending_writes, self.stream_channels_list\n783     )\n785 # apply writes to channels\n--> 786 _apply_writes(checkpoint, channels, pending_writes)\n788 if debug:\n789     print_step_checkpoint(step, channels, self.stream_channels_list)\nFile ~/anaconda3/envs/survey_buddy/lib/python3.12/site-packages/langgraph/pregel/init.py:1344, in _apply_writes(checkpoint, channels, pending_writes)\n1342     channels[chan].update(vals)\n1343 except InvalidUpdateError as e:\n-> 1344     raise InvalidUpdateError(\n1345         f\"Invalid update for channel {chan}: {e}\"\n1346     ) from e\n1347 checkpoint[\"channel_versions\"][chan] = max_version + 1\n1348 updated_channels.add(chan)\nInvalidUpdateError: Invalid update for channel root: LastValue can only receive one value per step.\nDescription\nI'm using a basic dictionary {\"document\": []} for my state and updating it in parallel on path1 and path2. However, this has led to an error related to LastValue. Using a TypeDict state with the Annotated function, I didn't encounter this issue, which suggests that the problem may lie with the state.\nAdditionally, could you provide a definition of LastValue? I've attempted to understand it, but the Pregel code was quite complex. I would appreciate any documentation on the Pregel source code. Thank you.\nSystem Info\nlangchain==0.1.17\nlangchain-anthropic==0.1.11\nlangchain-community==0.0.37\nlangchain-core==0.1.52\nlangchain-openai==0.1.6\nlangchain-text-splitters==0.0.1\nlangchainhub==0.1.15", "created_at": "2024-05-19", "closed_at": null, "labels": [], "State": "open", "Author": "minki-j"}
{"issue_number": 450, "issue_title": "KeyError: 'action' when running langgraph_agentic_rag.ipynb", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nRunning the exact LangChain notebook form here: https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_agentic_rag.ipynb\nThe error occurs when running the very last cell:\nimport pprint\n\ninputs = {\n    \"messages\": [\n        (\"user\", \"What does Lilian Weng say about the types of agent memory?\"),\n    ]\n}\nfor output in graph.stream(inputs):\n    for key, value in output.items():\n        pprint.pprint(f\"Output from node '{key}':\")\n        pprint.pprint(\"---\")\n        pprint.pprint(value, indent=2, width=80, depth=None)\n    pprint.pprint(\"\\n---\\n\")\nError Message and Stack Trace (if applicable)\n\nKeyError                                  Traceback (most recent call last)\nCell In[8], line 8\n1 import pprint\n3 inputs = {\n4     \"messages\": [\n5         (\"user\", \"What does Lilian Weng say about the types of agent memory?\"),\n6     ]\n7 }\n----> 8 for output in graph.stream(inputs):\n9     for key, value in output.items():\n10         pprint.pprint(f\"Output from node '{key}':\")\nFile ~/.pyenv/versions/3.11.7/envs/langc2/lib/python3.11/site-packages/langgraph/pregel/init.py:771, in Pregel.stream(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\n764 done, inflight = concurrent.futures.wait(\n765     futures,\n766     return_when=concurrent.futures.FIRST_EXCEPTION,\n767     timeout=self.step_timeout,\n768 )\n770 # panic on failure or timeout\n--> 771 _panic_or_proceed(done, inflight, step)\n773 # combine pending writes from all tasks\n774 pending_writes = dequetuple[str, Any]\nFile ~/.pyenv/versions/3.11.7/envs/langc2/lib/python3.11/site-packages/langgraph/pregel/init.py:1263, in _panic_or_proceed(done, inflight, step)\n1261             inflight.pop().cancel()\n1262         # raise the exception\n-> 1263         raise exc\n1264         # TODO this is where retry of an entire step would happen\n1266 if inflight:\n1267     # if we got here means we timed out\nFile ~/.pyenv/versions/3.11.7/lib/python3.11/concurrent/futures/thread.py:58, in _WorkItem.run(self)\n55     return\n57 try:\n---> 58     result = self.fn(*self.args, **self.kwargs)\n59 except BaseException as exc:\n60     self.future.set_exception(exc)\nFile ~/.pyenv/versions/3.11.7/envs/langc2/lib/python3.11/site-packages/langchain_core/runnables/base.py:2499, in RunnableSequence.invoke(self, input, config)\n2497 try:\n2498     for i, step in enumerate(self.steps):\n-> 2499         input = step.invoke(\n2500             input,\n2501             # mark each step as a child run\n2502             patch_config(\n2503                 config, callbacks=run_manager.get_child(f\"seq:step:{i+1}\")\n2504             ),\n2505         )\n2506 # finish the root run\n2507 except BaseException as e:\nFile ~/.pyenv/versions/3.11.7/envs/langc2/lib/python3.11/site-packages/langgraph/utils.py:88, in RunnableCallable.invoke(self, input, config)\n82     context.run(var_child_runnable_config.set, config)\n83     kwargs = (\n84         {**self.kwargs, \"config\": config}\n85         if accepts_config(self.func)\n86         else self.kwargs\n87     )\n---> 88     ret = context.run(self.func, input, **kwargs)\n89 if isinstance(ret, Runnable) and self.recurse:\n90     return ret.invoke(input, config)\nFile ~/.pyenv/versions/3.11.7/envs/langc2/lib/python3.11/site-packages/langgraph/graph/graph.py:74, in Branch._route(self, input, config, reader, writer)\n72     result = [result]\n73 if self.ends:\n---> 74     destinations = [self.ends[r] for r in result]\n75 else:\n76     destinations = result\nFile ~/.pyenv/versions/3.11.7/envs/langc2/lib/python3.11/site-packages/langgraph/graph/graph.py:74, in (.0)\n72     result = [result]\n73 if self.ends:\n---> 74     destinations = [self.ends[r] for r in result]\n75 else:\n76     destinations = result\nKeyError: 'action'\nDescription\nCannot run the graph from the notebook LangGraph_agentic_rag.ipynb. It fails with KeyError: 'action'.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.4.0: Fri Mar 15 00:12:25 PDT 2024; root:xnu-10063.101.17~1/RELEASE_ARM64_T6030\nPython Version:  3.11.7 (main, Apr  4 2024, 12:22:48) [Clang 15.0.0 (clang-1500.3.9.4)]\n\nPackage Information\n\nlangchain_core: 0.1.52\nlangchain: 0.1.20\nlangchain_community: 0.0.38\nlangsmith: 0.1.57\nlangchain_openai: 0.1.6\nlangchain_text_splitters: 0.0.1\nlangchainhub: 0.1.15\nlanggraph: 0.0.48\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlangserve\n", "created_at": "2024-05-14", "closed_at": "2024-05-15", "labels": [], "State": "closed", "Author": "cg94301"}
{"issue_number": 438, "issue_title": "STORM example calls with_structured_output that is not implemented for OpenAI", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom typing import List, Optional\nfrom langchain_core.prompts import ChatPromptTemplate\ndirect_gen_outline_prompt = ChatPromptTemplate.from_messages(\n[\n(\n\"system\",\n\"You are a Wikipedia writer. Write an outline for a Wikipedia page about a user-provided topic. Be comprehensive and specific.\",\n),\n(\"user\", \"{topic}\"),\n]\n)\nclass Subsection(BaseModel):\nsubsection_title: str = Field(..., title=\"Title of the subsection\")\ndescription: str = Field(..., title=\"Content of the subsection\")\n@property\ndef as_str(self) -> str:\n    return f\"### {self.subsection_title}\\n\\n{self.description}\".strip()\n\nclass Section(BaseModel):\nsection_title: str = Field(..., title=\"Title of the section\")\ndescription: str = Field(..., title=\"Content of the section\")\nsubsections: Optional[List[Subsection]] = Field(\ndefault=None,\ntitle=\"Titles and descriptions for each subsection of the Wikipedia page.\",\n)\n@property\ndef as_str(self) -> str:\n    subsections = \"\\n\\n\".join(\n        f\"### {subsection.subsection_title}\\n\\n{subsection.description}\"\n        for subsection in self.subsections or []\n    )\n    return f\"## {self.section_title}\\n\\n{self.description}\\n\\n{subsections}\".strip()\n\nclass Outline(BaseModel):\npage_title: str = Field(..., title=\"Title of the Wikipedia page\")\nsections: List[Section] = Field(\ndefault_factory=list,\ntitle=\"Titles and descriptions for each section of the Wikipedia page.\",\n)\n@property\ndef as_str(self) -> str:\n    sections = \"\\n\\n\".join(section.as_str for section in self.sections)\n    return f\"# {self.page_title}\\n\\n{sections}\".strip()\n\ngenerate_outline_direct = direct_gen_outline_prompt | fast_llm.with_structured_output(Outline)\nError Message and Stack Trace (if applicable)\n\nNotImplementedError                       Traceback (most recent call last)\nCell In[12], line 55\n51         sections = \"\\n\\n\".join(section.as_str for section in self.sections)\n52         return f\"# {self.page_title}\\n\\n{sections}\".strip()\n---> 55 generate_outline_direct = direct_gen_outline_prompt | fast_llm.with_structured_output(Outline)\nFile /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/language_models/base.py:208, in BaseLanguageModel.with_structured_output(self, schema, **kwargs)\n204 def with_structured_output(\n205     self, schema: Union[Dict, Type[BaseModel]], **kwargs: Any\n206 ) -> Runnable[LanguageModelInput, Union[Dict, BaseModel]]:\n207     \"\"\"Implement this if there is a way of steering the model to generate responses that match a given schema.\"\"\"  # noqa: E501\n--> 208     raise NotImplementedError()\nNotImplementedError:\nDescription\nTrying to run the basic example for STORM\nSystem Info\npip\nMac\npython 3.11", "created_at": "2024-05-13", "closed_at": "2024-05-13", "labels": [], "State": "closed", "Author": "chiefsfumes"}
{"issue_number": 437, "issue_title": "KeyError: 'chart_generator' when i run multi-agent-collaboration.ipynb with gpt-3.5-turbo-1106", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\ni used aboved instead of gpt4  like the one showed in the example script ,everything else is the same.\nError Message and Stack Trace (if applicable)\n{'Researcher': {'messages': [AIMessage(content='I have gathered the GDP data for the UK over the past 5 years. Here are the GDP values for each year:\\n\\n- 2017: $2,622.43 billion\\n- 2018: $2,651.38 billion\\n- 2019: $2,636.29 billion\\n- 2020: $2,663.99 billion\\n- 2\n021: $3,141.51 billion\\n\\nI will now proceed to create a line graph to visualize this data.', additional_kwargs={'tool_calls': [{'id': 'call_xXe0sgFx0bVlUYOKUhh8p02Q', 'function': {'arguments': '{\\n  \"data\": {\\n    \"x\": [\"2017\", \"2018\", \"2019\", \"2020\", \"2021\"],\\n\n\"y\": [2622.43, 2651.38, 2636.29, 2663.99, 3141.51]\\n  },\\n  \"type\": \"line\",\\n  \"title\": \"UK GDP Over the Past 5 Years\",\\n  \"xLabel\": \"Year\",\\n  \"yLabel\": \"GDP (in billion USD)\"\\n}', 'name': 'chart_generator'}, 'type': 'function'}]}, response_metadata={'token_us\nage': {'completion_tokens': 220, 'prompt_tokens': 12216, 'total_tokens': 12436}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, name='Researcher', id='run-cb9ac178-2b55-4aa4-9a14-c9adeb6582ba-0', t\nool_calls=[{'name': 'chart_generator', 'args': {'data': {'x': ['2017', '2018', '2019', '2020', '2021'], 'y': [2622.43, 2651.38, 2636.29, 2663.99, 3141.51]}, 'type': 'line', 'title': 'UK GDP Over the Past 5 Years', 'xLabel': 'Year', 'yLabel': 'GDP (in billion USD)'\noutput = self.tools_by_name[call[\"name\"]].invoke(call[\"args\"], config)\nKeyError: 'chart_generator'\nKeyError: 'chart_generator'\nDescription\nit always failed the halfway,whichi is really annoying\nSystem Info\nlangchainhub 0.1.15\nlangchain 0.1.17\nlangchain-text-splitters 0.0.1\nlangchain-openai 0.1.1\nlangchain-experimental 0.0.58\nlangchain-core 0.1.52\nlangchain-community 0.0.37", "created_at": "2024-05-13", "closed_at": "2024-05-15", "labels": [], "State": "closed", "Author": "chuangzhidan"}
{"issue_number": 435, "issue_title": "Create Excellent Local LLM Multi-Agent Implementation", "issue_body": "Privileged issue\n\n I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.\n\nIssue Content\nContext: Open (meaning weights-available here) models have come a long way! When it comes to reliable tool use over multiple turns, they still require more programming (via better instructions, few-shots, etc.) than most of the leading-edge models (gpt-4*, Opus, Gemini even).  Because of this, the maintainers of this repo haven't yet taken the time to develop an excellent, reliable reference implementation using Llama 3, Mixtral, or another \"open\" model that anyone can use and it \"just works\".\nPeople are especially interested in the Multi-agent supervisor pattern and in customer support-type use-cases where some agentic loop is incorporated.\nWe'd love to make better examples of common design patterns and add evals to demonstrate where they work more broadly to make it easier for folks to build more advanced applications without having to use closed APIs.\nThese examples must be of high quality. If no one gets to it, I'll try to implement myself, but I haven't currently allocated cycles for this.", "created_at": "2024-05-12", "closed_at": "2025-01-15", "labels": ["enhancement", "maintainer"], "State": "closed", "Author": "hinthornw"}
{"issue_number": 431, "issue_title": "web_voyager.ipynb met with a \"NotImplementedError\" when i run the example script", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\n**exactly the same code in the web_voyager.ipynb\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nNotImplementedError                       Traceback (most recent call last)\nCell In[10], line 5\n      2 from IPython import display\n      3 from playwright.async_api import async_playwright\n----> 5 browser = await async_playwright().start()\n      7 # We will set headless=False so we can watch the agent navigate the web.\n      8 browser = await browser.chromium.launch(headless=False, args=None)\n\nFile ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\playwright\\async_api\\_context_manager.py:51, in PlaywrightContextManager.start(self)\n     50 async def start(self) -> AsyncPlaywright:\n---> 51     return await self.__aenter__()\n\nFile ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\playwright\\async_api\\_context_manager.py:46, in PlaywrightContextManager.__aenter__(self)\n     44 if not playwright_future.done():\n     45     playwright_future.cancel()\n---> 46 playwright = AsyncPlaywright(next(iter(done)).result())\n     47 playwright.stop = self.__aexit__  # type: ignore\n     48 return playwright\n\nFile ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\playwright\\_impl\\_connection.py:272, in Connection.run(self)\n    269 async def init() -> None:\n    270     self.playwright_future.set_result(await self._root_object.initialize())\n--> 272 await self._transport.connect()\n    273 self._init_task = self._loop.create_task(init())\n    274 await self._transport.run()\n\nFile ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\playwright\\_impl\\_transport.py:133, in PipeTransport.connect(self)\n    131 except Exception as exc:\n    132     self.on_error_future.set_exception(exc)\n--> 133     raise exc\n    135 self._output = self._proc.stdin\n\nFile ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\playwright\\_impl\\_transport.py:120, in PipeTransport.connect(self)\n    117         startupinfo.wShowWindow = subprocess.SW_HIDE\n    119     executable_path, entrypoint_path = compute_driver_executable()\n--> 120     self._proc = await asyncio.create_subprocess_exec(\n    121         executable_path,\n    122         entrypoint_path,\n    123         \"run-driver\",\n    124         stdin=asyncio.subprocess.PIPE,\n    125         stdout=asyncio.subprocess.PIPE,\n    126         stderr=_get_stderr_fileno(),\n    127         limit=32768,\n    128         env=env,\n    129         startupinfo=startupinfo,\n    130     )\n    131 except Exception as exc:\n    132     self.on_error_future.set_exception(exc)\n\nFile ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\subprocess.py:236, in create_subprocess_exec(program, stdin, stdout, stderr, loop, limit, *args, **kwds)\n    229     warnings.warn(\"The loop argument is deprecated since Python 3.8 \"\n    230                   \"and scheduled for removal in Python 3.10.\",\n    231                   DeprecationWarning,\n    232                   stacklevel=2\n    233     )\n    234 protocol_factory = lambda: SubprocessStreamProtocol(limit=limit,\n    235                                                     loop=loop)\n--> 236 transport, protocol = await loop.subprocess_exec(\n    237     protocol_factory,\n    238     program, *args,\n    239     stdin=stdin, stdout=stdout,\n    240     stderr=stderr, **kwds)\n    241 return Process(transport, protocol, loop)\n\nFile ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py:1676, in BaseEventLoop.subprocess_exec(self, protocol_factory, program, stdin, stdout, stderr, universal_newlines, shell, bufsize, encoding, errors, text, *args, **kwargs)\n   1674     debug_log = f'execute program {program!r}'\n   1675     self._log_subprocess(debug_log, stdin, stdout, stderr)\n-> 1676 transport = await self._make_subprocess_transport(\n   1677     protocol, popen_args, False, stdin, stdout, stderr,\n   1678     bufsize, **kwargs)\n   1679 if self._debug and debug_log is not None:\n   1680     logger.info('%s: %r', debug_log, transport)\n\nFile ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py:498, in BaseEventLoop._make_subprocess_transport(self, protocol, args, shell, stdin, stdout, stderr, bufsize, extra, **kwargs)\n    494 async def _make_subprocess_transport(self, protocol, args, shell,\n    495                                      stdin, stdout, stderr, bufsize,\n    496                                      extra=None, **kwargs):\n    497     \"\"\"Create subprocess transport.\"\"\"\n--> 498     raise NotImplementedError\n\nNotImplementedError: \nDescription\ni restarted  the kernel to use updated packages. search the internet, did not find a solution\nplaywright Version: 1.43.0\nSummary: A high-level API to automate web browsers\nHome-page: https://github.com/Microsoft/playwright-python\nAuthor: Microsoft Corporation\nAuthor-email:\nLicense: Apache-2.0\nLocation: c:\\users\\gavin\\appdata\\local\\programs\\python**\\python39**lib\\site-packages\nRequires: greenlet, pyee\nRequired-by:\nSystem Info\nlangchain==0.1.17\nlangchain-community==0.0.37\nlangchain-core==0.1.52\nlangchain-openai==0.1.1\nlangchain-text-splitters==0.0.1\nlangchainhub==0.1.15\nlanggraph==0.0.45\nlangsmith==0.1.54", "created_at": "2024-05-11", "closed_at": "2024-05-15", "labels": [], "State": "closed", "Author": "chuangzhidan"}
{"issue_number": 486, "issue_title": "Langgraph's Question on Function_call in OpenAI", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\nExample Code\nprompt = hub.pull(\"hwchase17/openai-functions-agent\")\n# Choose the LLM that will drive the agent\nllm = ChatOpenAI(model=\"gpt-4-turbo-preview\", openai_api_key=openai_api_key, openai_api_base=openai_api_base)\ntools = search_tools\n# \u53ef\u4ee5\u6267\u884cPython\u4ee3\u7801\npython_repl_tool = PythonREPLTool()\n\n\ndef create_agent(llm, tools, system_prompt):\n    # \u6bcf\u4e2a\u5de5\u4f5c\u7684\u8282\u70b9agent\u90fd\u6709\u4e00\u4e2a\u540d\u5b57\u548c\u4e00\u4e9b\u5de5\u5177\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                system_prompt\n            ),\n            MessagesPlaceholder(variable_name=\"messages\"),\n            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n        ]\n    )\n    agent = create_openai_tools_agent(llm, tools, prompt)\n    executor = AgentExecutor(agent=agent, tools=tools)\n    return executor\n\n\ndef agent_node(state, agent, name):\n    result = agent.invoke(state)\n    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}\n\n\n# \u5b9a\u4e49agent supervisor\nmembers = [\"Researcher\", \"Coder\"]\nsystem_prompt = (\n    \"You are a supervisor tasked with managing a conversation between the\"\n    \" following workers:  {members}. Given the following user request,\"\n    \" respond with the worker to act next. Each worker will perform a\"\n    \" task and respond with their results and status. When finished,\"\n    \" respond with FINISH.\"\n)\noptions = [\"FINISH\"] + members\n# \u4f7f\u7528openAI\u7684function_call\nfunction_def = {\n    \"name\": \"route\",\n    \"description\": \"Select the next role.\",\n    \"parameters\": {\n        \"title\": \"routeSchema\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"next\": {\n                \"title\": \"Next\",\n                \"anyOf\": [\n                    {\"enum\": options},\n                ],\n            }\n        },\n        \"required\": [\"next\"],\n    },\n}\nsupervisor_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system_prompt),\n        MessagesPlaceholder(variable_name=\"messages\"),\n        (\n            \"system\",\n            \"Given the conversation above, who should act next?\"\n            \" Or should we FINISH? Select one of: {options}\",\n         ),\n    ]\n).partial(options=str(options),members=\",\".join(members))\nsupervisor_chain=supervisor_prompt \\\n                 | llm.bind_functions(functions=[function_def],function_call=\"route\") \\\n                 | JsonOutputFunctionsParser()\n# \u6784\u5efa\u56fe\nclass AgentState(TypedDict):\n    # The annotation tells the graph that new messages will always\n    # be added to the current states\n    messages: Annotated[Sequence[BaseMessage], operator.add]\n    # The 'next' field indicates where to route to next\n    next: str\nresearche_agent=create_agent(llm,search_tools,\"You are a web researcher.\")\n# from functools import partial\n# def power(base, exponent):\n#     return base ** exponent\n# # \u521b\u5efa\u4e00\u4e2a\u504f\u51fd\u6570\uff0c\u56fa\u5b9abase\u4e3a2\n# square = partial(power, base=2)\n# print(square(3))  # \u8f93\u51fa8\uff0c\u76f8\u5f53\u4e8e\u8c03\u7528\u4e86power(2, 3)\nresearch_node=functools.partial(agent_node,agent=researche_agent,name=\"Researcher\")\ncode_agent = create_agent(\n    llm,\n    [python_repl_tool],\n    \"You may generate safe python code to analyze data and generate charts using matplotlib.\",\n)\ncode_node = functools.partial(agent_node, agent=code_agent, name=\"Coder\")\nworkflow = StateGraph(AgentState)\nworkflow.add_node(\"Researcher\", research_node)\nworkflow.add_node(\"Coder\", code_node)\nworkflow.add_node(\"supervisor\", supervisor_chain)\nfor member in members:\n    # worker\u6267\u884c\u5b8c\u4e4b\u540e\u8fd4\u56desupervisor\u8282\u70b9\n    workflow.add_edge(member, \"supervisor\")\nconditional_map = {k: k for k in members}\nconditional_map[\"FINISH\"] = END\nworkflow.add_conditional_edges(\"supervisor\",lambda x: x[\"next\"],conditional_map)\nworkflow.set_entry_point(\"supervisor\")\ngraph = workflow.compile()\nfor s in graph.stream(\n    {\n        \"messages\": [\n            HumanMessage(content=\"Code hello world and print it to the terminal\")\n        ]\n    }\n):\n    if \"__end__\" not in s:\n        print(s)\n        print(\"----\")\n\nError Message and Stack Trace (if applicable)\nError Message and Stack Trace (if applicable)\nraise OutputParserException(f\"Could not parse function call: {exc}\")\nlangchain_core.exceptions.OutputParserException: Could not parse function call: 'function_call'\nDescription\nThis is a problem that I encountered while studying Langgraph's multi-agent blog. This problem not only occurs here, but also when I was studying Langgraph's Planning Agent Examples module\nSystem Info\nLatest version", "created_at": "2024-05-11", "closed_at": "2024-08-27", "labels": [], "State": "closed", "Author": "147258369777"}
{"issue_number": 429, "issue_title": "Incorrect Validation in Graph class", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nOn github, in langgraph/langgraph/graph/graph.py:\n97   class Graph:\n...\n241     def validate(self, interrupt: Optional[Sequence[str]] = None) -> None:\n...\n259         for source in all_sources:\n260             if node not in self.nodes and node != START:\n261                 raise ValueError(f\"Found edge starting at unknown node '{source}'\")\n\nError Message and Stack Trace (if applicable)\nN/A\nDescription\nnode in line 260 of langgraph/langgraph/graph/graph.py should instead be source to properly validate that all edges have a known source.\nSystem Info\nN/A", "created_at": "2024-05-10", "closed_at": "2024-05-14", "labels": [], "State": "closed", "Author": "jtstener"}
{"issue_number": 428, "issue_title": "ASCII Graph shows conditional edges not as expected", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nSlightly extended code from langgraph's 'multiply example'.\nWhy are there conditional edges from router to pprint_A and pprint_B in the ASCII graph?\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.runnables import RunnableLambda\n\nfrom langchain_core.tools import tool\nfrom langgraph.prebuilt import ToolNode\n\nfrom langchain_core.messages import HumanMessage\nfrom langgraph.graph import END, MessageGraph\n\n@tool\ndef multiply(first_number: int, second_number: int):\n    \"\"\"Multiplies two numbers together.\"\"\"\n    return first_number * second_number\n\n\ndef fn_pprint_A(obj):\n    print(\"------pA-----\")\n    print(obj)\n    print(\"-------------\")\n    return obj\n\npprint_A = RunnableLambda(fn_pprint_A)\n\ndef fn_pprint_B(obj):\n    print(\"------pB-----\")\n    print(obj)\n    print(\"-------------\")\n    return obj\n\npprint_B = RunnableLambda(fn_pprint_B)\n\nmodel = ChatOpenAI()\nmodel_with_tools = model.bind_tools([multiply])\n\ntool_node = ToolNode([multiply])\n\nbuilder = MessageGraph()\n\nbuilder.add_node(\"oracle\", model_with_tools)\nbuilder.add_node(\"multiply\", tool_node)\nbuilder.add_node(\"pprint_A\", pprint_A)\nbuilder.add_node(\"pprint_B\", pprint_B)\n\n\n\n\nbuilder.add_edge(\"multiply\", \"pprint_A\")\nbuilder.add_edge(\"pprint_A\", \"pprint_B\")\nbuilder.add_edge(\"pprint_B\", END)\n\nbuilder.set_entry_point(\"oracle\")\n\nfrom typing import Literal\nfrom typing import List\nfrom langchain_core.messages.base import BaseMessage\n\ndef router(state: List[BaseMessage]) -> Literal[\"multiply\", \"__end__\"]:\n    tool_calls = state[-1].additional_kwargs.get(\"tool_calls\", [])\n    if len(tool_calls):\n        return \"multiply\"\n    else:\n        return \"__end__\"\n\nbuilder.add_conditional_edges(\"oracle\", router)\n\nrunnable = builder.compile()\n\n# runnable.invoke(HumanMessage(\"What is 123 * 456?\"))\n\nrunnable.get_graph().print_ascii()\n\n#                                 +-----------+                            \n#                                 | __start__ |                            \n#                                 +-----------+                            \n#                                        *                                 \n#                                        *                                 \n#                                        *                                 \n#                                   +--------+                             \n#                                   | oracle |                             \n#                                   +--------+                             \n#                                        *                                 \n#                                        *                                 \n#                                        *                                 \n#                                   +--------+                             \n#                                 ..| router |...                          \n#                           ......  +--------+   ......                    \n#                      .....       ..         ..       ......              \n#                ......          ..             ..           .....         \n#             ...              ..                 ..              ......   \n# +----------+                .                     .                   ...\n# | multiply |              ..                      .                     .\n# +----------+            ..                        .                     .\n#            **         ..                          .                     .\n#              **     ..                            .                     .\n#                *   .                              .                     .\n#            +----------+                          ..                     .\n#            | pprint_A |                      ....                       .\n#            +----------+                   ...                           .\n#                       **              ....                              .\n#                         **        ....                                  .\n#                           *     ..                                      .\n#                       +----------+                                    ...\n#                       | pprint_B |                              ......   \n#                       +----------+                         .....         \n#                                  **                  ......              \n#                                    **          ......                    \n#                                      *      ...                          \n#                                   +---------+                            \n#                                   | __end__ |                            \n#                                   +---------+                            \nError Message and Stack Trace (if applicable)\nNo response\nDescription\n\nI would expect that the router only has conditional edges to 'multiply' and 'end'\nAs shown in the ASCII graph, the router also points to 'pprint_A' and 'pprint_B'\n\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.19045\nPython Version:  3.11.9 | packaged by Anaconda, Inc. | (main, Apr 19 2024, 16:40:41) [MSC v.1916 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.1.46\nlangchain: 0.1.16\nlangchain_community: 0.0.34\nlangsmith: 0.1.51\nlangchain_experimental: 0.0.57\nlangchain_openai: 0.1.4\nlangchain_text_splitters: 0.0.1\nlanggraph: 0.0.39\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlangserve\n", "created_at": "2024-05-10", "closed_at": "2024-05-10", "labels": [], "State": "closed", "Author": "arthurGrigo"}
{"issue_number": 413, "issue_title": "Unable to define a conditional edge", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\n    class WorkFlow():\n\tdef __init__(self):\n\t\tnodes = Nodes()\n\t\tworkflow = StateGraph(PersonState)\n\t\tworkflow.add_node(\"userInput\", nodes.userInput)\n\t\tworkflow.add_node(\"text2PersonInfo\", nodes.text2PersonInfo)\n\t\tworkflow.add_node(\"personInfoValidate\", nodes.personInfoValidate)\n\t\tworkflow.add_node(\"missingDataAgent\", nodes.missingDataAgent)\n\t\tworkflow.set_entry_point(\"userInput\")\n\t\tworkflow.add_edge('userInput', 'text2PersonInfo')\n\t\tworkflow.add_edge('text2PersonInfo', 'personInfoValidate')\n\t\tworkflow.add_conditional_edges(\n\t\t\t\"personInfoValidate\",\n\t\t\tnodes.personInfoValidate,\n\t\t\t{\n\t\t\t\t\"incomplete\": \"missingDataAgent\",\n\t\t\t\t\"end\": END\n\t\t\t}\n\t\t)\n\t\tself.app = workflow.compile()\n\n\n\n Traceback (most recent call last):\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/Users/jindal/AppDevelopment/30DaysofAI/LLMHandsOn/lang_graph/main.py\", line 15, in <module>\n    app.invoke({})\n  File \"/Users/jindal/Library/Caches/pypoetry/virtualenvs/llm-handson-m_vF4p_v-py3.10/lib/python3.10/site-packages/langgraph/pregel/__init__.py\", line 1171, in invoke\n    for chunk in self.stream(\n  File \"/Users/jindal/Library/Caches/pypoetry/virtualenvs/llm-handson-m_vF4p_v-py3.10/lib/python3.10/site-packages/langgraph/pregel/__init__.py\", line 774, in stream\n    _panic_or_proceed(done, inflight, step)\n  File \"/Users/jindal/Library/Caches/pypoetry/virtualenvs/llm-handson-m_vF4p_v-py3.10/lib/python3.10/site-packages/langgraph/pregel/__init__.py\", line 1260, in _panic_or_proceed\n    raise exc\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.14/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/Users/jindal/Library/Caches/pypoetry/virtualenvs/llm-handson-m_vF4p_v-py3.10/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2499, in invoke\n    input = step.invoke(\n  File \"/Users/jindal/Library/Caches/pypoetry/virtualenvs/llm-handson-m_vF4p_v-py3.10/lib/python3.10/site-packages/langgraph/utils.py\", line 88, in invoke\n    ret = context.run(self.func, input, **kwargs)\n  File \"/Users/jindal/Library/Caches/pypoetry/virtualenvs/llm-handson-m_vF4p_v-py3.10/lib/python3.10/site-packages/langgraph/graph/graph.py\", line 74, in _route\n    destinations = [self.ends[r] for r in result]\n  File \"/Users/jindal/Library/Caches/pypoetry/virtualenvs/llm-handson-m_vF4p_v-py3.10/lib/python3.10/site-packages/langgraph/graph/graph.py\", line 74, in <listcomp>\n    destinations = [self.ends[r] for r in result]\nTypeError: unhashable type: 'dict'\n\n[tool.poetry.dependencies]\npython = \">=3.10.0,<3.11\"\nollama = \"^0.1.8\"\nload-dotenv = \"^0.1.0\"\nlanggraph = \"0.0.45\"\nlangchain = \"^0.1.17\"\n\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nwhenever i remove conditional node it works fine\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 22.2.0: Fri Nov 11 02:04:44 PST 2022; root:xnu-8792.61.2~4/RELEASE_ARM64_T8103\nPython Version:  3.10.14 (main, Mar 19 2024, 21:46:16) [Clang 15.0.0 (clang-1500.1.0.2.5)]\n\nPackage Information\n\nlangchain_core: 0.1.52\nlangchain: 0.1.17\nlangchain_community: 0.0.37\nlangsmith: 0.1.54\nlangchain_openai: 0.0.5\nlangchain_text_splitters: 0.0.1\nlanggraph: 0.0.45\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlangserve\n", "created_at": "2024-05-07", "closed_at": "2024-05-15", "labels": [], "State": "closed", "Author": "jindalAnuj"}
{"issue_number": 397, "issue_title": "Error parsing langchain example notebook 'langgraph_rag_agent_llama3_local.ipynb': SyntaxError", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nAny help on correcting this would be appreciated!\nError opening the notebook https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb.  Opened on different Jupyter NB and lab, online JSON verifier and online Jupyter notebook (CoCalc).\nError on my ubuntu 24.04 installation:\nUnreadable Notebook: /mnt/c/Users/wdonn/NLP1/langchain/rag/langgraph_rag_agent_llama3_local (2).ipynb NotJSONError(\"Notebook does not appear to be JSON: '\\n\\n\\n\\n\\n\\n\\n<html\\n l...\")\nCoCalc error:\nError parsing the ipynb file 'langgraph_rag_agent_llama3_local (2).ipynb': SyntaxError: Unexpected token < in JSON at position 6.  You must fix the ipynb file somehow before continuing.\nBest wishes,\nBill\nError Message and Stack Trace (if applicable)\nUnreadable Notebook: /mnt/c/Users/wdonn/NLP1/langchain/rag/langgraph_rag_agent_llama3_local (2).ipynb NotJSONError(\"Notebook does not appear to be JSON: '\\n\\n\\n\\n\\n\\n\\n<html\\n l...\")\nCoCalc error:\nError parsing the ipynb file 'langgraph_rag_agent_llama3_local (2).ipynb': SyntaxError: Unexpected token < in JSON at position 6.  You must fix the ipynb file somehow before continuing.\nDescription\nError in current version of file in github:\nhttps://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb\nSystem Info\nNot relevant: file does not even open", "created_at": "2024-05-05", "closed_at": "2024-05-05", "labels": [], "State": "closed", "Author": "wdonno"}
{"issue_number": 396, "issue_title": "Edge naming in docs", "issue_body": "Issue with current documentation:\nEdges and nodes have the same name in the example. It took me some time to figure out what was what. Impatient, yes. Unnecessary, also yes.\nIdea or request for content:\nImprovement: add _edge to edges in the examples, such as:\ndef router(state: List[BaseMessage]):\n    tool_calls = state[-1].additional_kwargs.get(\"tool_calls\", [])\n    if len(tool_calls):\n        return \"multiply_edge\"\n    else:\n        return \"end_edge\"\n\ngraph.add_conditional_edges(\"oracle\", router, {\n    \"multiply\": \"multiply\",\n    \"end_edge\": END,\n})\n", "created_at": "2024-05-04", "closed_at": "2024-05-07", "labels": [], "State": "closed", "Author": "carbonemys"}
{"issue_number": 395, "issue_title": "Langgraph's Question on Function_call in OpenAI", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nprompt = hub.pull(\"hwchase17/openai-functions-agent\")\n# Choose the LLM that will drive the agent\nllm = ChatOpenAI(model=\"gpt-4-turbo-preview\", openai_api_key=openai_api_key, openai_api_base=openai_api_base)\ntools = search_tools\n# \u53ef\u4ee5\u6267\u884cPython\u4ee3\u7801\npython_repl_tool = PythonREPLTool()\n\n\ndef create_agent(llm, tools, system_prompt):\n    # \u6bcf\u4e2a\u5de5\u4f5c\u7684\u8282\u70b9agent\u90fd\u6709\u4e00\u4e2a\u540d\u5b57\u548c\u4e00\u4e9b\u5de5\u5177\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                system_prompt\n            ),\n            MessagesPlaceholder(variable_name=\"messages\"),\n            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n        ]\n    )\n    agent = create_openai_tools_agent(llm, tools, prompt)\n    executor = AgentExecutor(agent=agent, tools=tools)\n    return executor\n\n\ndef agent_node(state, agent, name):\n    result = agent.invoke(state)\n    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}\n\n\n# \u5b9a\u4e49agent supervisor\nmembers = [\"Researcher\", \"Coder\"]\nsystem_prompt = (\n    \"You are a supervisor tasked with managing a conversation between the\"\n    \" following workers:  {members}. Given the following user request,\"\n    \" respond with the worker to act next. Each worker will perform a\"\n    \" task and respond with their results and status. When finished,\"\n    \" respond with FINISH.\"\n)\noptions = [\"FINISH\"] + members\n# \u4f7f\u7528openAI\u7684function_call\nfunction_def = {\n    \"name\": \"route\",\n    \"description\": \"Select the next role.\",\n    \"parameters\": {\n        \"title\": \"routeSchema\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"next\": {\n                \"title\": \"Next\",\n                \"anyOf\": [\n                    {\"enum\": options},\n                ],\n            }\n        },\n        \"required\": [\"next\"],\n    },\n}\nsupervisor_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system_prompt),\n        MessagesPlaceholder(variable_name=\"messages\"),\n        (\n            \"system\",\n            \"Given the conversation above, who should act next?\"\n            \" Or should we FINISH? Select one of: {options}\",\n         ),\n    ]\n).partial(options=str(options),members=\",\".join(members))\nsupervisor_chain=supervisor_prompt \\\n                 | llm.bind_functions(functions=[function_def],function_call=\"route\") \\\n                 | JsonOutputFunctionsParser()\n# \u6784\u5efa\u56fe\nclass AgentState(TypedDict):\n    # The annotation tells the graph that new messages will always\n    # be added to the current states\n    messages: Annotated[Sequence[BaseMessage], operator.add]\n    # The 'next' field indicates where to route to next\n    next: str\nresearche_agent=create_agent(llm,search_tools,\"You are a web researcher.\")\n# from functools import partial\n# def power(base, exponent):\n#     return base ** exponent\n# # \u521b\u5efa\u4e00\u4e2a\u504f\u51fd\u6570\uff0c\u56fa\u5b9abase\u4e3a2\n# square = partial(power, base=2)\n# print(square(3))  # \u8f93\u51fa8\uff0c\u76f8\u5f53\u4e8e\u8c03\u7528\u4e86power(2, 3)\nresearch_node=functools.partial(agent_node,agent=researche_agent,name=\"Researcher\")\ncode_agent = create_agent(\n    llm,\n    [python_repl_tool],\n    \"You may generate safe python code to analyze data and generate charts using matplotlib.\",\n)\ncode_node = functools.partial(agent_node, agent=code_agent, name=\"Coder\")\nworkflow = StateGraph(AgentState)\nworkflow.add_node(\"Researcher\", research_node)\nworkflow.add_node(\"Coder\", code_node)\nworkflow.add_node(\"supervisor\", supervisor_chain)\nfor member in members:\n    # worker\u6267\u884c\u5b8c\u4e4b\u540e\u8fd4\u56desupervisor\u8282\u70b9\n    workflow.add_edge(member, \"supervisor\")\nconditional_map = {k: k for k in members}\nconditional_map[\"FINISH\"] = END\nworkflow.add_conditional_edges(\"supervisor\",lambda x: x[\"next\"],conditional_map)\nworkflow.set_entry_point(\"supervisor\")\ngraph = workflow.compile()\nfor s in graph.stream(\n    {\n        \"messages\": [\n            HumanMessage(content=\"Code hello world and print it to the terminal\")\n        ]\n    }\n):\n    if \"__end__\" not in s:\n        print(s)\n        print(\"----\")\nError Message and Stack Trace (if applicable)\nraise OutputParserException(f\"Could not parse function call: {exc}\")\nlangchain_core.exceptions.OutputParserException: Could not parse function call: 'function_call'\nDescription\nThis is a problem that I encountered while studying Langgraph's multi-agent blog. This problem not only occurs here, but also when I was studying Langgraph's Planning Agent Examples module\nSystem Info\nLatest version", "created_at": "2024-05-04", "closed_at": "2024-06-03", "labels": [], "State": "closed", "Author": "147258369777"}
{"issue_number": 373, "issue_title": "DOC: Agent supervisor error after Changing ChatOpenAi", "issue_body": "Issue with current documentation:\nI'm trying to run the agent-supervisor example. I've change the ChatOpenAi to ChatGroq and And also the tavily tool for the python_repl_tool instead and I'm getting an error, which I think it comes from the create_openai_tools_agent()\nThis is the error I'm getting:\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[28], [line 1](vscode-notebook-cell:?execution_count=28&line=1)\n----> [1](vscode-notebook-cell:?execution_count=28&line=1) for s in graph.stream(\n      [2](vscode-notebook-cell:?execution_count=28&line=2)     {\n      [3](vscode-notebook-cell:?execution_count=28&line=3)         \"messages\": [\n      [4](vscode-notebook-cell:?execution_count=28&line=4)             HumanMessage(content=\"Code hello world and print it to the terminal\")\n      [5](vscode-notebook-cell:?execution_count=28&line=5)         ]\n      [6](vscode-notebook-cell:?execution_count=28&line=6)     }\n      [7](vscode-notebook-cell:?execution_count=28&line=7) ):\n      [8](vscode-notebook-cell:?execution_count=28&line=8)     if \"__end__\" not in s:\n      [9](vscode-notebook-cell:?execution_count=28&line=9)         print(s)\n\nFile [~/Desktop/Python/pdf/local-model/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:710](https://file+.vscode-resource.vscode-cdn.net/Users/ruben/Desktop/Python/pdf/local-model/notebooks/~/Desktop/Python/pdf/local-model/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:710), in Pregel.stream(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\n    [703](https://file+.vscode-resource.vscode-cdn.net/Users/ruben/Desktop/Python/pdf/local-model/notebooks/~/Desktop/Python/pdf/local-model/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:703) done, inflight = concurrent.futures.wait(\n    [704](https://file+.vscode-resource.vscode-cdn.net/Users/ruben/Desktop/Python/pdf/local-model/notebooks/~/Desktop/Python/pdf/local-model/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:704)     futures,\n    [705](https://file+.vscode-resource.vscode-cdn.net/Users/ruben/Desktop/Python/pdf/local-model/notebooks/~/Desktop/Python/pdf/local-model/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:705)     return_when=concurrent.futures.FIRST_EXCEPTION,\n    [706](https://file+.vscode-resource.vscode-cdn.net/Users/ruben/Desktop/Python/pdf/local-model/notebooks/~/Desktop/Python/pdf/local-model/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:706)     timeout=self.step_timeout,\n    [707](https://file+.vscode-resource.vscode-cdn.net/Users/ruben/Desktop/Python/pdf/local-model/notebooks/~/Desktop/Python/pdf/local-model/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:707) )\n    [709](https://file+.vscode-resource.vscode-cdn.net/Users/ruben/Desktop/Python/pdf/local-model/notebooks/~/Desktop/Python/pdf/local-model/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:709) # panic on failure or timeout\n--> [710](https://file+.vscode-resource.vscode-cdn.net/Users/ruben/Desktop/Python/pdf/local-model/notebooks/~/Desktop/Python/pdf/local-model/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:710) _panic_or_proceed(done, inflight, step)\n    [712](https://file+.vscode-resource.vscode-cdn.net/Users/ruben/Desktop/Python/pdf/local-model/notebooks/~/Desktop/Python/pdf/local-model/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:712) # combine pending writes from all tasks\n    [713](https://file+.vscode-resource.vscode-cdn.net/Users/ruben/Desktop/Python/pdf/local-model/notebooks/~/Desktop/Python/pdf/local-model/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:713) pending_writes = deque[tuple[str, Any]]()\n\nFile [~/Desktop/Python/pdf/local-model/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1126](https://file+.vscode-resource.vscode-cdn.net/Users/ruben/Desktop/Python/pdf/local-model/notebooks/~/Desktop/Python/pdf/local-model/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1126), in _panic_or_proceed(done, inflight, step)\n...\n    [241](https://file+.vscode-resource.vscode-cdn.net/Users/ruben/Desktop/Python/pdf/local-model/notebooks/~/Desktop/Python/pdf/local-model/.venv/lib/python3.10/site-packages/langchain_groq/chat_models.py:241) }\n--> [242](https://file+.vscode-resource.vscode-cdn.net/Users/ruben/Desktop/Python/pdf/local-model/notebooks/~/Desktop/Python/pdf/local-model/.venv/lib/python3.10/site-packages/langchain_groq/chat_models.py:242) response = self.client.create(messages=message_dicts, **params)\n    [243](https://file+.vscode-resource.vscode-cdn.net/Users/ruben/Desktop/Python/pdf/local-model/notebooks/~/Desktop/Python/pdf/local-model/.venv/lib/python3.10/site-packages/langchain_groq/chat_models.py:243) return self._create_chat_result(response)\n\nTypeError: Completions.create() got an unexpected keyword argument 'functions'\nOutput is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?d4edf1e0-942d-4502-98e6-8fe38e079a64) or open in a [text editor](command:workbench.action.openLargeOutput?d4edf1e0-942d-4502-98e6-8fe38e079a64). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)...\n\nI've tried to use other functions like create_structured_chat_agent but I've got different errors. This time when trying to compile the graph, says\nValueError: Prompt missing required variables: {'tools', 'tool_names'}\nOutput is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?9154a43a-66ac-4161-8429-870a95e44a24) or open in a [text editor](command:workbench.action.openLargeOutput?9154a43a-66ac-4161-8429-870a95e44a24). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)...\n\nIdea or request for content:\nDiferent Implementations with open source Chat models like ChatGroq or ChatOllama", "created_at": "2024-05-01", "closed_at": null, "labels": [], "State": "open", "Author": "SatouKuzuma1"}
{"issue_number": 371, "issue_title": "OpenAI Agent Not Function Calling", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\ndef call_model(state):\n    messages = state['messages']\n    response = model.invoke(messages)\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": [response]}\n\n# Define the function to execute tools\ndef call_tool(state):\n    messages = state['messages']\n    # Based on the continue condition\n    # we know the last message involves a function call\n    last_message = messages[-1]\n    # We construct an ToolInvocation from the function_call\n    action = ToolInvocation(\n        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n        tool_input=json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"]),\n    )\n    # We call the tool_executor and get back a response\n    response = tool_exec.invoke(action)\n    # We use the response to create a FunctionMessage\n    function_message = FunctionMessage(content=str(response), name=action.tool)\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": [function_message]}\n\nclass RetrieverAgent():\n    def __init__(self) -> None:\n        self.workflow = StateGraph(AgentState)\n    \n        # Define the two nodes we will cycle between\n        self.workflow.add_node(\"agent\", call_model)\n        self.workflow.add_node(\"action\", call_tool)\n\n        # Set the entrypoint as `agent`\n        # This means that this node is the first one called\n        self.workflow.set_entry_point(\"agent\")\n\n        self.workflow.add_edge(\"agent\", \"action\")\n        self.workflow.add_conditional_edges(\n            \"action\",\n            should_end,\n            {\n                \"continue\": \"agent\",\n                \"end\": END\n            }\n        )\n        \n        self.app = self.workflow.compile()\n    \n    def run(self, human_input):\n        inputs = {\"messages\": [HumanMessage(content=human_input)]}\n        \n        with get_openai_callback() as cb:\n            result = self.app.invoke(inputs)\n\n        return result['messages'][-1].content\n    \nagent = RetrieverAgent()\nError Message and Stack Trace (if applicable)\ntool=last_message.additional_kwargs[\"function_call\"][\"name\"],\\n\\n\\nKeyError: 'function_call'\"\nDescription\n\nI am trying to create an agent that has to call single or multiple tools based on the user query\nthe first step of the agent has to be function calling\ncurrently, based on certain user queries that isn't very clear, the agent will not do function calling and instead generate an answer\n\nSystem Info\nlangchain==0.1.17\nlanggraph==0.0.40", "created_at": "2024-05-01", "closed_at": "2024-05-07", "labels": [], "State": "closed", "Author": "jasonngap1"}
{"issue_number": 358, "issue_title": "Storm example contains outdated check for END in stream output", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nfrom typing_extensions import TypedDict\nclass ResearchState(TypedDict):\n    \n    topic: str\n    # outline: Outline\n    # editors: List[Editor]\n    # interview_results: List[InterviewState]\n    # # The final sections output\n    # sections: List[WikiSection]\n    # article: str\n\nimport asyncio\nfrom langgraph.graph import StateGraph, END\nfrom IPython.display import Image\n\n\nasync def initialize_research(state: ResearchState):\n    print(\"initialize_research\")\n    return {\n        **state,\n    }\n\n\nasync def conduct_interviews(state: ResearchState):\n    print(\"conduct_interview\")\n    return {\n        **state,\n    }\n\n\nasync def refine_outline(state: ResearchState):\n    print(\"refine_outline\")\n    return {\n        **state,\n    }\n\n\nasync def index_references(state: ResearchState):\n    print(\"index_references\")\n    return {\n        **state,\n    }\n\n\nasync def write_sections(state: ResearchState):\n    print(\"write_sections\")\n    return {\n        **state,\n    }\n\n\nasync def write_article(state: ResearchState):\n    print(\"write_article\")\n    return {\n        **state,\n    }\n\nbuilder_of_storm = StateGraph(ResearchState)\n\nnodes = [\n    (\"init_research\", initialize_research),\n    (\"conduct_interviews\", conduct_interviews),\n    (\"refine_outline\", refine_outline),\n    (\"index_references\", index_references),\n    (\"write_sections\", write_sections),\n    (\"write_article\", write_article),\n]\nfor i in range(len(nodes)):\n    name, node = nodes[i]\n    builder_of_storm.add_node(name, node)\n    if i > 0:\n        builder_of_storm.add_edge(nodes[i - 1][0], name)\n\nbuilder_of_storm.set_entry_point(nodes[0][0])\nbuilder_of_storm.set_finish_point(nodes[-1][0])\nstorm = builder_of_storm.compile()\n\nImage(storm.get_graph().draw_png())\n\n\nasync for step in storm.astream(\n    {\n        \"topic\": \"Practical processes for implementing ORPO. ORPO: Monolithic Preference Optimization without Reference Model found at https://arxiv.org/abs/2403.07691\",\n    }\n):\n    name = next(iter(step))\n    print(name)\n    print(\"-- \", str(step[name])[:300])\n    if END in step:\n        results = step\n\nprint(results)\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 92\n     89     if END in step:\n     90         results = step\n---> 92 print(results)\n\nNameError: name 'results' is not defined\n\nDescription\nI am trying to go through the Storm tutorial and the END is not being run. I verified the conditions were satisfied and threw END up front but still couldn't catch the condition. It is processing through the graph according to LangSmith.\nThis proof of concept shows that although the endpoint is set in the Graph, it's not being reached when specifically set with set_finish_point\nSystem Info\nlangchain==0.1.16\nlangchain-anthropic==0.1.11\nlangchain-community==0.0.34\nlangchain-core==0.1.46\nlangchain-fireworks==0.1.2\nlangchain-openai==0.1.4\nlangchain-text-splitters==0.0.1\nlangchainhub==0.1.15", "created_at": "2024-04-28", "closed_at": null, "labels": [], "State": "open", "Author": "csellis"}
{"issue_number": 351, "issue_title": "DOC: Manually updating the state causes a bad request error", "issue_body": "Issue with current documentation:\nIn this section: https://langchain-ai.github.io/langgraph/tutorials/introduction/#what-if-you-want-to-overwrite-existing-messages\nevents = graph.stream(None, config)\nfor event in events:\n    for value in event.values():\n        if isinstance(value[\"messages\"][-1], BaseMessage):\n            print(\"Assistant:\", value[\"messages\"][-1].content)\n\nThe tutorial is using Claude 3 haiku but, using any of the claude 3 models, Claud 3 returns a bad request:\n\"name\": \"BadRequestError\",\n\"message\": \"Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'messages.2: Did not find 1 tool_result block(s) at the beginning of this message. Messages following tool_use blocks must begin with a matching number of tool_result blocks.'}}\"\ngpt-4 is working as expected though.\nIdea or request for content:\ngpt-4 is doing it correctly but is less verbose so the first part of the tutorial gives less interesting feedback so it should probably be updated so that the expectations of Claude are met.", "created_at": "2024-04-26", "closed_at": "2024-05-07", "labels": [], "State": "closed", "Author": "TVScoundrel"}
{"issue_number": 348, "issue_title": "DOC: Tutorial introduction is using tools_condition which is not in the current version 0.0.38", "issue_body": "Issue with current documentation:\nThis page:\nhttps://langchain-ai.github.io/langgraph/tutorials/introduction/#part-2-enhancing-the-chatbot-with-tools\nfrom langgraph.prebuilt import tools_condition\n\n# The `tools_condition` function returns \"action\" if the chatbot asks to use a tool, and \"__end__\" if\n# it is fine directly responding. This conditional routing defines the main agent loop.\ngraph_builder.add_conditional_edges(\n    \"chatbot\",\n    tools_condition,\n    # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node\n    # It defaults to the identity function, but if you\n    # want to use a node named something else apart from \"action\",\n    # You can update the value of the dictionary to something else\n    # e.g., \"action\": \"my_tools\"\n    {\"action\": \"action\", \"__end__\": \"__end__\"},\n)\n# Any time a tool is called, we return to the chatbot to decide the next step\ngraph_builder.add_edge(\"action\", \"chatbot\")\ngraph_builder.set_entry_point(\"chatbot\")\ngraph = graph_builder.compile()\n\nThe current version https://pypi.org/project/langgraph/ is at 0.0.38 which does not have the prebuilt tools_condition yet.\nIdea or request for content:\nProbably either mention that this is bleeding edge, or do a new release on https://pypi.org/project/langgraph/", "created_at": "2024-04-25", "closed_at": "2024-05-04", "labels": [], "State": "closed", "Author": "TVScoundrel"}
{"issue_number": 347, "issue_title": "Compiled app.getGraph() throws an exception", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nThe graph was initialized as the following:\ngraph = StateGraph(GraphState)\nfrom IPython.display import display, HTML\nimport base64\n\ndef display_image(image_bytes: bytes, width=300):\n    decoded_img_bytes = base64.b64encode(image_bytes).decode('utf-8')\n    html = f'<img src=\"data:image/png;base64,{decoded_img_bytes}\" style=\"width: {width}px;\" />'\n    display(HTML(html))\nimport nest_asyncio\nfrom langchain_core.runnables.graph import CurveStyle, NodeColors, MermaidDrawMethod\n\nnest_asyncio.apply() # Required for Jupyter Notebook to run async functions\n\ndisplay_image(app.get_graph().draw_mermaid_png(\n    curve_style=CurveStyle.LINEAR,\n    node_colors=NodeColors(start=\"#ffdfba\", end=\"#baffc9\", other=\"#fad7de\"),\n    wrap_label_n_words=9,\n    output_file_path=None,\n    draw_method=MermaidDrawMethod.PYPPETEER,\n    background_color=\"white\",\n    padding=10\n))\nError Message and Stack Trace (if applicable)\nRuntimeError                              Traceback (most recent call last) Cell In[84], line 6       2 from langchain_core.runnables.graph import CurveStyle, NodeColors, MermaidDrawMethod       4 nest_asyncio.apply() # Required for Jupyter Notebook to run async functions ----> 6 display_image(app.get_graph().draw_mermaid_png(       7     curve_style=CurveStyle.LINEAR,       8     node_colors=NodeColors(start=\"#ffdfba\", end=\"#baffc9\", other=\"#fad7de\"),       9     wrap_label_n_words=9,      10     output_file_path=None,      11     draw_method=MermaidDrawMethod.PYPPETEER,      12     background_color=\"white\",      13     padding=10      14 )) ... --> 760         raise RuntimeError(f'error checking inheritance of {type_!r} (type: {display_as_type(type_)})')     762 if config.arbitrary_types_allowed:     763     yield make_arbitrary_type_validator(type_)  RuntimeError: error checking inheritance of <built-in function any> (type: builtin_function_or_method)\nDescription\nNo exception, graph drawn and visialized within the Jupyter notebook\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #111-Ubuntu SMP Tue Mar 5 20:16:58 UTC 2024\nPython Version:  3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\n\nPackage Information\n\nlangchain_core: 0.1.45\nlangchain: 0.1.16\nlangchain_community: 0.0.34\nlangsmith: 0.1.45\nlangchain_chroma: 0.1.0\nlangchain_experimental: 0.0.55\nlangchain_openai: 0.1.1\nlangchain_text_splitters: 0.0.1\nlangchainhub: 0.1.15\nlanggraph: 0.0.38\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlangserve\n", "created_at": "2024-04-25", "closed_at": "2024-05-06", "labels": [], "State": "closed", "Author": "DanielProkhorov"}
{"issue_number": 346, "issue_title": "Local RAG agent with LLaMA3 error: Ollama call failed with status code 400. Details: {\"error\":\"unexpected server status: 1\"}", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nnotebook example code in https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb\nError Message and Stack Trace (if applicable)\n{'score': 'yes'}\nAccording to the context, agent memory refers to a long-term memory module (external database) that records a comprehensive list of agents' experience in natural language. This memory stream is used by generative agents to enable them to behave conditioned on past experience and interact with other agents.\nTraceback (most recent call last):\n  File \"/home/luca/pymaindir_icos/autocoders/lc_coder/lama3/local_llama3.py\", line 139, in <module>\n    answer_grader.invoke({\"question\": question,\"generation\": generation})\n  File \"/home/luca/anaconda3/envs/lc_coder/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2499, in invoke\n    input = step.invoke(\n            ^^^^^^^^^^^^\n  File \"/home/luca/anaconda3/envs/lc_coder/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 158, in invoke\n    self.generate_prompt(\n  File \"/home/luca/anaconda3/envs/lc_coder/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 560, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luca/anaconda3/envs/lc_coder/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 421, in generate\n    raise e\n  File \"/home/luca/anaconda3/envs/lc_coder/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 411, in generate\n    self._generate_with_cache(\n  File \"/home/luca/anaconda3/envs/lc_coder/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 632, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/home/luca/anaconda3/envs/lc_coder/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py\", line 259, in _generate\n    final_chunk = self._chat_stream_with_aggregation(\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luca/anaconda3/envs/lc_coder/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py\", line 190, in _chat_stream_with_aggregation\n    for stream_resp in self._create_chat_stream(messages, stop, **kwargs):\n  File \"/home/luca/anaconda3/envs/lc_coder/lib/python3.11/site-packages/langchain_community/chat_models/ollama.py\", line 162, in _create_chat_stream\n    yield from self._create_stream(\n               ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/luca/anaconda3/envs/lc_coder/lib/python3.11/site-packages/langchain_community/llms/ollama.py\", line 251, in _create_stream\n    raise ValueError(\nValueError: Ollama call failed with status code 400. Details: {\"error\":\"unexpected server status: 1\"}\nDescription\nrunning the example code i get the above error, this is not happening with mistral so i guess my ollama is ok. I also get the first \"yes\" from the llama3 if I'm not mistaken, so I supsect it's related to something not working here:\nfrom pprint import pprint\ninputs = {\"question\": \"What are the types of agent memory?\"}\nfor output in app.stream(inputs):\n    for key, value in output.items():\n        pprint(f\"Finished running: {key}:\")\npprint(value[\"generation\"])\nSystem Info\nUbuntu 22.04.4 LTS\nAnaconda and VSC.", "created_at": "2024-04-25", "closed_at": null, "labels": [], "State": "open", "Author": "luca-git"}
{"issue_number": 789, "issue_title": "AsyncSqliteSaver hangs when using astream_event", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nimport asyncio\nfrom pprint import pprint\n\nfrom langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph\n\nbuilder = StateGraph(int)\nbuilder.add_node(\"add_one\", lambda x: x + 1)\nbuilder.set_entry_point(\"add_one\")\nbuilder.set_finish_point(\"add_one\")\n\n\nasync def memory():\n    memory = MemorySaver()\n    graph = builder.compile(checkpointer=memory)\n    config = {\"configurable\": {\"thread_id\": \"thread-1\"}}\n    async for event in graph.astream_events(10, config, version=\"v1\"):\n        pprint(event)\n\n\nasync def asyncSqlite():\n    memory = AsyncSqliteSaver.from_conn_string(\":memory:\")\n    graph = builder.compile(checkpointer=memory)\n    config = {\"configurable\": {\"thread_id\": \"thread-1\"}}\n    async for event in graph.astream_events(10, config, version=\"v1\"):\n        pprint(event)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(memory())  # <--- Works\n    asyncio.run(asyncSqlite())  # <--- Hangs here\nError Message and Stack Trace (if applicable)\nException ignored in: <module 'threading' from 'lib/python3.11/threading.py'>\nTraceback (most recent call last):\n  File \" lib/python3.11/threading.py\", line 1590, in _shutdown\n    lock.acquire()\nKeyboardInterrupt:\nDescription\nI am trying to add langraph with astream_events to fastapi and it hangs after the first request is complete. i was a able to isolate the issue using a cli tool\nSystem Info\nlangchain==0.1.20\nlangchain-community==0.0.38\nlangchain-core==0.1.52\nlangchain-openai==0.1.6\nlangchain-text-splitters==0.0.1\nopeninference-instrumentation-langchain==0.1.14", "created_at": "2024-06-24", "closed_at": "2024-06-27", "labels": [], "State": "closed", "Author": "ewianda"}
{"issue_number": 775, "issue_title": "MemorySaver doesn't store checkpoints in descending order by timestamp", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom pprint import pprint\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph\n\nbuilder = StateGraph(int)\nbuilder.add_node(\"add_one\", lambda x: x + 1)\nbuilder.set_entry_point(\"add_one\")\nbuilder.set_finish_point(\"add_one\")\n\nmemory = MemorySaver()\ngraph = builder.compile(checkpointer=memory)\nconfig = {\"configurable\": {\"thread_id\": \"thread-1\"}}\n\npprint(graph.invoke(1, config))\npprint(list(graph.get_state_history(config)))\npprint(list(memory.list(config)))\nprint(memory.list.__doc__)\nError Message and Stack Trace (if applicable)\nNo response\nDescription\n\nI'm trying to reproduce the example from the home page of the LangGraph documentation\nI expect to see that MemorySaver stores checkpoints in descending order by timestamp as described in the docstring of the list method\nInstead, it is not stored in descending order by timestamp, but probably insertion order of defaultdict is used, because MemorySaver's storage is currently based on defaultdict and there is no explicit sorting of the states (which is used, for example, by SqliteSaver)\n\nSystem Info\nPackage Information\n\nlangchain_core: 0.2.9\nlangchain: 0.2.5\nlangchain_community: 0.2.5\nlangsmith: 0.1.81\nlangchain_experimental: 0.0.61\nlangchain_text_splitters: 0.2.1\nlangchainhub: 0.1.20\nlanggraph: 0.1.1\nlangserve: 0.2.2\n", "created_at": "2024-06-23", "closed_at": "2024-08-24", "labels": [], "State": "closed", "Author": "labdmitriy"}
{"issue_number": 766, "issue_title": "langgraph.errors.InvalidUpdateError: Invalid update for channel medical with values", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nHere is mine workflow:\ndef workflow(one:dict):\n    workflow = StateGraph(GraphState)\n    # Define the nodes\n    workflow.add_node(\"generate_new_extraction\", generate_new_extraction)  \n\n    workflow.add_node(\"medical_mentor\", medical_mentor)  # check imports\n\n    workflow.add_node(\"medical_mentor_pass\", medical_mentor_pass)  # \u7528\u6765\u8fc7\u6ee4\u4e8b\u4ef6\u62bd\u53d6\u7684agent\n\n\n    # Build graph\n    workflow.set_entry_point(\"generate_new_extraction\")\n\n    workflow.add_edge(\"generate_new_extraction\", \"medical_mentor\")\n    workflow.add_edge(\"medical_mentor\", \"medical_mentor_pass\")\n    workflow.add_edge(\"medical_mentor_pass\", END)\n\n    workflow.add_conditional_edges(\n        \"medical_mentor\",\n        decide_to_finish,\n        {\n            \"medical_mentor_pass\": \"medical_mentor_pass\",\n            \"generate_new_extraction\": \"generate_new_extraction\",\n        },\n    )\n    app = workflow.compile()\n    sss = app.invoke({\"medical\":{'dialogue':dialogue,'extrction':extrction,'medical_record':medical_record,'medical_iterations':0}})\n  \ngenerate_new_extraction return :\nreturn {\n        \"medical\": {\"new_generation\": medical_solution,'dialogue':dialogue,'extrction':extrction,'medical_record':medical_record,'medical_iterations':medical_iter_num}\n    }\n\nmedical_mentor return:\nreturn {\n        \"medical\": {\n            \"feedback\": mentor['feedback'],\n            \"correct\": mentor['correct'],\n            \"new_generation\": generate_new_extraction_1,\n            \"dialogue\": dialogue,\n            \"extrction\": extrction,\n            \"medical_record\": medical_record,\n            \"medical_iterations\": medical_iter_num,\n        }\n    }\n\nmedical_mentor_pass return :\nreturn {\n        \"medical\": {\n            \"feedback\":'\u5b8c\u6210\u8fc7\u6ee4',\n            \"correct\": True,\n            \"new_generation\": str(mentor.content),\n            \"dialogue\": dialogue,\n            \"extrction\": extrction,\n            \"medical_record\": medical_record,\n            \"medical_iterations\": medical_iter_num,\n        }\n    }\n\n\nWhere the error is reported:\nfor chan, vals in pending_writes_by_channel.items():\n        if chan in channels:\n            try:\n                \n                # if len(vals) > 1:\n                #     vals = [vals[-1]]\n                # pdb.set_trace()\n                # if chan== 'medical' and len(vals)>1:\n                #     vals=  [vals[-1]]\n                channels[chan].update(vals)\n\n            except InvalidUpdateError as e:\n                \n                pdb.set_trace()\n                raise InvalidUpdateError(\n                    f\"Invalid update for channel {chan} with values {vals}\"\n                ) from e\n            checkpoint[\"channel_versions\"][chan] = max_version + 1\n            updated_channels.add(chan)\n\n\nCause of error:langgraph.errors.InvalidUpdateError: Invalid update for channel medical with values\nError Message and Stack Trace (if applicable)\nlanggraph.errors.InvalidUpdateError: Invalid update for channel medical with values\n File \"/Users/wushiqi/anaconda3/envs/01bot/lib/python3.9/site-packages/langgraph/pregel/__init__.py\", line 978, in stream\n    _apply_writes(checkpoint, channels, pending_writes)\n  File \"/Users/wushiqi/anaconda3/envs/01bot/lib/python3.9/site-packages/langgraph/pregel/__init__.py\", line 1600, in _apply_writes\n    raise InvalidUpdateError(\nlanggraph.errors.InvalidUpdateError: Invalid update for channel medical with values [{'new_generation': '\\n{\\n  \"\u75c7\u72b6\u4f53\u5f81\": [\\n    {\\n      \"\u540d\u79f0\": \"\u54b3\u55fd\",\\n      \"\u6301\u7eed\u65f6\u95f4\": \"\u4e24\u5929\",\\n      \"\u8be6\u7ec6\u63cf\u8ff0\": \"\u60a3\u513f\u6301\u7eed\u54b3\u55fd\uff0c\u6709\u75f0\uff0c\u767d\u5929\u665a\u4e0a\u90fd\u4f1a\u54b3\uff0c\u4f46\u54b3\u55fd\u4e0d\u7b97\u5389\u5bb3\u3002\",\\n      \"\u4e25\u91cd\u7a0b\u5ea6\": \"\u8f7b\u5fae\",\\n      \"\u9891\u7387\": \"\u9891\u7e41\",\\n      \"\u90e8\u4f4d\": \"\u547c\u5438\u9053\"\\n    }\\n  ],\\n  \"\u68c0\u67e5\u68c0\u9a8c\": [],\\n  \"\u836f\u54c1\": [\\n    {\\n      \"\u540d\u79f0\": \"\u6d88\u708e\u836f\",\\n      \"\u5242\u91cf\": \"\u6839\u636e\u533b\u751f\u5efa\u8bae\",\\n      \"\u8be6\u7ec6\u63cf\u8ff0\": \"\u7528\u4e8e\u6cbb\u7597\u60a3\u513f\u7684\u54b3\u55fd\u75c7\u72b6\",\\n      \"\u9891\u6b21\": \"\u6bcf\u65e5\u6309\u7167\u533b\u5631\"\\n    },\\n    {\\n      \"\u540d\u79f0\": \"\u5316\u75f0\u6b62\u54b3\u836f\",\\n      \"\u5242\u91cf\": \"\u6839\u636e\u533b\u751f\u5efa\u8bae\",\\n      \"\u8be6\u7ec6\u63cf\u8ff0\": \"\u5e2e\u52a9\u60a3\u513f\u7f13\u89e3\u54b3\u55fd\u548c\u6392\u75f0\",\\n      \"\u9891\u6b21\": \"\u6bcf\u65e5\u6309\u7167\u533b\u5631\"\\n    }\\n  ],\\n  \"\u624b\u672f\": [],\\n  \"\u8bca\u65ad\": [\\n    {\\n      \"\u540d\u79f0\": \"\u54b3\u55fd\u5f85\u67e5\",\\n      \"\u8be6\u7ec6\u63cf\u8ff0\": \"\u6839\u636e\u60a3\u513f\u7684\u75c7\u72b6\u548c\u4f53\u5f81\uff0c\u5f85\u8fdb\u4e00\u6b65\u68c0\u67e5\u548c\u8bca\u65ad\",\\n      \"\u6301\u7eed\u65f6\u95f4\": \"\u5f85\u67e5\"\\n    }\\n  ]\\n}\\n', 'dialogue': '\u533b\u751f:\u4f60\u597d\uff0c\u5bb6\u957f\u5b9d\u5b9d\u7684\u54b3\u55fd\u60c5\u51b5\u600e\u4e48\u6837\uff1f\u591a\u5927\u7684\u5b69\u5b50\\n\u60a3\u8005:3\u5c81\u4e86\uff0c\u4e0d\u600e\u4e48\u54b3\u4e86\uff0c\u6709\u75f0\\n\u533b\u751f:\u767d\u5929\u54b3\u8fd8\u662f\u665a\u4e0a\u54b3\u554a\\n\u533b\u751f:\u6ca1\u6709\u9f3b\u585e\u6d41\u9f3b\u6d95\u6253\u55b7\u568f\u5440\\n\u60a3\u8005:\u6ca1\u6709\uff0c\u767d\u5929\u665a\u4e0a\u90fd\u4f1a\\n\u533b\u751f:\u96fe\u5316\u4e0d\u662f\u516c\u7528\u7684\u5440\uff0c\u90fd\u6709\u6d88\u6bd2\u7684\uff0c\u8fd9\u4e2a\u4e0d\u8981\u62c5\u5fc3\u3002\\n\u60a3\u8005:\u54b3\u7684\u4e0d\u7b97\u5389\u5bb3\\n\u60a3\u8005:\u80fd\u6d88\u6bd2\u5e72\u51c0\u554a\uff0c\u9547\u4e0a\u7684\u536b\u751f\u9662\\n\u533b\u751f:\u54b3\u55fd\u6709\u75f0\u6ca1\u6709\u9f3b\u585e\u6d41\u9f3b\u6d95\u6253\u55b7\u568f\u7684\u60c5\u51b5\u8003\u8651\u662f\u652f\u6c14\u7ba1\u708e\uff0c\u75f0\u662f\u708e\u75c7\u7684\u5206\u6ccc\u7269\u3002\\n\u60a3\u8005:\u73b0\u5728\u4e00\u822c\u4e0d\u90fd\u662f\u4e00\u6b21\u6027\u7ba1\u5b50\u4e86\u5417\uff0c\u600e\u4e48\u8fd8\u6709\u8fd9\u79cd\u516c\u7528\u7684\\n\u533b\u751f:\u533b\u9662\u5e94\u8be5\u6d88\u6bd2\u5c31\u662f\u5e72\u51c0\u7684\u5440\u3002\\n\u533b\u751f:\u73b0\u5728\u4e00\u822c\u7684\u90fd\u662f\u81ea\u5df1\u4e70\u4e00\u4e2a\uff0c\u5982\u679c\u6ca1\u6709\u7684\u8bdd\uff0c\u516c\u7528\u7684\uff0c\u4e5f\u4f1a\u8fdb\u884c\u6d88\u6bd2\u3002\\n\u60a3\u8005:\u536b\u751f\u9662\u4e5f\u7b97\u662f\u533b\u9662\u5427\\n\u533b\u751f:\u55ef\u55ef\\n\u60a3\u8005:\u90a3\u91cc\u6ca1\u6709\u81ea\u5df1\u4e70\u7684\\n\u533b\u751f:\u90a3\u5c31\u6ca1\u6709\u5173\u7cfb\u554a\uff0c\u4ed6\u4eec\u80af\u5b9a\u4f1a\u6d88\u6bd2\u7684\uff0c\u53ea\u8981\u662f\u770b\u75c5\u7684\u5730\u65b9\u6d88\u6bd2\u90fd\u4f1a\u505a\u5230\u7684\u3002\\n\u533b\u751f:\u4e0d\u8981\u62c5\u5fc3\u3002\\n\u60a3\u8005:\u6240\u4ee5\u6211\u542c\u5230\u5fc3\u91cc\u90fd\u96be\u53d7\u7684\u4e0d\u884c\uff0c\u5c31\u6015\u4e0d\u5e72\u51c0\uff0c\u5f97\u4ec0\u4e48\u4f20\u67d3\u75c5\\n\u533b\u751f:\u7406\u89e3\\n\u533b\u751f:\u8fd8\u6ca1\u6709\u542c\u8bf4\u96fe\u5316\u6709\u8fc7\u4f20\u67d3\uff0c\u90a3\u786e\u5b9e\u561b\uff0c\u4ee5\u524d\u662f\u96c6\u4f53\u6d88\u6bd2\uff0c\u73b0\u5728\u90fd\u662f\u6bcf\u4eba\u4e70\u4e00\u4e2a\u3002\\n\u60a3\u8005:\u597d\u7684\uff0c\u8c22\u8c22\u533b\u751f\\n\u533b\u751f:\u90a3\u73b0\u5728\u54b3\u55fd\u6709\u75f0\uff0c\u52a0\u5f3a\u4fdd\u6696\uff0c\u591a\u559d\u6c34\uff0c\u52e4\u62cd\u80cc\u3002\\n\u533b\u751f:\u53ef\u4ee5\u53e3\u670d\u4e00\u70b9\u6d88\u708e\u836f\u548c\u5316\u75f0\u6b62\u54b3\u836f\u3002', 'extrction': '{\\n  \"\u75c7\u72b6\u4f53\u5f81\": [\\n    {\\n      \"\u540d\u79f0\": \"\u54b3\u55fd\",\\n      \"\u6301\u7eed\u65f6\u95f4\": \"\u4e24\u5929\",\\n      \"\u8be6\u7ec6\u63cf\u8ff0\": \"\u60a3\u513f\u6709\u54b3\u55fd\u75c7\u72b6\uff0c\u6709\u75f0\uff0c\u767d\u5929\u665a\u4e0a\u90fd\u4f1a\u54b3\uff0c\u4f46\u4e0d\u7b97\u5389\u5bb3\u3002\",\\n      \"\u4e25\u91cd\u7a0b\u5ea6\": \"\u4e0d\u7b97\u5389\u5bb3\"\\n    }\\n  ],\\n  \"\u68c0\u67e5\u68c0\u9a8c\": [],\\n  \"\u836f\u54c1\": [\\n    {\\n      \"\u540d\u79f0\": \"\u6d88\u708e\u836f\",\\n      \"\u8be6\u7ec6\u63cf\u8ff0\": \"\u53e3\u670d\u6d88\u708e\u836f\uff0c\u7528\u4e8e\u6cbb\u7597\u60a3\u513f\u7684\u54b3\u55fd\u75c7\u72b6\u3002\",\\n      \"\u5242\u91cf\": \"\u672a\u77e5\",\\n      \"\u9891\u6b21\": \"\u672a\u77e5\"\\n    },\\n    {\\n      \"\u540d\u79f0\": \"\u5316\u75f0\u6b62\u54b3\u836f\",\\n      \"\u8be6\u7ec6\u63cf\u8ff0\": \"\u53e3\u670d\u5316\u75f0\u6b62\u54b3\u836f\uff0c\u5e2e\u52a9\u60a3\u513f\u7f13\u89e3\u54b3\u55fd\u548c\u6392\u75f0\u3002\",\\n      \"\u5242\u91cf\": \"\u672a\u77e5\",\\n      \"\u9891\u6b21\": \"\u672a\u77e5\"\\n    }\\n  ],\\n  \"\u624b\u672f\": [],\\n  \"\u8bca\u65ad\": [\\n    {\\n      \"\u540d\u79f0\": \"\u652f\u6c14\u7ba1\u708e\",\\n      \"\u8be6\u7ec6\u63cf\u8ff0\": \"\u6839\u636e\u60a3\u513f\u7684\u75c7\u72b6\uff0c\u533b\u751f\u521d\u6b65\u8bca\u65ad\u4e3a\u652f\u6c14\u7ba1\u708e\u3002\",\\n      \"\u6301\u7eed\u65f6\u95f4\": \"\u672a\u77e5\"\\n    }\\n  ]\\n}', 'medical_record': '\u4e3b\u8bc9:\u54b3\u55fd\u4e24\u5929\u3002\\n\u73b0\u75c5\u53f2:\u60a3\u513f\u54b3\u55fd\u4e24\u5929\uff0c\u505a\u8fc7\u96fe\u5316\u3002\\n\u8f85\u52a9\u68c0\u67e5:\u6682\u7f3a\u3002\\n\u65e2\u5f80\u53f2:\u4e0d\u8be6\u3002\\n\u8bca\u65ad:\u54b3\u55fd\u5f85\u67e5\u3002\\n\u5efa\u8bae:\u6d88\u708e\u836f\uff0c\u5316\u75f0\u6b62\u54b3\u836f\uff0c\u4fdd\u6696\uff0c\u591a\u559d\u6c34\uff0c\u52e4\u62cd\u80cc\u3002', 'medical_iterations': 2}, {'feedback': '\u5b8c\u6210\u8fc7\u6ee4', 'correct': True, 'new_generation': '\u4e8b\u4ef6\u62bd\u53d6\uff1a\\n\\n{\\n  \"\u75c7\u72b6\u4f53\u5f81\": [\\n    {\\n      \"\u540d\u79f0\": \"\u54b3\u55fd\",\\n      \"\u6301\u7eed\u65f6\u95f4\": \"\u4e24\u5929\",\\n      \"\u8be6\u7ec6\u63cf\u8ff0\": \"\u60a3\u513f\u6301\u7eed\u54b3\u55fd\uff0c\u6709\u75f0\uff0c\u767d\u5929\u665a\u4e0a\u90fd\u4f1a\u54b3\uff0c\u4f46\u54b3\u55fd\u4e0d\u7b97\u5389\u5bb3\u3002\",\\n      \"\u4e25\u91cd\u7a0b\u5ea6\": \"\u8f7b\u5fae\",\\n      \"\u9891\u7387\": \"\u9891\u7e41\",\\n      \"\u90e8\u4f4d\": \"\u547c\u5438\u9053\"\\n    }\\n  ],\\n  \"\u68c0\u67e5\u68c0\u9a8c\": [],\\n  \"\u836f\u54c1\": [\\n    {\\n      \"\u540d\u79f0\": \"\u6d88\u708e\u836f\",\\n      \"\u5242\u91cf\": \"\u672a\u77e5\",\\n      \"\u8be6\u7ec6\u63cf\u8ff0\": \"\u7528\u4e8e\u6cbb\u7597\u60a3\u513f\u7684\u54b3\u55fd\u75c7\u72b6\",\\n      \"\u9891\u6b21\": \"\u672a\u77e5\"\\n    },\\n    {\\n      \"\u540d\u79f0\": \"\u5316\u75f0\u6b62\u54b3\u836f\",\\n      \"\u5242\u91cf\": \"\u672a\u77e5\",\\n      \"\u8be6\u7ec6\u63cf\u8ff0\": \"\u5e2e\u52a9\u60a3\u513f\u7f13\u89e3\u54b3\u55fd\u548c\u6392\u75f0\",\\n      \"\u9891\u6b21\": \"\u672a\u77e5\"\\n    }\\n  ],\\n  \"\u624b\u672f\": [],\\n  \"\u8bca\u65ad\": [\\n    {\\n      \"\u540d\u79f0\": \"\u652f\u6c14\u7ba1\u708e\",\\n      \"\u8be6\u7ec6\u63cf\u8ff0\": \"\u6839\u636e\u60a3\u513f\u7684\u75c7\u72b6\u548c\u4f53\u5f81\uff0c\u533b\u751f\u8bca\u65ad\u4e3a\u652f\u6c14\u7ba1\u708e\",\\n      \"\u6301\u7eed\u65f6\u95f4\": \"\u672a\u77e5\uff08\u6025\u6027\u6216\u6162\u6027\u5f85\u67e5\uff09\"\\n    }\\n  ]\\n}\\n\\n\\n\u662f\u5426\u4fee\u6539\uff0c\u539f\u56e0\uff1a\\n- \u4fee\u6539\u4e86\u201c\u75c7\u72b6\u4f53\u5f81\u201d\u4e2d\u7684\u201c\u989c\u8272\u201d\u5b57\u6bb5\uff0c\u56e0\u4e3a\u5bf9\u8bdd\u4e2d\u6ca1\u6709\u63d0\u53ca\u75f0\u7684\u989c\u8272\uff0c\u6240\u4ee5\u5c06\u5176\u5220\u9664\u3002\\n- \u5176\u4ed6\u5b57\u6bb5\u4fdd\u6301\u4e0d\u53d8\uff0c\u56e0\u4e3a\u5b83\u4eec\u90fd\u662f\u6839\u636e\u5bf9\u8bdd\u548c\u7535\u5b50\u75c5\u5386\u4e2d\u7684\u4fe1\u606f\u51c6\u786e\u63d0\u53d6\u7684\u3002', 'dialogue': '\u533b\u751f:\u4f60\u597d\uff0c\u5bb6\u957f\u5b9d\u5b9d\u7684\u54b3\u55fd\u60c5\u51b5\u600e\u4e48\u6837\uff1f\u591a\u5927\u7684\u5b69\u5b50\\n\u60a3\u8005:3\u5c81\u4e86\uff0c\u4e0d\u600e\u4e48\u54b3\u4e86\uff0c\u6709\u75f0\\n\u533b\u751f:\u767d\u5929\u54b3\u8fd8\u662f\u665a\u4e0a\u54b3\u554a\\n\u533b\u751f:\u6ca1\u6709\u9f3b\u585e\u6d41\u9f3b\u6d95\u6253\u55b7\u568f\u5440\\n\u60a3\u8005:\u6ca1\u6709\uff0c\u767d\u5929\u665a\u4e0a\u90fd\u4f1a\\n\u533b\u751f:\u96fe\u5316\u4e0d\u662f\u516c\u7528\u7684\u5440\uff0c\u90fd\u6709\u6d88\u6bd2\u7684\uff0c\u8fd9\u4e2a\u4e0d\u8981\u62c5\u5fc3\u3002\\n\u60a3\u8005:\u54b3\u7684\u4e0d\u7b97\u5389\u5bb3\\n\u60a3\u8005:\u80fd\u6d88\u6bd2\u5e72\u51c0\u554a\uff0c\u9547\u4e0a\u7684\u536b\u751f\u9662\\n\u533b\u751f:\u54b3\u55fd\u6709\u75f0\u6ca1\u6709\u9f3b\u585e\u6d41\u9f3b\u6d95\u6253\u55b7\u568f\u7684\u60c5\u51b5\u8003\u8651\u662f\u652f\u6c14\u7ba1\u708e\uff0c\u75f0\u662f\u708e\u75c7\u7684\u5206\u6ccc\u7269\u3002\\n\u60a3\u8005:\u73b0\u5728\u4e00\u822c\u4e0d\u90fd\u662f\u4e00\u6b21\u6027\u7ba1\u5b50\u4e86\u5417\uff0c\u600e\u4e48\u8fd8\u6709\u8fd9\u79cd\u516c\u7528\u7684\\n\u533b\u751f:\u533b\u9662\u5e94\u8be5\u6d88\u6bd2\u5c31\u662f\u5e72\u51c0\u7684\u5440\u3002\\n\u533b\u751f:\u73b0\u5728\u4e00\u822c\u7684\u90fd\u662f\u81ea\u5df1\u4e70\u4e00\u4e2a\uff0c\u5982\u679c\u6ca1\u6709\u7684\u8bdd\uff0c\u516c\u7528\u7684\uff0c\u4e5f\u4f1a\u8fdb\u884c\u6d88\u6bd2\u3002\\n\u60a3\u8005:\u536b\u751f\u9662\u4e5f\u7b97\u662f\u533b\u9662\u5427\\n\u533b\u751f:\u55ef\u55ef\\n\u60a3\u8005:\u90a3\u91cc\u6ca1\u6709\u81ea\u5df1\u4e70\u7684\\n\u533b\u751f:\u90a3\u5c31\u6ca1\u6709\u5173\u7cfb\u554a\uff0c\u4ed6\u4eec\u80af\u5b9a\u4f1a\u6d88\u6bd2\u7684\uff0c\u53ea\u8981\u662f\u770b\u75c5\u7684\u5730\u65b9\u6d88\u6bd2\u90fd\u4f1a\u505a\u5230\u7684\u3002\\n\u533b\u751f:\u4e0d\u8981\u62c5\u5fc3\u3002\\n\u60a3\u8005:\u6240\u4ee5\u6211\u542c\u5230\u5fc3\u91cc\u90fd\u96be\u53d7\u7684\u4e0d\u884c\uff0c\u5c31\u6015\u4e0d\u5e72\u51c0\uff0c\u5f97\u4ec0\u4e48\u4f20\u67d3\u75c5\\n\u533b\u751f:\u7406\u89e3\\n\u533b\u751f:\u8fd8\u6ca1\u6709\u542c\u8bf4\u96fe\u5316\u6709\u8fc7\u4f20\u67d3\uff0c\u90a3\u786e\u5b9e\u561b\uff0c\u4ee5\u524d\u662f\u96c6\u4f53\u6d88\u6bd2\uff0c\u73b0\u5728\u90fd\u662f\u6bcf\u4eba\u4e70\u4e00\u4e2a\u3002\\n\u60a3\u8005:\u597d\u7684\uff0c\u8c22\u8c22\u533b\u751f\\n\u533b\u751f:\u90a3\u73b0\u5728\u54b3\u55fd\u6709\u75f0\uff0c\u52a0\u5f3a\u4fdd\u6696\uff0c\u591a\u559d\u6c34\uff0c\u52e4\u62cd\u80cc\u3002\\n\u533b\u751f:\u53ef\u4ee5\u53e3\u670d\u4e00\u70b9\u6d88\u708e\u836f\u548c\u5316\u75f0\u6b62\u54b3\u836f\u3002', 'extrction': '{\\n  \"\u75c7\u72b6\u4f53\u5f81\": [\\n    {\\n      \"\u540d\u79f0\": \"\u54b3\u55fd\",\\n      \"\u6301\u7eed\u65f6\u95f4\": \"\u4e24\u5929\",\\n      \"\u8be6\u7ec6\u63cf\u8ff0\": \"\u60a3\u513f\u6709\u54b3\u55fd\u75c7\u72b6\uff0c\u6709\u75f0\uff0c\u767d\u5929\u665a\u4e0a\u90fd\u4f1a\u54b3\uff0c\u4f46\u4e0d\u7b97\u5389\u5bb3\u3002\",\\n      \"\u4e25\u91cd\u7a0b\u5ea6\": \"\u4e0d\u7b97\u5389\u5bb3\"\\n    }\\n  ],\\n  \"\u68c0\u67e5\u68c0\u9a8c\": [],\\n  \"\u836f\u54c1\": [\\n    {\\n      \"\u540d\u79f0\": \"\u6d88\u708e\u836f\",\\n      \"\u8be6\u7ec6\u63cf\u8ff0\": \"\u53e3\u670d\u6d88\u708e\u836f\uff0c\u7528\u4e8e\u6cbb\u7597\u60a3\u513f\u7684\u54b3\u55fd\u75c7\u72b6\u3002\",\\n      \"\u5242\u91cf\": \"\u672a\u77e5\",\\n      \"\u9891\u6b21\": \"\u672a\u77e5\"\\n    },\\n    {\\n      \"\u540d\u79f0\": \"\u5316\u75f0\u6b62\u54b3\u836f\",\\n      \"\u8be6\u7ec6\u63cf\u8ff0\": \"\u53e3\u670d\u5316\u75f0\u6b62\u54b3\u836f\uff0c\u5e2e\u52a9\u60a3\u513f\u7f13\u89e3\u54b3\u55fd\u548c\u6392\u75f0\u3002\",\\n      \"\u5242\u91cf\": \"\u672a\u77e5\",\\n      \"\u9891\u6b21\": \"\u672a\u77e5\"\\n    }\\n  ],\\n  \"\u624b\u672f\": [],\\n  \"\u8bca\u65ad\": [\\n    {\\n      \"\u540d\u79f0\": \"\u652f\u6c14\u7ba1\u708e\",\\n      \"\u8be6\u7ec6\u63cf\u8ff0\": \"\u6839\u636e\u60a3\u513f\u7684\u75c7\u72b6\uff0c\u533b\u751f\u521d\u6b65\u8bca\u65ad\u4e3a\u652f\u6c14\u7ba1\u708e\u3002\",\\n      \"\u6301\u7eed\u65f6\u95f4\": \"\u672a\u77e5\"\\n    }\\n  ]\\n}', 'medical_record': '\u4e3b\u8bc9:\u54b3\u55fd\u4e24\u5929\u3002\\n\u73b0\u75c5\u53f2:\u60a3\u513f\u54b3\u55fd\u4e24\u5929\uff0c\u505a\u8fc7\u96fe\u5316\u3002\\n\u8f85\u52a9\u68c0\u67e5:\u6682\u7f3a\u3002\\n\u65e2\u5f80\u53f2:\u4e0d\u8be6\u3002\\n\u8bca\u65ad:\u54b3\u55fd\u5f85\u67e5\u3002\\n\u5efa\u8bae:\u6d88\u708e\u836f\uff0c\u5316\u75f0\u6b62\u54b3\u836f\uff0c\u4fdd\u6696\uff0c\u591a\u559d\u6c34\uff0c\u52e4\u62cd\u80cc\u3002', 'medical_iterations': 1}]\nDescription\nI did some tests: I found that when chan='medical', after completing the mentor_pass task, len(vals)==2, which is the reason for the error.\nIf I force the switch\nif chan== 'medical' and len(vals)>1:\nvals= [vals[-1]]\nIt can run, but it will always loop in medical_mentor and medical_mentor_pass and cannot jump out of end. Why is this?\nSystem Info\nlanggraph==0.0.66", "created_at": "2024-06-22", "closed_at": null, "labels": ["stale"], "State": "open", "Author": "xtu-xiaoc"}
{"issue_number": 740, "issue_title": "langgraph.errors.InvalidUpdateError: Must write to at least one of ['input', 'plan', 'past_steps', 'response']", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nasync def execute_step(state: PlanExecute):\n    objective = state[\"input\"]\n    task = state[\"plan\"][0]\n    agent_response = await agent_executor.ainvoke({\"objective\": objective, \"input\": task, \"chat_history\": []})\n    return {\n        \"past_steps\": [(task, agent_response[\"result\"])],\n    }\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\n  File \"D:\\githcoalagent\\demo3\\demo3_zysss.py\", line 118, in <module>\n    asyncio.run(handle_events(inputs, config))\n  File \"D:\\conda-pachaall-llm\\lib\\asyncio\\runners.py\", line 44, in run\n    return loop.run_until_complete(main)\n  File \"D:\\conda-pach-llm\\lib\\asyncio\\base_events.py\", line 649, in run_until_complete\n    return future.result()\n  File \"D:\\glagent\\demo3\\demo3_zysss.py\", line 107, in handle_events\n    async for event in app.astream(inputs, config=config):\n  File \"D:\\conda-pachage\\envs\\recall-llm\\lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 1333, in astream\n    _panic_or_proceed(done, inflight, step)\n  File \"D:\\conda-pachage\\envs\\recall-llm\\lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 1537, in _panic_or_proceed\n    raise exc\n  File \"D:\\conda-pachage\\envs\\recall-llm\\lib\\site-packages\\langgraph\\pregel\\retry.py\", line 120, in arun_with_retry\n    await task.proc.ainvoke(task.input, task.config)\n  File \"D:\\conda-pachage\\envs\\recall-llm\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2542, in ainvoke\n    input = await step.ainvoke(input, config)\n  File \"D:\\conda-pachage\\envs\\recall-llm\\lib\\site-packages\\langgraph\\utils.py\", line 107, in ainvoke\n    ret = await self._acall_with_config(\n  File \"D:\\conda-pachage\\envs\\recall-llm\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 1650, in _acall_with_config\n    output = await coro\n  File \"D:\\conda-pachageb\\site-packages\\langgraph\\pregel\\write.py\", line 141, in _awrite\n    self.do_write(\n  File \"D:\\conggraph\\pregel\\write.py\", line 157, in do_write\n    raise InvalidUpdateError(\nlanggraph.errors.InvalidUpdateError: Must write to at least one of ['input', 'plan', 'past_steps', 'response']\nDescription\nFile \"D:\\conda-pachage\\envs\\recall-llm\\lib\\site-packages\\langgraph\\pregel\\write.py\", line 141, in _awrite\nself.do_write(\nFile \"D:\\conda-pachage\\envs\\recall-llm\\lib\\site-packages\\langgraph\\pregel\\write.py\", line 157, in do_write\nraise InvalidUpdateError(\nlanggraph.errors.InvalidUpdateEr\nSystem Info\nFile \"D:\\conda-pachage\\envs\\recall-llm\\lib\\site-packages\\langgraph\\pregel\\write.py\", line 141, in _awrite\nself.do_write(\nFile \"D:\\conda-pachage\\envs\\recall-llm\\lib\\site-packages\\langgraph\\pregel\\write.py\", line 157, in do_write\nraise InvalidUpdateError(\nlanggraph.errors.InvalidUpdateEr", "created_at": "2024-06-21", "closed_at": null, "labels": [], "State": "open", "Author": "cristianohello"}
{"issue_number": 721, "issue_title": "Possible error in `update_state` Deciding last updating node because of sorting null values", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nelif as_node is None:\n            last_seen_by_node = sorted(\n                (v, n)\n                for n, seen in checkpoint[\"versions_seen\"].items()\n                for v in seen.values()\n            )\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nThis code is taken from https://github.com/langchain-ai/langgraph/blob/51dbbc71b3dfadbc94bd8a8faf8f6fb0b37f63a3/libs/langgraph/langgraph/pregel/__init__.py#L525C1-L530C14 is likely to ignore any node mapped to empty. For instance\n{'__start__': {'__start__': '00000000000000000000000000000001.ffee86dada62e095026eb3583d4506fa'},\n 'chatbot': {'start:chatbot': '00000000000000000000000000000002.d6f25946c3108fc12f27abbcf9b4cedc'},\n 'tools': {}}\n\nSystem Info\nlanggraph==0.0.69", "created_at": "2024-06-20", "closed_at": null, "labels": [], "State": "open", "Author": "xtfocus"}
{"issue_number": 720, "issue_title": "langgraph tool calls not working", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom typing import Literal\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\n\n\n@tool\ndef get_weather(city: Literal[\"nyc\", \"sf\"]):\n    \"\"\"Use this to get weather information.\"\"\"\n    if city == \"nyc\":\n        return \"It might be cloudy in nyc\"\n    elif city == \"sf\":\n        return \"It's always sunny in sf\"\n    else:\n        raise AssertionError(\"Unknown city\")\n\n\ntools = [get_weather]\n\nmodel = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0, base_url=\"https://xxxx.xxx/v1/\",\n                   api_key=\"sk-xxxxx\")\n\n\ndef print_stream(stream):\n    for s in stream:\n        message = s[\"messages\"][-1]\n        if isinstance(message, tuple):\n            print(message)\n        else:\n            message.pretty_print()\n\n\ngraph = create_react_agent(model, tools=tools, debug=True)\ninputs = {\"messages\": [(\"user\", \"what is the weather in sf?\")]}\nprint_stream(graph.stream(inputs, stream_mode=\"values\"))\nError Message and Stack Trace (if applicable)\n[0:tasks] Starting step 0 with 1 task:\n- __start__ -> {'messages': [('user', 'what is the weather in sf?')]}\n[0:writes] Finished step 0 with writes to 1 channel:\n- messages -> [('user', 'what is the weather in sf?')]\n================================ Human Message =================================\n\nwhat is the weather in sf?\n[1:tasks] Starting step 1 with 1 task:\n- agent -> {'is_last_step': False,\n 'messages': [HumanMessage(content='what is the weather in sf?', id='9c5bb8bd-cef8-454f-93f1-69877b6916c3')]}\n[1:writes] Finished step 1 with writes to 1 channel:\n- messages -> [AIMessage(content='\\n> search(\"current weather in San Francisco\")Currently, in San Francisco, the weather is sunny with a temperature around 70\u00b0F. The wind is coming from the west, with speeds ranging from 13 to 18 mph and gusts reaching up to 28 mph. Tonight, the sky will remain mostly clear, with temperatures dropping to a low around 54\u00b0F. The winds will continue from the west, with speeds between 8 to 18 mph and gusts as high as 30 mph .', response_metadata={'token_usage': {'completion_tokens': 103, 'prompt_tokens': 14, 'total_tokens': 117}, 'model_name': 'gpt-4-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-d5f92165-9e4e-4b79-a8a5-31bc1cce3afa-0', usage_metadata={'input_tokens': 14, 'output_tokens': 103, 'total_tokens': 117})]\n================================== Ai Message ==================================\n\n\n> search(\"current weather in San Francisco\")Currently, in San Francisco, the weather is sunny with a temperature around 70\u00b0F. The wind is coming from the west, with speeds ranging from 13 to 18 mph and gusts reaching up to 28 mph. Tonight, the sky will remain mostly clear, with temperatures dropping to a low around 54\u00b0F. The winds will continue from the west, with speeds between 8 to 18 mph and gusts as high as 30 mph .\nDescription\ni'm trying to create react agent as documents:https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/\nbut it's still answer from llm not the tools.\nSystem Info\n\nOS:  Windows\nOS Version:  10.0.22000\nPython Version:  3.11.8 | packaged by Anaconda, Inc. | (main, Feb 26 2024, 21:34:05) [MSC v.1916 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.2.9\nlangchain: 0.2.1\nlangchain_community: 0.0.19\nlangsmith: 0.1.81\nlangchain_experimental: 0.0.47\nlangchain_openai: 0.1.8\nlangchain_text_splitters: 0.2.0\nlanggraph: 0.0.69\n", "created_at": "2024-06-20", "closed_at": "2024-07-08", "labels": [], "State": "closed", "Author": "lastrei"}
{"issue_number": 712, "issue_title": "langgraph up not working on windows because of os.killpg", "issue_body": "Example Code\nlanggraph up\nError Message and Stack Trace (if applicable)\nFile \"C:\\Users\\geoff\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 654, in run_until_complete\nreturn future.result()\n^^^^^^^^^^^^^^^\nFile \"c:\\Users\\geoff\\OneDrive\\Documents\\GitHub\\pcd-data-eu-genai-poko-orch.venv\\Lib\\site-packages\\langgraph_cli\\exec.py\", line 103, in subp_exec\nos.killpg(os.getpgid(proc.pid), signal.SIGINT)\n^^^^^^^^^\nAttributeError: module 'os' has no attribute 'killpg'. Did you mean: 'kill'?\nException ignored in: <function BaseSubprocessTransport.del at 0x000001C170BA19E0>\nDescription\nHi all,\nWhile running langgraph up on windows i get the os.killpg error due to non support by windows.:\n\n\n\nlanggraph/libs/cli/langgraph_cli/exec.py\n\n\n         Line 103\n      in\n      19ff4a1\n\n\n\n\n\n\n os.killpg(os.getpgid(proc.pid), signal.SIGINT) \n\n\n\n\n\nI know it's a pain to support windows :)\nThanks for your help\nSystem Info\nPlatform windows:\nlanggraph==0.0.69\nlanggraph-cli==0.1.39", "created_at": "2024-06-19", "closed_at": "2024-06-20", "labels": [], "State": "closed", "Author": "Freezaa9"}
{"issue_number": 701, "issue_title": "Can not invoke/stream a compiled graph.", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom typing_extensions import TypedDict\nfrom langchain_community.chat_models import ChatOllama\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.output_parsers import JsonOutputParser, StrOutputParser\nfrom langgraph.graph import END, StateGraph\n\nMODEL_ID = 'qwen:32b-chat-v1.5-q8_0'\n\nllm = ChatOllama(model=MODEL_ID,\n                 temperature=0.,\n                 format='json')\n\nchat = ChatOllama(model=MODEL_ID,\n                 temperature=0.6)\n\nprompt_next_router = PromptTemplate(\n    template=\"\"\"You are a route planner, extracting the route from the user's input.\\n\n    There is only two route 'A' and 'B', extract only the uppercase letters 'A' and 'B'.\\n\n    Return an JSON object that contains only one key-value pair with the key being 'next'.\\n\n    user input\uff1a {question}\"\"\",\n    input_variables=[\"question\"],\n)\nnext_router = prompt_next_router | llm | JsonOutputParser()\n\nprompt_A = PromptTemplate(\n    template=\"\"\"Tell the user that the feature for Route A is still under development.\\n\n    user input\uff1a {question}\"\"\",\n    input_variables=[\"question\"],\n)\na_generator = prompt_A | chat | StrOutputParser()\n\nprompt_B = PromptTemplate(\n    template=\"\"\"Tell the user that the feature for Route B is still under development.\\n\n    user input\uff1a {question}\"\"\",\n    input_variables=[\"question\"],\n)\nb_generator = prompt_B | chat | StrOutputParser()\n\n# nodes\nclass GraphState(TypedDict):\n    \"\"\"\n    Graph\n\n    Attributes:\n        question: input\n        generation: response\n    \"\"\"\n    question: str\n    generation: str\n\ndef a_ex(state):\n    \"\"\"\n    a node\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): Updates generation key\n    \"\"\"\n    question = state[\"question\"]\n    response = a_generator.invoke({\"question\": question})\n\n    return {\"generation\": response}\n\ndef b_ex(state):\n    \"\"\"\n    b node\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): Updates generation key\n    \"\"\"\n    question = state[\"question\"]\n    response = b_generator.invoke({\"question\": question})\n\n    return {\"generation\": response}\n\n# next entry edge\ndef route_question(state):\n    \"\"\"\n    next node\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n    print(state)\n    question = state[\"question\"]\n    next = next_router.invoke({\"question\": question})\n    if next[\"next\"] == \"A\":\n        return \"A\"\n    elif next[\"next\"] == \"B\":\n        return \"B\"\n\nworkflow = StateGraph(GraphState)\n\n# Define the nodes\nworkflow.add_node(\"a\", a_ex)\nworkflow.add_node(\"b\", b_ex)\n\n# Build graph\nworkflow.set_conditional_entry_point(\n    next_router,\n    {\n        \"A\": \"a\",\n        \"B\": \"b\",\n    },\n)\n\nworkflow.add_edge(\"a\", END)\nworkflow.add_edge(\"b\", END)\n\n# Compile\nagent = workflow.compile()\n\nagent.invoke({'question': 'I choose route A.'})\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[13], line 1\n----> 1 agent.invoke({'question': 'I choose route A.'})\n      2 #next_router.invoke({'question': 'I choose route A.'})\n\nFile ~/anaconda3/envs/LLM/lib/python3.9/site-packages/langgraph/pregel/__init__.py:1384, in Pregel.invoke(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug, **kwargs)\n   1382 else:\n   1383     chunks = []\n-> 1384 for chunk in self.stream(\n   1385     input,\n   1386     config,\n   1387     stream_mode=stream_mode,\n   1388     output_keys=output_keys,\n   1389     input_keys=input_keys,\n   1390     interrupt_before=interrupt_before,\n   1391     interrupt_after=interrupt_after,\n   1392     debug=debug,\n   1393     **kwargs,\n   1394 ):\n   1395     if stream_mode == \"values\":\n   1396         latest = chunk\n\nFile ~/anaconda3/envs/LLM/lib/python3.9/site-packages/langgraph/pregel/__init__.py:949, in Pregel.stream(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\n    946         del fut, task\n    948 # panic on failure or timeout\n--> 949 _panic_or_proceed(done, inflight, step)\n    950 # don't keep futures around in memory longer than needed\n    951 del done, inflight, futures\n\nFile ~/anaconda3/envs/LLM/lib/python3.9/site-packages/langgraph/pregel/__init__.py:1473, in _panic_or_proceed(done, inflight, step)\n   1471             inflight.pop().cancel()\n   1472         # raise the exception\n-> 1473         raise exc\n   1475 if inflight:\n   1476     # if we got here means we timed out\n   1477     while inflight:\n   1478         # cancel all pending tasks\n\nFile ~/anaconda3/envs/LLM/lib/python3.9/concurrent/futures/thread.py:58, in _WorkItem.run(self)\n     55     return\n     57 try:\n---> 58     result = self.fn(*self.args, **self.kwargs)\n     59 except BaseException as exc:\n     60     self.future.set_exception(exc)\n\nFile ~/anaconda3/envs/LLM/lib/python3.9/site-packages/langgraph/pregel/retry.py:66, in run_with_retry(task, retry_policy)\n     64 task.writes.clear()\n     65 # run the task\n---> 66 task.proc.invoke(task.input, task.config)\n     67 # if successful, end\n     68 break\n\nFile ~/anaconda3/envs/LLM/lib/python3.9/site-packages/langchain_core/runnables/base.py:2504, in RunnableSequence.invoke(self, input, config, **kwargs)\n   2502             input = step.invoke(input, config, **kwargs)\n   2503         else:\n-> 2504             input = step.invoke(input, config)\n   2505 # finish the root run\n   2506 except BaseException as e:\n\nFile ~/anaconda3/envs/LLM/lib/python3.9/site-packages/langgraph/utils.py:95, in RunnableCallable.invoke(self, input, config, **kwargs)\n     93     if accepts_config(self.func):\n     94         kwargs[\"config\"] = config\n---> 95     ret = context.run(self.func, input, **kwargs)\n     96 if isinstance(ret, Runnable) and self.recurse:\n     97     return ret.invoke(input, config)\n\nFile ~/anaconda3/envs/LLM/lib/python3.9/site-packages/langgraph/graph/graph.py:81, in Branch._route(self, input, config, reader, writer)\n     79     value = input\n     80 result = self.path.invoke(value, config)\n---> 81 return self._finish(writer, input, result)\n\nFile ~/anaconda3/envs/LLM/lib/python3.9/site-packages/langgraph/graph/graph.py:108, in Branch._finish(self, writer, input, result)\n    106     result = [result]\n    107 if self.ends:\n--> 108     destinations = [r if isinstance(r, Send) else self.ends[r] for r in result]\n    109 else:\n    110     destinations = result\n\nFile ~/anaconda3/envs/LLM/lib/python3.9/site-packages/langgraph/graph/graph.py:108, in <listcomp>(.0)\n    106     result = [result]\n    107 if self.ends:\n--> 108     destinations = [r if isinstance(r, Send) else self.ends[r] for r in result]\n    109 else:\n    110     destinations = result\n\nTypeError: unhashable type: 'dict'\nDescription\nI create a simple Graph to test router with Ollama in local.\nThe graph is successfully compiled. Structure shows below.\n\nI can't invoke the graph to get response for the error TypeError: unhashable type: 'dict'.\nPlease let me know how to solve this problem.\nSystem Info\nlangchain==0.2.5\nlangchain-community==0.2.5\nlangchain-core==0.2.8\nlangchain-experimental==0.0.61\nlangchain-huggingface==0.0.1\nlangchain-openai==0.1.8\nlangchain-text-splitters==0.2.0\nlangchainhub==0.1.17\nubuntu 20.04\npython==3.9", "created_at": "2024-06-19", "closed_at": "2024-06-19", "labels": [], "State": "closed", "Author": "DiaQusNet"}
{"issue_number": 741, "issue_title": "Tentative Plan for for langgraph Checkpointer Support ?", "issue_body": "Will langgraph checkpointer be supported in future release, is there any roadmap?\nSince there is lack of persistent database support in langgraph checkpointer this was my langchain-postgres==0.0.5 go to method for that (please let me know if there are better ways).\nApologies, since I am new I am not sure how I can help but will be happy to pitch in wherever I can.", "created_at": "2024-06-19", "closed_at": "2024-07-08", "labels": [], "State": "closed", "Author": "sushantMoon"}
{"issue_number": 692, "issue_title": "draw_mermaid() can't accept input other than 'str'", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom typing import Annotated, TypedDict\n\nfrom langchain_core.messages import AnyMessage\nfrom langgraph.graph import END, START, StateGraph\nfrom langgraph.graph.message import add_messages\n\n\nclass State(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n\n\ndef branch(state: State) -> bool:\n    return 1 + 1 == 3\n\n\ngraph_builder = StateGraph(State)\ngraph_builder.add_node(\"foo\", lambda state: {\"messages\": [(\"ai\", \"foo\")]})\ngraph_builder.add_node(\"bar\", lambda state: {\"messages\": [(\"ai\", \"bar\")]})\n\ngraph_builder.add_conditional_edges(\n    START,\n    branch,\n    path_map={True: \"foo\", False: \"bar\"},\n    then=END,\n)\n\napp = graph_builder.compile()\nprint(app.invoke({\"messages\": []}))  # can work\n\n# GOOD, But only 'True' is displayed on the path\napp.get_graph().draw_png(output_file_path=\"graph.png\")\n\n# ERROR\nprint(app.get_graph().draw_mermaid())\nError Message and Stack Trace (if applicable)\nFile \"/home/gbaian10/.local/lib/python3.10/site-packages/langchain_core/runnables/graph.py\", line 423, in draw_mermaid\n    return draw_mermaid(\n  File \"/home/gbaian10/.local/lib/python3.10/site-packages/langchain_core/runnables/graph_mermaid.py\", line 84, in draw_mermaid\n    words = edge_data.split()  # Split the string into words\nAttributeError: 'bool' object has no attribute 'split'\nDescription\nThis question seems like a follow-up to #620.  The graph can work.\nBut mermaid_syntax can't be generated. Therefore mermaid_png can't be generated either.\nIn addition, in draw_png(), it only displays True and does not display False.\nIf change part of the above code to the following, everything will work fine. But this seems to defeat the purpose of using bool\ndef branch(state: State) -> str:\n    return str(1 + 1 == 3)\n\ngraph_builder.add_conditional_edges(\n    START,\n    branch,\n    path_map={\"True\": \"foo\", \"False\": \"bar\"},\n    then=END,\n)\nSystem Info\nlanggraph==0.0.69\nlangchain==0.2.5\npython_version==3.12.4\nplatform==linux", "created_at": "2024-06-18", "closed_at": "2024-06-27", "labels": [], "State": "closed", "Author": "gbaian10"}
{"issue_number": 691, "issue_title": "ModuleNotFoundError: No module named 'langchain_core.tracers._streaming' when import from lang graph", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langgraph.graph import END, START\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nFile /home/heybo789/Project/Python/ai_content_gen/custom_modules/tools/asf.py:1\n----> 1 from langgraph.graph import END, START\n\nFile ~/Project/Python/ai_content_gen/lib/python3.10/site-packages/langgraph/graph/__init__.py:1\n----> 1 from langgraph.graph.graph import END, START, Graph\n      2 from langgraph.graph.message import MessageGraph, MessagesState, add_messages\n      3 from langgraph.graph.state import StateGraph\n\nFile ~/Project/Python/ai_content_gen/lib/python3.10/site-packages/langgraph/graph/graph.py:30\n     28 from langgraph.checkpoint import BaseCheckpointSaver\n     29 from langgraph.constants import TAG_HIDDEN\n---> 30 from langgraph.pregel import Channel, Pregel\n     31 from langgraph.pregel.read import PregelNode\n     32 from langgraph.pregel.types import All\n\nFile ~/Project/Python/ai_content_gen/lib/python3.10/site-packages/langgraph/pregel/__init__.py:48\n     34 from langchain_core.runnables.config import (\n     35     RunnableConfig,\n     36     ensure_config,\n   (...)\n     41     patch_config,\n     42 )\n     43 from langchain_core.runnables.utils import (\n     44     ConfigurableFieldSpec,\n     45     create_model,\n     46     get_unique_config_specs,\n     47 )\n---> 48 from langchain_core.tracers._streaming import _StreamingCallbackHandler\n     49 from typing_extensions import Self\n     51 from langgraph.channels.base import (\n     52     AsyncChannelsManager,\n     53     BaseChannel,\n   (...)\n     56     create_checkpoint,\n     57 )\n\nModuleNotFoundError: No module named 'langchain_core.tracers._streaming'\nDescription\nWhenever i try to import from langgraph.graph. the bug happen. I try to reinstall langchain.core but it not solve the problem.\nSystem Info\naiohttp==3.9.5\naiosignal==1.3.1\nalembic==1.13.1\nannotated-types==0.6.0\nanthropic==0.28.1\nanyio==4.3.0\nappdirs==1.4.4\nasgiref==3.8.1\nasttokens==2.4.1\nasync-timeout==4.0.3\nattrs==23.2.0\nBabel==2.15.0\nbackoff==2.2.1\nbcrypt==4.1.3\nbeautifulsoup4==4.12.3\nblinker==1.8.2\nboto3==1.34.127\nbotocore==1.34.127\nBrotli==1.1.0\nbs4==0.0.2\nbuild==1.2.1\ncachetools==5.3.3\ncatalogue==2.0.10\ncertifi==2024.2.2\ncffi==1.16.0\ncharset-normalizer==3.3.2\nchroma-hnswlib==0.7.3\nchromadb==0.4.24\nclick==8.1.7\ncoloredlogs==15.0.1\ncomm==0.2.2\ncourlan==1.1.0\ncrewai==0.28.8\ncrewai-tools==0.2.3\ncryptography==42.0.6\ndataclasses-json==0.6.5\ndateparser==1.2.0\ndebugpy==1.8.1\ndecorator==5.1.1\ndefusedxml==0.7.1\nDeprecated==1.2.14\ndeprecation==2.1.0\ndirtyjson==1.0.8\ndistro==1.9.0\ndocstring-parser==0.15\nembedchain==0.1.102\nexceptiongroup==1.2.1\nexecuting==2.0.1\nfaiss-cpu==1.8.0\nfaiss-gpu==1.7.2\nfast-pytorch-kmeans==0.2.0.1\nfastapi==0.110.3\nfilelock==3.14.0\nflatbuffers==24.3.25\nfree-proxy==1.1.1\nfrozenlist==1.4.1\nfsspec==2024.3.1\ngit-python==1.0.3\ngitdb==4.0.11\nGitPython==3.1.43\ngoogle==3.0.0\ngoogle-ai-generativelanguage==0.6.4\ngoogle-api-core==2.19.0\ngoogle-api-python-client==2.133.0\ngoogle-auth==2.29.0\ngoogle-auth-httplib2==0.2.0\ngoogle-cloud-aiplatform==1.50.0\ngoogle-cloud-bigquery==3.21.0\ngoogle-cloud-core==2.4.1\ngoogle-cloud-resource-manager==1.12.3\ngoogle-cloud-storage==2.16.0\ngoogle-crc32c==1.5.0\ngoogle-generativeai==0.5.4\ngoogle-resumable-media==2.7.0\ngoogleapis-common-protos==1.63.0\ngptcache==0.1.43\ngraphviz==0.20.3\ngreenlet==3.0.3\ngroq==0.5.0\ngrpc-google-iam-v1==0.13.0\ngrpcio==1.63.0\ngrpcio-status==1.62.2\nh11==0.14.0\nhtml2text==2024.2.26\nhtmldate==1.8.1\nhttpcore==1.0.5\nhttplib2==0.22.0\nhttptools==0.6.1\nhttpx==0.27.0\nhuggingface-hub==0.23.0\nhumanfriendly==10.0\nidna==3.7\nimportlib-metadata==7.0.0\nimportlib_resources==6.4.0\niniconfig==2.0.0\ninstructor==0.5.2\nipykernel==6.29.4\nipython==8.24.0\nitsdangerous==2.2.0\njedi==0.19.1\nJinja2==3.1.3\njiter==0.4.2\njmespath==1.0.1\njoblib==1.4.2\njsonpatch==1.33\njsonpointer==2.4\njupyter_client==8.6.1\njupyter_core==5.7.2\njusText==3.0.0\nkubernetes==29.0.0\nlancedb==0.5.7\nlangchain==0.1.15\nlangchain-anthropic==0.1.11\nlangchain-aws==0.1.3\nlangchain-chroma==0.1.0\nlangchain-community==0.0.38\nlangchain-core==0.1.52\nlangchain-experimental==0.0.60\nlangchain-google-genai==1.0.3\nlangchain-groq==0.1.3\nlangchain-openai==0.1.6\nlangchain-text-splitters==0.0.2\nlangchainhub==0.1.18\nlanggraph==0.0.57\nlangsmith==0.1.53\nlark==1.1.9\nllama-index==0.10.36\nllama-index-agent-openai==0.2.4\nllama-index-cli==0.1.12\nllama-index-embeddings-openai==0.1.9\nllama-index-indices-managed-llama-cloud==0.1.6\nllama-index-llms-openai==0.1.18\nllama-index-multi-modal-llms-openai==0.1.5\nllama-index-program-openai==0.1.6\nllama-index-question-gen-openai==0.1.3\nllama-index-readers-file==0.1.22\nllama-index-readers-llama-parse==0.1.4\nllama-parse==0.4.2\nlxml==5.1.1\nMako==1.3.3\nmarkdown-it-py==3.0.0\nMarkupSafe==2.1.5\nmarshmallow==3.21.2\nmatplotlib-inline==0.1.7\nmdurl==0.1.2\nminify_html==0.15.0\nmmh3==4.1.0\nmonotonic==1.6\nmpmath==1.3.0\nmultidict==6.0.5\nmutagen==1.47.0\nmypy-extensions==1.0.0\nnest-asyncio==1.6.0\nnetworkx==3.3\nnodeenv==1.8.0\nnumpy==1.26.4\nnvidia-cublas-cu12==12.1.3.1\nnvidia-cuda-cupti-cu12==12.1.105\nnvidia-cuda-nvrtc-cu12==12.1.105\nnvidia-cuda-runtime-cu12==12.1.105\nnvidia-cudnn-cu12==8.9.2.26\nnvidia-cufft-cu12==11.0.2.54\nnvidia-curand-cu12==10.3.2.106\nnvidia-cusolver-cu12==11.4.5.107\nnvidia-cusparse-cu12==12.1.0.106\nnvidia-nccl-cu12==2.20.5\nnvidia-nvjitlink-cu12==12.4.127\nnvidia-nvtx-cu12==12.1.105\noauthlib==3.2.2\nonnxruntime==1.17.3\nopenai==1.25.1\nopentelemetry-api==1.24.0\nopentelemetry-exporter-otlp-proto-common==1.24.0\nopentelemetry-exporter-otlp-proto-grpc==1.24.0\nopentelemetry-exporter-otlp-proto-http==1.24.0\nopentelemetry-instrumentation==0.45b0\nopentelemetry-instrumentation-asgi==0.45b0\nopentelemetry-instrumentation-fastapi==0.45b0\nopentelemetry-proto==1.24.0\nopentelemetry-sdk==1.24.0\nopentelemetry-semantic-conventions==0.45b0\nopentelemetry-util-http==0.45b0\norjson==3.10.2\noutcome==1.3.0.post0\noverrides==7.7.0\npackaging==23.2\npandas==2.2.2\nparso==0.8.4\npexpect==4.9.0\npillow==10.3.0\nplatformdirs==4.2.1\nplaywright==1.43.0\npluggy==1.5.0\nposthog==3.5.0\nprompt-toolkit==3.0.43\nproto-plus==1.23.0\nprotobuf==4.25.3\npsutil==5.9.8\nptyprocess==0.7.0\npulsar-client==3.5.0\npure-eval==0.2.2\npy==1.11.0\npyarrow==16.0.0\npyarrow-hotfix==0.6\npyasn1==0.6.0\npyasn1_modules==0.4.0\npycparser==2.22\npycryptodomex==3.20.0\npydantic==2.7.1\npydantic_core==2.18.2\npyee==11.1.0\nPyGithub==1.59.1\nPygments==2.18.0\nPyJWT==2.8.0\npylance==0.9.18\nPyNaCl==1.5.0\npyparsing==3.1.2\npypdf==4.2.0\nPyPika==0.48.9\npyproject_hooks==1.1.0\npyright==1.1.361\npysbd==0.3.4\nPySocks==1.7.1\npytest==8.2.0\npython-dateutil==2.9.0.post0\npython-dotenv==1.0.1\npytube==15.0.0\npytz==2024.1\nPyYAML==6.0.1\npyzmq==26.0.3\nrandom-user-agent==1.0.1\nrank-bm25==0.2.2\nratelimiter==1.2.0.post0\nredis==5.0.4\nregex==2023.12.25\nrequests==2.31.0\nrequests-file==2.0.0\nrequests-oauthlib==2.0.0\nretry==0.9.2\nrich==13.7.1\nrsa==4.9\ns3transfer==0.10.1\nsafetensors==0.4.3\nschema==0.7.7\nscikit-learn==1.4.2\nscipy==1.13.0\nselenium==4.20.0\nsemver==3.0.2\nsentence-transformers==2.7.0\nshapely==2.0.4\nsix==1.16.0\nsmmap==5.0.1\nsniffio==1.3.1\nsortedcontainers==2.4.0\nsoupsieve==2.5\nSQLAlchemy==2.0.29\nstack-data==0.6.3\nstarlette==0.37.2\nstriprtf==0.0.26\nsympy==1.12\ntavily-python==0.3.3\ntenacity==8.2.3\nthreadpoolctl==3.5.0\ntiktoken==0.6.0\ntld==0.13\ntldextract==5.1.2\ntokenizers==0.19.1\ntomli==2.0.1\ntorch==2.3.0\ntornado==6.4\ntqdm==4.66.4\ntrafilatura==1.9.0\ntraitlets==5.14.3\ntransformers==4.40.1\ntrio==0.25.0\ntrio-websocket==0.11.1\ntriton==2.3.0\ntyper==0.9.4\ntypes-requests==2.32.0.20240602\ntyping-inspect==0.9.0\ntyping_extensions==4.11.0\ntzdata==2024.1\ntzlocal==5.2\nujson==5.9.0\nundetected-playwright==0.3.0\nuritemplate==4.1.1\nurllib3==2.2.1\nuuid6==2024.1.12\nuvicorn==0.29.0\nuvloop==0.19.0\nwatchfiles==0.21.0\nwcwidth==0.2.13\nwebsocket-client==1.8.0\nwebsockets==12.0\nwrapt==1.16.0\nwsproto==1.2.0\nyarl==1.9.4\nyoutube-transcript-api==0.6.2\nyt-dlp==2023.12.30\nzipp==3.18.1\nplatform  = Ubuntu 22.04.4 LTS (Jammy Jellyfish)](https://releases.ubuntu.com/jammy)\npython ver 3.10.12\nI", "created_at": "2024-06-18", "closed_at": "2024-06-18", "labels": [], "State": "closed", "Author": "HEYBOY789"}
{"issue_number": 684, "issue_title": "use update_state,the state change,but event not show latest message", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\ndef should_continue(data, config):\n\tmessages = data[\"messages\"]\n\tlast_message = messages[-1]\n\taction = ReActFormatParser().parse(last_message.content)\n\tif isinstance(action, AgentFinish):\n\t\tmsg = action.return_values[\"output\"]\n\t\tnew_message = AIMessage(content=msg, id=last_message.id)\n\t\tapp.update_state(config, {\"messages\": [new_message]})\n\t\treturn \"end\"\n\t# Otherwise, an AgentAction is returned\n\t# Here we return `continue` string\n\t# This will be used when setting up the graph to define the flow\n\telse:\n\t\treturn \"continue\"\nError Message and Stack Trace (if applicable)\nnew_message is update, but in this code :\n        elif event[\"event\"] == \"on_chain_end\" and event[\"run_id\"] == root_run_id:\n            messages = event.get(\"data\", {}).get('output', {}).get('messages', [])\n            if len(messages) > 1:\n                msg = messages[-1]\n                if isinstance(msg, AIMessage):\n                    last_messages_list.append(msg)\n                    message_info = MessageStreamDetail(id=root_run_id,\n                                                       message=last_messages_list[-1],\n                                                       type=\"final\")\n                    return message_info, root_run_id, last_messages_list, last_stream_run_id\n\nthe message not the latest.\nDescription\nI hope the message is the latest\nSystem Info\nlangchain = \"==0.2.3\"\nlanggraph = \"==0.0.66\"", "created_at": "2024-06-17", "closed_at": null, "labels": [], "State": "open", "Author": "wangxinzhang"}
{"issue_number": 679, "issue_title": "DOC: Grammar and punctuation improvements in LangGraph introduction tutorial", "issue_body": "Issue with current documentation:\nOverview\nI've recently read through the LangGraph introduction tutorial and noticed a few minor grammar and punctuation errors. Fixing these will make the docs easier to understand and reduce the chances of misinterpretation.\nErrors\n\nIncorrect closure of back ticks to delineate objects in Markdown (section 2)\nUnnecessary double new-line in docstring of route_tools function (section 2)\nTypo when describing checkpointer memory (section 3)\nGrammar error: using \"arbitrary\" instead of \"arbitrarily\" when describing checkpointing (section 3)\nUsing \"is\" instead of \"are\" to describe plural (section 3)\nIncomplete wrapping of words in double underscore for Markdown formatting (section 7)\n\nIdea or request for content:\nNo response", "created_at": "2024-06-16", "closed_at": "2024-06-20", "labels": [], "State": "closed", "Author": "ruankie"}
{"issue_number": 653, "issue_title": "Spurious validation error", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nThe code below works.\n\nfrom langchain_core.messages import AnyMessage, HumanMessage, SystemMessage, AIMessage\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.utils.function_calling import convert_to_openai_function\n\nimport json\n\ndef main():\n    '''\n    This tool uses the OpenAI API to generate a response based on the last message in the state.\n    '''\n    print(\"choose_next_action\")\n    messages = [SystemMessage(content=\"You are a sociologist conducting a semi-structured interview. Respond only in valid JSON.\"), AIMessage(content=\"Why do you have a cat?\"), HumanMessage(content=\"I am a student.\") ]\n\n    print(messages)\n\n    tools = [\n                {\n                    \"name\": \"evaluate_answer\",\n                    \"description\": \"Decide what to do based on the content of an interviewee's response\",\n                    \"parameters\": {\n                        \"required\": [\n                            \"next_action\"\n                        ],\n                        \"properties\": {\n                            \"next_action\": {\n                                \"enum\": [\n                                    \"probing_question\",\n                                    \"clarifying_question\",\n                                    \"next_question\"\n                                ]\n                            }\n                        },\n                        \"type\": \"object\"\n                    }\n                }\n            ]\n\n    functions = [convert_to_openai_function(t) for t in tools]\n\n    print(functions[0])\n\n    # prompt = AIMessagePromptTemplate.from_messages(messages)\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n    response = llm.invoke(messages, functions=functions).content\n\n    next_action = json.loads(response)\n    \n    print(next_action[\"next_action\"])\n\n    return response\n\nif __name__ == \"__main__\":\n    main()\n\n\nWhen the same function is converted to a ToolNode, it crashes.\nError Message and Stack Trace (if applicable)\n/Users/francis/PycharmProjects/langgraph-test/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 0.3.0. Use invoke instead.\n  warn_deprecated(\n2024-06-12 13:29:32 - 1 validation error for choose_next_actionSchema\nstate\n  field required (type=value_error.missing)\nTraceback (most recent call last):\n  File \"/Users/francis/PycharmProjects/langgraph-test/.venv/lib/python3.11/site-packages/chainlit/utils.py\", line 44, in wrapper\n    return await user_function(**params_values)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/francis/PycharmProjects/langgraph-test/demo.py\", line 102, in on_message\n    state = await graph.ainvoke(state)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/francis/PycharmProjects/langgraph-test/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 1456, in ainvoke\n    async for chunk in self.astream(\n  File \"/Users/francis/PycharmProjects/langgraph-test/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 1292, in astream\n    _panic_or_proceed(done, inflight, step)\n  File \"/Users/francis/PycharmProjects/langgraph-test/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 1489, in _panic_or_proceed\n    raise exc\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n    result = coro.send(None)\n             ^^^^^^^^^^^^^^^\n  File \"/Users/francis/PycharmProjects/langgraph-test/.venv/lib/python3.11/site-packages/langgraph/pregel/retry.py\", line 114, in arun_with_retry\n    await task.proc.ainvoke(task.input, task.config)\n  File \"/Users/francis/PycharmProjects/langgraph-test/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2531, in ainvoke\n    input = await step.ainvoke(input, config, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/francis/PycharmProjects/langgraph-test/.venv/lib/python3.11/site-packages/langgraph/utils.py\", line 117, in ainvoke\n    ret = await asyncio.create_task(\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py\", line 287, in __await__\n    yield self  # This tells Task to wait for completion.\n    ^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n    future.result()\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py\", line 203, in result\n    raise self._exception.with_traceback(self._exception_tb)\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py\", line 279, in __step\n    result = coro.throw(exc)\n             ^^^^^^^^^^^^^^^\n  File \"/Users/francis/PycharmProjects/langgraph-test/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 547, in run_in_executor\n    return await asyncio.get_running_loop().run_in_executor(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py\", line 287, in __await__\n    yield self  # This tells Task to wait for completion.\n    ^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py\", line 349, in __wakeup\n    future.result()\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/futures.py\", line 203, in result\n    raise self._exception.with_traceback(self._exception_tb)\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/francis/PycharmProjects/langgraph-test/demo.py\", line 120, in evaluate_answer_node\n    next_action = json.loads(choose_next_action(state))[\"next_action\"]\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/francis/PycharmProjects/langgraph-test/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py\", line 148, in warning_emitting_wrapper\n    return wrapped(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/francis/PycharmProjects/langgraph-test/.venv/lib/python3.11/site-packages/langchain_core/tools.py\", line 567, in __call__\n    return self.run(tool_input, callbacks=callbacks)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/francis/PycharmProjects/langgraph-test/.venv/lib/python3.11/site-packages/langchain_core/tools.py\", line 417, in run\n    raise e\n  File \"/Users/francis/PycharmProjects/langgraph-test/.venv/lib/python3.11/site-packages/langchain_core/tools.py\", line 406, in run\n    parsed_input = self._parse_input(tool_input)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/francis/PycharmProjects/langgraph-test/.venv/lib/python3.11/site-packages/langchain_core/tools.py\", line 304, in _parse_input\n    result = input_args.parse_obj(tool_input)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/francis/PycharmProjects/langgraph-test/.venv/lib/python3.11/site-packages/pydantic/v1/main.py\", line 526, in parse_obj\n    return cls(**obj)\n           ^^^^^^^^^^\n  File \"/Users/francis/PycharmProjects/langgraph-test/.venv/lib/python3.11/site-packages/pydantic/v1/main.py\", line 341, in __init__\n    raise validation_error\npydantic.v1.error_wrappers.ValidationError: 1 validation error for choose_next_actionSchema\nstate\n  field required (type=value_error.missing)\nDescription\nSince the function in question is clearly working outside of Langgraph, this appears to be a spurious error. If it is not, the error message does not provide any useful information and in any case is not formatted correctly.\nSystem Info\nlanggraph 0.0.66\npython 3.11\nmac os 14.5", "created_at": "2024-06-12", "closed_at": "2024-07-11", "labels": ["stale"], "State": "closed", "Author": "francisjervis"}
{"issue_number": 640, "issue_title": "Test example", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nTest example\nError Message and Stack Trace (if applicable)\ndef foo():\n    return \"bar\"\nDescription\nThis is a really long description\nSystem Info\nsomething", "created_at": "2024-06-11", "closed_at": "2024-06-11", "labels": [], "State": "closed", "Author": "hinthornw"}
{"issue_number": 639, "issue_title": "(Python3.12.4) TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nfrom langgraph.graph import StateGraph\nError Message and Stack Trace (if applicable)\nTypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'\nDescription\nLooks the same as these issues\nlangchain #22692\npydantic #9612\nIs this a problem caused by pydantic?\nIf pydantic has not fixed the problem, can it only be back to 3.12.3?\nSystem Info\npython_version==3.12.4\nlanggraph==0.0.66\nlangchain==0.2.3\npydantic==2.7.3\nplatform==windows", "created_at": "2024-06-11", "closed_at": "2024-06-13", "labels": [], "State": "closed", "Author": "gbaian10"}
{"issue_number": 637, "issue_title": "How to add an initial greeting with no input", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.graph.message import add_messages\nfrom langchain_core.messages import AIMessage, HumanMessage, ToolMessage\nfrom typing import Annotated\n\nfrom typing_extensions import TypedDict\n\nfrom langchain_core.runnables import chain, RunnableLambda\n\nclass State(TypedDict):\n    # Messages have the type \"list\". The `add_messages` function\n    # in the annotation defines how this state key should be updated\n    # (in this case, it appends messages to the list, rather than overwriting them)\n    messages: Annotated[list, add_messages]\n\n\ngraph_state = StateGraph(State)\n\nwelcome_message = AIMessage(content=\"Welcome to the hello, world! state machine.\")\n\n\ndef emit_greeting() -> AIMessage:\n    print(\"emit_greeting\")\n    return welcome_message\n\n\n\n\ndef handle_greeting_node(state):\n    print(\"handle_greeting_node\")\n    print(state)\n    state.add_message(welcome_message)\n    print(state)\n    return state\n\n\n\n\ndef handle_user_input(state):\n    print(\"handle_user_input\")\n    print(state)\n\n\ngraph_state.add_node(\"handle_greeting_node\", lambda state: handle_greeting_node(state))\n# graph_state.add_node(\"handle_user_input\", handle_user_input)\ngraph_state.set_entry_point(\"handle_greeting_node\")\n# graph_state.add_edge(\"handle_greeting_node\", \"handle_user_input\")\ngraph_state.add_edge(\"handle_greeting_node\", END)\napp = graph_state.compile()\n\napp.invoke()\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\nFile \"scratch_4.py\", line 51, in \napp.invoke()\nTypeError: Pregel.invoke() missing 1 required positional argument: 'input'\nDescription\nIt appears to be impossible to set up a LangGraph agent to send an initial message to the user from the start node. This means that the user is always presented with a \"blank screen\" which is unacceptable in production.  This is due to \"input\" being required when it should be optional.\nSystem Info\nlanggraph 0.0.66\npython 3.11\nmac os 14.5", "created_at": "2024-06-11", "closed_at": "2024-07-03", "labels": [], "State": "closed", "Author": "francisjervis"}
{"issue_number": 634, "issue_title": "Shouldn't thread_ts be set to a datetime object instead of checkpoint[\"id\"]?", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nThe following code:\nimport datetime\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nfrom langgraph.graph import StateGraph\nimport uuid\n\n\nbuilder = StateGraph(State)\n\n## This part is my implementation, not the part of the problem\nprimary_assistant = PrimaryAssistant()\nprimary_assistant.build_workflow(builder)\n## End of my implementation\n\nmemory = SqliteSaver.from_conn_string(\"checkpoints\")\ngraph = builder.compile(checkpointer=memory)\n\nconfig = {\n        \"configurable\": {\n            \"thread_id\": uuid.uuid4(),\n            \"thread_ts\": datetime.datetime.utcnow()\n        }\n    }\n\ngraph.invoke({\"messages\": \"Hello!\"}, config, debug=True)\nFollowing graph.invoke(), stream method sets the thread_ts to checkpoint[\"id\"] instead of the passed thread_ts in config\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nI'm trying to create a persistent storage for LangGraph's messages. While trying it out, I noticed that thread_ts argument that I pass gets changed to checkpoint[\"id\"] by stream\nAm I missing something or is this something to be fixed? If so, I'd like to create a PR for this.\nSystem Info\nlangchain==0.2.1\nlangchain-community==0.2.1\nlangchain-core==0.2.1\nlangchain-mongodb==0.1.5\nlangchain-openai==0.1.7\nlangchain-pinecone==0.1.1\nlangchain-text-splitters==0.2.0", "created_at": "2024-06-10", "closed_at": "2024-08-24", "labels": [], "State": "closed", "Author": "umithyo"}
{"issue_number": 630, "issue_title": "SystemMessage Not Effective", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nsystem_prompt = \"Don't speak antyhing.\"\nagent = AgentM(model, tools)\n\n# the initial task input\n# messages = [HumanMessage(content=\"What is score of NBA Final Game 1?\")]\nmessages = [HumanMessage(content=\"The total number of students in my class is to use my student id and plus 10. So how many students are there in my class? \")]\nmessages = [SystemMessage(content=system_prompt)] + messages\n\n# Execute the task\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ntry: \n  result = agent.graph.invoke({\"messages\": messages}, config=config)\n  print(result['messages'][-1].content)\n  print(\"*\" * 20)\n  print(agent.graph.get_state(config))\nexcept UserExitError as e:\n  print(e.message)\n\nError Message and Stack Trace (if applicable)\n****************************% python tutorial.py\nPlease provide your student ID so I can assist you in calculating the total number of students in your class.\n\nStateSnapshot(values={'messages': [SystemMessage(content=\"Don't say antyhing.\", id='ffc8061c-796f-430e-9d7f-9c82852e4a12'), HumanMessage(content='The total number of students in my class is to use my student id and plus 10. So how many students are there in my class? ', id='a1e0d963-eda8-46f4-b3c3-3ae26780c08b'), AIMessage(content='Please provide your student ID so I can assist you in calculating the total number of students in your class.', response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 150, 'total_tokens': 172}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_319be4768e', 'finish_reason': 'stop', 'logprobs': None}, id='run-f5cdb365-3cc9-4025-80f0-0d80fd2d6d34-0', usage_metadata={'input_tokens': 150, 'output_tokens': 22, 'total_tokens': 172})], 'query_human': False}, next=(), config={'configurable': {'thread_id': '1', 'thread_ts': '1ef26ada-893a-6892-8001-0bd642095671'}}, metadata={'source': 'loop', 'step': 1, 'writes': {'llm': {'messages': [AIMessage(content='Please provide your student ID so I can assist you in calculating the total number of students in your class.', response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 150, 'total_tokens': 172}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_319be4768e', 'finish_reason': 'stop', 'logprobs': None}, id='run-f5cdb365-3cc9-4025-80f0-0d80fd2d6d34-0', usage_metadata={'input_tokens': 150, 'output_tokens': 22, 'total_tokens': 172})], 'query_human': False}}}, created_at='2024-06-09T22:14:37.122152+00:00', parent_config={'configurable': {'thread_id': '1', 'thread_ts': '1ef26ada-80c4-6d52-8000-dd00aa835319'}})\nDescription\n\nSystemMessage can be correctly appended in the beginning, by printing out graph.get_state(), but it seems the agent can't be guided by any info in the SystemMessage.\n\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.3.0: Wed Dec 20 21:30:44 PST 2023; root:xnu-10002.81.5~7/RELEASE_ARM64_T6000\nPython Version:  3.9.13 (v3.9.13:6de2ca5339, May 17 2022, 11:37:23)\n[Clang 13.0.0 (clang-1300.0.29.30)]\n\nPackage Information\n\nlangchain_core: 0.2.4\nlangchain: 0.2.2\nlangchain_community: 0.2.3\nlangsmith: 0.1.74\nlangchain_anthropic: 0.1.15\nlangchain_openai: 0.1.8\nlangchain_text_splitters: 0.2.1\nlanggraph: 0.0.64\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlangserve\n", "created_at": "2024-06-09", "closed_at": "2024-06-10", "labels": [], "State": "closed", "Author": "babelpainterwell"}
{"issue_number": 628, "issue_title": "After changing chat_models from ChatOpenAI to ChatVertexAI, i get a error 'ValueError: SystemMessage should be the first in the history.'", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\ncalculate = get_math_tool(ChatVertexAI(model_name=\"gemini-pro\"))\nllm = ChatVertexAI(model_name=\"gemini-pro\")\n\n### Error Message and Stack Trace (if applicable)\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[38], line 3\n      1 example_question = \"What's the temperature in SF raised to the 3rd power?\"\n----> 3 for task in planner.stream([HumanMessage(content=example_question)]):\n      4     print(task[\"tool\"], task[\"args\"])\n\nFile /opt/conda/lib/python3.10/site-packages/langchain_core/runnables/base.py:2873, in RunnableSequence.stream(self, input, config, **kwargs)\n   2867 def stream(\n   2868     self,\n   2869     input: Input,\n   2870     config: Optional[RunnableConfig] = None,\n   2871     **kwargs: Optional[Any],\n   2872 ) -> Iterator[Output]:\n-> 2873     yield from self.transform(iter([input]), config, **kwargs)\n\nFile /opt/conda/lib/python3.10/site-packages/langchain_core/runnables/base.py:2860, in RunnableSequence.transform(self, input, config, **kwargs)\n   2854 def transform(\n   2855     self,\n   2856     input: Iterator[Input],\n   2857     config: Optional[RunnableConfig] = None,\n   2858     **kwargs: Optional[Any],\n   2859 ) -> Iterator[Output]:\n-> 2860     yield from self._transform_stream_with_config(\n   2861         input,\n   2862         self._transform,\n   2863         patch_config(config, run_name=(config or {}).get(\"run_name\") or self.name),\n   2864         **kwargs,\n   2865     )\n\nFile /opt/conda/lib/python3.10/site-packages/langchain_core/runnables/base.py:1865, in Runnable._transform_stream_with_config(self, input, transformer, config, run_type, **kwargs)\n   1863 try:\n   1864     while True:\n-> 1865         chunk: Output = context.run(next, iterator)  # type: ignore\n   1866         yield chunk\n   1867         if final_output_supported:\n\nFile /opt/conda/lib/python3.10/site-packages/langchain_core/runnables/base.py:2822, in RunnableSequence._transform(self, input, run_manager, config, **kwargs)\n   2819     else:\n   2820         final_pipeline = step.transform(final_pipeline, config)\n-> 2822 for output in final_pipeline:\n   2823     yield output\n\nFile /opt/conda/lib/python3.10/site-packages/langchain_core/output_parsers/transform.py:50, in BaseTransformOutputParser.transform(self, input, config, **kwargs)\n     44 def transform(\n     45     self,\n     46     input: Iterator[Union[str, BaseMessage]],\n     47     config: Optional[RunnableConfig] = None,\n     48     **kwargs: Any,\n     49 ) -> Iterator[T]:\n---> 50     yield from self._transform_stream_with_config(\n     51         input, self._transform, config, run_type=\"parser\"\n     52     )\n\nFile /opt/conda/lib/python3.10/site-packages/langchain_core/runnables/base.py:1829, in Runnable._transform_stream_with_config(self, input, transformer, config, run_type, **kwargs)\n   1827 input_for_tracing, input_for_transform = tee(input, 2)\n   1828 # Start the input iterator to ensure the input runnable starts before this one\n-> 1829 final_input: Optional[Input] = next(input_for_tracing, None)\n   1830 final_input_supported = True\n   1831 final_output: Optional[Output] = None\n\nFile /opt/conda/lib/python3.10/site-packages/langchain_core/runnables/base.py:1179, in Runnable.transform(self, input, config, **kwargs)\n   1176             final = ichunk\n   1178 if got_first_val:\n-> 1179     yield from self.stream(final, config, **kwargs)\n\nFile /opt/conda/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:265, in BaseChatModel.stream(self, input, config, stop, **kwargs)\n    258 except BaseException as e:\n    259     run_manager.on_llm_error(\n    260         e,\n    261         response=LLMResult(\n    262             generations=[[generation]] if generation else []\n    263         ),\n    264     )\n--> 265     raise e\n    266 else:\n    267     run_manager.on_llm_end(LLMResult(generations=[[generation]]))\n\nFile /opt/conda/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:245, in BaseChatModel.stream(self, input, config, stop, **kwargs)\n    243 generation: Optional[ChatGenerationChunk] = None\n    244 try:\n--> 245     for chunk in self._stream(messages, stop=stop, **kwargs):\n    246         if chunk.message.id is None:\n    247             chunk.message.id = f\"run-{run_manager.run_id}\"\n\nFile /opt/conda/lib/python3.10/site-packages/langchain_google_vertexai/chat_models.py:1011, in ChatVertexAI._stream(self, messages, stop, run_manager, **kwargs)\n   1007     yield from self._stream_non_gemini(\n   1008         messages, stop=stop, run_manager=run_manager, **kwargs\n   1009     )\n   1010     return\n-> 1011 yield from self._stream_gemini(\n   1012     messages=messages, stop=stop, run_manager=run_manager, **kwargs\n   1013 )\n   1014 return\n\nFile /opt/conda/lib/python3.10/site-packages/langchain_google_vertexai/chat_models.py:1023, in ChatVertexAI._stream_gemini(self, messages, stop, run_manager, **kwargs)\n   1016 def _stream_gemini(\n   1017     self,\n   1018     messages: List[BaseMessage],\n   (...)\n   1021     **kwargs: Any,\n   1022 ) -> Iterator[ChatGenerationChunk]:\n-> 1023     request = self._prepare_request_gemini(messages=messages, stop=stop, **kwargs)\n   1024     response_iter = _completion_with_retry(\n   1025         self.prediction_client.stream_generate_content,\n   1026         max_retries=self.max_retries,\n   (...)\n   1030         **kwargs,\n   1031     )\n   1032     for response_chunk in response_iter:\n\nFile /opt/conda/lib/python3.10/site-packages/langchain_google_vertexai/chat_models.py:796, in ChatVertexAI._prepare_request_gemini(self, messages, stop, stream, tools, functions, tool_config, safety_settings, **kwargs)\n    785 def _prepare_request_gemini(\n    786     self,\n    787     messages: List[BaseMessage],\n   (...)\n    794     **kwargs,\n    795 ) -> GenerateContentRequest:\n--> 796     system_instruction, contents = _parse_chat_history_gemini(messages)\n    797     formatted_tools = self._tools_gemini(tools=tools, functions=functions)\n    798     tool_config = self._tool_config_gemini(tool_config=tool_config)\n\nFile /opt/conda/lib/python3.10/site-packages/langchain_google_vertexai/chat_models.py:244, in _parse_chat_history_gemini(history, project, convert_system_message_to_human)\n    242 prev_ai_message = None\n    243 if i != 0:\n--> 244     raise ValueError(\"SystemMessage should be the first in the history.\")\n    245 if system_instruction is not None:\n    246     raise ValueError(\n    247         \"Detected more than one SystemMessage in the list of messages.\"\n    248         \"Gemini APIs support the insertion of only one SystemMessage.\"\n    249     )\n\nValueError: SystemMessage should be the first in the history.\n\nDescription\nAfter changing chat_models from ChatOpenAI to ChatVertexAI, i get a error 'ValueError: SystemMessage should be the first in the history.'\nfile : LLMCompiler.ipynb\nSystem Info\npython -m langchain_core.sys_info", "created_at": "2024-06-08", "closed_at": null, "labels": [], "State": "open", "Author": "weatherbetter"}
{"issue_number": 621, "issue_title": "Stream_events call fails with custom RawToolMessage class", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\ntools = [create_plot, query_data]\ntools_by_name = {tool.name: tool for tool in tools}\n\nclass RawToolMessage(ToolMessage):\n    \"\"\"\n    Customized Tool message that lets us pass around the raw tool outputs (along with string contents for passing back to the model).\n    \"\"\"\n\n    raw: dict\n    \"\"\"Arbitrary (non-string) tool outputs. Won't be sent to model.\"\"\"\n    tool_name: str\n    \"\"\"Name of tool that generated output.\"\"\"\n\ndef execute_tools(state: State, config) -> dict:\n    \"\"\"\n    Execute the called tools\n    \"\"\"\n    messages = []\n\n    last_ai_msg = [msg for msg in state[\"messages\"] if isinstance(msg, AIMessage)][-1]\n    for tool_call in last_ai_msg.tool_calls:\n        try:\n            result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"], config)\n        except Exception as e:\n            messages.append(\n                ToolMessage(\n                    content=f\"Error: {repr(e)}\\n please fix your mistakes.\",\n                    tool_call_id=tool_call[\"id\"],\n                    tool_name=tool_call[\"name\"],\n                )\n            )\n            continue\n\n        if tool_call[\"name\"] == \"create_plot\":\n            result_repr = f\"Created a plot with title: {result[\"plot_echarts_option\"][\"title\"][\"text\"]}\"\n        elif tool_call[\"name\"] == \"query_data\":\n            result_repr = str({k: (v if k!= \"data\" else v[:5]) for k, v in result.items()})\n        messages.append(\n            RawToolMessage(\n                result_repr,\n                raw=result,\n                tool_call_id=tool_call[\"id\"],\n                tool_name=tool_call[\"name\"],\n            )\n        )\n    return {\"messages\": messages}\nError Message and Stack Trace (if applicable)\nHere is the complete trace but the error is actually happening before:\nError in LogStreamCallbackHandler.on_llm_end callback: ValueError(\"Trying to deserialize something that cannot be deserialized in current version of langchain-core: ('langchain', 'schema', 'messages', 'RawToolMessage')\")\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile ~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpointer.py:288, in JsonPointer.walk(self, doc, part)\n    [287](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpointer.py:287) try:\n--> [288](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpointer.py:288)     return doc[part]\n    [290](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpointer.py:290) except KeyError:\n\nKeyError: 'AzureChatOpenAI:4'\n\nDuring handling of the above exception, another exception occurred:\n\nJsonPointerException                      Traceback (most recent call last)\nCell In[4], [line 1](vscode-notebook-cell:?execution_count=4&line=1)\n----> [1](vscode-notebook-cell:?execution_count=4&line=1) async for x in app.astream_events({\"messages\": [HumanMessage(\"show my top 5 vendors by expense in a pie chart\")]}, config={\"configurable\": {\"db_schema\": \"greenlanterneastllc_test\"}}, version=\"v1\"):\n      [2](vscode-notebook-cell:?execution_count=4&line=2)     print(x, flush=True)\n\nFile ~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py:1145, in Runnable.astream_events(self, input, config, version, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\n   [1134](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py:1134) # Ignoring mypy complaint about too many different union combinations\n   [1135](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py:1135) # This arises because many of the argument types are unions\n   [1136](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py:1136) async for log in _astream_log_implementation(  # type: ignore[misc]\n   [1137](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py:1137)     self,\n   [1138](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py:1138)     input,\n   (...)\n   [1143](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py:1143)     **kwargs,\n   [1144](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py:1144) ):\n-> [1145](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py:1145)     run_log = run_log + log\n   [1147](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py:1147)     if not encountered_start_event:\n   [1148](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py:1148)         # Yield the start event for the root runnable.\n   [1149](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py:1149)         encountered_start_event = True\n\nFile ~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/langchain_core/tracers/log_stream.py:135, in RunLog.__add__(self, other)\n    [133](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/langchain_core/tracers/log_stream.py:133) if type(other) == RunLogPatch:\n    [134](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/langchain_core/tracers/log_stream.py:134)     ops = self.ops + other.ops\n--> [135](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/langchain_core/tracers/log_stream.py:135)     state = jsonpatch.apply_patch(self.state, other.ops)\n    [136](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/langchain_core/tracers/log_stream.py:136)     return RunLog(*ops, state=state)\n    [138](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/langchain_core/tracers/log_stream.py:138) raise TypeError(\n    [139](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/langchain_core/tracers/log_stream.py:139)     f\"unsupported operand type(s) for +: '{type(self)}' and '{type(other)}'\"\n    [140](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/langchain_core/tracers/log_stream.py:140) )\n\nFile ~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpatch.py:157, in apply_patch(doc, patch, in_place, pointer_cls)\n    [155](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpatch.py:155) else:\n    [156](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpatch.py:156)     patch = JsonPatch(patch, pointer_cls=pointer_cls)\n--> [157](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpatch.py:157) return patch.apply(doc, in_place)\n\nFile ~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpatch.py:692, in JsonPatch.apply(self, obj, in_place)\n    [689](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpatch.py:689)     obj = copy.deepcopy(obj)\n    [691](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpatch.py:691) for operation in self._ops:\n--> [692](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpatch.py:692)     obj = operation.apply(obj)\n    [694](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpatch.py:694) return obj\n\nFile ~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpatch.py:282, in AddOperation.apply(self, obj)\n    [278](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpatch.py:278) except KeyError as ex:\n    [279](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpatch.py:279)     raise InvalidJsonPatch(\n    [280](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpatch.py:280)         \"The operation does not contain a 'value' member\")\n--> [282](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpatch.py:282) subobj, part = self.pointer.to_last(obj)\n    [284](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpatch.py:284) if isinstance(subobj, MutableSequence):\n    [285](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpatch.py:285)     if part == '-':\n\nFile ~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpointer.py:196, in JsonPointer.to_last(self, doc)\n    [193](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpointer.py:193)     return doc, None\n    [195](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpointer.py:195) for part in self.parts[:-1]:\n--> [196](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpointer.py:196)     doc = self.walk(doc, part)\n    [198](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpointer.py:198) return doc, JsonPointer.get_part(doc, self.parts[-1])\n\nFile ~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpointer.py:291, in JsonPointer.walk(self, doc, part)\n    [288](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpointer.py:288)     return doc[part]\n    [290](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpointer.py:290) except KeyError:\n--> [291](https://vscode-remote+wsl-002bubuntu.vscode-resource.vscode-cdn.net/home/mec/personal/cc/repos/cc-llm-engine/notebooks/~/.cache/pypoetry/virtualenvs/cc-llm-engine-uD5CZwxz-py3.12/lib/python3.12/site-packages/jsonpointer.py:291)     raise JsonPointerException(\"member '%s' not found in %s\" % (part, doc))\n\nJsonPointerException: member 'AzureChatOpenAI:4' not found in { [[deleted private data]] }\nDescription\nI'm trying to stream_events in a graph that uses a custom RawToolMessage class as described in this example notebook. If I use the .invoke method like in the original notebook it works OK, but when I call .stream_events I get an error:\nError in LogStreamCallbackHandler.on_llm_end callback: ValueError(\"Trying to deserialize something that cannot be deserialized in current version of langchain-core: ('langchain', 'schema', 'messages', 'RawToolMessage')\")\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP Thu Jan 11 04:09:03 UTC 2024\nPython Version:  3.12.3 (main, May 23 2024, 22:52:35) [GCC 9.4.0]\n\nPackage Information\n\nlangchain_core: 0.1.52\nlangchain: 0.1.20\nlangchain_community: 0.0.38\nlangsmith: 0.1.62\nlangchain_openai: 0.1.7\nlangchain_postgres: 0.0.6\nlangchain_text_splitters: 0.0.2\nlanggraph: 0.0.46\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlangserve\n", "created_at": "2024-06-07", "closed_at": "2024-07-11", "labels": [], "State": "closed", "Author": "meliascosta"}
{"issue_number": 620, "issue_title": "add_conditional_edges Type Hint", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nfrom typing import Annotated, TypedDict\n\nfrom langchain_core.messages import AnyMessage\nfrom langgraph.graph import END, START, StateGraph\nfrom langgraph.graph.message import add_messages\n\n\nclass State(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n\n\ndef branch(state: State) -> bool:\n    return 1 + 1 == 3\n\n\ngraph_builder = StateGraph(State)\ngraph_builder.add_node(\"foo\", lambda state: {\"messages\": [(\"ai\", \"foo\")]})\ngraph_builder.add_node(\"bar\", lambda state: {\"messages\": [(\"ai\", \"bar\")]})\n\ngraph_builder.add_conditional_edges(\n    START,\n    branch,   # type hint error\n    path_map={True: \"foo\", False: \"bar\"},   # type hint error\n    then=END,\n)\n\napp = graph_builder.compile()\nprint(app.invoke({\"messages\": []}))\nError Message and Stack Trace (if applicable)\nIt can work, but the type hint is wrong.\nDescription\nWhen the return type of path is not str or the key of path_map is not str, the IDE will complain and mypy will complain.\ndef add_conditional_edges(\n        self,\n        source: str,\n        path: Union[\n            Callable[..., Union[str, list[str]]],\n            Callable[..., Awaitable[Union[str, list[str]]]],\n            Runnable[Any, Union[str, list[str]]],\n        ],\n        path_map: Optional[Union[dict[str, str], list[str]]] = None,\n        then: Optional[str] = None,\n    ) -> None: ...\nIs there a better Type Hint?\nSystem Info\nlanggraph==0.0.64\npython_version==3.12.3", "created_at": "2024-06-07", "closed_at": "2024-06-07", "labels": [], "State": "closed", "Author": "gbaian10"}
{"issue_number": 606, "issue_title": "DOC: force-calling-a-tool-first.ipynb tool_calls invalid_request_error", "issue_body": "Issue with current documentation:\nWhen I try to execute the code in the documentation (I'm not modifying nothing at all) I get the following error:\nError code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[2].role', 'code': None}}\nhttps://github.com/langchain-ai/langgraph/blob/main/examples/force-calling-a-tool-first.ipynb\nIdea or request for content:\nNo response", "created_at": "2024-06-06", "closed_at": "2024-06-06", "labels": [], "State": "closed", "Author": "Daggle24"}
{"issue_number": 597, "issue_title": "astream_events can't work with checkpointer", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nimport asyncio\nimport uuid\nfrom typing import Annotated, TypedDict\n\nfrom dotenv import load_dotenv\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nfrom langgraph.graph import StateGraph, add_messages\n\nload_dotenv()\n\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\n\nmodel = ChatOpenAI()\n\n\nasync def call_model(state: State):\n    messages = state[\"messages\"]\n    response = await model.ainvoke(messages)\n    return {\"messages\": response}\n\n\nworkflow = StateGraph(State)\nworkflow.add_node(\"model\", call_model)\nworkflow.set_entry_point(\"model\")\nworkflow.set_finish_point(\"model\")\nmemory = SqliteSaver.from_conn_string(\":memory:\")\napp = workflow.compile(checkpointer=memory)\n\n\nasync def main():\n    inputs = [(\"human\", \"Who are you?\")]\n    config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n    async for event in app.astream_events({\"messages\": inputs}, config, version=\"v2\"):\n        kind = event[\"event\"]\n        if kind == \"on_chat_model_stream\":\n            content = event[\"data\"][\"chunk\"].content\n            if content:\n                print(content, end=\"|\", flush=True)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nError Message and Stack Trace (if applicable)\n/home/gbaian10/.cache/pypoetry/virtualenvs/langchain-backend-eZX4qY_I-py3.12/lib/python3.12/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: This API is in beta and may change in the future.\nwarn_beta(\nTraceback (most recent call last):\nFile \"/home/gbaian10/online_code/langchain-backend/playground/temp.py\", line 46, in \nasyncio.run(main())\nFile \"/usr/lib/python3.12/asyncio/runners.py\", line 194, in run\nreturn runner.run(main)\n^^^^^^^^^^^^^^^^\nFile \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\nreturn self._loop.run_until_complete(task)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/usr/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\nreturn future.result()\n^^^^^^^^^^^^^^^\nFile \"/home/gbaian10/online_code/langchain-backend/playground/temp.py\", line 37, in main\nasync for event in app.astream_events({\"messages\": inputs}, config, version=\"v2\"):\nFile \"/home/gbaian10/.cache/pypoetry/virtualenvs/langchain-backend-eZX4qY_I-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 1137, in astream_events\nasync for event in event_stream:\nFile \"/home/gbaian10/.cache/pypoetry/virtualenvs/langchain-backend-eZX4qY_I-py3.12/lib/python3.12/site-packages/langchain_core/tracers/event_stream.py\", line 860, in _astream_events_implementation_v2\nawait task\nFile \"/home/gbaian10/.cache/pypoetry/virtualenvs/langchain-backend-eZX4qY_I-py3.12/lib/python3.12/site-packages/langchain_core/tracers/event_stream.py\", line 820, in consume_astream\nasync for _ in event_streamer.tap_output_aiter(\nFile \"/home/gbaian10/.cache/pypoetry/virtualenvs/langchain-backend-eZX4qY_I-py3.12/lib/python3.12/site-packages/langchain_core/tracers/event_stream.py\", line 125, in tap_output_aiter\nfirst = await py_anext(output, default=sentinel)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/home/gbaian10/.cache/pypoetry/virtualenvs/langchain-backend-eZX4qY_I-py3.12/lib/python3.12/site-packages/langchain_core/utils/aiter.py\", line 62, in anext_impl\nreturn await anext(iterator)\n^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/home/gbaian10/.cache/pypoetry/virtualenvs/langchain-backend-eZX4qY_I-py3.12/lib/python3.12/site-packages/langgraph/pregel/init.py\", line 1084, in astream\nawait self.checkpointer.aget_tuple(config)\nFile \"/home/gbaian10/.cache/pypoetry/virtualenvs/langchain-backend-eZX4qY_I-py3.12/lib/python3.12/site-packages/langgraph/checkpoint/base.py\", line 191, in aget_tuple\nraise NotImplementedError\nNotImplementedError\nDescription\nastream_events can't work with checkpointer.\nBut if checkpointer is not used, it will be normal.\nSystem Info\nlangchain==0.2.2\nlanggraph==0.0.62\nlangchain-core==0.2.4\nlangchain-openai==0.1.8\nplatfor== linux\npython-version==3.12.3", "created_at": "2024-06-05", "closed_at": "2024-06-05", "labels": [], "State": "closed", "Author": "gbaian10"}
{"issue_number": 593, "issue_title": "Cannot draw graph w/ xray=True when graph contains a subgraph w/ edges targeting the subgraph entry point", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nfrom typing import TypedDict, Annotated, List\nfrom langchain_core.messages import AnyMessage\nfrom langgraph.graph import StateGraph, add_messages\nfrom IPython.display import Image, display\nimport random\n\n\nclass State(TypedDict):\n    messages: Annotated[List[AnyMessage], add_messages]\n\n\ndef node(name):\n    def _node(state: State):\n        return {'messages': [('human', f'entered {name} node')]}\n\n    return _node\n\n\nparent = StateGraph(State)\nchild = StateGraph(State)\n\nchild.add_node('c_one', node('c_one'))\nchild.add_node('c_two', node('c_two'))\n\nchild.add_edge('__start__', 'c_one')\nchild.add_edge('c_two', 'c_one')\n\nchild.add_conditional_edges('c_one',\n                            lambda x: str(random.randrange(0, 2)),\n                            {\n                                '0': 'c_two',\n                                '1': '__end__'\n                                \n                            })\n\nparent.add_node('p_one', node('p_one'))\nparent.add_node('p_two', child.compile())\n\nparent.add_edge('__start__', 'p_one')\nparent.add_edge('p_two', 'p_one')\n\nparent.add_conditional_edges('p_one',\n                            lambda x: str(random.randrange(0, 2)),\n                            {\n                                '0': 'p_two',\n                                '1': '__end__'\n                                \n                            })\n\n\napp = parent.compile()\n\nfor event in app.stream({'messages': [('human', 'start')]}):\n    print(event)\n\n\ndisplay(Image(app.get_graph(xray=True).draw_mermaid_png())) # this fails\n-->\n{'p_one': {'messages': [('human', 'entered p_one node')]}}\n{'p_two': {'messages': [HumanMessage(content='start', id='53318df0-564d-4f9b-b1ad-a8bcf03e2219'), HumanMessage(content='entered p_one node', id='c67dcfa5-e1cd-4bc0-a9b5-2ca6676150bd'), HumanMessage(content='entered c_one node', id='35b095bb-3113-4d93-8afa-b8225cb72042'), HumanMessage(content='entered c_two node', id='8f231424-9c4f-49d2-a61e-12917d953454'), HumanMessage(content='entered c_one node', id='b8bdc719-ff93-40ef-adf5-c4e90d541a2a')]}}\n{'p_one': {'messages': [('human', 'entered p_one node')]}}\n{'p_two': {'messages': [HumanMessage(content='start', id='53318df0-564d-4f9b-b1ad-a8bcf03e2219'), HumanMessage(content='entered p_one node', id='c67dcfa5-e1cd-4bc0-a9b5-2ca6676150bd'), HumanMessage(content='entered c_one node', id='35b095bb-3113-4d93-8afa-b8225cb72042'), HumanMessage(content='entered c_two node', id='8f231424-9c4f-49d2-a61e-12917d953454'), HumanMessage(content='entered c_one node', id='b8bdc719-ff93-40ef-adf5-c4e90d541a2a'), HumanMessage(content='entered p_one node', id='8cf2a906-baa2-41c6-84f8-615546a90dcf'), HumanMessage(content='entered c_one node', id='05991dcf-6d6e-4968-bb3a-5f1b39bbac0b')]}}\n{'p_one': {'messages': [('human', 'entered p_one node')]}}\n{'p_two': {'messages': [HumanMessage(content='start', id='53318df0-564d-4f9b-b1ad-a8bcf03e2219'), HumanMessage(content='entered p_one node', id='c67dcfa5-e1cd-4bc0-a9b5-2ca6676150bd'), HumanMessage(content='entered c_one node', id='35b095bb-3113-4d93-8afa-b8225cb72042'), HumanMessage(content='entered c_two node', id='8f231424-9c4f-49d2-a61e-12917d953454'), HumanMessage(content='entered c_one node', id='b8bdc719-ff93-40ef-adf5-c4e90d541a2a'), HumanMessage(content='entered p_one node', id='8cf2a906-baa2-41c6-84f8-615546a90dcf'), HumanMessage(content='entered c_one node', id='05991dcf-6d6e-4968-bb3a-5f1b39bbac0b'), HumanMessage(content='entered p_one node', id='a67bd7a7-dbb0-418b-913f-dc69f02592a7'), HumanMessage(content='entered c_one node', id='5785cb97-4bcd-4c76-a8d6-bb3e62e967f4')]}}\n{'p_one': {'messages': [('human', 'entered p_one node')]}}\nError Message and Stack Trace (if applicable)\nsee below\nDescription\nI'm trying to draw a graph which contains a subgraph as a node.\nThe subgraph has edges which target the entry node, which seems to be the problem in get_graph(xray=True)\nI expect to see an image of the graph.\nInstead I get an exception.\nSystem Info\nsee below", "created_at": "2024-06-04", "closed_at": "2024-08-19", "labels": [], "State": "closed", "Author": "mhmaguire"}
{"issue_number": 585, "issue_title": "DOC: how to stream content inside my custom node? (not from chatOpenAI)", "issue_body": "Issue with current documentation:\nThe langgrpah documentation includes two ways to stream.\n\ngraph.stream()\ngraph.stream_event()\n\nThe first way can only stream the output of a node.\nThe second way worked for ChatOpenAI method, but my situation is:\nI have a custom function, and i add it to langgraph node using \"graph.add_nodes\", the behavior of the function is to generate a long text like a LLM(but it is not a LLM), so I want to stream the output of my custom function, so that the user don't need to wait for the whole text to be generated.\nI read the docs for stream_event()\uff0c and find a similar approch is:\nfrom langchain_core.runnables import RunnableLambda\n\nasync def reverse(s: str) -> str:\n    return s[::-1]\n\nchain = RunnableLambda(func=reverse)\n\nevents = [\n    event async for event in chain.astream_events(\"hello\", version=\"v1\")\n]\n\nthis will produce the following events (run_id has been omitted for brevity):\n[\n               {\n                   \"data\": {\"input\": \"hello\"},\n                   \"event\": \"on_chain_start\",\n                   \"metadata\": {},\n                   \"name\": \"reverse\",\n                   \"tags\": [],\n               },\n               {\n                   \"data\": {\"chunk\": \"olleh\"},\n                   \"event\": \"on_chain_stream\",\n                   \"metadata\": {},\n                   \"name\": \"reverse\",\n                   \"tags\": [],\n               },\n               {\n                   \"data\": {\"output\": \"olleh\"},\n                   \"event\": \"on_chain_end\",\n                   \"metadata\": {},\n                   \"name\": \"reverse\",\n                   \"tags\": [],\n               },\n           ]\n\nBut there is only one chunk, I want to generate more chunks like chunk1: o, chunk2: lle, chunks3: h, like the message chunks from a LLM, so that maybe i can stream the content inside my custom functions.\nAnd I find the ToolNode from prebuilt maybe helpful in my situation, but the problem is there is also only one chunk from stream event.\nIdea or request for content:\nCan you provide a doc for streaming any custom nodes? It will be more flexible for developers because not only LLM stream tokens but other functions may need stream tokens.", "created_at": "2024-06-04", "closed_at": "2024-06-04", "labels": [], "State": "closed", "Author": "arlenkkk"}
{"issue_number": 584, "issue_title": "DOC: streaming-llm-tokens", "issue_body": "Issue with current documentation:\nhttps://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/#streaming-llm-tokens\nSample code does not produce expected results. (\"on_chat_model_stream\" part)\nThis is my code\nfrom typing import Annotated, TypedDict\n\nfrom dotenv import load_dotenv\nfrom langchain_core.messages import HumanMessage\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import END, StateGraph, add_messages\nfrom langgraph.prebuilt import ToolNode\n\nload_dotenv()\n\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\n\n@tool\ndef search(query: str):\n    \"\"\"Call to surf the web.\"\"\"\n    return [\"Cloudy with a chance of hail.\"]\n\n\ntools = [search]\ntool_node = ToolNode(tools)\n\nmodel = ChatOpenAI(model=\"gpt-3.5-turbo\", streaming=True)\nmodel = model.bind_tools(tools)\n\n\ndef should_continue(state: State):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if not last_message.tool_calls:\n        return \"end\"\n    else:\n        return \"continue\"\n\n\nasync def call_model(state: State):\n    messages = state[\"messages\"]\n    response = await model.ainvoke(messages)\n    return {\"messages\": response}\n\n\nworkflow = StateGraph(State)\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"action\", tool_node)\nworkflow.set_entry_point(\"agent\")\nworkflow.add_conditional_edges(\n    \"agent\",\n    should_continue,\n    {\"continue\": \"action\", \"end\": END},\n)\nworkflow.add_edge(\"action\", \"agent\")\napp = workflow.compile()\n\n\nasync def main():\n    inputs = [HumanMessage(content=\"what is the weather in sf\")]\n    async for event in app.astream_events({\"messages\": inputs}, version=\"v1\"):\n        kind = event[\"event\"]\n        if kind == \"on_chat_model_stream\":\n            content = event[\"data\"][\"chunk\"].content\n            if content:\n                print(content, end=\"|\")\n        elif kind == \"on_tool_start\":\n            print(\"--\")\n            print(\n                f\"Starting tool: {event['name']} with inputs: {event['data'].get('input')}\"\n            )\n        elif kind == \"on_tool_end\":\n            print(f\"Done tool: {event['name']}\")\n            print(f\"Tool output was: {event['data'].get('output')}\")\n            print(\"--\")\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(main())\noutput:\n--\nStarting tool: search with inputs: {'query': 'weather in San Francisco'}\nDone tool: search\nTool output was: ['Cloudy with a chance of hail.']\n--\n\npython = 3.10\nlanggraph = 0.0.62\nlangchain-core = 0.2.3\nlangchain-openai = 0.1.8\nIdea or request for content:\nNo response", "created_at": "2024-06-04", "closed_at": "2024-06-04", "labels": [], "State": "closed", "Author": "gbaian10"}
{"issue_number": 574, "issue_title": "Graph stream labels \"HumanMessage\" but it is a \"AiMessage\" ", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nworkflow = StateGraph(sv.GraphState)\n# Router intial\nworkflow.add_conditional_edges(START, gu.route_user)\nworkflow.add_node(\"rdv\", gu.rdv_generate)\n\n# Define the nodes we will cycle between\nworkflow.add_node(\"agent\", gu.agent)  # agent\nworkflow.add_node(\"retrieve\", gu.tool_node_retrieve())  # retrieval\n\n\nworkflow.add_node(\n    \"generate\", gu.generate\n)  # Generating a response after we know the documents are relevant\n\n# Decide whether to retrieve\nworkflow.add_conditional_edges(\n    \"agent\",\n    # Assess agent decision\n    tools_condition,\n    {\n        # Translate the condition outputs to nodes in our graph\n        \"tools\": \"retrieve\",\n        END: END,\n    },\n)\n\n# Edges taken after the `action` node is called.\nworkflow.add_edge(\"retrieve\", \"generate\")\nworkflow.add_edge(\"generate\", END)\nworkflow.add_edge(\"rdv\", END)\n\n# Compile\ngraph = workflow.compile()\n\nreturn graph\n\nHere is how it looks like displayed \n\n\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nUsing a graph I read that:\nfor output in graph.stream(inputs, stream_mode=\"values\"):\n            print(\"__output__\", output)\n\nmades output a concatenate list of different steos through graph path.\nMy path graph is input -> agent -> retriever -> generate and I got something like this :\n{'messages': [HumanMessage(content='my_input?', id='3c2e2af5-f38e-4471-aee3-07502e3a5494'),\nAIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'bdR7xmCGm', 'function': {'name': 'my_function', 'arguments': '{\"query\": \"new_query\"}'}}]}, response_metadata={'token_usage': {'prompt_tokens': 116, 'total_tokens': 154, 'completion_tokens': 38}, 'model': 'open-mixtral-8x22b', 'finish_reason': 'tool_calls'}, id='run-1e8ddb4a-751b-4f46-9ce3-02ddad099549-0', tool_calls=[{'name': 'my_tools', 'args': {'query': 'my_query}, 'id': 'bdR7xmCGm'}]),\nToolMessage(content='blobloblo', tool_call_id='bdR7xmCGm'),\nHumanMessage(content=\"blablabla\", id='739239a9-b3e6-4ef4-a01a-642d4a992d17')]}\nThe  lastest object in my list labeled as \"HumanMessage\" but is generated by AI. Using Langsmith I do not have this error.\nThis is very confusing because I cannot just look an the event steps from my graph if I want to store it.\nSystem Info\nlangchain==0.1.20\nlangchain-community==0.0.38\nlangchain-core==0.2.1\nlangchain-mistralai==0.1.7\nlangchain-postgres==0.0.6\nlangchain-text-splitters==0.0.1\nlangchainhub==0.1.15\nlanggraph==0.0.53\nlangsmith==0.1.57", "created_at": "2024-06-03", "closed_at": null, "labels": [], "State": "open", "Author": "HGInfoNancy"}
{"issue_number": 563, "issue_title": "DOC: when streaming events, how can we get the final state of StateGraph at the end of execution?", "issue_body": "Issue with current documentation:\nOriginally posted this as a discussion but reposting as issue because there's no docs on it and we're currently blocked by this issue - cannot upgrade langchain without upgrading langgraph, and cannot upgrade langgraph without breaking our app.\nI was previously on version 0.30 and I could get the final graph state for a StateGraph by doing:\nasync for event in parent_runnable.astream_events(version=\"v1\"):          \n      if event[\"event\"] == \"on_chain_end\" and event[\"name\"] == \"langgraph_graph\":\n            output = event[\"data\"][\"output\"]\n            if not output:\n                final_state = output.get(\"__end__\", {})\nHowever, now that I updated to the last version, it seems like the final event is triggered by the penultimate node (not the end node) and the final state is no longer available? How can I access the final state when the graph is done executing?", "created_at": "2024-05-31", "closed_at": "2024-05-31", "labels": [], "State": "closed", "Author": "ldorigo"}
{"issue_number": 547, "issue_title": "Non-informative / Confusing error message when invoking async graph nodes using sync calls (TypeError: None is not a callable object)", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nfrom langgraph.graph import StateGraph\nfrom langchain_google_vertexai import ChatVertexAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom typing import TypedDict\n\n\nclass FlowState(TypedDict):\n    original_utterance: str\n    pirate_utterance: str\n\n\nllm = ChatVertexAI(\n    model_name=\"gemini-1.5-flash-001\",\n    project=\"rd-bdlab-9c32\",\n    location=\"europe-west1\",\n    temperature=0.0,\n    max_retries=0,\n)\n\nsingle_message_template = \"\"\"Given the following utterance, rewrite it as if you were a pirate. Utterance: {original_utterance}\"\"\"\nprompt_template = ChatPromptTemplate.from_messages([(\"human\", single_message_template)])\n\nrewriter = prompt_template | llm | StrOutputParser()\n\n\nasync def rewriter_node(state: FlowState):\n    original_utterance = state[\"original_utterance\"]\n    pirate_utterance = await rewriter.ainvoke(state)  # type: ignore\n    return {\n        \"original_utterance\": original_utterance,\n        \"pirate_utterance\": pirate_utterance,\n    }\n\n\nworkflow = StateGraph(FlowState)\n\nworkflow.add_node(\"rewriter\", rewriter_node)\nworkflow.set_entry_point(\"rewriter\")\nworkflow.set_finish_point(\"rewriter\")\n\ngraph = workflow.compile()\n\nresult = graph.invoke(\n    {\n        \"original_utterance\": \"The weather is nice today.\",\n    },\n    debug=True,\n)\n\nError Message and Stack Trace (if applicable)\n[0:tasks] Starting step 0 with 1 task:\n- __start__ -> {'original_utterance': 'The weather is nice today.'}\n[0:writes] Finished step 0 with writes to 1 channel:\n- original_utterance -> 'The weather is nice today.'\n[0:checkpoint] State at the end of step 0:\n{'original_utterance': 'The weather is nice today.'}\n[1:tasks] Starting step 1 with 1 task:\n- rewriter -> {'original_utterance': 'The weather is nice today.', 'pirate_utterance': None}\n[1:writes] Finished step 1 with writes to 2 channels:\n- original_utterance -> 'The weather is nice today.'\n- pirate_utterance -> (\"Ahoy, matey! The sun be shinin' like a doubloon, and the wind be blowin' \"\n \"fair. A fine day for sailin', I'd say! \\n\")\n[1:checkpoint] State at the end of step 1:\n{'original_utterance': 'The weather is nice today.',\n 'pirate_utterance': \"Ahoy, matey! The sun be shinin' like a doubloon, and the \"\n                     \"wind be blowin' fair. A fine day for sailin', I'd \"\n                     'say! \\n'}\nlangserver-py3.12\u279c  ~/git/rewe-digital-ecom/pi-ai-shop-assistant/langserver git:(feature/NOTICKET/new-flow) \u2717 poetry run python graph/example.py\n[0:tasks] Starting step 0 with 1 task:\n- __start__ -> {'original_utterance': 'The weather is nice today.'}\n[0:writes] Finished step 0 with writes to 1 channel:\n- original_utterance -> 'The weather is nice today.'\n[0:checkpoint] State at the end of step 0:\n{'original_utterance': 'The weather is nice today.'}\n[1:tasks] Starting step 1 with 1 task:\n- rewriter -> {'original_utterance': 'The weather is nice today.', 'pirate_utterance': None}\nTraceback (most recent call last):\n  File \"example.py\", line 44, in <module>\n    result = graph.invoke(\n             ^^^^^^^^^^^^^\n  File \"lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 1333, in invoke\n    for chunk in self.stream(\n  File \"lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 876, in stream\n    _panic_or_proceed(done, inflight, step)\n  File \"lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 1422, in _panic_or_proceed\n    raise exc\n  File \"lib/python3.12/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"lib/python3.12/site-packages/langgraph/pregel/retry.py\", line 66, in run_with_retry\n    task.proc.invoke(task.input, task.config)\n  File \"lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 2393, in invoke\n    input = step.invoke(\n            ^^^^^^^^^^^^\n  File \"lib/python3.12/site-packages/langgraph/utils.py\", line 86, in invoke\n    if accepts_config(self.func)\n       ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"lib/python3.12/site-packages/langchain_core/runnables/utils.py\", line 86, in accepts_config\n    return signature(callable).parameters.get(\"config\") is not None\n           ^^^^^^^^^^^^^^^^^^^\n  File \"lib/python3.12/inspect.py\", line 3310, in signature\n    return Signature.from_callable(obj, follow_wrapped=follow_wrapped,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"lib/python3.12/inspect.py\", line 3054, in from_callable\n    return _signature_from_callable(obj, sigcls=cls,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"lib/python3.12/inspect.py\", line 2491, in _signature_from_callable\n    raise TypeError('{!r} is not a callable object'.format(obj))\n\nTypeError: None is not a callable object\n\nDescription\n\nI am trying to run a graph containing async nodes using a normal sync invoke call\nI expect to get a clear and informative error stating that this is not supported\nInstead, I get a non-informative / confusing error from langchain_core/runnables/utils.py (TypeError: None is not a callable object)\nAs a consequence, the code is not trivial to debug.\n\nSystem Info\nSystem Information\n------------------\n> OS:  Darwin\n> OS Version:  Darwin Kernel Version 23.5.0: Wed May  1 20:12:58 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T6000\n> Python Version:  3.12.3 (main, May 23 2024, 10:10:53) [Clang 15.0.0 (clang-1500.3.9.4)]\n\nPackage Information\n-------------------\n> langchain_core: 0.2.1\n> langchain: 0.2.1\n> langsmith: 0.1.57\n> langchain_chroma: 0.1.1\n> langchain_google_vertexai: 1.0.4\n> langchain_text_splitters: 0.2.0\n> langgraph: 0.0.55\n\nPackages not installed (Not Necessarily a Problem)\n--------------------------------------------------\nThe following packages were not found:\n\n> langserve\n", "created_at": "2024-05-29", "closed_at": "2024-05-29", "labels": [], "State": "closed", "Author": "vreyespue"}
{"issue_number": 539, "issue_title": "Please don't break semantic versioning (& better changelogs/releases?)", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nIn a recent langgraph version, do\n        graph.add_conditional_edges(\n            start_key=\"foo\",\n            condition=select_next_node,\n            conditional_edge_mapping={\n                'foo': 'bar'\n            },\n        )\n\n\nThis will break, since the parameter names changed.\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nHello, due to a combination of\n\nNot using releases\nUsing very nondescript commit messages\n\nNot following semantic versioning (i.e. introducing breaking changes in minor versions)\n\nThis library introduces quite a maintenance burden and is hard to keep track of.\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP PREEMPT_DYNAMIC Fri, 17 May 2024 11:49:30 +0000\nPython Version:  3.12.3 (main, Apr 23 2024, 09:16:07) [GCC 13.2.1 20240417]\n\nPackage Information\n\nlangchain_core: 0.1.52\nlangchain: 0.1.20\nlangchain_community: 0.0.38\nlangsmith: 0.1.63\nlangchain_cohere: 0.1.4\nlangchain_mongodb: 0.1.5\nlangchain_openai: 0.1.6\nlangchain_text_splitters: 0.0.2\nlangchainhub: 0.1.16\nlanggraph: 0.0.30\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlangserve\n", "created_at": "2024-05-28", "closed_at": "2024-05-28", "labels": [], "State": "closed", "Author": "ldorigo"}
{"issue_number": 538, "issue_title": "Can not change the name of the langgraph in langsmith", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nAny langgraph case or example is OK. You just need to run any langgraph program and see the effect in langsmith.\nError Message and Stack Trace (if applicable)\nNone\nDescription\nIn langgraph langsmith, all the names are langgraph, I can not customize the name.\n\nI would like to know, first of all, whether it is feasible to modify the name, and second, if it is feasible, how to modify it? If it doesn't work, we need a solution.\nSystem Info\n\npython -m langchain_core.sys_info\n\nSystem Information\n------------------\n> OS:  Windows\n> OS Version:  10.0.22631\n> Python Version:  3.10.14 | packaged by Anaconda, Inc. | (main, Mar 21 2024, 16:20:14) [MSC v.1916 64 bit (AMD64)]\n\nPackage Information\n-------------------\n> langchain_core: 0.2.1\n> langchain: 0.2.1\n> langchain_community: 0.2.1\n> langsmith: 0.1.63\n> langchain_openai: 0.1.7\n> langchain_text_splitters: 0.2.0\n> langchainhub: 0.1.16\n> langgraph: 0.0.55\n\nPackages not installed (Not Necessarily a Problem)\n--------------------------------------------------\nThe following packages were not found:\n\n> langserve\n", "created_at": "2024-05-27", "closed_at": "2024-05-29", "labels": [], "State": "closed", "Author": "Undertone0809"}
{"issue_number": 531, "issue_title": "Using CheckPointer makes the tool call break.", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n\nExample Code\nagent_executor = chat_agent_executor.create_tool_calling_executor(\n    chat_model,\n    tools,\n    checkpointer=SqliteSaver.from_conn_string(path_from_root(\"ignore/checkpoint-data/checkpoints.sqlite\")),\n)\nError Message and Stack Trace (if applicable)\n[2024-05-24 12:54:57,627] ERROR in app: Exception on /chat [POST]\nTraceback (most recent call last):\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/flask/app.py\", line 1473, in wsgi_app\n    response = self.full_dispatch_request()\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/flask/app.py\", line 882, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/flask/app.py\", line 880, in full_dispatch_request\n    rv = self.dispatch_request()\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/flask/app.py\", line 865, in dispatch_request\n    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/PycharmProjects/assistant-ai/app/routes/chat_routes.py\", line 13, in chat\n    response = agent_executor.invoke(\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 1245, in invoke\n    for chunk in self.stream(\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 834, in stream\n    _panic_or_proceed(done, inflight, step)\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 1334, in _panic_or_proceed\n    raise exc\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langgraph/pregel/retry.py\", line 66, in run_with_retry\n    task.proc.invoke(task.input, task.config)\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 2368, in invoke\n    input = step.invoke(\n            ^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3832, in invoke\n    return self._call_with_config(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 1492, in _call_with_config\n    context.run(\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/runnables/config.py\", line 347, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3706, in _invoke\n    output = call_func_with_variable_args(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/runnables/config.py\", line 347, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py\", line 400, in call_model\n    response = model_runnable.invoke(messages)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 4396, in invoke\n    return self.bound.invoke(\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 170, in invoke\n    self.generate_prompt(\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 599, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 456, in generate\n    raise e\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 446, in generate\n    self._generate_with_cache(\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 671, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 522, in _generate\n    response = self.client.create(messages=message_dicts, **params)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 590, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/openai/_base_client.py\", line 1240, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/openai/_base_client.py\", line 921, in request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"/Users/ozan/Library/Caches/pypoetry/virtualenvs/assistant-ai-atkT-oAf-py3.12/lib/python3.12/site-packages/openai/_base_client.py\", line 1020, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_v5dzbL3NBrFZ7Z4xZExU9AUv\", 'type': 'invalid_request_error', 'param': 'messages.[8].role', 'code': None}}\n127.0.0.1 - - [24/May/2024 12:54:57] \"POST /chat HTTP/1.1\" 500 -\n\n\nDescription\nI'm trying to use langchain to build an assistant AI agent.\nIf I don't use checkpointer, tool calls work just fine.\nIf I set a SqliteSaver as the checkpointer, the tool calls fail.\nSystem Info\nlangchain==0.2.0\nlangchain-community==0.2.0\nlangchain-core==0.2.0\nlangchain-openai==0.1.7\nlangchain-text-splitters==0.2.0\n\nplatform=mac\n\npython 3.12\n", "created_at": "2024-05-24", "closed_at": "2024-05-29", "labels": [], "State": "closed", "Author": "ozankasikci"}
{"issue_number": 1122, "issue_title": "LlamaCpp with mistral fails with [TypeError: can only concatenate str (not \"NoneType\") to str] when function calling.", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nmodel_get_schema_sys=\"\"\"You are an SQL engineer and you have to choose, given the user questions and the list of tables present in your datababase,\nthe most relevant tables that will be used in the query. You must select ONLY the tables names provided to you and make sure the spelling \nis correct. Call the tool and return ONLY the names of this tables separate by a comma to the tool.\nMake sure the arguments are ONLY names from the available tables and double check the spelling.\n\"\"\"\nmodel_get_schema_prompt = ChatPromptTemplate.from_messages(\n    [(\"system\", model_get_schema_sys), (\"placeholder\", \"{messages}\")])\n\n\nget_schema_bound_tool = model_get_schema_prompt | llm.bind_tools(\n    [get_schema_tool],tool_choice=True)\n\n###LETS TEST IT\n\nbig_openai_message={'messages': [HumanMessage(content='Which artist has the most songs after Iron Maiden?', id='639046f6-f4d0-4f4c-95da-0926243df500'), AIMessage(content='', id='73f957a5-6623-4cde-be02-a4f87b72989d', tool_calls=[{'name': 'sql_db_list_tables', 'args': {}, 'id': 'tool_abcd123', 'type': 'tool_call'}]), ToolMessage(content='Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track', name='sql_db_list_tables', id='935a7225-2719-482c-a032-be0f754893df', tool_call_id='tool_abcd123')]}\n\n#Llamacpp+Mistral will NOT work with this\nget_schema_bound_tool.invoke(big_openai_message)\n\n#Llamacpp+Mistral WILL work with this. The message is just a mock message but it makes me think #is a formatting issue?\nget_schema_bound_tool.invoke({\"messages\": [(\"user\", \"What is the artist with most tracks? Tables available on the Database are Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\")]})\nError Message and Stack Trace (if applicable)\n4651 def invoke(\n   4652     self,\n   4653     input: Input,\n   4654     config: Optional[RunnableConfig] = None,\n   4655     **kwargs: Optional[Any],\n   4656 ) -> Output:\n-> 4657     return self.bound.invoke(\n   4658         input,\n   4659         self._merge_configs(config),\n...\n--> 939 raise rewrite_traceback_stack(source=source)\n\nFile <template>:2, in top-level template code()\n\nTypeError: can only concatenate str (not \"NoneType\") to str\nDescription\nI am trying to adapt your SQL langgraph Agent tutorial for a local model, using ChatLlamaCpp and Mistral (both support tool calling). It wouldn't work for that Type error so I troubled shot it until here. I added this prompt instead of just binding it right away. I dug out the state that is passed to the tool when using openAI model (in the code as big_openai_message) and try to test the tool with that but it fails. It does work with a mock message that I provided that is formatted differently. Any suggestions?\nSystem Info\nlanggraph==0.1.9", "created_at": "2024-07-24", "closed_at": "2025-01-15", "labels": [], "State": "closed", "Author": "GabHoo"}
{"issue_number": 1084, "issue_title": "ToolMessage `artifact` failed to save in memory", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\n(content='success', name='sql_database_query', id='e419a154-04ef-4df5-b04a-e3e6961abb81', tool_call_id='call_bD8KUN0fUAVx4BmcONICNVnv', artifact=[('Customer#000143500', Decimal('7012696.48')), ('Customer#000095257', Decimal('6563511.23')), ('Customer#000087115', Decimal('6457526.26')), ('Customer#000131113', Decimal('6311428.86')), ('Customer#000103834', Decimal('6306524.23'))])``\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[4], line 1\n----> 1 sql_agent.stream(\"I want to get the top 5 customers which making the most purchases\", tool_mode=ToolMode.CONTINUOUS, display=True)\n\nFile ~/dev/aita/aita/aita/agent/graph.py:171, in GraphAgent.stream(self, question, tool_mode, max_iterations, display)\n    169         if display:\n    170             print(f\"Iteration {iterations}\")\n--> 171             self._print_graph_events(events, _printed)\n    172         current_state = self.get_current_state()\n    173 if self.output_parser:\n\nFile ~/dev/aita/aita/aita/agent/graph.py:257, in GraphAgent._print_graph_events(self, events, printed, max_length)\n    255 if isinstance(events, dict):\n    256     events = [events]\n--> 257 for event in events:\n    258     messages = self._get_event_message(event)\n    259     for message in messages:\n\nFile ~/Library/Caches/pypoetry/virtualenvs/aita-YD6hEonJ-py3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py:884, in Pregel.stream(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\n    882 start = saved.metadata.get(\"step\", -2) + 1 if saved else -1\n    883 # create channels from checkpoint\n--> 884 with BackgroundExecutor(config) as submit, ChannelsManager(\n    885     self.channels, checkpoint, config\n    886 ) as channels, ManagedValuesManager(\n    887     self.managed_values_dict, config, self\n    888 ) as managed:\n    890     def put_writes(task_id: str, writes: Sequence[tuple[str, Any]]) -> None:\n    891         if self.checkpointer is not None:\n\nFile /opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:144, in _GeneratorContextManager.__exit__(self, typ, value, traceback)\n    142 if typ is None:\n    143     try:\n--> 144         next(self.gen)\n    145     except StopIteration:\n    146         return False\n\nFile ~/Library/Caches/pypoetry/virtualenvs/aita-YD6hEonJ-py3.12/lib/python3.12/site-packages/langgraph/pregel/executor.py:70, in BackgroundExecutor(config)\n     67         # executor waits for all tasks to finish on exit\n     68 for task in tasks:\n     69     # the first task to have raised an exception will be re-raised here\n---> 70     task.result()\n\nFile /opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:449, in Future.result(self, timeout)\n    447     raise CancelledError()\n    448 elif self._state == FINISHED:\n--> 449     return self.__get_result()\n    451 self._condition.wait(timeout)\n    453 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\nFile /opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401, in Future.__get_result(self)\n    399 if self._exception:\n    400     try:\n--> 401         raise self._exception\n    402     finally:\n    403         # Break a reference cycle with the exception in self._exception\n    404         self = None\n\nFile ~/Library/Caches/pypoetry/virtualenvs/aita-YD6hEonJ-py3.12/lib/python3.12/site-packages/langgraph/pregel/executor.py:43, in BackgroundExecutor.<locals>.done(task)\n     41 def done(task: concurrent.futures.Future) -> None:\n     42     try:\n---> 43         task.result()\n     44     except BaseException:\n     45         pass\n\nFile /opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:449, in Future.result(self, timeout)\n    447     raise CancelledError()\n    448 elif self._state == FINISHED:\n--> 449     return self.__get_result()\n    451 self._condition.wait(timeout)\n    453 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\nFile /opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401, in Future.__get_result(self)\n    399 if self._exception:\n    400     try:\n--> 401         raise self._exception\n    402     finally:\n    403         # Break a reference cycle with the exception in self._exception\n    404         self = None\n\nFile /opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/thread.py:58, in _WorkItem.run(self)\n     55     return\n     57 try:\n---> 58     result = self.fn(*self.args, **self.kwargs)\n     59 except BaseException as exc:\n     60     self.future.set_exception(exc)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/aita-YD6hEonJ-py3.12/lib/python3.12/site-packages/langgraph/checkpoint/memory.py:168, in MemorySaver.put(self, config, checkpoint, metadata)\n    147 def put(\n    148     self,\n    149     config: RunnableConfig,\n    150     checkpoint: Checkpoint,\n    151     metadata: CheckpointMetadata,\n    152 ) -> RunnableConfig:\n    153     \"\"\"Save a checkpoint to the in-memory storage.\n    154 \n    155     This method saves a checkpoint to the in-memory storage. The checkpoint is associated\n   (...)\n    163         RunnableConfig: The updated config containing the saved checkpoint's timestamp.\n    164     \"\"\"\n    165     self.storage[config[\"configurable\"][\"thread_id\"]].update(\n    166         {\n    167             checkpoint[\"id\"]: (\n--> 168                 self.serde.dumps(checkpoint),\n    169                 self.serde.dumps(metadata),\n    170             )\n    171         }\n    172     )\n    173     return {\n    174         \"configurable\": {\n    175             \"thread_id\": config[\"configurable\"][\"thread_id\"],\n    176             \"thread_ts\": checkpoint[\"id\"],\n    177         }\n    178     }\n\nFile ~/Library/Caches/pypoetry/virtualenvs/aita-YD6hEonJ-py3.12/lib/python3.12/site-packages/langgraph/serde/jsonplus.py:98, in JsonPlusSerializer.dumps(self, obj)\n     97 def dumps(self, obj: Any) -> bytes:\n---> 98     return json.dumps(obj, default=self._default, ensure_ascii=False).encode(\n     99         \"utf-8\", \"ignore\"\n    100     )\n\nFile /opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py:238, in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\n    232 if cls is None:\n    233     cls = JSONEncoder\n    234 return cls(\n    235     skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n    236     check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n    237     separators=separators, default=default, sort_keys=sort_keys,\n--> 238     **kw).encode(obj)\n\nFile /opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/encoder.py:200, in JSONEncoder.encode(self, o)\n    196         return encode_basestring(o)\n    197 # This doesn't pass the iterator directly to ''.join() because the\n    198 # exceptions aren't as detailed.  The list call should be roughly\n    199 # equivalent to the PySequence_Fast that ''.join() would do.\n--> 200 chunks = self.iterencode(o, _one_shot=True)\n    201 if not isinstance(chunks, (list, tuple)):\n    202     chunks = list(chunks)\n\nFile /opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/encoder.py:258, in JSONEncoder.iterencode(self, o, _one_shot)\n    253 else:\n    254     _iterencode = _make_iterencode(\n    255         markers, self.default, _encoder, self.indent, floatstr,\n    256         self.key_separator, self.item_separator, self.sort_keys,\n    257         self.skipkeys, _one_shot)\n--> 258 return _iterencode(o, 0)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/aita-YD6hEonJ-py3.12/lib/python3.12/site-packages/langgraph/serde/jsonplus.py:72, in JsonPlusSerializer._default(self, obj)\n     68     return self._encode_constructor_args(\n     69         obj.__class__, kwargs={\"node\": obj.node, \"arg\": obj.arg}\n     70     )\n     71 else:\n---> 72     raise TypeError(\n     73         f\"Object of type {obj.__class__.__name__} is not JSON serializable\"\n     74     )\n\nTypeError: Object of type Decimal is not JSON serializable\nDescription\nWhen the tool message contains artifact, it failed to save to memory because it is raw and not stringified\nerror:\nTypeError: Object of type Decimal is not JSON serializable\nSystem Info\nNA", "created_at": "2024-07-22", "closed_at": "2024-07-22", "labels": [], "State": "closed", "Author": "HaoXuAI"}
{"issue_number": 1069, "issue_title": "BadRequestError with create_react_agent, tavily tool and ChatOpenAI model", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.messages import BaseMessage\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\n\nfrom operator import add\nfrom typing import TypedDict, Annotated\n\n\nSYSTEM_INIT_PROMPT = \"\"\"\nYou are a helpful assistant.\n\nToday is {today}.\n\"\"\"\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", SYSTEM_INIT_PROMPT),\n        (\"placeholder\", \"{messages}\"),\n    ]\n)\n\nclass AgentState(TypedDict):\n    today: str\n    messages: Annotated[list[BaseMessage], add]\n    is_last_step: str\n\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\nnew_react_agent = create_react_agent(model, [TavilySearchResults(max_results=3)], state_schema=AgentState, state_modifier=prompt)\nnew_react_agent.invoke({\n    \"messages\": [(\"human\", \"what's the weather in sf? make sure to mention today's date\")], \n    \"today\": \"July 12, 2004\"\n})\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nBadRequestError                           Traceback (most recent call last)\nCell In[10], line 33\n     30 model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n     32 new_react_agent = create_react_agent(model, [TavilySearchResults(max_results=3)], state_schema=AgentState, state_modifier=prompt)\n---> 33 new_react_agent.invoke({\n     34     \"messages\": [(\"human\", \"what's the weather in sf? make sure to mention today's date\")], \n     35     \"today\": \"July 12, 2004\"\n     36 })\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1668, in Pregel.invoke(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug, **kwargs)\n   1666 else:\n   1667     chunks = []\n-> 1668 for chunk in self.stream(\n   1669     input,\n   1670     config,\n   1671     stream_mode=stream_mode,\n   1672     output_keys=output_keys,\n   1673     input_keys=input_keys,\n   1674     interrupt_before=interrupt_before,\n   1675     interrupt_after=interrupt_after,\n   1676     debug=debug,\n   1677     **kwargs,\n   1678 ):\n   1679     if stream_mode == \"values\":\n   1680         latest = chunk\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1111, in Pregel.stream(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\n   1108         del fut, task\n   1110 # panic on failure or timeout\n-> 1111 _panic_or_proceed(done, inflight, step)\n   1112 # don't keep futures around in memory longer than needed\n   1113 del done, inflight, futures\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1758, in _panic_or_proceed(done, inflight, step, timeout_exc_cls)\n   1756             inflight.pop().cancel()\n   1757         # raise the exception\n-> 1758         raise exc\n   1760 if inflight:\n   1761     # if we got here means we timed out\n   1762     while inflight:\n   1763         # cancel all pending tasks\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/langgraph/pregel/executor.py:43, in BackgroundExecutor.<locals>.done(task)\n     41 def done(task: concurrent.futures.Future) -> None:\n     42     try:\n---> 43         task.result()\n     44     except BaseException:\n     45         pass\n\nFile /opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:449, in Future.result(self, timeout)\n    447     raise CancelledError()\n    448 elif self._state == FINISHED:\n--> 449     return self.__get_result()\n    451 self._condition.wait(timeout)\n    453 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\nFile /opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401, in Future.__get_result(self)\n    399 if self._exception:\n    400     try:\n--> 401         raise self._exception\n    402     finally:\n    403         # Break a reference cycle with the exception in self._exception\n    404         self = None\n\nFile /opt/homebrew/Cellar/python@3.12/3.12.3/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/thread.py:58, in _WorkItem.run(self)\n     55     return\n     57 try:\n---> 58     result = self.fn(*self.args, **self.kwargs)\n     59 except BaseException as exc:\n     60     self.future.set_exception(exc)\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/langgraph/pregel/retry.py:25, in run_with_retry(task, retry_policy)\n     23 task.writes.clear()\n     24 # run the task\n---> 25 task.proc.invoke(task.input, task.config)\n     26 # if successful, end\n     27 break\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:2822, in RunnableSequence.invoke(self, input, config, **kwargs)\n   2818 config = patch_config(\n   2819     config, callbacks=run_manager.get_child(f\"seq:step:{i+1}\")\n   2820 )\n   2821 if i == 0:\n-> 2822     input = step.invoke(input, config, **kwargs)\n   2823 else:\n   2824     input = step.invoke(input, config)\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:4387, in RunnableLambda.invoke(self, input, config, **kwargs)\n   4373 \"\"\"Invoke this Runnable synchronously.\n   4374 \n   4375 Args:\n   (...)\n   4384     TypeError: If the Runnable is a coroutine function.\n   4385 \"\"\"\n   4386 if hasattr(self, \"func\"):\n-> 4387     return self._call_with_config(\n   4388         self._invoke,\n   4389         input,\n   4390         self._config(config, self.func),\n   4391         **kwargs,\n   4392     )\n   4393 else:\n   4394     raise TypeError(\n   4395         \"Cannot invoke a coroutine function synchronously.\"\n   4396         \"Use `ainvoke` instead.\"\n   4397     )\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:1734, in Runnable._call_with_config(self, func, input, config, run_type, **kwargs)\n   1730     context = copy_context()\n   1731     context.run(_set_config_context, child_config)\n   1732     output = cast(\n   1733         Output,\n-> 1734         context.run(\n   1735             call_func_with_variable_args,  # type: ignore[arg-type]\n   1736             func,  # type: ignore[arg-type]\n   1737             input,  # type: ignore[arg-type]\n   1738             config,\n   1739             run_manager,\n   1740             **kwargs,\n   1741         ),\n   1742     )\n   1743 except BaseException as e:\n   1744     run_manager.on_chain_error(e)\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:379, in call_func_with_variable_args(func, input, config, run_manager, **kwargs)\n    377 if run_manager is not None and accepts_run_manager(func):\n    378     kwargs[\"run_manager\"] = run_manager\n--> 379 return func(input, **kwargs)\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:4243, in RunnableLambda._invoke(self, input, run_manager, config, **kwargs)\n   4241                 output = chunk\n   4242 else:\n-> 4243     output = call_func_with_variable_args(\n   4244         self.func, input, config, run_manager, **kwargs\n   4245     )\n   4246 # If the output is a Runnable, invoke it\n   4247 if isinstance(output, Runnable):\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:379, in call_func_with_variable_args(func, input, config, run_manager, **kwargs)\n    377 if run_manager is not None and accepts_run_manager(func):\n    378     kwargs[\"run_manager\"] = run_manager\n--> 379 return func(input, **kwargs)\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py:579, in create_react_agent.<locals>.call_model(state, config)\n    575 def call_model(\n    576     state: AgentState,\n    577     config: RunnableConfig,\n    578 ):\n--> 579     response = model_runnable.invoke(state, config)\n    580     if state[\"is_last_step\"] and response.tool_calls:\n    581         return {\n    582             \"messages\": [\n    583                 AIMessage(\n   (...)\n    587             ]\n    588         }\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:2824, in RunnableSequence.invoke(self, input, config, **kwargs)\n   2822             input = step.invoke(input, config, **kwargs)\n   2823         else:\n-> 2824             input = step.invoke(input, config)\n   2825 # finish the root run\n   2826 except BaseException as e:\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5006, in RunnableBindingBase.invoke(self, input, config, **kwargs)\n   5000 def invoke(\n   5001     self,\n   5002     input: Input,\n   5003     config: Optional[RunnableConfig] = None,\n   5004     **kwargs: Optional[Any],\n   5005 ) -> Output:\n-> 5006     return self.bound.invoke(\n   5007         input,\n   5008         self._merge_configs(config),\n   5009         **{**self.kwargs, **kwargs},\n   5010     )\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:265, in BaseChatModel.invoke(self, input, config, stop, **kwargs)\n    254 def invoke(\n    255     self,\n    256     input: LanguageModelInput,\n   (...)\n    260     **kwargs: Any,\n    261 ) -> BaseMessage:\n    262     config = ensure_config(config)\n    263     return cast(\n    264         ChatGeneration,\n--> 265         self.generate_prompt(\n    266             [self._convert_input(input)],\n    267             stop=stop,\n    268             callbacks=config.get(\"callbacks\"),\n    269             tags=config.get(\"tags\"),\n    270             metadata=config.get(\"metadata\"),\n    271             run_name=config.get(\"run_name\"),\n    272             run_id=config.pop(\"run_id\", None),\n    273             **kwargs,\n    274         ).generations[0][0],\n    275     ).message\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:698, in BaseChatModel.generate_prompt(self, prompts, stop, callbacks, **kwargs)\n    690 def generate_prompt(\n    691     self,\n    692     prompts: List[PromptValue],\n   (...)\n    695     **kwargs: Any,\n    696 ) -> LLMResult:\n    697     prompt_messages = [p.to_messages() for p in prompts]\n--> 698     return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:555, in BaseChatModel.generate(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\n    553         if run_managers:\n    554             run_managers[i].on_llm_error(e, response=LLMResult(generations=[]))\n--> 555         raise e\n    556 flattened_outputs = [\n    557     LLMResult(generations=[res.generations], llm_output=res.llm_output)  # type: ignore[list-item]\n    558     for res in results\n    559 ]\n    560 llm_output = self._combine_llm_outputs([res.llm_output for res in results])\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:545, in BaseChatModel.generate(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\n    542 for i, m in enumerate(messages):\n    543     try:\n    544         results.append(\n--> 545             self._generate_with_cache(\n    546                 m,\n    547                 stop=stop,\n    548                 run_manager=run_managers[i] if run_managers else None,\n    549                 **kwargs,\n    550             )\n    551         )\n    552     except BaseException as e:\n    553         if run_managers:\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:770, in BaseChatModel._generate_with_cache(self, messages, stop, run_manager, **kwargs)\n    768 else:\n    769     if inspect.signature(self._generate).parameters.get(\"run_manager\"):\n--> 770         result = self._generate(\n    771             messages, stop=stop, run_manager=run_manager, **kwargs\n    772         )\n    773     else:\n    774         result = self._generate(messages, stop=stop, **kwargs)\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:589, in BaseChatOpenAI._generate(self, messages, stop, run_manager, **kwargs)\n    587     generation_info = {\"headers\": dict(raw_response.headers)}\n    588 else:\n--> 589     response = self.client.create(**payload)\n    590     generation_info = None\n    591 return self._create_chat_result(response, generation_info)\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py:277, in required_args.<locals>.inner.<locals>.wrapper(*args, **kwargs)\n    275             msg = f\"Missing required argument: {quote(missing[0])}\"\n    276     raise TypeError(msg)\n--> 277 return func(*args, **kwargs)\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/openai/resources/chat/completions.py:643, in Completions.create(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\n    609 @required_args([\"messages\", \"model\"], [\"messages\", \"model\", \"stream\"])\n    610 def create(\n    611     self,\n   (...)\n    641     timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,\n    642 ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n--> 643     return self._post(\n    644         \"/chat/completions\",\n    645         body=maybe_transform(\n    646             {\n    647                 \"messages\": messages,\n    648                 \"model\": model,\n    649                 \"frequency_penalty\": frequency_penalty,\n    650                 \"function_call\": function_call,\n    651                 \"functions\": functions,\n    652                 \"logit_bias\": logit_bias,\n    653                 \"logprobs\": logprobs,\n    654                 \"max_tokens\": max_tokens,\n    655                 \"n\": n,\n    656                 \"parallel_tool_calls\": parallel_tool_calls,\n    657                 \"presence_penalty\": presence_penalty,\n    658                 \"response_format\": response_format,\n    659                 \"seed\": seed,\n    660                 \"service_tier\": service_tier,\n    661                 \"stop\": stop,\n    662                 \"stream\": stream,\n    663                 \"stream_options\": stream_options,\n    664                 \"temperature\": temperature,\n    665                 \"tool_choice\": tool_choice,\n    666                 \"tools\": tools,\n    667                 \"top_logprobs\": top_logprobs,\n    668                 \"top_p\": top_p,\n    669                 \"user\": user,\n    670             },\n    671             completion_create_params.CompletionCreateParams,\n    672         ),\n    673         options=make_request_options(\n    674             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n    675         ),\n    676         cast_to=ChatCompletion,\n    677         stream=stream or False,\n    678         stream_cls=Stream[ChatCompletionChunk],\n    679     )\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/openai/_base_client.py:1266, in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)\n   1252 def post(\n   1253     self,\n   1254     path: str,\n   (...)\n   1261     stream_cls: type[_StreamT] | None = None,\n   1262 ) -> ResponseT | _StreamT:\n   1263     opts = FinalRequestOptions.construct(\n   1264         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1265     )\n-> 1266     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/openai/_base_client.py:942, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    933 def request(\n    934     self,\n    935     cast_to: Type[ResponseT],\n   (...)\n    940     stream_cls: type[_StreamT] | None = None,\n    941 ) -> ResponseT | _StreamT:\n--> 942     return self._request(\n    943         cast_to=cast_to,\n    944         options=options,\n    945         stream=stream,\n    946         stream_cls=stream_cls,\n    947         remaining_retries=remaining_retries,\n    948     )\n\nFile ~/Documents/Workshop/.venv/lib/python3.12/site-packages/openai/_base_client.py:1046, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)\n   1043         err.response.read()\n   1045     log.debug(\"Re-raising status error\")\n-> 1046     raise self._make_status_error_from_response(err.response) from None\n   1048 return self._process_response(\n   1049     cast_to=cast_to,\n   1050     options=options,\n   (...)\n   1053     stream_cls=stream_cls,\n   1054 )\n\nBadRequestError: Error code: 400 - {'error': {'message': \"Missing required parameter: 'messages[3].content[0].type'.\", 'type': 'invalid_request_error', 'param': 'messages[3].content[0].type', 'code': 'missing_required_parameter'}}\nDescription\nI'm trying to use Tavily tool with the latest code to use create_react_agent from #1023 PR\nSystem Info\nlangchain==0.2.9\nlangchain-anthropic==0.1.20\nlangchain-cli==0.0.25\nlangchain-community==0.2.7\nlangchain-core==0.2.21\nlangchain-experimental==0.0.62\nlangchain-openai==0.1.17\nlangchain-postgres==0.0.9\nlangchain-text-splitters==0.2.2\nlangchainhub==0.1.20\nlanggraph==0.1.9", "created_at": "2024-07-19", "closed_at": "2024-07-24", "labels": [], "State": "closed", "Author": "arthberman"}
{"issue_number": 1068, "issue_title": "Docstrings are outdated for some methods of `AsyncSqliteSaver`, `SqliteSaver`, and `MemorySaver`", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nasync def alist(\n    self,\n    config: Optional[RunnableConfig],\n    *,\n    filter: Optional[Dict[str, Any]] = None,\n    before: Optional[RunnableConfig] = None,\n    limit: Optional[int] = None,\n) -> AsyncIterator[CheckpointTuple]:\n    \"\"\"List checkpoints from the database asynchronously.\n\n    This method retrieves a list of checkpoint tuples from the SQLite database based\n    on the provided config. The checkpoints are ordered by timestamp in descending order.\n\n    Args:\n        config (RunnableConfig): The config to use for listing the checkpoints.\n        before (Optional[RunnableConfig]): If provided, only checkpoints before the specified timestamp are returned. Defaults to None.\n        limit (Optional[int]): The maximum number of checkpoints to return. Defaults to None.\n\n    Yields:\n        AsyncIterator[CheckpointTuple]: An asynchronous iterator of checkpoint tuples.\n    \"\"\"\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nIn AsyncSqliteSaver and SqliteSaver, alist is missing filter, aput is missing metadata, and aput_writes have no docstring. MemorySaver has even more outdated docstrings.\nThis is making me having a hard time confirming that my own CheckpointSaver is functioning identically to the official implementations.\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.19045\nPython Version:  3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.2.20\nlangchain: 0.2.8\nlangchain_community: 0.2.7\nlangsmith: 0.1.88\nlangchain_anthropic: 0.1.20\nlangchain_cohere: 0.1.9\nlangchain_experimental: 0.0.62\nlangchain_openai: 0.1.16\nlangchain_text_splitters: 0.2.2\nlangchainhub: 0.1.20\nlanggraph: 0.1.8\nlangserve: 0.2.2\n", "created_at": "2024-07-19", "closed_at": "2024-07-20", "labels": [], "State": "closed", "Author": "Glinte"}
{"issue_number": 1053, "issue_title": "interrupt_after interrupt before condition logic", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nimport random\nfrom typing import TypedDict, Sequence, Annotated\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langchain_core.messages import BaseMessage, AIMessage, HumanMessage\nfrom langgraph.graph.message import add_messages\nfrom langgraph.graph import StateGraph, END\nfrom pydantic import BaseModel, Field\n\nclass ActionResult[T](BaseModel):\n    code: Annotated[int, Field(description='code')]\n    data: Annotated[T, Field(description='data')]\n    msg: Annotated[str, Field(description='msg')]\n\nclass State(TypedDict):\n    name: str\n    next: str\n    data: str\n    messages: Annotated[Sequence[BaseMessage], add_messages]\n\n\ndef fa() -> ActionResult[str]:\n    x = random.randint(1, 10)\n    print(x)\n    # if x > 6:\n    #     return ActionResult(code=0, data='test', msg='')\n    # else:\n    #     return ActionResult(code=1, data='', msg='error')\n    return ActionResult(code=0, data='test', msg='')\n\ndef llm(state: State) -> dict:\n    res = fa()\n    if res.code == 0:\n        return {'next': 'a', 'data': res.data}\n    else:\n        return {'next': 'b'}\n\ndef cond(state: State) -> str:\n    return state.get('next')\n\ndef a(state: State) -> dict:\n    print('a')\n    return {'messages': [AIMessage(content=state.get('data'))]}\n\ndef b(state: State) -> dict:\n    print('b')\n    return {'messages': [AIMessage(content='node b')]}\n\ndef c(state: State) -> dict:\n    print('c')\n    return {'messages': [AIMessage(content='node c')]}\n\ndef d(state: State) -> dict:\n    print('d')\n    return {'messages': [AIMessage(content='node d')]}\n\ndef continue_exec(state: State) -> dict:\n    print('continue? A: continue B: End')\n    return {'messages': [AIMessage(content='continue? A: continue B: End')]}\n\ndef continue_judge(state: State) -> str:\n    msg = state.get('messages')[-1].content\n    if msg == 'A':\n        return 'd'\n    else:\n        return END\n\ngraph = StateGraph(State)\ngraph.add_node('llm', llm)\ngraph.add_node('a', a)\ngraph.add_node('b', b)\ngraph.add_node('c', c)\ngraph.add_node('continue', continue_exec)\ngraph.add_node('d', d)\ngraph.add_conditional_edges('llm', cond)\ngraph.add_edge('a', 'c')\ngraph.add_edge('c', 'continue')\ngraph.add_conditional_edges('continue', continue_judge)\ngraph.add_edge('b', END)\ngraph.add_edge('d', END)\ngraph.set_entry_point('llm')\n\nmemory = MemorySaver()\napp = graph.compile(checkpointer=memory, interrupt_after=['continue'])\ncfg = {\"configurable\": {\"thread_id\": \"thread-1\"}}\nres = app.invoke(State(name='xxx', next='', data='', messages=[]), cfg)\nprint(res)\nsnapshot = app.get_state({\"configurable\": {\"thread_id\": \"thread-1\"}})\nsnapshot.values['messages'].append(HumanMessage(content='A'))\napp.update_state(cfg, snapshot.values)\nr = app.invoke(None, config=cfg, stream_mode='values')\nprint(r)\nError Message and Stack Trace (if applicable)\nNo response\nDescription\ni want the graph interrupt after the node logic but the befor at condition jump logic, the interrupt is human input node, when i get the input the condition logic will invoke\nSystem Info\nlangchain==0.2.1\nlangchain-community==0.2.4\nlangchain-core==0.2.3\nlangchain-openai==0.1.8\nlangchain-text-splitters==0.2.0\nlanggraph==0.0.69\nlangsmith==0.1.65\nlinux\npython 3.12", "created_at": "2024-07-18", "closed_at": "2024-07-22", "labels": [], "State": "closed", "Author": "cjdxhjj"}
{"issue_number": 1040, "issue_title": "The bug report template has links to the langchain repository rather than langgraph", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nDescription\nOn github go to Issues | New Issue, everything is linking to langchain.\n\n", "created_at": "2024-07-17", "closed_at": "2024-07-19", "labels": [], "State": "closed", "Author": "Glinte"}
{"issue_number": 1028, "issue_title": "Incorrect `add_node` types for `Callable`", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langgraph.graph import StateGraph\nfrom langchain_core.messages import HumanMessage\nfrom typing import TypedDict\nfrom langchain_core.runnables.config import RunnableConfig\n\n\nclass MyState(TypedDict, total=False):\n    input: HumanMessage\n\n\ndef some_node(state: MyState, config: RunnableConfig) -> MyState:\n    return state\n\n\nworkflow = StateGraph(MyState)\nworkflow.add_node(\"some_node\", some_node) # Will have a type error\nError Message and Stack Trace (if applicable)\nerror: Argument 2 to \"add_node\" of \"StateGraph\" has incompatible type \"Callable[[MyState, RunnableConfig], MyState]\"; expected \"Runnable[Any, Any] | Callable[[Any], Any] | Callable[[Any], Awaitable[Any]] | Callable[[Iterator[Any]], Iterator[Any]] | Callable[[AsyncIterator[Any]], AsyncIterator[Any]] | Mapping[str, Any]\"  [arg-type]\nDescription\nadd_node has a type of RunnableLike, however RunnableLike's type does not account for a Callable that takes both the state and the RunnableConfig. In the how-to guides for passing runtime values into tools, it mentions that the second argument is a RunnableConfig.\nSystem Info\npython=3.10.4\nlanggraph==0.1.8\nlangchain==0.2.8\nlangchain-community==0.2.7\nlangchain-core==0.2.19\nlangchain-experimental==0.0.61\nlangchain-openai==0.1.16\nlangchain-text-splitters==0.2.2\n", "created_at": "2024-07-15", "closed_at": "2024-09-18", "labels": [], "State": "closed", "Author": "chadhietala"}
{"issue_number": 1020, "issue_title": "How to Pass thread_id to langserve chat playground for langgraph?", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\napp = FastAPI(\n    title=\"Biscuit AI LangChain Server\",\n    version=\"1.0\",\n    description=\"Api server using LangChain's Runnable interfaces\",\n)\n\n\nclass InputChat(BaseModel):\n    \"\"\"Input for the chat endpoint.\"\"\"\n\n    messages: List[Union[HumanMessage, AIMessage, SystemMessage]] = Field(\n        ...,\n        description=\"The chat messages representing the current conversation.\",\n    )\n\n    input: str\n\n\ndef output_parsing_for_playground(agent_output):\n    data = agent_output[-1]\n\n    content = next(iter(data.values()))['messages'][0].content\n\n    print(\"graph output : \",agent_output[-1])\n    print(\"content : \",content)\n\n    return content\n\n\nadd_routes(\n    app,\n    (graph | RunnableLambda(output_parsing_for_playground)).with_types(input_type=InputChat, output_type=str),\n    playground_type=\"chat\"\n)\n\nif __name__ == \"__main__\":\n    import uvicorn\n    host = \"localhost\"\n    port = 8181\n\n    uvicorn.run(app, host=host, port=port)\nError Message and Stack Trace (if applicable)\n----------------------------------------------------------------------------------------------------\nINFO:     Started server process [96599]\nINFO:     Waiting for application startup.\n\n __          ___      .__   __.   _______      _______. _______ .______     ____    ____  _______\n|  |        /   \\     |  \\ |  |  /  _____|    /       ||   ____||   _  \\    \\   \\  /   / |   ____|\n|  |       /  ^  \\    |   \\|  | |  |  __     |   (----`|  |__   |  |_)  |    \\   \\/   /  |  |__\n|  |      /  /_\\  \\   |  . `  | |  | |_ |     \\   \\    |   __|  |      /      \\      /   |   __|\n|  `----./  _____  \\  |  |\\   | |  |__| | .----)   |   |  |____ |  |\\  \\----.  \\    /    |  |____\n|_______/__/     \\__\\ |__| \\__|  \\______| |_______/    |_______|| _| `._____|   \\__/     |_______|\n\nLANGSERVE: Playground for chain \"/\" is live at:\nLANGSERVE:  \u2502\nLANGSERVE:  \u2514\u2500\u2500> /playground/\nLANGSERVE:\nLANGSERVE: See all available routes at /docs/\n\nLANGSERVE: \u26a0\ufe0f Using pydantic 2.8.2. OpenAPI docs for invoke, batch, stream, stream_log endpoints will not be generated. API endpoints and playground should work as expected. If you need to see the docs, you can downgrade to pydantic 1. For example, `pip install pydantic==1.10.13`. See https://github.com/tiangolo/fastapi/issues/10360 for details.\n\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://localhost:8181 (Press CTRL+C to quit)\nINFO:     127.0.0.1:50628 - \"GET /playground/ HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:50628 - \"GET /playground/assets/index-86d4d9c0.js HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:50632 - \"GET /playground/assets/index-434ff580.css HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:50632 - \"GET /c/N4XyA/input_schema HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:50628 - \"GET /c/N4XyA/output_schema HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:50632 - \"GET /playground/favicon.ico HTTP/1.1\" 200 OK\nINFO:     127.0.0.1:50628 - \"POST /stream_log HTTP/1.1\" 200 OK\nERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/sse_starlette/sse.py\", line 282, in __call__\n    await wrap(partial(self.listen_for_disconnect, receive))\n  File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/sse_starlette/sse.py\", line 271, in wrap\n    await func()\n  File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/sse_starlette/sse.py\", line 221, in listen_for_disconnect\n    message = await receive()\n  File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 553, in receive\n    await self.message_event.wait()\n  File \"/usr/lib/python3.10/asyncio/locks.py\", line 214, in wait\n    await fut\nasyncio.exceptions.CancelledError: Cancelled by cancel scope 71986c155330\n\nDuring handling of the above exception, another exception occurred:\n\n  + Exception Group Traceback (most recent call last):\n  |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 399, in run_asgi\n  |     result = await app(  # type: ignore[func-returns-value]\n  |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 70, in __call__\n  |     return await self.app(scope, receive, send)\n  |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/fastapi/applications.py\", line 1054, in __call__\n  |     await super().__call__(scope, receive, send)\n  |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/starlette/applications.py\", line 123, in __call__\n  |     await self.middleware_stack(scope, receive, send)\n  |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n  |     raise exc\n  |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n  |     await self.app(scope, receive, _send)\n  |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 65, in __call__\n  |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n  |     raise exc\n  |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n  |     await app(scope, receive, sender)\n  |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/starlette/routing.py\", line 756, in __call__\n  |     await self.middleware_stack(scope, receive, send)\n  |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/starlette/routing.py\", line 776, in app\n  |     await route.handle(scope, receive, send)\n  |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/starlette/routing.py\", line 297, in handle\n  |     await self.app(scope, receive, send)\n  |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/starlette/routing.py\", line 77, in app\n  |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n  |     raise exc\n  |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n  |     await app(scope, receive, sender)\n  |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/starlette/routing.py\", line 75, in app\n  |     await response(scope, receive, send)\n  |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/sse_starlette/sse.py\", line 268, in __call__\n  |     async with anyio.create_task_group() as task_group:\n  |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 680, in __aexit__\n  |     raise BaseExceptionGroup(\n  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/sse_starlette/sse.py\", line 271, in wrap\n    |     await func()\n    |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/sse_starlette/sse.py\", line 251, in stream_response\n    |     async for data in self.body_iterator:\n    |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/langserve/api_handler.py\", line 1214, in _stream_log\n    |     async for chunk in self._runnable.astream_log(\n    |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 969, in astream_log\n    |     async for item in _astream_log_implementation(  # type: ignore\n    |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/langchain_core/tracers/log_stream.py\", line 635, in _astream_log_implementation\n    |     await task\n    |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/langchain_core/tracers/log_stream.py\", line 589, in consume_astream\n    |     async for chunk in runnable.astream(input, config, **kwargs):\n    |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5161, in astream\n    |     async for item in self.bound.astream(\n    |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3199, in astream\n    |     async for chunk in self.atransform(input_aiter(), config, **kwargs):\n    |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3182, in atransform\n    |     async for chunk in self._atransform_stream_with_config(\n    |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2114, in _atransform_stream_with_config\n    |     chunk = cast(Output, await py_anext(iterator))\n    |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/langchain_core/tracers/log_stream.py\", line 239, in tap_output_aiter\n    |     async for chunk in output:\n    |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3152, in _atransform\n    |     async for output in final_pipeline:\n    |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 4585, in atransform\n    |     async for output in self._atransform_stream_with_config(\n    |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2067, in _atransform_stream_with_config\n    |     final_input: Optional[Input] = await py_anext(input_for_tracing, None)\n    |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/langchain_core/utils/aiter.py\", line 66, in anext_impl\n    |     return await __anext__(iterator)\n    |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/langchain_core/utils/aiter.py\", line 101, in tee_peer\n    |     item = await iterator.__anext__()\n    |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 1281, in atransform\n    |     async for output in self.astream(final, config, **kwargs):\n    |   File \"/home/arslan/.virtualenvs/biscuit_ai_langgraph/lib/python3.10/site-packages/langgraph/pregel/__init__.py\", line 1297, in astream\n    |     raise ValueError(\n    | ValueError: Checkpointer requires one or more of the following 'configurable' keys: ['thread_id', 'thread_ts']\nDescription\n.\nSystem Info\n.", "created_at": "2024-07-15", "closed_at": "2024-09-03", "labels": [], "State": "closed", "Author": "Arslan-Mehmood1"}
{"issue_number": 1019, "issue_title": "DOC: Table of Content incomplete in Quickstart Tutorial", "issue_body": "Issue with current documentation:\nThis is what I am seeing in Quickstart Tutorial\n\nEverything past \"Part 3: Adding Memory to the Chatbot\" is not visible in Table of Content.\nIdea or request for content:\nNo response", "created_at": "2024-07-15", "closed_at": "2024-07-15", "labels": [], "State": "closed", "Author": "AdityaPrasad275"}
{"issue_number": 1006, "issue_title": "DOC: Code Error in Self-RAG using local LLMs", "issue_body": "Issue with current documentation:\nin the Graph State code\nthere is a spell error in the print statement which causes an error\n\nIdea or request for content:\nNo response", "created_at": "2024-07-12", "closed_at": "2024-07-16", "labels": [], "State": "closed", "Author": "unworld11"}
{"issue_number": 1002, "issue_title": "Use the firefunction-v2 model of ChatFireworks for multi-agent, and encountered an error: fireworks.client.error.InternalServerError: {'error': {'object': 'error', 'type': 'internal_server_error', 'message': 'server had an error while processing your request, please retry again after a brief wait'}}", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nimport json\nfrom langchain_core.messages import (\n    AIMessage,\n    BaseMessage,\n    ChatMessage,\n    FunctionMessage,\n    HumanMessage,\n)\nfrom langchain.tools.render import format_tool_to_openai_function\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langgraph.graph import END, StateGraph\nfrom langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\nfrom langchain_core.utils.function_calling import convert_to_openai_function\nfrom langchain_core.tools import tool\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_experimental.utilities import PythonREPL\nfrom typing import Annotated\nfrom typing import Dict\nimport subprocess\n\nimport os\nfrom langchain_fireworks import ChatFireworks\n# \u8bbe\u7f6e\u548c\u521d\u59cb\u5316\nllm = ChatFireworks(\n    api_key=\"UAbZbw6jUOa0PCKm6IXebmLlrpDUfqoMkBLr3hKMnV7aTACm\",\n    model=\"accounts/fireworks/models/firefunction-v2\",\n)\n\n\n# nmap\u5de5\u5177\n@tool\n\ndef get_mojuan_price(name: str):\n    \"\"\"\n    To get the price of the Morimaki brand burger.\n    \"\"\"\n    return 66\n\n# \u793a\u4f8b\u8c03\u7528\n\n# \u5c06\u5de5\u5177\u5bfc\u5165\u4e00\u4e2a\u5217\u8868\u4e2d\ntools = [get_mojuan_price]\n\n# \u521b\u5efa\u72b6\u6001\nimport operator\nfrom typing import Annotated, List, Sequence, Tuple, TypedDict, Union\n\nfrom langchain.agents import create_openai_functions_agent\nfrom langchain.tools.render import format_tool_to_openai_function\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_openai import ChatOpenAI\nfrom typing_extensions import TypedDict\n\n\n# \u4e3a\u6bcf\u4e2a\u4ee3\u7406\u548c\u5de5\u5177\u521b\u5efa\u4e0d\u540c\u7684\u8282\u70b9\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], operator.add]\n    sender: str\n\n\ntool_executor = ToolExecutor(tools)\n\n\ndef tool_node(state):\n    \"\"\"This runs tools in the graph. It takes in an agent action and calls that tool and returns the result.\"\"\"\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    tool_call = last_message.additional_kwargs[\"tool_calls\"][0]\n    tool_name = tool_call[\"function\"][\"name\"]\n    tool_input = json.loads(tool_call[\"function\"][\"arguments\"])\n    action = ToolInvocation(\n        tool=tool_name,\n        tool_input=tool_input,\n    )\n    response = tool_executor.invoke(action)\n    function_message = FunctionMessage(\n        content=f\"{tool_name} response: {str(response)}\", name=action.tool\n    )\n    return {\"messages\": [function_message]}\n\ndef router(state):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if \"tool_calls\" in last_message.additional_kwargs:\n        if \"function\" in last_message.additional_kwargs[\"tool_calls\"][0]:\n            return \"call_tool\"\n    if \"FINAL ANSWER\" in last_message.content:\n        return \"end\"\n    return \"continue\"\n\ndef create_agent(llm, tools, system_message: str):\n    \"\"\"Create an agent.\"\"\"\n    functions = [convert_to_openai_function(t) for t in tools]\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                \"You are a helpful AI assistant, collaborating with other assistants.\"\n            ),\n            MessagesPlaceholder(variable_name=\"messages\"),\n        ]\n    )\n    prompt = prompt.partial(system_message=system_message)\n    prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n    return prompt | llm.bind_functions(functions)\n\ndef agent_node(state, agent, name):\n    #state[\"messages\"] = convert_tool_calls_to_function_calls(state[\"messages\"])\n    #print(f\"\u8fd9\u662fagent_node:{state[\"messages\"]}\")\n\n    result = agent.invoke(state)\n    if isinstance(result, FunctionMessage):\n        pass\n    else:\n        result = HumanMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n    return {\n        \"messages\": [result],\n        \"sender\": name,\n    }\n\nresearch_agent = create_agent(\n    llm,\n    [],\n    system_message=\"You should provide the corresponding precise attack command to use for the attack\",\n)\n\nnmap_agent = create_agent(\n    llm,\n    [get_mojuan_price],\n    system_message=\"Users can see any results you scan.\",\n)\n\nimport functools\n\nresearch_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\nchart_node = functools.partial(agent_node, agent=nmap_agent, name=\"Scan_Generator\")\n\nworkflow = StateGraph(AgentState)\n\nworkflow.add_node(\"Researcher\", research_node)\nworkflow.add_node(\"Scan_Generator\", chart_node)\nworkflow.add_node(\"call_tool\", tool_node)\n\nworkflow.add_conditional_edges(\n    \"Researcher\",\n    router,\n    {\"continue\": \"Scan_Generator\", \"call_tool\": \"call_tool\", \"end\": END},\n)\nworkflow.add_conditional_edges(\n    \"Scan_Generator\",\n    router,\n    {\"continue\": \"Researcher\", \"call_tool\": \"call_tool\", \"end\": END},\n)\nworkflow.add_conditional_edges(\n    \"call_tool\",\n    lambda x: x[\"sender\"],\n    {\n        \"Researcher\": \"Researcher\",\n        \"Scan_Generator\": \"Scan_Generator\",\n    },\n)\n\nworkflow.set_entry_point(\"Researcher\")\ngraph = workflow.compile()\n\nfor s in graph.stream(\n    {\n        \"messages\": [\n            HumanMessage(\n                content=\"How much is the Morimaki Beef Burger? \"\n            )\n        ],\n    },\n    {\"recursion_limit\": 50},\n):\n    print(s)\n    print(\"----\")\nError Message and Stack Trace (if applicable)\n{'Researcher': {'messages': [HumanMessage(content='I need to know the current menu prices to answer this question. Can you please provide the current menu prices?', response_metadata={'token_usage': {'prompt_tokens': 280, 'total_tokens': 303, 'completion_tokens': 23}, 'model_name': 'accounts/fireworks/models/firefunction-v2', 'system_fingerprint': '', 'finish_reason': 'stop', 'logprobs': None}, name='Researcher', id='run-b4df2be4-131e-43eb-bf6a-ee53981f8501-0', tool_calls=[], usage_metadata={'input_tokens': 280, 'output_tokens': 23, 'total_tokens': 303}, invalid_tool_calls=[])], 'sender': 'Researcher'}}\n----\n{'Scan_Generator': {'messages': [HumanMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_w3VH2fJG1Q9LoZYLcr92VicZ', 'type': 'function', 'function': {'name': 'get_mojuan_price', 'arguments': '{\"name\": \"Morimaki Beef Burger\"}'}}]}, response_metadata={'token_usage': {'prompt_tokens': 385, 'total_tokens': 411, 'completion_tokens': 26}, 'model_name': 'accounts/fireworks/models/firefunction-v2', 'system_fingerprint': '', 'finish_reason': 'tool_calls', 'logprobs': None}, name='Scan_Generator', id='run-72e5902a-49ec-422d-86e5-855e7a14678a-0', tool_calls=[{'name': 'get_mojuan_price', 'args': {'name': 'Morimaki Beef Burger'}, 'id': 'call_w3VH2fJG1Q9LoZYLcr92VicZ'}], usage_metadata={'input_tokens': 385, 'output_tokens': 26, 'total_tokens': 411}, invalid_tool_calls=[])], 'sender': 'Scan_Generator'}}\n----\n{'call_tool': {'messages': [FunctionMessage(content='get_mojuan_price response: 66', name='get_mojuan_price')]}}\n----\nTraceback (most recent call last):\n  File \"E:\\python-test\\VulnMapAI-main\\test6.py\", line 166, in <module>\n    for s in graph.stream(\n  File \"D:\\python310\\lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 1073, in stream\n    _panic_or_proceed(done, inflight, step)\n  File \"D:\\python310\\lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 1643, in _panic_or_proceed\n    raise exc\n  File \"D:\\python310\\lib\\concurrent\\futures\\thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"D:\\python310\\lib\\site-packages\\langgraph\\pregel\\retry.py\", line 72, in run_with_retry\n    task.proc.invoke(task.input, task.config)\n  File \"D:\\python310\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2505, in invoke\n    input = step.invoke(input, config, **kwargs)\n  File \"D:\\python310\\lib\\site-packages\\langgraph\\utils.py\", line 95, in invoke\n    ret = context.run(self.func, input, **kwargs)\n  File \"E:\\python-test\\VulnMapAI-main\\test6.py\", line 111, in agent_node\n    result = agent.invoke(state)\n  File \"D:\\python310\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2507, in invoke\n    input = step.invoke(input, config)\n  File \"D:\\python310\\lib\\site-packages\\langchain_core\\runnables\\base.py\", line 4588, in invoke\n    return self.bound.invoke(\n  File \"D:\\python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 248, in invoke\n    self.generate_prompt(\n  File \"D:\\python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 677, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n  File \"D:\\python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 534, in generate\n    raise e\n  File \"D:\\python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 524, in generate\n    self._generate_with_cache(\n  File \"D:\\python310\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 749, in _generate_with_cache\n    result = self._generate(\n  File \"D:\\python310\\lib\\site-packages\\langchain_fireworks\\chat_models.py\", line 477, in _generate\n    response = self.client.create(messages=message_dicts, **params)\n  File \"D:\\python310\\lib\\site-packages\\fireworks\\client\\chat_completion.py\", line 40, in create\n    return super().create(\n  File \"D:\\python310\\lib\\site-packages\\fireworks\\client\\base_completion.py\", line 76, in create\n    return cls._create_non_streaming(\n  File \"D:\\python310\\lib\\site-packages\\fireworks\\client\\base_completion.py\", line 158, in _create_non_streaming\n    response = client.post_request_non_streaming(\n  File \"D:\\python310\\lib\\site-packages\\fireworks\\client\\api_client.py\", line 131, in post_request_non_streaming\n    self._error_handling(response)\n  File \"D:\\python310\\lib\\site-packages\\fireworks\\client\\api_client.py\", line 112, in _error_handling\n    self._raise_for_status(resp)\n  File \"D:\\python310\\lib\\site-packages\\fireworks\\client\\api_client.py\", line 94, in _raise_for_status\n    raise InternalServerError(get_error_message(resp, \"internal_server_error\"))\nfireworks.client.error.InternalServerError: {'error': {'object': 'error', 'type': 'internal_server_error', 'message': 'server had an error while processing your request, please retry again after a brief wait'}}\n\nProcess finished with exit code 1\nDescription\nI want to know how to use other models, or open-source large models in a multi-agent system.\nSystem Info\nlangchain-community                      0.2.6\nlangchain-core                           0.2.10\nlangchain-experimental                   0.0.62\nlangchain-fireworks                      0.1.4\nlangchain-groq                           0.1.5\nlangchain-openai                         0.1.10\nlangchain-text-splitters                 0.2.0\nlangchainhub                             0.1.20\nlangdetect                               1.0.9\nlanggraph                                0.1.5", "created_at": "2024-07-12", "closed_at": "2024-07-22", "labels": [], "State": "closed", "Author": "SENVENHUHU"}
{"issue_number": 987, "issue_title": "`add_conditional_edges` does not work without optional argument `path_map`", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom typing import TypedDict, Annotated, List\nfrom langgraph.graph import StateGraph\nfrom langgraph.graph.graph import END, START, CompiledGraph\nfrom langgraph.graph.message import add_messages\nimport random, operator\nfrom IPython.display import Image, display\n\n\nclass TestState(TypedDict):\n    messages: Annotated[List[str], operator.add]\n\ntest_workflow = StateGraph(TestState)\n\ndef node1(state: TestState):\n    return {\"messages\": [\"Hello from node 1\"]}\n\ndef node2(state: TestState):\n    return {\"messages\": [\"Hello from node 2\"]}\n\ndef node3(state: TestState):\n    return {\"messages\": [\"Hello from node 3\"]}\n\ndef node4(state: TestState):\n    return {\"messages\": [\"Hello from node 4\"]}\n\ndef node5(state: TestState):\n    return {\"messages\": [\"Hello from node 5\"]}\n\ndef route(state: TestState):\n    if random.choice([True, False]):\n        return \"node5\"\n    return \"__end__\"\n\ntest_workflow.add_node(\"node1\", node1)\ntest_workflow.add_node(\"node2\", node2)\ntest_workflow.add_node(\"node3\", node3)\ntest_workflow.add_node(\"node4\", node4)\ntest_workflow.add_node(\"node5\", node5)\n\ntest_workflow.add_edge(START, \"node1\")\ntest_workflow.add_edge(\"node1\", \"node2\")\ntest_workflow.add_edge(\"node2\", \"node3\")\ntest_workflow.add_edge(\"node3\", \"node4\")\ntest_workflow.add_edge(\"node5\", \"node4\")\n\ntest_workflow.add_conditional_edges(\"node4\", route)\n\ndisplay(Image(test_workflow.compile().get_graph().draw_mermaid_png()))\nError Message and Stack Trace (if applicable)\nNo error message.\nDescription\nThe previous code does not work as expected ( according to Langgraph API reference ) as it generates the following graph with unexpected conditional edges from node4 to every other node:\n\nThis is also visible when inspecting the branches attribute of the workflow:\ntest_workflow.branches\nreturns :\ndefaultdict(dict,\n            {'node4': {'route': Branch(path=route(recurse=True), ends=None, then=None)}})\nFix for expected behavior\nReplacing\ntest_workflow.add_conditional_edges(\"node4\", route)\nby giving the additional map_path argument such as\ntest_workflow.add_conditional_edges(\"node4\", route, {\"node5\": \"node5\", \"__end__\": \"__end__\"})\nfixes the issue, as it can be seen by inspecting the branches attribute of the workflow.\ntest_workflow.branches\nnow returns :\ndefaultdict(dict,\n            {'node4': {'route': Branch(path=route(recurse=True), ends={'node5': 'node5', '__end__': '__end__'}, then=None)}})\nand the expected Mermaid graph:\n\nSystem Info\nI use the latest versions installed with pip:\npython = \"3.12\"\nlanggraph = \"0.1.6\"\n", "created_at": "2024-07-10", "closed_at": "2024-07-22", "labels": [], "State": "closed", "Author": "xabier64"}
{"issue_number": 982, "issue_title": "Langraph Agent, Code works with OpenAI, but throws an error with Claude: (ValueError: System message must be at the beginning of the message list.)", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_chroma import Chroma\nfrom langgraph.prebuilt import ToolNode\nfrom langgraph.checkpoint import MemorySaver\nfrom langgraph.graph import StateGraph, MessagesState\nfrom langgraph.checkpoint import MemorySaver\n\nimport os\nfrom dotenv import load_dotenv\nfrom typing import Literal\nfrom langchain_anthropic import ChatAnthropic\n\n\n# Import LangSmith tracer\nfrom langchain.callbacks.tracers.langchain import LangChainTracer\n\n# Import tools from todo_tools.py\nfrom todo_tools import (\n    read_first_inbox_item, delete_inbox_item, read_tasks, read_inbox, read_projects,\n    read_task, read_project, create_task, create_project, create_inbox_item,\n    update_tasks, update_project, reinitialize_database, starting_context\n)\n\n\n# Function to create and compile the bot\ndef create_todo_agent(channelid: str):\n    system_massage = f\"\"\"You are an assistant that manages tasks, projects, and task inboxes.\n    Use the provided tool to get the job done. Whenever a tool asks for a channel ID, the channel ID is '{channelid}'.\n    Help the user with their Getting Things Done (GTD) workflow. In GTD, you first collect stuff in the inbox bucket. \n    It includes everything from a URL to a thought to a task like \"do taxes.\"\n    When a user gives you some input that you can't quite understand, put it as-is into the inbox using the inbox tool.\n    It can be anything from just a URL to instructions, to buy food, to call XYZ.\n    At some point, when the user asks you to, you will help process the inbox. \n    To do this, get the first item from the inbox and ask the user what it is. Request context from the user. \n    They need to determine whether it has multiple steps, in which case it is a project. \n    Add it to projects using the tool and create a first next action, \n    adding it as a task connected to the project by ID (you get the ID after creating the project).\n    If it is a single action, create a task directly. It should be a next action. \n    Only once the inbox item is processed and added to tasks, should you delete it (with delete_inbox_item). \n    Continue processing new inbox items until the user tells you to stop.\n    At a different time, the user will want to get something done. Get the starting context first, then help them pick a task. \n    Be helpful and supportive. Once the task or project is done, mark the status as \"done\" and \"completed\" as true. \n    You can do this with the update tools. Whenever a tool asks for a channel ID, the channel ID is '{channelid}'.\n    \"\"\"\n\n    # Load environment variables from .env file\n    load_dotenv()\n\n    # Retrieve the API key from environment variables\n    api_key = os.getenv(\"OPENAI_API_KEY\")   \n    api_key = os.getenv(\"ANTHROPIC_API_KEY\")   \n\n    # Define a flag to control tracing\n    ENABLE_TRACING = os.getenv(\"LANGCHAIN_TRACING_V2\") == \"false\"\n\n    # Initialize LangSmith tracer conditionally\n    callbacks = []\n    if ENABLE_TRACING:\n        tracer = LangChainTracer()\n        callbacks.append(tracer)\n \n\n    model = ChatAnthropic(model='claude-3.5-sonnet', api_key=api_key, callbacks=callbacks)\n    #model = ChatOpenAI(model=\"gpt-4o\", api_key=api_key, callbacks=callbacks)\n\n    #db = Chroma(persist_directory=\"./chroma_db\", embedding_function=OpenAIEmbeddings())\n\n    # Add imported tools to the tools list\n    tools = [\n        read_first_inbox_item, delete_inbox_item, read_tasks,\n        read_inbox, read_projects, read_task, read_project, create_task, create_project,\n        create_inbox_item, update_tasks, update_project, reinitialize_database,\n        starting_context\n    ]\n\n    # Bind tools to the model\n    tool_node = ToolNode(tools)\n    model = model.bind_tools(tools)\n\n    def initialize_system_prompt(state: MessagesState, channelid: str):\n\n        system_message = {\n            \"role\": \"system\",\n            \"content\": system_massage\n        }\n        state[\"messages\"].append(system_message)\n        return state\n\n\n    def invoke_starting_context(state: MessagesState, channelid: str):\n        context = starting_context.invoke({\"channelid\": channelid})  # Ensure the correct field name is used\n        # Ensure the response contains 'role' and 'content' keys\n        if isinstance(context, dict) and 'role' in context and 'content' in context:\n            formatted_context = context\n        else:\n            formatted_context = {\n                \"role\": \"system\",  # or another appropriate role\n                \"content\": str(context)  # Convert context to string if it's not already\n            }\n        return {\"messages\": [formatted_context]}\n\n\n    def should_continue(state: MessagesState) -> Literal[\"action\", \"__end__\"]:\n        \"\"\"Return the next node to execute.\"\"\"\n        last_message = state[\"messages\"][-1]\n        # If there is no function call, then we finish\n        if not last_message.tool_calls:\n            return \"__end__\"\n        # Otherwise if there is, we continue\n        return \"action\"\n\n    # Define the function that calls the model\n    def call_model(state: MessagesState):\n        messages = filter_messages(state[\"messages\"])\n        response = model.invoke(messages)\n        # We return a list, because this will get added to the existing list\n        return {\"messages\": response}\n\n    def filter_messages(messages: list):\n        # This is a simple helper function which only ever uses the last six messages\n        if len(messages) > 6:\n            return messages[-6:]\n        else:\n            return messages\n    \n    # Create the graph\n    workflow = StateGraph(MessagesState)\n\n    # Add the system prompt initialization node\n    workflow.add_node(\"initialize_system_prompt\", lambda state: initialize_system_prompt(state, channelid))\n\n    # Define the two nodes we will cycle between\n    workflow.add_node(\"agent\", call_model)\n    workflow.add_node(\"action\", tool_node)\n\n    workflow.set_entry_point(\"initialize_system_prompt\")\n    # Set the entry point as `initialize_system_prompt`\n\n    # Connect the initialization node to the agent node\n    workflow.add_edge(\"initialize_system_prompt\", \"agent\")\n\n    # We now add a conditional edge\n    workflow.add_conditional_edges(\n        \"agent\",\n        should_continue,\n    )\n\n    # We now add a normal edge from `tools` to `agent`.\n    workflow.add_edge(\"action\", \"agent\")\n    # Initialize memory to persist state between graph runs\n    checkpointer = MemorySaver()\n\n    # Compile the graph\n    app = workflow.compile(checkpointer=checkpointer)\n    return app\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\n  File \"C:\\Users\\merli\\Development\\Python\\assistant\\lg.py\", line 176, in <module>\n    messages = app.invoke({\"messages\": [(\"human\", query)]}, config={\"configurable\": {\"thread_id\": \"your_thread_id\"}})\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\merli\\miniconda3\\envs\\assistant\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 1554, in invoke\n    for chunk in self.stream(\n  File \"C:\\Users\\merli\\miniconda3\\envs\\assistant\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 1073, in stream\n    _panic_or_proceed(done, inflight, step)\n  File \"C:\\Users\\merli\\miniconda3\\envs\\assistant\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 1643, in _panic_or_proceed\n    raise exc\n  File \"C:\\Users\\merli\\miniconda3\\envs\\assistant\\Lib\\concurrent\\futures\\thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\merli\\miniconda3\\envs\\assistant\\Lib\\site-packages\\langgraph\\pregel\\retry.py\", line 72, in run_with_retry\n    task.proc.invoke(task.input, task.config)\n  File \"C:\\Users\\merli\\miniconda3\\envs\\assistant\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 2497, in invoke\n    input = step.invoke(input, config, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\merli\\miniconda3\\envs\\assistant\\Lib\\site-packages\\langgraph\\utils.py\", line 95, in invoke\n    ret = context.run(self.func, input, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\merli\\Development\\Python\\assistant\\lg.py\", line 129, in call_model\n    response = model.invoke(messages)\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\merli\\miniconda3\\envs\\assistant\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 4580, in invoke\n    return self.bound.invoke(\n           ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\merli\\miniconda3\\envs\\assistant\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 280, in invoke\n    self.generate_prompt(\n  File \"C:\\Users\\merli\\miniconda3\\envs\\assistant\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 713, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\merli\\miniconda3\\envs\\assistant\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 570, in generate\n    raise e\n  File \"C:\\Users\\merli\\miniconda3\\envs\\assistant\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 560, in generate\n    self._generate_with_cache(\n  File \"C:\\Users\\merli\\miniconda3\\envs\\assistant\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 785, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"C:\\Users\\merli\\miniconda3\\envs\\assistant\\Lib\\site-packages\\langchain_anthropic\\chat_models.py\", line 755, in _generate\n    payload = self._get_request_payload(messages, stop=stop, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\merli\\miniconda3\\envs\\assistant\\Lib\\site-packages\\langchain_anthropic\\chat_models.py\", line 647, in _get_request_payload\n    system, formatted_messages = _format_messages(messages)\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\merli\\miniconda3\\envs\\assistant\\Lib\\site-packages\\langchain_anthropic\\chat_models.py\", line 161, in _format_messages\n    raise ValueError(\"System message must be at beginning of message list.\")\nValueError: System message must be at beginning of message list.\nDescription\nI have code for an agent that I have written. It worked fine when I used OpenAI models. But the same code, with Anthropic, throws an error. The OpenAI model is commented out. If I uncomment it and comment out Claude, it's working.\nSeems to be the same or related to #656 [https://github.com/langchain-ai/langchain/issues/20835] [https://github.com/langchain-ai/langchain/issues/18909] [https://github.com/enso-labs/llm-server/issues/21]\nCould be duplicated ?\nSystem Info\nlangchain==0.2.7\nlangchain-anthropic==0.1.19\nlangchain-chroma==0.1.2\nlangchain-community==0.2.7\nlangchain-core==0.2.12\nlangchain-google-genai==1.0.7\nlangchain-openai==0.1.14\nlangchain-text-splitters==0.2.2\nPlatform: Windows\nPython 3.11.9", "created_at": "2024-07-10", "closed_at": "2024-07-10", "labels": [], "State": "closed", "Author": "Merlinvt"}
{"issue_number": 978, "issue_title": "plan-and-execute example => TypeError: can only concatenate list (not \"tuple\") to list", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nSimply running the [Plan-and-Execute](https://langchain-ai.github.io/langgraph/tutorials/plan-and-execute/plan-and-execute) code\nError Message and Stack Trace (if applicable)\n{\n\t\"name\": \"TypeError\",\n\t\"message\": \"can only concatenate list (not \\\"tuple\\\") to list\",\n\t\"stack\": \"---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[47], line 3\n      1 config = {\\\"recursion_limit\\\": 50}\n      2 inputs = {\\\"input\\\": \\\"what is the hometown of the 2024 Australia open winner?\\\"}\n----> 3 async for event in app.astream(inputs, config=config):\n      4     for k, v in event.items():\n      5         if k != \\\"__end__\\\":\n\nFile /usr/local/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1446, in Pregel.astream(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\n   1441     print_step_writes(\n   1442         step, pending_writes, self.stream_channels_list\n   1443     )\n   1445 # apply writes to channels\n-> 1446 _apply_writes(\n   1447     checkpoint,\n   1448     channels,\n   1449     pending_writes,\n   1450     (\n   1451         self.checkpointer.get_next_version\n   1452         if self.checkpointer\n   1453         else _increment\n   1454     ),\n   1455 )\n   1457 # yield current values\n   1458 if \\\"values\\\" in stream_modes:\n\nFile /usr/local/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1758, in _apply_writes(checkpoint, channels, pending_writes, get_next_version)\n   1756 if chan in channels:\n   1757     try:\n-> 1758         updated = channels[chan].update(vals)\n   1759     except InvalidUpdateError as e:\n   1760         raise InvalidUpdateError(\n   1761             f\\\"Invalid update for channel {chan} with values {vals}\\\"\n   1762         ) from e\n\nFile /usr/local/lib/python3.11/site-packages/langgraph/channels/binop.py:96, in BinaryOperatorAggregate.update(self, values)\n     94     values = values[1:]\n     95 for value in values:\n---> 96     self.value = self.operator(self.value, value)\n     97 return True\n\nTypeError: can only concatenate list (not \\\"tuple\\\") to list\"\n}\nDescription\nI'm running the code in a Jupyter notebook in a VS Code devcontainer on my macbook.\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP PREEMPT Wed Dec  6 17:08:31 UTC 2023\nPython Version:  3.11.9 (main, Jul  2 2024, 21:48:22) [GCC 10.2.1 20210110]\n\nPackage Information\n\nlangchain_core: 0.2.12\nlangchain: 0.2.7\nlangchain_community: 0.2.7\nlangsmith: 0.1.84\nlangchain_google_vertexai: 1.0.6\nlangchain_groq: 0.1.6\nlangchain_openai: 0.1.14\nlangchain_text_splitters: 0.2.2\nlangchain_weaviate: 0.0.2\nlangchainhub: 0.1.20\nlanggraph: 0.1.6\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlangserve\n", "created_at": "2024-07-10", "closed_at": "2025-01-15", "labels": [], "State": "closed", "Author": "nick-youngblut"}
{"issue_number": 963, "issue_title": "The difference between the use of configurable and metadata in RunnableConfig?", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langchain_core.messages import AIMessage\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import StateGraph, MessagesState\n\n\ndef print_config(state: MessagesState, config: RunnableConfig) -> MessagesState:\n    print(config[\"metadata\"])  # The value of container type does not appear here\n    # These values must be use configurable to get\n    print(config[\"configurable\"][\"cherries\"])\n    print(config[\"configurable\"][\"dates\"])\n    print(config[\"configurable\"][\"egg\"])\n    return {\"messages\": [AIMessage(content=\"foo\")]}\n\n\ngraph_builder = StateGraph(MessagesState)\ngraph_builder.set_entry_point(\"foo\")\ngraph_builder.add_node(\"foo\", print_config)\ngraph_builder.set_finish_point(\"foo\")\n\nconfig = {\n    \"configurable\": {\n        \"apple\": 1,\n        \"banana\": \"two\",\n        \"cherries\": [3, \"four\", 5.0],\n        \"dates\": {\"Alice\": 6, \"Bob\": \"seven\"},\n        \"egg\": (1, 2),\n    },\n}\n\napp = graph_builder.compile()\napp.invoke({\"messages\": [(\"human\", \"Hello World!\")]}, config)\nError Message and Stack Trace (if applicable)\n`config[\"metadata\"]` cannot retrieve some of the data entered from `configurable`\nDescription\nWhen I use\n{\n    \"configurable\": {\n        \"apple\": 1,\n        \"banana\": \"two\",\n        \"cherries\": [3, \"four\", 5.0],\n        \"dates\": {\"Alice\": 6, \"Bob\": \"seven\"},\n        \"egg\": (1, 2),\n    },\n}\nconfig[\"metadata\"] seems to only save basic types of content?\nIs this normal output? What is the principle of saving metadata? When should I use metadata or configurable?\nconfigurable contains a lot of information, while metadata is much cleaner. I encountered this problem when trying to use metadata to get some of my own data.\n\nWhen I use metadata to input, all values \u200b\u200bcan be obtained normally.\n{\n    \"metadata\": {\n        \"apple\": 1,\n        \"banana\": \"two\",\n        \"cherries\": [3, \"four\", 5.0],\n        \"dates\": {\"Alice\": 6, \"Bob\": \"seven\"},\n        \"egg\": (1, 2),\n    },\n}\nSystem Info\nlanggraph == 0.1.5\nlangchain == 0.2.6\npython version == 3.10.12", "created_at": "2024-07-09", "closed_at": "2024-07-09", "labels": [], "State": "closed", "Author": "gbaian10"}
{"issue_number": 962, "issue_title": "dynamic jump to node", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\ndef __load_key_indicators(self, state: ZxhState) -> dict:\n        self.push_msg(state, 'caption', '\u4ea7\u4ed4\u5206\u6790,\u52a0\u8f7d\u5173\u952e\u6307\u6807')\n        res = self.dp.get_product_core_birth_report(state.get('pig_farm_id'))\n        if res.code != 0:\n            self.push_msg(state, 'reply', f'\u4ea7\u4ed4\u5206\u6790,\u52a0\u8f7d\u5173\u952e\u6307\u6807\u5931\u8d25:{res.msg}')\n            return {'messages': []}\n        else:\n            return {'data': ('parturition_analysis_core_indicators', res.data)}\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nin gradph node, i invoke some extern interface, they may fail with code, i check the code, if error occur\uff0c i want do another node,\nnow i have to define an temp node, check the result and add an condiftion edge to do that things, but i have at least one hundred, that may a hevay job\nSystem Info\nlangchain==0.2.1\nlangchain-community==0.2.4\nlangchain-core==0.2.3\nlangchain-openai==0.1.8\nlangchain-text-splitters==0.2.0\nlanggraph==0.0.69\nlangsmith==0.1.65\nlinux\npython 3.12", "created_at": "2024-07-09", "closed_at": "2024-12-16", "labels": [], "State": "closed", "Author": "cjdxhjj"}
{"issue_number": 954, "issue_title": "Final node doesn't wait for all other incoming nodes", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nimport asyncio\nfrom typing import Annotated\n\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph.message import add_messages\nfrom langgraph.graph import END, StateGraph, START\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\n\nasync def call_node(state: State):\n    print(\"call_node\")\n    return state\n\nasync def call_node_2(state: State):\n    print(\"call_node 2 start\")\n    await asyncio.sleep(5)\n    print(\"call_node 2 finish\")\n    return state\n\nasync def call_node_2_2(state: State):\n    print(\"call_node 2_2 start\")\n    await asyncio.sleep(3)\n    print(\"call_node 2_2 finish\")\n    return state\n\n# Define a new graph\nworkflow = StateGraph(State)\n\n# Define the two nodes we will cycle between\nworkflow.add_node(\"agent\", lambda x: print('agent'))\nworkflow.add_node(\"agent_2\", call_node_2)\nworkflow.add_node(\"agent_2_1\", lambda x: print('agent_2_1'))\nworkflow.add_node(\"agent_2_2\", call_node_2_2)\nworkflow.add_node(\"agent_2_3\", lambda x: print('agent_2_3'))\nworkflow.add_node(\"agent_3\", lambda x: print('agent_3'))\nworkflow.add_node(\"agent_4\", lambda x: print('agent_4'))\n\n# Set the entrypoint as `agent`\n# This means that this node is the first one called\nworkflow.add_edge(START, \"agent\")\n\n# We now add a conditional edge\nworkflow.add_conditional_edges(\n    \"agent\",\n    lambda x: [\"agent_2\", \"agent_3\"],\n    {\n        \"agent_2\": \"agent_2\",\n        \"agent_3\": \"agent_3\",\n    },\n)\n\n# We now add a conditional edge\nworkflow.add_conditional_edges(\n    \"agent_2_1\",\n    lambda x: \"True\",\n    {\n        \"True\": \"agent_2_2\",\n        \"False\": \"agent_2_3\",\n    },\n)\nworkflow.add_edge(\"agent_2\", \"agent_2_1\")\nworkflow.add_edge(\"agent_3\", \"agent_4\")\nworkflow.add_edge(\"agent_2_2\", \"agent_4\")\nworkflow.add_edge(\"agent_2_3\", \"agent_4\")\nworkflow.add_edge(\"agent_4\", END)\n\napp = workflow.compile()\n\nasync def main():\n    inputs = {\"messages\": []}\n    print(app.get_graph().draw_ascii())\n    await app.ainvoke(inputs)\n    \nasyncio.run(main())\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nHello, guys\nPlease, help me out with this problem\nI've upgraded to latest langgraph\nLook at the image\nWhen there's agent_2 and agent_3 - agent_4 waits until both of them finish and then continues BUT when I add agent_2_1 condition - it ruins. agent_4 must wait for agent_2_2  and agent_3, but in fact it allows agent_3 to finish, then agent_4 calls, after this finishes agent_2_2 and AGAIN agent_4 calls\nThe order of calling is this:\nagent\ncall_node 2 start\nagent_3\ncall_node 2 finish\nagent_2_1\nagent_4\ncall_node 2_2 start\ncall_node 2_2 finish\nagent_4\n\nThis problem is present both in older versions of langgraph and in newest\nHow it looks now (click to expand):\n\nSystem Info\nlanggraph == 0.1.5 OR (try on older version) == 0.0.48", "created_at": "2024-07-08", "closed_at": "2024-07-09", "labels": [], "State": "closed", "Author": "mrbuslov"}
{"issue_number": 953, "issue_title": "Final node doesn't wait all other incoming nodes", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nimport asyncio\nfrom typing import Annotated\n\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph.message import add_messages\nfrom langgraph.graph import END, StateGraph, START\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\n\nasync def call_node(state: State):\n    print(\"call_node\")\n    return state\n\nasync def call_node_2(state: State):\n    print(\"call_node 2 start\")\n    await asyncio.sleep(5)\n    print(\"call_node 2 finish\")\n    return state\n\nasync def call_node_2_2(state: State):\n    print(\"call_node 2_2 start\")\n    await asyncio.sleep(3)\n    print(\"call_node 2_2 finish\")\n    return state\n\n# Define a new graph\nworkflow = StateGraph(State)\n\n# Define the two nodes we will cycle between\nworkflow.add_node(\"agent\", lambda x: print('agent'))\nworkflow.add_node(\"agent_2\", call_node_2)\nworkflow.add_node(\"agent_2_1\", lambda x: print('agent_2_1'))\nworkflow.add_node(\"agent_2_2\", call_node_2_2)\nworkflow.add_node(\"agent_2_3\", lambda x: print('agent_2_3'))\nworkflow.add_node(\"agent_3\", lambda x: print('agent_3'))\nworkflow.add_node(\"agent_4\", lambda x: print('agent_4'))\n\n# Set the entrypoint as `agent`\n# This means that this node is the first one called\nworkflow.add_edge(START, \"agent\")\n\n# We now add a conditional edge\nworkflow.add_conditional_edges(\n    \"agent\",\n    lambda x: [\"agent_2\", \"agent_3\"],\n    {\n        \"agent_2\": \"agent_2\",\n        \"agent_3\": \"agent_3\",\n    },\n)\n\n# We now add a conditional edge\nworkflow.add_conditional_edges(\n    \"agent_2_1\",\n    lambda x: \"True\",\n    {\n        \"True\": \"agent_2_2\",\n        \"False\": \"agent_2_3\",\n    },\n)\nworkflow.add_edge(\"agent_2\", \"agent_2_1\")\nworkflow.add_edge(\"agent_3\", \"agent_4\")\nworkflow.add_edge(\"agent_2_2\", \"agent_4\")\nworkflow.add_edge(\"agent_2_3\", \"agent_4\")\nworkflow.add_edge(\"agent_4\", END)\n\napp = workflow.compile()\n\nasync def main():\n    inputs = {\"messages\": []}\n    print(app.get_graph().draw_ascii())\n    await app.ainvoke(inputs)\n    \nasyncio.run(main())\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nHello, guys\nPlease, help me out with this problem\nI've upgraded to latest langgraph\nLook at the image\nWhen there's agent_2`` and agent_3-agent_4waits until both of them finish and then continues BUT when I addagent_2_1condition - it ruins.agent_4must wait foragent_2_2andagent_3, but in fact it allows agent_3to finish, thenagent_4calls, after this finishesagent_2_2and AGAINagent_4 calls`\nThe order of calling is this:\nagent\ncall_node 2 start\nagent_3\ncall_node 2 finish\nagent_2_1\nagent_4\ncall_node 2_2 start\ncall_node 2_2 finish\nagent_4\n\nThis problem is present both in older versions of langgraph and in newest\nSystem Info\nlanggraph == 0.1.5 OR (try on older version) == 0.0.48", "created_at": "2024-07-08", "closed_at": "2024-07-08", "labels": [], "State": "closed", "Author": "BuslovGart"}
{"issue_number": 944, "issue_title": "cannot import ToolNode from Langgraph", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langchain_core.runnables import RunnableLambda, RunnableWithFallbacks\nfrom langgraph.prebuilt import ToolNode\n\n\ndef create_tool_node_with_fallback(tools: list) -> RunnableWithFallbacks[Any, dict]:\n    \"\"\"\n    Create a ToolNode with a fallback to handle errors and surface them to the agent.\n    \"\"\"\n    return ToolNode(tools).with_fallbacks(\n        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n    )\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[66], line 2\n      1 from langchain_core.runnables import RunnableLambda, RunnableWithFallbacks\n----> 2 from langgraph.prebuilt import ToolNode\n      5 def create_tool_node_with_fallback(tools: list) -> RunnableWithFallbacks[Any, dict]:\n      6     \"\"\"\n      7     Create a ToolNode with a fallback to handle errors and surface them to the agent.\n      8     \"\"\"\n\nImportError: cannot import name 'ToolNode' from 'langgraph.prebuilt' (/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langgraph/prebuilt/__init__.py)\nDescription\nI am getting this error today, when I was able to execute it before\nSystem Info\nfrom langgraph.prebuilt import ToolNode", "created_at": "2024-07-08", "closed_at": "2024-07-16", "labels": ["stale"], "State": "closed", "Author": "rajidhamon31"}
{"issue_number": 942, "issue_title": "\"TypeError: string indices must be integers\" in line: \"for output in app.stream(inputs):\"", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nIn the final web search example from the local crag notebook, \n\nfrom pprint import pprint\n\n# Compile\napp = workflow.compile()\ninputs = {\"question\": \"Who won the 2024 UK general elections?\"}\nfor output in app.stream(inputs):\n    for key, value in output.items():\n        pprint(f\"Finished running: {key}:\")\npprint(value[\"generation\"])\nError Message and Stack Trace (if applicable)\nTypeError                                 Traceback (most recent call last)\nCell In[24], line 6\n      4 app = workflow.compile()\n      5 inputs = {\"question\": \"Who won the 2024 UK general elections?\"}\n----> 6 for output in app.stream(inputs):\n      7     for key, value in output.items():\n      8         pprint(f\"Finished running: {key}:\")\n\nFile ~/GitHub/rag_cookbook/.rag_env/lib/python3.10/site-packages/langgraph/pregel/__init__.py:986, in Pregel.stream(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\n    983         del fut, task\n    985 # panic on failure or timeout\n--> 986 _panic_or_proceed(done, inflight, step)\n    987 # don't keep futures around in memory longer than needed\n    988 del done, inflight, futures\n\nFile ~/GitHub/rag_cookbook/.rag_env/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1540, in _panic_or_proceed(done, inflight, step)\n   1538             inflight.pop().cancel()\n   1539         # raise the exception\n-> 1540         raise exc\n   1542 if inflight:\n   1543     # if we got here means we timed out\n   1544     while inflight:\n   1545         # cancel all pending tasks\n...\n--> 124 web_results = \"\\n\".join([d[\"content\"] for d in docs])\n    125 web_results = Document(page_content=web_results)\n    126 if documents is not None:\n\nTypeError: string indices must be integers\nDescription\nthe previous index search example works, but the web search one fails. Could it be because i've provided a generic value to the API KEYS ?\nSystem Info\nPip list of relevant libraries are as follows:\nPython 3.10.12\nlangchain==0.2.5\nlangchain-community==0.2.5\nlangchain-core==0.2.9\nlangchain-nomic==0.1.2\nlangchain-text-splitters==0.2.1\nlangchainhub==0.1.20\nollama==0.2.1", "created_at": "2024-07-08", "closed_at": "2024-07-08", "labels": [], "State": "closed", "Author": "pranavkr29"}
{"issue_number": 941, "issue_title": "DOC: Add more information about checkpointing and time travel", "issue_body": "Issue with current documentation:\nNo response\nIdea or request for content:\nIt would be really helpful to have content to understand where/how often checkpointing is invoked during the graph execution lifecycle.\nThen, it would be helpful to have documentation / a notebook example to show how to correspond a checkpoint's timestamp to a node's execution so that we can implement \"retrying\" from a certain node in the graph execution.", "created_at": "2024-07-08", "closed_at": "2024-07-08", "labels": [], "State": "closed", "Author": "riyavsinha"}
{"issue_number": 928, "issue_title": "Send raise ValueError: too many values to unpack (expected 2)", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom typing import Literal\nfrom langchain_core.messages import HumanMessage\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.checkpoint import MemorySaver\nfrom langgraph.graph import END, StateGraph, MessagesState, START\nfrom langgraph.prebuilt import ToolNode\nfrom langgraph.constants import Send\n\ntools = [TavilySearchResults(max_results=1)]\ntool_node = ToolNode(tools)\nmodel = ChatOpenAI(temperature=0).bind_tools(tools)\n\ndef should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if last_message.tool_calls:\n        return \"tools\"\n    return END\n\ndef call_model(state: MessagesState):\n    messages = state[\"messages\"]\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n\nworkflow = StateGraph(MessagesState)\nworkflow.add_node(\"tools\", tool_node)\nworkflow.add_node(\"agent\", call_model)\n\n##NEW HERE\ndef should_start(state: MessagesState) -> Literal[\"agent\"]:\n    return [Send(\"agent\", {\"messages\": state[\"messages\"]})]\n\nworkflow.add_conditional_edges(START, should_start)\n##########\n\nworkflow.add_conditional_edges(\"agent\", should_continue)\nworkflow.add_edge(\"tools\", \"agent\")\n\ncheckpointer = MemorySaver()\napp = workflow.compile(checkpointer=checkpointer)\nfinal_state = app.invoke(\n    {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]},\n    config={\"configurable\": {\"thread_id\": 42}},\n)\nError Message and Stack Trace (if applicable)\n{\n\t\"name\": \"ValueError\",\n\t\"message\": \"too many values to unpack (expected 2)\",\n\t\"stack\": \"---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[7], line 39\n     37 checkpointer = MemorySaver()\n     38 app = workflow.compile(checkpointer=checkpointer)\n---> 39 final_state = app.invoke(\n     40     {\\\"messages\\\": [HumanMessage(content=\\\"what is the weather in sf\\\")]},\n     41     config={\\\"configurable\\\": {\\\"thread_id\\\": 42}},\n     42 )\n\nFile ~/anaconda3/envs/rag/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1550, in Pregel.invoke(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug, **kwargs)\n   1548 else:\n   1549     chunks = []\n-> 1550 for chunk in self.stream(\n   1551     input,\n   1552     config,\n   1553     stream_mode=stream_mode,\n   1554     output_keys=output_keys,\n   1555     input_keys=input_keys,\n   1556     interrupt_before=interrupt_before,\n   1557     interrupt_after=interrupt_after,\n   1558     debug=debug,\n   1559     **kwargs,\n   1560 ):\n   1561     if stream_mode == \\\"values\\\":\n   1562         latest = chunk\n\nFile ~/anaconda3/envs/rag/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1073, in Pregel.stream(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\n   1070         del fut, task\n   1072 # panic on failure or timeout\n-> 1073 _panic_or_proceed(done, inflight, step)\n   1074 # don't keep futures around in memory longer than needed\n   1075 del done, inflight, futures\n\nFile ~/anaconda3/envs/rag/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1639, in _panic_or_proceed(done, inflight, step)\n   1637             inflight.pop().cancel()\n   1638         # raise the exception\n-> 1639         raise exc\n   1641 if inflight:\n   1642     # if we got here means we timed out\n   1643     while inflight:\n   1644         # cancel all pending tasks\n\nFile ~/anaconda3/envs/rag/lib/python3.12/concurrent/futures/thread.py:58, in _WorkItem.run(self)\n     55     return\n     57 try:\n---> 58     result = self.fn(*self.args, **self.kwargs)\n     59 except BaseException as exc:\n     60     self.future.set_exception(exc)\n\nFile ~/anaconda3/envs/rag/lib/python3.12/site-packages/langgraph/pregel/retry.py:72, in run_with_retry(task, retry_policy)\n     70 task.writes.clear()\n     71 # run the task\n---> 72 task.proc.invoke(task.input, task.config)\n     73 # if successful, end\n     74 break\n\nFile ~/anaconda3/envs/rag/lib/python3.12/site-packages/langchain_core/runnables/base.py:2507, in RunnableSequence.invoke(self, input, config, **kwargs)\n   2505             input = step.invoke(input, config, **kwargs)\n   2506         else:\n-> 2507             input = step.invoke(input, config)\n   2508 # finish the root run\n   2509 except BaseException as e:\n\nFile ~/anaconda3/envs/rag/lib/python3.12/site-packages/langgraph/utils.py:95, in RunnableCallable.invoke(self, input, config, **kwargs)\n     93     if accepts_config(self.func):\n     94         kwargs[\\\"config\\\"] = config\n---> 95     ret = context.run(self.func, input, **kwargs)\n     96 if isinstance(ret, Runnable) and self.recurse:\n     97     return ret.invoke(input, config)\n\nFile ~/anaconda3/envs/rag/lib/python3.12/site-packages/langgraph/graph/graph.py:70, in Branch._route(self, input, config, reader, writer)\n     61 def _route(\n     62     self,\n     63     input: Any,\n   (...)\n     67     writer: Callable[[list[str]], Optional[Runnable]],\n     68 ) -> Runnable:\n     69     if reader:\n---> 70         value = reader(config)\n     71         # passthrough additional keys from node to branch\n     72         # only doable when using dict states\n     73         if isinstance(value, dict) and isinstance(input, dict):\n\nFile ~/anaconda3/envs/rag/lib/python3.12/site-packages/langgraph/pregel/read.py:94, in ChannelRead.do_read(config, channel, fresh, mapper)\n     89     raise RuntimeError(\n     90         \\\"Not configured with a read function\\\"\n     91         \\\"Make sure to call in the context of a Pregel process\\\"\n     92     )\n     93 if mapper:\n---> 94     return mapper(read(channel, fresh))\n     95 else:\n     96     return read(channel, fresh)\n\nFile ~/anaconda3/envs/rag/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1697, in _local_read(checkpoint, channels, writes, config, select, fresh)\n   1691     with ChannelsManager(\n   1692         {k: v for k, v in channels.items() if k not in context_channels},\n   1693         checkpoint,\n   1694         config,\n   1695     ) as channels:\n   1696         all_channels = {**channels, **context_channels}\n-> 1697         _apply_writes(copy_checkpoint(checkpoint), all_channels, writes, None)\n   1698         return read_channels(all_channels, select)\n   1699 else:\n\nFile ~/anaconda3/envs/rag/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1737, in _apply_writes(checkpoint, channels, pending_writes, get_next_version)\n   1735 pending_writes_by_channel: dict[str, list[Any]] = defaultdict(list)\n   1736 # Group writes by channel\n-> 1737 for chan, val in pending_writes:\n   1738     if chan == TASKS:\n   1739         checkpoint[\\\"pending_sends\\\"].append(val)\n\nValueError: too many values to unpack (expected 2)\"\n}\nDescription\nI add a conditional edge, which use 'SEND' to delivery the messages to the agent model, but it does not work and raises  ValueError: too many values to unpack (expected 2).\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #112~20.04.1-Ubuntu SMP Thu Mar 14 14:28:24 UTC 2024\nPython Version:  3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:50:58) [GCC 12.3.0]\n\nPackage Information\n\nlangchain_core: 0.2.10\nlangchain: 0.2.6\nlangchain_community: 0.2.6\nlangsmith: 0.1.82\nlangchain_cohere: 0.1.5\nlangchain_openai: 0.1.14\nlangchain_text_splitters: 0.2.2\nlanggraph: 0.1.4\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlangserve\n", "created_at": "2024-07-04", "closed_at": "2024-07-11", "labels": [], "State": "closed", "Author": "Acabbage"}
{"issue_number": 911, "issue_title": "pandas agent not working with langgraph", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\npandas_agent = create_pandas_dataframe_agent(\n    llm,\n    df,\n    verbose=True,\n    agent_type=AgentType.OPENAI_FUNCTIONS,\n    # agent_type=\"openai-tools\",\n    allow_dangerous_code=True\n)\npandas_node = functools.partial(agent_node, agent=pandas_agent, name=\"Analyst\")\n\n\nworkflow = StateGraph(AgentState)\nworkflow.add_node(\"Analyst\", pandas_node)\n\nfor member in members:\n    # We want our workers to ALWAYS \"report back\" to the supervisor when done\n    workflow.add_edge(member, \"supervisor\")\n# The supervisor populates the \"next\" field in the graph state\n# which routes to a node or finishes\nconditional_map = {k: k for k in members}\nconditional_map[\"FINISH\"] = END\nworkflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n# Finally, add entrypoint\nworkflow.set_entry_point(\"supervisor\")\n\ngraph = workflow.compile()\n\nfor s in graph.stream(\n    {\n        \"messages\": [\n            HumanMessage(content=\"summarize the columns in the dataframe provided to you\")\n        ]\n    }\n):\n    if \"__end__\" not in s:\n        print(s)\n        print(\"----\")\nError Message and Stack Trace (if applicable)\nValueError                                Traceback (most recent call last)\nCell In[63], line 1\n----> 1 for s in graph.stream(\n      2     {\n      3         \"messages\": [\n      4             HumanMessage(content=\"summarize the columns in the dataframe provided to you\")\n      5         ]\n      6     }\n      7 ):\n      8     if \"__end__\" not in s:\n      9         print(s)\n\nFile c:\\Users\\AzharMajeed\\Documents\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1073, in Pregel.stream(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\n   1070         del fut, task\n   1072 # panic on failure or timeout\n-> 1073 _panic_or_proceed(done, inflight, step)\n   1074 # don't keep futures around in memory longer than needed\n   1075 del done, inflight, futures\n\nFile c:\\Users\\AzharMajeed\\Documents\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1643, in _panic_or_proceed(done, inflight, step)\n   1641             inflight.pop().cancel()\n   1642         # raise the exception\n-> 1643         raise exc\n   1645 if inflight:\n   1646     # if we got here means we timed out\n   1647     while inflight:\n   1648         # cancel all pending tasks\n\nFile ~\\.pyenv\\pyenv-win\\versions\\3.11.2\\Lib\\concurrent\\futures\\thread.py:58, in _WorkItem.run(self)\n     55     return\n     57 try:\n---> 58     result = self.fn(*self.args, **self.kwargs)\n     59 except BaseException as exc:\n     60     self.future.set_exception(exc)\n\nFile c:\\Users\\AzharMajeed\\Documents\\.venv\\Lib\\site-packages\\langgraph\\pregel\\retry.py:72, in run_with_retry(task, retry_policy)\n     70 task.writes.clear()\n     71 # run the task\n---> 72 task.proc.invoke(task.input, task.config)\n     73 # if successful, end\n     74 break\n\nFile c:\\Users\\AzharMajeed\\Documents\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2499, in invoke(self, input, config)\n   2497 # invoke all steps in sequence\n   2498 try:\n-> 2499     for i, step in enumerate(self.steps):\n   2500         # mark each step as a child run\n   2501         config = patch_config(\n   2502             config, callbacks=run_manager.get_child(f\"seq:step:{i+1}\")\n   2503         )\n   2504         if i == 0:\n\nFile c:\\Users\\AzharMajeed\\Documents\\.venv\\Lib\\site-packages\\langgraph\\utils.py:95, in RunnableCallable.invoke(self, input, config, **kwargs)\n     93     if accepts_config(self.func):\n     94         kwargs[\"config\"] = config\n---> 95     ret = context.run(self.func, input, **kwargs)\n     96 if isinstance(ret, Runnable) and self.recurse:\n     97     return ret.invoke(input, config)\n\nCell In[24], line 2\n      1 def agent_node(state, agent, name):\n----> 2     result = agent.invoke(state)\n      3     return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}\n\nFile c:\\Users\\AzharMajeed\\Documents\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:163, in invoke(self, input, config, **kwargs)\n    154     self._validate_inputs(inputs)\n    155     outputs = (\n    156         self._call(inputs, run_manager=run_manager)\n    157         if new_arg_supported\n    158         else self._call(inputs)\n    159     )\n    161     final_outputs: Dict[str, Any] = self.prep_outputs(\n    162         inputs, outputs, return_only_outputs\n--> 163     )\n    164 except BaseException as e:\n    165     run_manager.on_chain_error(e)\n\nFile c:\\Users\\AzharMajeed\\Documents\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:151, in invoke(self, input, config, **kwargs)\n    136 callback_manager = CallbackManager.configure(\n    137     callbacks,\n    138     self.callbacks,\n   (...)\n    143     self.metadata,\n    144 )\n    145 new_arg_supported = inspect.signature(self._call).parameters.get(\"run_manager\")\n    147 run_manager = callback_manager.on_chain_start(\n    148     dumpd(self),\n    149     inputs,\n    150     run_id,\n--> 151     name=run_name,\n    152 )\n    153 try:\n    154     self._validate_inputs(inputs)\n\nFile c:\\Users\\AzharMajeed\\Documents\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:279, in _validate_inputs(self, inputs)\n    273         _input_keys = _input_keys.difference(self.memory.memory_variables)\n    274     if len(_input_keys) != 1:\n    275         raise ValueError(\n    276             f\"A single string input was passed in, but this chain expects \"\n    277             f\"multiple inputs ({_input_keys}). When a chain expects \"\n    278             f\"multiple inputs, please call it by passing in a dictionary, \"\n--> 279             \"eg `chain({'foo': 1, 'bar': 2})`\"\n    280         )\n    282 missing_keys = set(self.input_keys).difference(inputs)\n    283 if missing_keys:\n\nValueError: Missing some input keys: {'input'}\nDescription\nI am trying to add a pandas agent in a mutli agent setup, similar to how it is shown in this notebook. https://github.com/langchainai/langgraph/blob/main/examples/multi_agent/agent_supervisor.ipynb\nI imported the pandas agent: from langchain_experimental.agents import create_pandas_dataframe_agent. Please let me know if this is the right way to add the pandas agent as a node in the graph. When I run the pandas agent separately it works fine, but when I add it do the graph I am getting this ValueError saying \"Missing some input keys: {'input'}\".\nPlease let me know if I need to provide any further details.\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.22631\nPython Version:  3.11.2 (tags/v3.11.2:878ead1, Feb  7 2023, 16:38:35) [MSC v.1934 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.2.10\nlangchain: 0.2.6\nlangchain_community: 0.2.6\nlangsmith: 0.1.82\nlangchain_experimental: 0.0.62\nlangchain_openai: 0.1.7\nlangchain_text_splitters: 0.2.2\nlangchainhub: 0.1.14\nlanggraph: 0.1.5\n", "created_at": "2024-07-03", "closed_at": "2024-07-05", "labels": [], "State": "closed", "Author": "azharmajeed-cml"}
{"issue_number": 893, "issue_title": "Cannot start \"langgraph up\"", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nlanggraph up\nError Message and Stack Trace (if applicable)\nlanggraph-api-1       | Not enough segments\nlanggraph-api-1       | 2024-06-28 00:22:33,709:ERROR:uvicorn.error Traceback (most recent call last):\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 732, in lifespan\nlanggraph-api-1       |     async with self.lifespan_context(app) as maybe_state:\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/contextlib.py\", line 210, in __aenter__\nlanggraph-api-1       |     return await anext(self.gen)\nlanggraph-api-1       |            ^^^^^^^^^^^^^^^^^^^^^\nlanggraph-api-1       |   File \"/api/langgraph_api/lifespan.py\", line 18, in lifespan\nlanggraph-api-1       | ValueError: License key is not valid\nlanggraph-api-1       | \nlanggraph-api-1       | 2024-06-28 00:22:33,709:ERROR:uvicorn.error Application startup failed. Exiting.\nlanggraph-api-1 exited with code 3\nDescription\nI count not do \"langgraph up\" .  I get the above error.\nSystem Info\nlanggraph up", "created_at": "2024-06-30", "closed_at": "2024-07-01", "labels": [], "State": "closed", "Author": "shanumas"}
{"issue_number": 889, "issue_title": "LANGGRAPH_AUTH_TYPE is not working properly.", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nWhen I use langgraph-cli for local development, I don't need the deployment of the cloud but the local startup, but it doesn't run as expected.\n\n//.env\n\nANTHROPIC_API_KEY=sk-proj-LfBOnaxzGYkId\nTAVILY_API_KEY=tvly-CW8gcAEU83s8zb37I8\n\nLANGGRAPH_AUTH_TYPE=noop\n\n// langgraph.json\n{\n    \"dependencies\": [\".\"],\n    \"graphs\": {\n        \"agent\": \"./agent.py:graph\"\n    },\n    \"env\": \".env\"\n}\n\n// agent.py\n\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langgraph.prebuilt import create_react_agent\n\nimport os\nos.environ[\"LANGGRAPH_AUTH_TYPE\"] = \"noop\"\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n\ntools = [TavilySearchResults(max_results=2)]\n\ngraph = create_react_agent(model, tools)\nError Message and Stack Trace (if applicable)\nlanggraph up                \nStarting LangGraph API server...\nFor local dev, requires env var LANGSMITH_API_KEY with access to LangGraph Cloud closed beta.\nFor production use, requires a license key in env var LANGGRAPH_CLOUD_LICENSE_KEY.\n| Building...#1 [langgraph-api internal] load .dockerignore\n#1 transferring context: 2B done\n#1 DONE 0.0s\n\n#2 [langgraph-api internal] load build definition from Dockerfile\n#2 transferring dockerfile: 753B done\n#2 DONE 0.0s\n\n#3 [langgraph-api internal] load metadata for docker.io/langchain/langgraph-api:3.11\n#3 DONE 0.0s\n\n#4 [langgraph-api 1/7] FROM docker.io/langchain/langgraph-api:3.11\n#4 DONE 0.0s\n\n#5 [langgraph-api internal] load build context\n#5 transferring context: 270B done\n#5 DONE 0.0s\n\n#6 [langgraph-api 4/7] ADD . /deps/__outer_graph-up/src\n#6 CACHED\n\n#7 [langgraph-api 2/7] ADD requirements.txt /deps/__outer_graph-up/src/requirements.txt\n#7 CACHED\n\n#8 [langgraph-api 3/7] RUN pip install -c /api/constraints.txt -r /deps/__outer_graph-up/src/requirements.txt\n#8 CACHED\n\n#9 [langgraph-api 6/7] RUN pip install -c /api/constraints.txt -e /deps/*\n#9 CACHED\n\n#10 [langgraph-api 5/7] RUN set -ex &&     for line in '[project]'                 'name = \"graph-up\"'                 'version = \"0.1\"'                 '[tool.setuptools.package-data]'                 '\"*\" = [\"**/*\"]'; do         echo \"\" >> /deps/__outer_graph-up/pyproject.toml;     done\n#10 CACHED\n\n#11 [langgraph-api 7/7] WORKDIR /deps/__outer_graph-up/src\n#11 CACHED\n\n#12 [langgraph-api] exporting to image\n#12 exporting layers done\n#12 writing image sha256:ee2d148333ee20ab826ef432402c9b61c3d23aaf7a933dc8ed29077e0fd2e335 done\n#12 naming to docker.io/library/graph-up-langgraph-api done\n#12 DONE 0.0s\nAttaching to graph-up-langgraph-api-1, graph-up-langgraph-postgres-1\ngraph-up-langgraph-postgres-1  | \ngraph-up-langgraph-postgres-1  | PostgreSQL Database directory appears to contain a database; Skipping initialization\ngraph-up-langgraph-postgres-1  | \ngraph-up-langgraph-postgres-1  | \ngraph-up-langgraph-api-1       | 2024-06-30 09:14:20,495:INFO:uvicorn.error Started server process [1]\ngraph-up-langgraph-api-1       | 2024-06-30 09:14:20,495:INFO:uvicorn.error Waiting for application startup.\ngraph-up-langgraph-api-1       | 2024-06-30 09:14:20,496:ERROR:uvicorn.error Traceback (most recent call last):\ngraph-up-langgraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 732, in lifespan\ngraph-up-langgraph-api-1       |     async with self.lifespan_context(app) as maybe_state:\ngraph-up-langgraph-api-1       |   File \"/usr/local/lib/python3.11/contextlib.py\", line 210, in __aenter__\ngraph-up-langgraph-api-1       |     return await anext(self.gen)\ngraph-up-langgraph-api-1       |            ^^^^^^^^^^^^^^^^^^^^^\ngraph-up-langgraph-api-1       |   File \"/api/langgraph_api/lifespan.py\", line 18, in lifespan\ngraph-up-langgraph-api-1       | ValueError: License key is not valid\ngraph-up-langgraph-api-1       | \ngraph-up-langgraph-api-1       | 2024-06-30 09:14:20,496:ERROR:uvicorn.error Application startup failed. Exiting.\ngraph-up-langgraph-api-1       | \ngraph-up-langgraph-api-1 exited with code 3\ntime=\"2024-06-30T17:14:14+08:00\" level=warning msg=\"The \\\"line\\\" variable is not set. Defaulting to a blank string.\"\n Container graph-up-langgraph-postgres-1  Created\n Container graph-up-langgraph-api-1  Created\ngraph-up-langgraph-postgres-1  | 2024-06-30 09:14:14.494 UTC [1] LOG:  starting PostgreSQL 16.3 (Debian 16.3-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit\ngraph-up-langgraph-postgres-1  | 2024-06-30 09:14:14.494 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\ngraph-up-langgraph-postgres-1  | 2024-06-30 09:14:14.494 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\ngraph-up-langgraph-postgres-1  | 2024-06-30 09:14:14.496 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\ngraph-up-langgraph-postgres-1  | 2024-06-30 09:14:14.499 UTC [29] LOG:  database system was shut down at 2024-06-30 09:13:28 UTC\ngraph-up-langgraph-postgres-1  | 2024-06-30 09:14:14.504 UTC [1] LOG:  database system is ready to accept connections\nAborting on container exit...\n Container graph-up-langgraph-api-1  Stopping\n Container graph-up-langgraph-api-1  Stopped\n Container graph-up-langgraph-postgres-1  Stopping\n Container graph-up-langgraph-postgres-1  Stopped\nDescription\nValueError: License key is not valid\nit's not work!\nSystem Info\npy311\nlanggraph-cli 1.47\nmac", "created_at": "2024-06-30", "closed_at": "2024-07-04", "labels": [], "State": "closed", "Author": "rxyshww"}
{"issue_number": 874, "issue_title": "Parallel tool calls causing errors in human-in-the-loop breakpoint example", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\ninputs = [HumanMessage(content=\"search for the weather in sf and in nyc now\")]\nError Message and Stack Trace (if applicable)\nBadRequestError\nCell In[96], line 1\n----> 1 for event in app.stream(None, thread, stream_mode=\"values\"):\n      2     event[\"messages\"][-1].pretty_print()\n\nFile /workspaces/waiv/backend/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1073, in Pregel.stream(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\n   1070         del fut, task\n   1072 # panic on failure or timeout\n-> 1073 _panic_or_proceed(done, inflight, step)\n   1074 # don't keep futures around in memory longer than needed\n   1075 del done, inflight, futures\n\nFile /workspaces/waiv/backend/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1639, in _panic_or_proceed(done, inflight, step)\n   1637             inflight.pop().cancel()\n   1638         # raise the exception\n-> 1639         raise exc\n   1641 if inflight:\n   1642     # if we got here means we timed out\n   1643     while inflight:\n   1644         # cancel all pending tasks\n\nFile /usr/local/lib/python3.12/concurrent/futures/thread.py:58, in _WorkItem.run(self)\n     55     return\n     57 try:\n...\n   (...)\n   1036     stream_cls=stream_cls,\n   1037 )\n\nBadRequestError: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': \"messages.2: the following `tool_use` ids were not found in `tool_result` blocks: {'toolu_01CWAWQdMDnYjrNBSneJfr3Z'}\"}}\nDescription\nI am running the breakpoints.ipynb notebook from the Human-in-the-Loop documentation and have modified the inputs variable (see above). This is the output when interacting with the agent with the modified input where it stops before calling the tool:\n================================ Human Message =================================\n\nsearch for the weather in sf and in nyc now\n================================== Ai Message ==================================\n\n[{'text': \"Certainly! I'll search for the current weather in both San Francisco (SF) and New York City (NYC) for you. To do this, I'll need to make two separate search queries using the available search function. Let me do that for you now.\", 'type': 'text'}, {'id': 'toolu_0134753sWzLXRGrbERPw7h8A', 'input': {'query': 'current weather in San Francisco'}, 'name': 'search', 'type': 'tool_use'}, {'id': 'toolu_01CWAWQdMDnYjrNBSneJfr3Z', 'input': {'query': 'current weather in New York City'}, 'name': 'search', 'type': 'tool_use'}]\nTool Calls:\n  search (toolu_0134753sWzLXRGrbERPw7h8A)\n Call ID: toolu_0134753sWzLXRGrbERPw7h8A\n  Args:\n    query: current weather in San Francisco\n  search (toolu_01CWAWQdMDnYjrNBSneJfr3Z)\n Call ID: toolu_01CWAWQdMDnYjrNBSneJfr3Z\n  Args:\n    query: current weather in New York City\n\nThis throws the above error after the first tool is run.\nSometimes the tools are called one after the other, which seems to work fine.\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP Thu May 23 08:36:57 UTC 2024\nPython Version:  3.12.1 (main, Feb  1 2024, 03:58:05) [GCC 12.2.0]\n\nPackage Information\n\nlangchain_core: 0.2.10\nlangchain: 0.2.1\nlangchain_community: 0.2.1\nlangsmith: 0.1.82\nlangchain_anthropic: 0.1.16\nlangchain_mistralai: 0.1.7\nlangchain_openai: 0.1.7\nlangchain_text_splitters: 0.2.0\nlanggraph: 0.1.3\n", "created_at": "2024-06-28", "closed_at": "2024-06-28", "labels": [], "State": "closed", "Author": "hannah-schiebener"}
{"issue_number": 872, "issue_title": "Failed to parse response from model when using OllamaFunctions with bind_tools()", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom typing import Annotated, Literal, Optional\nfrom typing_extensions import TypedDict\nfrom langgraph.graph.message import AnyMessage, add_messages\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field\nfrom langchain_core.runnables import Runnable, RunnableConfig\nfrom langchain_core.runnables import RunnableLambda\nfrom langchain_core.messages import ToolMessage\nfrom langgraph.prebuilt import ToolNode\nfrom typing import Callable\nfrom langchain_core.messages import ToolMessage\nfrom typing import Literal\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nfrom langgraph.graph import END, StateGraph\nfrom langgraph.prebuilt import tools_condition\nfrom langchain_openai import ChatOpenAI\nfrom langchain_experimental.llms.ollama_functions import OllamaFunctions\nfrom typing import Optional, List\nfrom langchain_core.tools import tool\nimport docx\nfrom datetime import datetime\nimport ast\nfrom langchain_community.tools import DuckDuckGoSearchRun\nfrom langchain_core.tools import Tool\n\n# stack to keep track of the current state of task\ndef update_dialog_stack(left: list[str], right: Optional[str]) -> list[str]:\n    \"\"\"Push or pop the state.\"\"\"\n    if right is None:\n        return left\n    if right == \"pop\":\n        return left[:-1]\n    return left + [right]\n\n# state of the LLM\nclass State(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n    user_info: str\n    dialog_state: Annotated[\n        list[\n        Literal[\"primary_assistant\", \n                \"search_assistant\",]],\n        update_dialog_stack,]\n        \nclass Assistant:\n    def __init__(self, runnable: Runnable):\n        self.runnable = runnable\n\n    def __call__(self, state: State, config: RunnableConfig):\n        while True:\n            result = self.runnable.invoke(state)\n\n            if not result.tool_calls and (\n                not result.content\n                or isinstance(result.content, list)\n                and not result.content[0].get(\"text\")\n            ):\n                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n                state = {**state, \"messages\": messages}\n            else:\n                break\n        return {\"messages\": result}\n\nclass CompleteOrEscalate(BaseModel):\n    \"\"\"A tool to mark the current task as completed and/or to escalate control of the dialog to the main assistant,\n    who can re-route the dialog based on the user's needs.\"\"\"\n\n    cancel: bool = True\n    reason: str\n\n    class Config:\n        schema_extra = {\n            \"example\": {\n                \"cancel\": True,\n                \"reason\": \"User changed their mind about the current task.\",\n            },\n            \"example 2\": {\n                \"cancel\": True,\n                \"reason\": \"I have fully completed the task.\",\n            },\n            \"example 3\": {\n                \"cancel\": False,\n                \"reason\": \"I need to have additional information from user to search.\",\n            },\n        }\n        \ndef handle_tool_error(state) -> dict:\n    error = state.get(\"error\")\n    debug_logs = state.get(\"debug_logs\", [])\n    tool_calls = state[\"messages\"][-1].tool_calls\n    return {\n        \"messages\": [\n            ToolMessage(\n                content=f\"Error: {repr(error)}\\nDebug Logs: {debug_logs}\\nPlease fix your mistakes and try to recall the function again.\",\n                tool_call_id=tc[\"id\"],\n            )\n            for tc in tool_calls\n        ]\n    }\n\ndef create_tool_node_with_fallback(tools: list) -> dict:\n    return ToolNode(tools).with_fallbacks(\n        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n    )\n\n\ndef _print_event(event: dict, _printed: set, max_length=3000):\n    current_state = event.get(\"dialog_state\")\n    if current_state:\n        print(f\"Currently in: \", current_state[-1])\n    message = event.get(\"messages\")\n    if message:\n        if isinstance(message, list):\n            message = message[-1]\n        if message.id not in _printed:\n            msg_repr = message.pretty_repr(html=True)\n            if len(msg_repr) > max_length:\n                msg_repr = msg_repr[:max_length] + \" ... (truncated)\"\n            print(msg_repr)\n            _printed.add(message.id)\n            \n\n# This node will be shared for exiting all specialized assistants\ndef pop_dialog_state(state: State) -> dict:\n    \"\"\"Pop the dialog stack and return to the main assistant.\n\n    This lets the full graph explicitly track the dialog flow and delegate control\n    to specific sub-graphs.\n    \"\"\"\n    messages = []\n    if state[\"messages\"][-1].tool_calls:\n        # Note: Doesn't currently handle the edge case where the llm performs parallel tool calls\n        messages.append(\n            ToolMessage(\n                content=\"Resuming dialog with the host assistant. Please reflect on the past conversation and assist the user as needed.\",\n                tool_call_id=state[\"messages\"][-1].tool_calls[0][\"id\"],\n            )\n        )\n    return {\n        \"dialog_state\": \"pop\",\n        \"messages\": messages,\n    }\n\n# LLM model to be used, low temperature to control the output possibilities\nllm = OllamaFunctions(model=\"phi3:14b-medium-4k-instruct-q8_0\", format = 'json', temperature = 0, verbose = True)\n\n## class to transfer from primary_assistant to search_assistant\nclass TosearchAssistant(BaseModel):\n    \"\"\"\n    Transfers work to a specialized assistant to search online\n    \"\"\"\n    \n    request: str = Field(\n        description = \"Any necessary followup questions the update flight assistant should clarify before proceeding.\"\n    )\n\n## Tool to identify the email to be used \n@tool\ndef online_query(request: str) -> str:\n    \"\"\"\n    Description:\n        query online\n    \n    Output:\n        returns the result\n    \"\"\"\n    search_tool =  DuckDuckGoSearchRun(max_results=2) \n    search_result = search_tool(request)\n    return search_result\n\n## prompt for the assistant\nsearch_assistant_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a specialized assistant working to search.\"\n            \"Remember that the report isn't completed until after the relevant tool has successfully been used.\"\n            \"Do not invoke the tool until all necessary information are obtained.\"\n            'If none of your tools are appropriate for it, then \"CompleteOrEscalate\" the dialog to the host assistant.'\n            \"Do not waste the user's time. Do not make up invalid tools or functions.\"\n            \"\\nCurrent time : {time}\",\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n).partial(time=datetime.now())\n\nsearch_assistant_safe_tools = [online_query]\nsearch_assistant_sensitive_tools = []\nsearch_assistant_tools = search_assistant_safe_tools + search_assistant_sensitive_tools\nsearch_assistant_runnable = search_assistant_prompt | llm.bind_tools(search_assistant_tools + [CompleteOrEscalate])\n\nfrom datetime import datetime \nfrom langchain_community.tools import DuckDuckGoSearchRun\n\nprimary_assistant_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a helpful support assistant. \"\n            \"Your primary role is to handle user's request. \"\n            \"Delegate the task to the appropriate specialized assistant by invoking the corresponding tool.\"\n            \"You can only search for president, any other search pass to search assistant.\"\n            \"You are not able to make these types of changes yourself.\"\n            \"Only the specialized assistants are given permission to do this for the user.\"\n            \"The user is not aware of the different specialized assistants, so do not mention them; just quietly delegate through function calls. \"\n            \"Provide detailed information to the customer, and always double-check before concluding that information is unavailable. \"\n            \"When searching, be persistent. Expand your query bounds once if the first search returns no results. \"\n            \"\\nCurrent time: {time}.\",\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n).partial(time=datetime.now())\nprimary_assistant_tools = [DuckDuckGoSearchRun(max_results=1)]\nassistant_runnable = primary_assistant_prompt | llm.bind_tools(primary_assistant_tools + [TosearchAssistant,])\n\nfrom typing import Callable\nfrom langchain_core.messages import ToolMessage\n\nfrom typing import Literal\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nfrom langgraph.graph import END, StateGraph\nfrom langgraph.prebuilt import tools_condition\n\n## initial entry node when switch to a new assistant to prompt\ndef create_entry_node(assistant_name: str, new_dialog_state: str) -> Callable:\n    def entry_node(state: State) -> dict:\n        tool_call_id = state[\"messages\"][-1].tool_calls[0][\"id\"]\n        return {\n            \"messages\": [\n                ToolMessage(\n                    content=f\"The assistant is now the {assistant_name}. Reflect on the above conversation between the host assistant and the user.\"\n                    f\" The user's intent is unsatisfied. Use the provided tools to assist the user. Remember, you are {assistant_name},\"\n                    \" and the task is not complete until after you have successfully invoked the appropriate tool.\"\n                    \" If the user changes their mind or needs help for other tasks, call the CompleteOrEscalate function to let the primary host assistant take control.\"\n                    \" Ask the user for information in order to perform the task. \",\n                    tool_call_id=tool_call_id,\n                )\n            ],\n            \"dialog_state\": new_dialog_state,\n        }\n\n    return entry_node\n\n## Building of graph (search assistant portion)\nbuilder = StateGraph(State)\n\nbuilder.add_node(\"enter_search_assistant\", create_entry_node(\"search Generation Assistant\", \"search_assistant\")) # transfering from Primary to search_assistant\nbuilder.add_node(\"search_assistant\", Assistant(search_assistant_runnable)) # actual node with search_assistant that has the tools to perform tasking\nbuilder.add_edge(\"enter_search_assistant\", \"search_assistant\") # connects the two nodes\nbuilder.add_node(\"search_assistant_sensitive_tools\",create_tool_node_with_fallback(search_assistant_sensitive_tools),)\nbuilder.add_node(\"search_assistant_safe_tools\",create_tool_node_with_fallback(search_assistant_safe_tools),)\n\n## Determine the possible routing for search_assistant\ndef route_search_Assistant(state: State,) -> Literal[\"search_assistant_sensitive_tools\", \"search_assistant_safe_tools\", \"leave_skill\", \"__end__\",]:\n    \n    route = tools_condition(state)\n    if route == END:\n        return END\n    tool_calls = state[\"messages\"][-1].tool_calls\n    did_cancel = any(tc[\"name\"] == CompleteOrEscalate.__name__ for tc in tool_calls)\n    if did_cancel:\n        return \"leave_skill\"\n    safe_toolnames = [t.name for t in search_assistant_safe_tools]\n    if all(tc[\"name\"] in safe_toolnames for tc in tool_calls):\n        return \"search_assistant_safe_tools\"\n    return \"search_assistant_sensitive_tools\"\n\n\nbuilder.add_edge(\"search_assistant_sensitive_tools\", \"search_assistant\")\nbuilder.add_edge(\"search_assistant_safe_tools\", \"search_assistant\")\nbuilder.add_conditional_edges(\"search_assistant\", route_search_Assistant)\n\n# Primary assistant\nbuilder.add_node(\"primary_assistant\", Assistant(assistant_runnable))\nbuilder.set_entry_point(\"primary_assistant\")\nbuilder.add_node(\"primary_assistant_tools\", create_tool_node_with_fallback(primary_assistant_tools))\n\n\ndef route_primary_assistant(state: State,) -> Literal[\"primary_assistant_tools\",\"enter_search_assistant\",\"__end__\",]:\n    route = tools_condition(state)\n    if route == END:\n        return END\n    tool_calls = state[\"messages\"][-1].tool_calls\n    if tool_calls:\n        if tool_calls[0][\"name\"] == TosearchAssistant.__name__:\n            return \"enter_search_assistant\"\n        return \"primary_assistant_tools\"\n    raise ValueError(\"Invalid route\")\n\n\n# The assistant can route to one of the delegated assistants, directly use a tool, or directly respond to the user\nbuilder.add_conditional_edges(\n    \"primary_assistant\",\n    route_primary_assistant,\n    {\n        \"enter_search_assistant\": \"enter_search_assistant\",\n        \"primary_assistant_tools\": \"primary_assistant_tools\",\n        END: END,\n    },\n)\nbuilder.add_edge(\"primary_assistant_tools\", \"primary_assistant\")\nbuilder.add_node(\"leave_skill\", pop_dialog_state)\nbuilder.add_edge(\"leave_skill\", \"primary_assistant\")\n\n\n# Each delegated workflow can directly respond to the user\n# When the user responds, we want to return to the currently active workflow\ndef route_to_workflow(state: State,) -> Literal[\"primary_assistant\",\"search_assistant\",]:\n    \"\"\"If we are in a delegated state, route directly to the appropriate assistant.\"\"\"\n    dialog_state = state.get(\"dialog_state\")\n    if not dialog_state:\n        return \"primary_assistant\"\n    return dialog_state[-1]\n\nconfig = {\"configurable\": {\"thread_id\": 403,}}\n\n# Compile graph\nmemory = SqliteSaver.from_conn_string(\":memory:\")\nfinal_graph = builder.compile(\n    checkpointer=memory,\n    # Let the user approve or deny the use of sensitive tools\n    interrupt_before=[\"search_assistant_sensitive_tools\",\n    ],\n)\n\n_printed = set()       \n\nwhile True:\n    user_input = input(\"Please type your request below (type 'exit' to quit):\\n\\n\")\n    \n    # Check if the user wants to exit\n    if user_input.lower() == \"exit\":\n        print(\"Exiting...\")\n        break\n    \n    # Put user input into the stream\n    events = final_graph.stream({\"messages\": (\"user\", user_input)}, config, stream_mode=\"values\")\n    \n    # Process events\n    for event in events:\n        _print_event(event, _printed)\n    snapshot = final_graph.get_state(config)\n    while snapshot.next:\n\n        # We have an interrupt! The agent is trying to use a sensitive tool, and the user can approve or deny it\n        user_approval = input(\n            \"Do you approve of the above actions? Type 'yes' to continue; otherwise, type 'no' and input a new request or type 'exit' to quit.\\n\\n\"\n        )\n        if user_approval.lower().strip() == \"yes\":\n            # Continue with the graph execution\n            result = final_graph.invoke(\n                None,\n                config,\n            )\n        else:\n            result = final_graph.invoke(\n                {\n                    \"messages\": [\n                        ToolMessage(\n                            tool_call_id=event[\"messages\"][-1].tool_calls[0][\"id\"],\n                            content=f\"API call denied by user. Reasoning: '{user_approval}'. Revert the user back to the previous state.\",\n                        )\n                    ]\n                },\n                config,\n            )\n        snapshot = final_graph.get_state(config)\nError Message and Stack Trace (if applicable)\nask the search assistant to find the population size of new york\n================================== Ai Message ==================================\nTool Calls:\n  duckduckgo_search (call_556e7a4ef6ea47038d98e5ece6d50563)\n Call ID: call_556e7a4ef6ea47038d98e5ece6d50563\n  Args:\n    query: New York State population\n================================= Tool Message =================================\nName: duckduckgo_search\n\nNew York. QuickFacts provides statistics for all states and counties. Also for cities and towns with a population of 5,000 or more. ... In Vintage 2022, as a result of the formal request from the state, Connecticut transitioned from eight counties to nine planning regions. 7,604,523. Persons per household, 2018-2022. 2.55. Living in same house 1 year ago, percent of persons age 1 year+, 2018-2022. 89.8%. Language other than English spoken at home, percent of persons age 5 years+, 2018-2022. 30.6%. Computer and Internet Use. Households with a computer, percent, 2018-2022. The population in New York aged 65+ grew by 30.2% from 2010 to 2020, while the 85+ population grew by 13%. In 2020, New York State had the fourth largest populations of both same-sex married (48,442) and same-sex unmarried couples (35,096) in the country. New York, constituent state of the U.S., one of the 13 original colonies and states. Its capital is Albany and its largest city is New York City, the cultural and financial center of American life. Until the 1960s New York was the country's leading state in nearly all population, cultural, and economic indexes. The population of New York continues to decline faster than any other state, according to the latest estimates out Tuesday from the U.S. Census Bureau. With a net loss of just under 102,000 ...\n================================== Ai Message ==================================\nTool Calls:\n  duckduckgo_search (call_5847cf1707c24ee8bf51c265b61f0f99)\n Call ID: call_5847cf1707c24ee8bf51c265b61f0f99\n  Args:\n    query: population size of New York\n================================= Tool Message =================================\nName: duckduckgo_search\n\nThis influx dramatically increased population density, making New York City one of the most densely populated cities in the United States. 1900s - The Modern Metropolis. ... New York City (NYC) Size: Approximately 468.9 square miles (1,214 km\u00b2). Population (as of 2021): Over 8.4 million. Basic Statistic New York-Newark-Jersey City metro area population in the U.S. 2010-2022 Premium Statistic Resident population of New York City, NY, by race 2022 New York City has a 2024 population of 7,931,147, according to data from World Population Review. World Population Review says New York City is currently declining at a rate of -2.49% annually and ... Yes, New York is the biggest city in the state of New York based on population. The second largest city, Buffalo, is 3% the size of New York. What is the population density of New York, New York? New York has a population density of 27,012.5 people per square mile. New York-Newark-Jersey City metro area population in the U.S. 2010-2022. Published by. Veera Korhonen , Nov 17, 2023. In 2022, about 19.62 million people populated the New York-Newark-Jersey City ...\nTraceback (most recent call last):\n  File \"/home/user/jett/test.py\", line 351, in <module>\n    for event in events:\n  File \"/home/user/anaconda3/envs/agentic/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 983, in stream\n    _panic_or_proceed(done, inflight, step)\n  File \"/home/user/anaconda3/envs/agentic/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 1537, in _panic_or_proceed\n    raise exc\n  File \"/home/user/anaconda3/envs/agentic/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/anaconda3/envs/agentic/lib/python3.12/site-packages/langgraph/pregel/retry.py\", line 72, in run_with_retry\n    task.proc.invoke(task.input, task.config)\n  File \"/home/user/anaconda3/envs/agentic/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 2502, in invoke\n    input = step.invoke(input, config, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/anaconda3/envs/agentic/lib/python3.12/site-packages/langgraph/utils.py\", line 95, in invoke\n    ret = context.run(self.func, input, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/jett/test.py\", line 40, in __call__\n    result = self.runnable.invoke(state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/anaconda3/envs/agentic/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 2504, in invoke\n    input = step.invoke(input, config)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/anaconda3/envs/agentic/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 4573, in invoke\n    return self.bound.invoke(\n           ^^^^^^^^^^^^^^^^^^\n  File \"/home/user/anaconda3/envs/agentic/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 170, in invoke\n    self.generate_prompt(\n  File \"/home/user/anaconda3/envs/agentic/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 599, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/anaconda3/envs/agentic/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 456, in generate\n    raise e\n  File \"/home/user/anaconda3/envs/agentic/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 446, in generate\n    self._generate_with_cache(\n  File \"/home/user/anaconda3/envs/agentic/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 671, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/home/user/anaconda3/envs/agentic/lib/python3.12/site-packages/langchain_experimental/llms/ollama_functions.py\", line 418, in _generate\n    raise ValueError(\nValueError: Failed to parse a response from phi3:14b-medium-4k-instruct-q8_0 output: {\n\n\n\"Population of New York State\": \"As of July 1, 2021, the estimated population of New York state was approximately 20.21 million according to the United States Census Bureau.\"}\nDescription\nI am trying to implement OllamaFunctions into the chatbot example part 4 shown in langGraph documentation. However, I face into an issue as when a tool is called, it arise this error of \"Failed to parse a response from model\" although the LLM have already retrieved the necessary information. The structure of the code and functions are all copied over from the example provided in documentation, with the exception of using OllamaFunctions instead of OpenAI model (which i have tried and it works without issue).\nI have always tried with changing the Ollama model such as llama and phi3, but it still brings about the same error.\nEDIT: I forgot to mention that the issue typically arises when I specify the initial chat to route to search_agent and use one of its tool (which is the online_query() in this case), and it will provide the error. However, if I were to use the initial primary agent and it's tool, it would not cause an error and is able to output normally.\nSystem Info\nplatform mac\npython version 3.12.4", "created_at": "2024-06-28", "closed_at": "2024-06-28", "labels": [], "State": "closed", "Author": "WenJett"}
{"issue_number": 868, "issue_title": "DOC: Typo (potential)", "issue_body": "Issue with current documentation:\nI was reading the following page - https://langchain-ai.github.io/langgraph/tutorials/rewoo/rewoo/\n\"The tasks are still executed in sequence, meaning the total execution time is impacted by every tool call, not just he longest-running in a given step.\"\nThere is a typo --> he --> the\nPlease see\nIdea or request for content:\nDocumentation is LLM generated. Is it?", "created_at": "2024-06-28", "closed_at": "2024-06-28", "labels": [], "State": "closed", "Author": "DhavalRepo18"}
{"issue_number": 853, "issue_title": "Error: 'ToolNode' object is not iterable", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\ntools = [retriever_tool, my_tool]\n\ndef agent(state):\n    \"\"\"\n    Invokes the agent model to generate a response based on the current state. Given\n    the question, it will decide to retrieve using the retriever tool, or simply end.\n\n    Args:\n        state (messages): The current state\n\n    Returns:\n        dict: The updated state with the agent response appended to messages\n    \"\"\"\n    \n    prompt = hub.pull(\"placeholder for the prompt\")\n    \n    print(\"---CALL AGENT---\")\n    messages = state[\"messages\"]\n    model = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0125\",streaming=True)\n    model = model.bind_tools(tools=tools)\n    agent = prompt | model\n    response = agent.invoke({\"question\": messages[-1].content,\"chat_history\": str(messages[:-1]),\"varos\":\"\",\"context\":\"\"})\n \n    return {\"messages\": [response]}\n\n*****\n\nfrom langgraph.graph import END, StateGraph\nfrom langgraph.prebuilt import ToolNode\n\n# Define a new graph\nworkflow = StateGraph(AgentState)\n\n# Define the nodes we will cycle between\nworkflow.add_node(\"topic_validator\",topic_validator)  # topic validator\nworkflow.add_node(\"router\", chain)  # agent\nworkflow.add_node(\"agent\", agent)  # agent\n\ntools = ToolNode(tools=[retriever_tool, my_tool])\nworkflow.add_node(\"tools_node\", tools) \n\nworkflow.add_node(\"standalone\", standalone)  # Re-writing the question\n\nworkflow.add_node(\"chains\", chain)\n\n\nworkflow.set_entry_point(\"topic_validator\")\nworkflow.add_edge(\"topic_validator\", \"router\")\nworkflow.add_edge(\"router\", \"standalone\")\nworkflow.add_edge(\"standalone\", \"agent\")\n\nworkflow.add_conditional_edges(\n    \"agent\",\n    should_continue\n)\n\n\nworkflow.add_edge(\"tools_node\", \"chains\")\nworkflow.add_edge(\"chains\", END)\n\n# Compile\ngraph = workflow.compile(debug=True)\n\n*****\n\ninputs = {\n    \"messages\": [\n        (\"user\", \"Question?\"),\n    ]\n}\n\ntry:\n    graph_result = graph.stream(inputs)\n    for output in graph_result:\n        print(\"OUTPUT:\")\n        for key, value in output.items():\n            print(f\"Output from node '{key}':\")\n            print(value)\n        print(\"\\n---\\n\")\nexcept Exception as e:\n    print(f\"Error: {e}\")\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[163], line 1\n----> 1 await graph.ainvoke(inputs)\n\nFile ~/.local/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1606, in Pregel.ainvoke(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug, **kwargs)\n   1604 else:\n   1605     chunks = []\n-> 1606 async for chunk in self.astream(\n   1607     input,\n   1608     config,\n   1609     stream_mode=stream_mode,\n   1610     output_keys=output_keys,\n   1611     input_keys=input_keys,\n   1612     interrupt_before=interrupt_before,\n   1613     interrupt_after=interrupt_after,\n   1614     debug=debug,\n   1615     **kwargs,\n   1616 ):\n   1617     if stream_mode == \"values\":\n   1618         latest = chunk\n\nFile ~/.local/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1431, in Pregel.astream(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\n   1428         del fut, task\n   1430 # panic on failure or timeout\n-> 1431 _panic_or_proceed(done, inflight, step)\n   1432 # don't keep futures around in memory longer than needed\n   1433 del done, inflight, futures\n\nFile ~/.local/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1639, in _panic_or_proceed(done, inflight, step)\n   1637             inflight.pop().cancel()\n   1638         # raise the exception\n-> 1639         raise exc\n   1641 if inflight:\n   1642     # if we got here means we timed out\n   1643     while inflight:\n   1644         # cancel all pending tasks\n\nFile ~/.local/lib/python3.10/site-packages/langgraph/pregel/retry.py:120, in arun_with_retry(task, retry_policy, stream)\n    118         pass\n    119 else:\n--> 120     await task.proc.ainvoke(task.input, task.config)\n    121 # if successful, end\n    122 break\n\nFile ~/.local/lib/python3.10/site-packages/langchain_core/runnables/base.py:2543, in RunnableSequence.ainvoke(self, input, config, **kwargs)\n   2539 config = patch_config(\n   2540     config, callbacks=run_manager.get_child(f\"seq:step:{i+1}\")\n   2541 )\n   2542 if i == 0:\n-> 2543     input = await step.ainvoke(input, config, **kwargs)\n   2544 else:\n   2545     input = await step.ainvoke(input, config)\n\nFile ~/.local/lib/python3.10/site-packages/langgraph/utils.py:121, in RunnableCallable.ainvoke(self, input, config, **kwargs)\n    117         ret = await asyncio.create_task(\n    118             self.afunc(input, **kwargs), context=context\n    119         )\n    120     else:\n--> 121         ret = await self.afunc(input, **kwargs)\n    122 if isinstance(ret, Runnable) and self.recurse:\n    123     return await ret.ainvoke(input, config)\n\nFile ~/.local/lib/python3.10/site-packages/langchain_core/runnables/config.py:557, in run_in_executor(executor_or_config, func, *args, **kwargs)\n    553         raise RuntimeError from exc\n    555 if executor_or_config is None or isinstance(executor_or_config, dict):\n    556     # Use default executor with context copied from current context\n--> 557     return await asyncio.get_running_loop().run_in_executor(\n    558         None,\n    559         cast(Callable[..., T], partial(copy_context().run, wrapper)),\n    560     )\n    562 return await asyncio.get_running_loop().run_in_executor(executor_or_config, wrapper)\n\nFile /usr/lib/python3.10/concurrent/futures/thread.py:58, in _WorkItem.run(self)\n     55     return\n     57 try:\n---> 58     result = self.fn(*self.args, **self.kwargs)\n     59 except BaseException as exc:\n     60     self.future.set_exception(exc)\n\nFile ~/.local/lib/python3.10/site-packages/langchain_core/runnables/config.py:548, in run_in_executor.<locals>.wrapper()\n    546 def wrapper() -> T:\n    547     try:\n--> 548         return func(*args, **kwargs)\n    549     except StopIteration as exc:\n    550         # StopIteration can't be set on an asyncio.Future\n    551         # it raises a TypeError and leaves the Future pending forever\n    552         # so we need to convert it to a RuntimeError\n    553         raise RuntimeError from exc\n\nCell In[155], line 20\n     18 messages = state[\"messages\"]\n     19 model = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0125\",streaming=True)\n---> 20 model = model.bind_tools(tools=tools)\n     21 agent = prompt | model\n     22 response = agent.invoke({\"question\": messages[-1].content,\"chat_history\": str(messages[:-1]),\"varos\":\"\",\"context\":\"\"})\n\nFile ~/.local/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:892, in BaseChatOpenAI.bind_tools(self, tools, tool_choice, **kwargs)\n    859 def bind_tools(\n    860     self,\n    861     tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],\n   (...)\n    866     **kwargs: Any,\n    867 ) -> Runnable[LanguageModelInput, BaseMessage]:\n    868     \"\"\"Bind tool-like objects to this chat model.\n    869 \n    870     Assumes model is compatible with OpenAI tool-calling API.\n   (...)\n    889             :class:`~langchain.runnable.Runnable` constructor.\n    890     \"\"\"\n--> 892     formatted_tools = [convert_to_openai_tool(tool) for tool in tools]\n    893     if tool_choice:\n    894         if isinstance(tool_choice, str):\n    895             # tool_choice is a tool/function name\n\nTypeError: 'ToolNode' object is not iterable\nDescription\nI'm using langgraph with a tool user llm, but i get error Error: 'ToolNode' object is not iterable. I provided the part of the code.\nSystem Info\n%pip install -U langgraph langchain openai langchain_openai langchain_community", "created_at": "2024-06-27", "closed_at": "2024-06-27", "labels": [], "State": "closed", "Author": "zsitvat"}
{"issue_number": 847, "issue_title": "storm notebook raises exception on final_state", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfinal_state = next(iter(final_step.values()))\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[25], line 1\n----> 1 final_state = next(iter(final_step.values()))\n\nAttributeError: 'NoneType' object has no attribute 'values'\nDescription\nI'm running through the cells of the storm.ipynb (from HEAD of main), one cell raises an error.\nSystem Info\n\u276f python -m langchain_core.sys_info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP PREEMPT_DYNAMIC Sun Jun 16 15:47:09 UTC 2024\nPython Version:  3.12.3 (main, Apr 17 2024, 00:00:00) [GCC 14.0.1 20240411 (Red Hat 14.0.1-0)]\n\nPackage Information\n\nlangchain_core: 0.2.10\nlangchain: 0.2.6\nlangchain_community: 0.2.6\nlangsmith: 0.1.82\nlangchain_anthropic: 0.1.15\nlangchain_chroma: 0.1.1\nlangchain_cli: 0.0.25\nlangchain_fireworks: 0.1.3\nlangchain_huggingface: 0.0.3\nlangchain_openai: 0.1.10\nlangchain_text_splitters: 0.2.1\nlanggraph: 0.1.2\nlangserve: 0.2.2\n", "created_at": "2024-06-27", "closed_at": null, "labels": ["bug", "documentation"], "State": "open", "Author": "goern"}
{"issue_number": 789, "issue_title": "AsyncSqliteSaver hangs when using astream_event", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nimport asyncio\nfrom pprint import pprint\n\nfrom langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph\n\nbuilder = StateGraph(int)\nbuilder.add_node(\"add_one\", lambda x: x + 1)\nbuilder.set_entry_point(\"add_one\")\nbuilder.set_finish_point(\"add_one\")\n\n\nasync def memory():\n    memory = MemorySaver()\n    graph = builder.compile(checkpointer=memory)\n    config = {\"configurable\": {\"thread_id\": \"thread-1\"}}\n    async for event in graph.astream_events(10, config, version=\"v1\"):\n        pprint(event)\n\n\nasync def asyncSqlite():\n    memory = AsyncSqliteSaver.from_conn_string(\":memory:\")\n    graph = builder.compile(checkpointer=memory)\n    config = {\"configurable\": {\"thread_id\": \"thread-1\"}}\n    async for event in graph.astream_events(10, config, version=\"v1\"):\n        pprint(event)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(memory())  # <--- Works\n    asyncio.run(asyncSqlite())  # <--- Hangs here\nError Message and Stack Trace (if applicable)\nException ignored in: <module 'threading' from 'lib/python3.11/threading.py'>\nTraceback (most recent call last):\n  File \" lib/python3.11/threading.py\", line 1590, in _shutdown\n    lock.acquire()\nKeyboardInterrupt:\nDescription\nI am trying to add langraph with astream_events to fastapi and it hangs after the first request is complete. i was a able to isolate the issue using a cli tool\nSystem Info\nlangchain==0.1.20\nlangchain-community==0.0.38\nlangchain-core==0.1.52\nlangchain-openai==0.1.6\nlangchain-text-splitters==0.0.1\nopeninference-instrumentation-langchain==0.1.14", "created_at": "2024-06-24", "closed_at": "2024-06-27", "labels": [], "State": "closed", "Author": "ewianda"}
{"issue_number": 1459, "issue_title": "I cannot get LLMCompiler work ", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langchain_openai import ChatOpenAI\n\n    llm = ChatOpenAI(model=llm_model_name) \n    calculate = get_math_tool(llm)\n\n\n\n    search = TavilySearchResults(\n        max_results=1,\n        description='tavily_search_results_json(query=\"the search query\") - a search engine.',\n    )\n\n    tools = [search, calculate]\n\n\n\n\n    # Part 2: Planner\n    # region Planner\n    from typing import Sequence\n\n    from langchain import hub\n    from langchain_core.language_models import BaseChatModel\n    from langchain_core.messages import (\n        BaseMessage,\n        FunctionMessage,\n        HumanMessage,\n        SystemMessage,\n    )\n    from langchain_core.prompts import ChatPromptTemplate\n    from langchain_core.runnables import RunnableBranch\n    from langchain_core.tools import BaseTool\n    from langchain_openai import ChatOpenAI\n    from utils.llm_compiler_output_parser import LLMCompilerPlanParser, Task\n\n    prompt = hub.pull(\"wfh/llm-compiler\")\n\n\n\n    def create_planner(\n    llm: BaseChatModel, tools: Sequence[BaseTool], base_prompt: ChatPromptTemplate\n):\n        tool_descriptions = \"\\n\".join(\n            f\"{i+1}. {tool.description}\\n\"\n            for i, tool in enumerate(\n                tools\n            )  # +1 to offset the 0 starting index, we want it count normally from 1.\n        )\n        planner_prompt = base_prompt.partial(\n            replan=\"\",\n            num_tools=len(tools)\n            + 1,  # Add one because we're adding the join() tool at the end.\n            tool_descriptions=tool_descriptions,\n        )\n        replanner_prompt = base_prompt.partial(\n            replan=' - You are given \"Previous Plan\" which is the plan that the previous agent created along with the execution results '\n            \"(given as Observation) of each plan and a general thought (given as Thought) about the executed results.\"\n            'You MUST use these information to create the next plan under \"Current Plan\".\\n'\n            ' - When starting the Current Plan, you should start with \"Thought\" that outlines the strategy for the next plan.\\n'\n            \" - In the Current Plan, you should NEVER repeat the actions that are already executed in the Previous Plan.\\n\"\n            \" - You must continue the task index from the end of the previous one. Do not repeat task indices.\", # type: ignore\n            num_tools=len(tools) + 1,\n            tool_descriptions=tool_descriptions,\n        )\n\n        def should_replan(state: list):\n            # Context is passed as a system message\n            return isinstance(state[-1], SystemMessage)\n\n        def wrap_messages(state: list):\n            return {\"messages\": state}\n\n        def wrap_and_get_last_index(state: list):\n            next_task = 0\n            for message in state[::-1]:\n                if isinstance(message, FunctionMessage):\n                    next_task = message.additional_kwargs[\"idx\"] + 1\n                    break\n            state[-1].content = state[-1].content + f\" - Begin counting at : {next_task}\"\n            return {\"messages\": state}\n\n        return (\n            RunnableBranch(\n                (should_replan, wrap_and_get_last_index | replanner_prompt),\n                wrap_messages | planner_prompt,\n            )\n            | llm\n            | LLMCompilerPlanParser(tools=tools)\n        )\n\n\n\n    # This is the primary \"agent\" in our application\n    planner = create_planner(llm, tools, prompt)\n\n    # endregion\n\n    # 3. Task Fetching Unit\n    # region Task Fetching Unit\n    import re\n    import time\n    from concurrent.futures import ThreadPoolExecutor, wait\n    from typing import Any, Dict, Iterable, List, Union\n\n    from langchain_core.runnables import (\n        chain as as_runnable,\n    )\n    from typing_extensions import TypedDict\n\n\n    def _get_observations(messages: List[BaseMessage]) -> Dict[int, Any]:\n        # Get all previous tool responses\n        results = {}\n        for message in messages[::-1]:\n            if isinstance(message, FunctionMessage):\n                results[int(message.additional_kwargs[\"idx\"])] = message.content\n        return results\n\n\n    class SchedulerInput(TypedDict):\n        messages: List[BaseMessage]\n        tasks: Iterable[Task]\n\n\n    def _execute_task(task, observations, config):\n        tool_to_use = task[\"tool\"]\n        if isinstance(tool_to_use, str):\n            return tool_to_use\n        args = task[\"args\"]\n        try:\n            if isinstance(args, str):\n                resolved_args = _resolve_arg(args, observations)\n            elif isinstance(args, dict):\n                resolved_args = {\n                    key: _resolve_arg(val, observations) for key, val in args.items()\n                }\n            else:\n                # This will likely fail\n                resolved_args = args\n        except Exception as e:\n            return (\n                f\"ERROR(Failed to call {tool_to_use.name} with args {args}.)\"\n                f\" Args could not be resolved. Error: {repr(e)}\"\n            )\n        try:\n            return tool_to_use.invoke(resolved_args, config)\n        except Exception as e:\n            return (\n                f\"ERROR(Failed to call {tool_to_use.name} with args {args}.\"\n                + f\" Args resolved to {resolved_args}. Error: {repr(e)})\"\n            )\n\n\n    def _resolve_arg(arg: Union[str, Any], observations: Dict[int, Any]):\n        # $1 or ${1} -> 1\n        ID_PATTERN = r\"\\$\\{?(\\d+)\\}?\"\n\n        def replace_match(match):\n            # If the string is ${123}, match.group(0) is ${123}, and match.group(1) is 123.\n\n            # Return the match group, in this case the index, from the string. This is the index\n            # number we get back.\n            idx = int(match.group(1))\n            return str(observations.get(idx, match.group(0)))\n\n        # For dependencies on other tasks\n        if isinstance(arg, str):\n            return re.sub(ID_PATTERN, replace_match, arg)\n        elif isinstance(arg, list):\n            return [_resolve_arg(a, observations) for a in arg]\n        else:\n            return str(arg)\n\n\n    @as_runnable\n    def schedule_task(task_inputs, config):\n        task: Task = task_inputs[\"task\"]\n        observations: Dict[int, Any] = task_inputs[\"observations\"]\n        try:\n            observation = _execute_task(task, observations, config)\n        except Exception:\n            import traceback\n\n            observation = traceback.format_exception()  # repr(e) +\n        observations[task[\"idx\"]] = observation\n\n\n    def schedule_pending_task(\n        task: Task, observations: Dict[int, Any], retry_after: float = 0.2\n    ):\n        while True:\n            deps = task[\"dependencies\"]\n            if deps and (any([dep not in observations for dep in deps])):\n                # Dependencies not yet satisfied\n                time.sleep(retry_after)\n                continue\n            schedule_task.invoke({\"task\": task, \"observations\": observations})\n            break\n\n\n    @as_runnable\n    def schedule_tasks(scheduler_input: SchedulerInput) -> List[FunctionMessage]:\n        \"\"\"Group the tasks into a DAG schedule.\"\"\"\n        # For streaming, we are making a few simplifying assumption:\n        # 1. The LLM does not create cyclic dependencies\n        # 2. That the LLM will not generate tasks with future deps\n        # If this ceases to be a good assumption, you can either\n        # adjust to do a proper topological sort (not-stream)\n        # or use a more complicated data structure\n        tasks = scheduler_input[\"tasks\"]\n        args_for_tasks = {}\n        messages = scheduler_input[\"messages\"]\n        # If we are re-planning, we may have calls that depend on previous\n        # plans. Start with those.\n        observations = _get_observations(messages)\n        task_names = {}\n        originals = set(observations)\n        # ^^ We assume each task inserts a different key above to\n        # avoid race conditions...\n        futures = []\n        retry_after = 0.25  # Retry every quarter second\n        with ThreadPoolExecutor() as executor:\n            for task in tasks:\n                deps = task[\"dependencies\"]\n                task_names[task[\"idx\"]] = (\n                    task[\"tool\"] if isinstance(task[\"tool\"], str) else task[\"tool\"].name\n                )\n                args_for_tasks[task[\"idx\"]] = task[\"args\"]\n                if (\n                    # Depends on other tasks\n                    deps\n                    and (any([dep not in observations for dep in deps]))\n                ):\n                    futures.append(\n                        executor.submit(\n                            schedule_pending_task, task, observations, retry_after\n                        )\n                    )\n                else:\n                    # No deps or all deps satisfied\n                    # can schedule now\n                    schedule_task.invoke(dict(task=task, observations=observations))\n                    # futures.append(executor.submit(schedule_task.invoke dict(task=task, observations=observations)))\n\n            # All tasks have been submitted or enqueued\n            # Wait for them to complete\n            wait(futures)\n        # Convert observations to new tool messages to add to the state\n        new_observations = {\n            k: (task_names[k], args_for_tasks[k], observations[k])\n            for k in sorted(observations.keys() - originals)\n        }\n        tool_messages = [\n            FunctionMessage(\n                name=name, content=str(obs), additional_kwargs={\"idx\": k, \"args\": task_args}\n            )\n            for k, (name, task_args, obs) in new_observations.items()\n        ]\n        return tool_messages\n    \n\n    import itertools\n\n\n    @as_runnable\n    def plan_and_schedule(state):\n        messages = state[\"messages\"]\n        tasks = planner.stream(messages)\n        # Begin executing the planner immediately\n        try:\n            tasks = itertools.chain([next(tasks)], tasks)\n        except StopIteration:\n            # Handle the case where tasks is empty.\n            tasks = iter([])\n        scheduled_tasks = schedule_tasks.invoke(\n            {\n                \"messages\": messages,\n                \"tasks\": tasks,\n            }\n        )\n        return {\"messages\": [scheduled_tasks]}\n    \n    # endregion\n    \n    # 4. \"Joiner\"\n    # region Joiner\n    from langchain.chains.openai_functions import create_structured_output_runnable\n    from   langchain_core.messages import AIMessage\n    from langchain_core.pydantic_v1 import BaseModel, Field\n\n\n    class FinalResponse(BaseModel):\n        \"\"\"The final response/answer.\"\"\"\n\n        response: str\n\n\n    class Replan(BaseModel):\n        feedback: str = Field(\n            description=\"Analysis of the previous attempts and recommendations on what needs to be fixed.\"\n        )\n\n\n    class JoinOutputs(BaseModel):\n        \"\"\"Decide whether to replan or whether you can return the final response.\"\"\"\n\n        thought: str = Field(\n            description=\"The chain of thought reasoning for the selected action\"\n        )\n        action: Union[FinalResponse, Replan]\n\n\n    joiner_prompt = hub.pull(\"wfh/llm-compiler-joiner\").partial(\n        examples=\"\"\n    )  # You can optionally add examples\n    \n\n    runnable = create_structured_output_runnable(JoinOutputs, llm, joiner_prompt)\n\n    \n    # We will select only the most recent messages in the state, and format the output to be more useful for the planner, should the agent need to loop.\n    def _parse_joiner_output(decision: JoinOutputs) -> List[BaseMessage]:\n        response = [AIMessage(content=f\"Thought: {decision.thought}\")]\n        if isinstance(decision.action, Replan):\n            return response + [\n                SystemMessage(\n                    content=f\"Context from last attempt: {decision.action.feedback}\"\n                )\n            ]\n        else:\n            return {\"messages\": response + [AIMessage(content=decision.action.response)]}\n\n\n    def select_recent_messages(state) -> dict:\n        messages = state[\"messages\"]\n        selected = []\n        for msg in messages[::-1]:\n            selected.append(msg)\n            if isinstance(msg, HumanMessage):\n                break\n        return {\"messages\": selected[::-1]}\n\n\n    joiner = select_recent_messages | runnable | _parse_joiner_output\n\n    # endregion\n\n    # 5. Compose using LangGraph\n    # region LangGraph\n    from langgraph.graph import END, StateGraph, START\n    from langgraph.graph.message import add_messages\n    from typing import Annotated\n\n\n    class State(TypedDict):\n        messages: Annotated[list, add_messages]\n\n    from langgraph.graph import MessageGraph, END\n    graph_builder = StateGraph(State)\n    # graph_builder = MessageGraph()\n\n    # 1.  Define vertices\n    # We defined plan_and_schedule above already\n    # Assign each node to a state variable to update\n    graph_builder.add_node(\"plan_and_schedule\", plan_and_schedule)\n    graph_builder.add_node(\"join\", joiner)\n\n\n    ## Define edges\n    graph_builder.add_edge(\"plan_and_schedule\", \"join\")\n\n    ### This condition determines looping logic\n\n\n    # def should_continue(state):\n    #     messages = state[\"messages\"]\n    #     if isinstance(messages[-1], AIMessage):\n    #         return END\n    #     return \"plan_and_schedule\"\n\n    from langgraph.graph import END, StateGraph, MessagesState\n    from typing import Annotated, Literal, TypedDict\n    \n        # Define the function that determines whether to continue or not\n    # def should_continue(state: MessagesState) -> Literal[\"plan_and_schedule\", END]:\n    #     messages = state['messages']\n    #     last_message = messages[-1]\n    #     if isinstance(messages[-1], AIMessage):\n    #         return END\n    #     return \"plan_and_schedule\"\n\n    def should_continue(state: List[BaseMessage]):\n        if isinstance(state[-1], AIMessage):\n            return END\n        return \"plan_and_schedule\"\n\n\n\n    graph_builder.add_conditional_edges(\n        \"join\",\n        # Next, we pass in the function that will determine which node is called next.\n        should_continue)\n    graph_builder.add_edge(START, \"plan_and_schedule\")\n    \n    chain = graph_builder.compile()\n    #endregion\n\n\n\n    steps = chain.stream(\n        {\"messages\": HumanMessage(content=\"What's the oldest parrot alive, and how much longer is that than the average?\")},\n        {\n            \"recursion_limit\": 100,\n        }\n    )\n    for step in steps:\n        print(step)\n        print(\"---\")\n    # final_state = chain.invoke(\n    # {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]},\n    # config={\"configurable\": {\"thread_id\": 42}}\n    # )\n    # print(final_state[\"messages\"][-1].content)\n    # print(steps[-1])\n\n    response = dict()\n    response['answer'] = ''\n    response['context'] = ''\n\n    return(response)\n    \n    # response['answer'] = final_state[\"messages\"][-1].content\n    # response['context'] = [x.content for x in final_state[\"messages\"]\n\n    #     Conclusion\n    # The planner output parsing format is fragile if your function requires more than 1 or 2 arguments. We could make it more robust by using streaming tool calling.\n    # Variable substitution is fragile in the example above. It could be made more robust by using a fine-tuned model and a more robust syntax (using e.g., Lark or a tool calling schema)\n    # The state can grow quite long if you require multiple re-planning runs. To handle, you could add a message compressor once you go above a certain token limit.\nError Message and Stack Trace (if applicable)\n\"content\": \"1. tavily_search_results_json(query=\\\"oldest parrot alive\\\")  \\n2. tavily_search_results_json(query=\\\"average lifespan of parrots\\\")  \\n3. join()  \\n<END_OF_PLAN>\",\n            \"response_metadata\": {\n              \"finish_reason\": \"stop\",\n              \"model_name\": \"gpt-4o-mini-2024-07-18\",\n              \"system_fingerprint\": \"fp_48196bc67a\"\n            },\n            \"type\": \"AIMessageChunk\",\n            \"id\": \"run-438d4b1f-9c34-40ee-adcc-bd84f9d7224e\",\n            \"tool_calls\": [],\n            \"invalid_tool_calls\": []\n          }\n        }\n      }\n    ]\n  ],\n  \"llm_output\": null,\n  \"run\": null\n}\n[chain/end] [chain:LangGraph > chain:plan_and_schedule > chain:plan_and_schedule > chain:RunnableSequence > parser:LLMCompilerPlanParser] [5.75s] Exiting Parser run with output:\n{\n  \"idx\": 3,\n  \"tool\": \"join\",\n  \"args\": [],\n  \"dependencies\": [\n    1,\n    2\n  ],\n  \"thought\": null\n}\n[chain/end] [chain:LangGraph > chain:plan_and_schedule > chain:plan_and_schedule > chain:RunnableSequence] [6.31s] Exiting \nChain run with output:\n{\n  \"idx\": 3,\n  \"tool\": \"join\",\n  \"args\": [],\n  \"dependencies\": [\n    1,\n    2\n  ],\n  \"thought\": null\n}\n[chain/end] [chain:LangGraph > chain:plan_and_schedule > chain:plan_and_schedule > chain:schedule_tasks] [5.63s] Exiting Chain run with output:\n[outputs]\n[chain/end] [chain:LangGraph > chain:plan_and_schedule > chain:plan_and_schedule] [6.33s] Exiting Chain run with output:   \n[outputs]\n[chain/start] [chain:LangGraph > chain:plan_and_schedule > chain:ChannelWrite<plan_and_schedule,messages>] Entering Chain run with input:\n[inputs]\n[chain/end] [chain:LangGraph > chain:plan_and_schedule > chain:ChannelWrite<plan_and_schedule,messages>] [3ms] Exiting Chain run with output:\n[outputs]\n[chain/end] [chain:LangGraph > chain:plan_and_schedule] [6.34s] Exiting Chain run with output:\n[outputs]\n{'plan_and_schedule': {'messages': [[FunctionMessage(content='[{\\'url\\': \\'https://en.wikipedia.org/wiki/Cookie_(cockatoo)\\', \\'content\\': \\'He was one of the longest-lived birds on record[4] and was recognised by the Guinness World Records as the oldest living parrot in the world.[5]\\\\nThe next-oldest pink cockatoo to be found in a zoological setting was a 31-year-old female bird located at Paradise Wildlife Sanctuary, England.[3] Information published by the World Parrot Trust states longevity for Cookie\\\\\\'s species in captivity is on average 40\u201360 years.[6]\\\\nLife[edit]\\\\nCookie was Brookfield Zoo\\\\\\'s oldest resident and the last surviving member of the animal collection from the time of the zoo\\\\\\'s opening in 1934, having arrived from Taronga Zoo of Sydney, New South Wales, Australia, in the same year and judged to be one year old at the time.[7]\\\\nIn the 1950s an attempt was made to introduce Cookie to a female pink cockatoo, but Cookie rejected her as \"she was \nnot nice to him\".[8]\\\\n In 2007, Cookie was diagnosed with, and placed on medication and nutritional supplements for, osteoarthritis and osteoporosis\\\\xa0\u2013 medical conditions which occur commonly in aging animals and humans alike,[7] although it \nis believed that the latter may also have been brought on as a result of being fed a seed-only diet for the first 40 years \nof his life, in the years before the dietary requirements of his species were fully understood.[9]\\\\nCookie was \"retired\" from exhibition at the zoo in 2009 (following a few months of weekend-only appearances) in order to preserve his health, after it was noticed by staff that his appetite, demeanor and stress levels improved markedly when not on public display. age.[11] A memorial at the zoo was unveiled in September 2017.[12]\\\\nIn 2020, Cookie became the subject of a poetry collection \nby Barbara Gregorich entitled Cookie the Cockatoo: Everything Changes.[13]\\\\nSee also[edit]\\\\nReferences[edit]\\\\nExternal links[edit] He was believed to be the oldest member of his species alive in captivity, at the age of 82 in June 2015,[1][2] \nhaving significantly exceeded the average lifespan for his kind.[3] He was moved to a permanent residence in the keepers\\\\\\' office of the zoo\\\\\\'s Perching Bird House, although he made occasional appearances for special events, such as his birthday celebration, which was held each June.[3]\\'}]', additional_kwargs={'idx': 1, 'args': {'query': 'oldest parrot alive'}}, name='tavily_search_results_json'), FunctionMessage(content='[{\\'url\\': \\'https://www.thesprucepets.com/how-long-do-parrots-and-other-pet-birds-live-1238433\\', \\'content\\': \"It\\'s possible that a pet bird can outlive its owners\\\\nThe Spruce / Adrienne Legault\\\\nParrots and other birds can live up to 10 to 50 years or more depending on the type and the conditions they live in. They vary in size from small birds that can fit in the palm of your hand to large birds the size of a cat and their lifespans are just as variable.\\\\n Also, for birds who live longer some owners have to make a plan of where the bird is going in the circumstance the bird outlives the owner.\\\\n In reality, there is a wide range in the age that pet birds might reach and certainly, some will live longer (or shorter amounts of time) than the ages listed.\\\\n Potential owners need to be aware of the longevity of their bird so they can be prepared to provide proper care for them for as long as they live.\\\\n\"}]', additional_kwargs={'idx': 2, 'args': {'query': 'average lifespan of parrots'}}, name='tavily_search_results_json'), FunctionMessage(content='join', additional_kwargs={'idx': 3, 'args': ()}, name='join')]]}}\n---\n[chain/error] [chain:LangGraph] [6.39s] Chain run errored with error:\n\"NotImplementedError(\\\"Unsupported message type: <class 'list'>\\\")Traceback (most recent call last):\\n\\n\\n  File \\\"C:\\\\Users\\\\erdem\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\langgraph\\\\pregel\\\\__init__.py\\\", line 910, in stream\\n    while loop.tick(\\n          ^^^^^^^^^^\\n\\n\\n  File \\\"C:\\\\Users\\\\erdem\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\langgraph\\\\pregel\\\\loop.py\\\", line 178, in tick\\n    apply_writes(\\n\\n\\n  File \\\"C:\\\\Users\\\\erdem\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\langgraph\\\\pregel\\\\algo.py\\\", line 195, in apply_writes\\n    updated = channels[chan].update(vals)\\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"C:\\\\Users\\\\erdem\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\langgraph\\\\channels\\\\binop.py\\\", line 104, in update\\n    self.value = self.operator(self.value, value)\\n\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"C:\\\\Users\\\\erdem\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\langgraph\\\\graph\\\\message.py\\\", line 70, in add_messages\\n    right = [message_chunk_to_message(m) for m in convert_to_messages(right)]\\n                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"C:\\\\Users\\\\erdem\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\langchain_core\\\\messages\\\\utils.py\\\", line 304, in convert_to_messages\\n    return [_convert_to_message(m) for m in messages]\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"C:\\\\Users\\\\erdem\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\langchain_core\\\\messages\\\\utils.py\\\", line 304, in <listcomp>\\n    return [_convert_to_message(m) for m in messages]\\n            ^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"C:\\\\Users\\\\erdem\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\langchain_core\\\\messages\\\\utils.py\\\", line 283, in _convert_to_message\\n    raise NotImplementedError(f\\\"Unsupported message type: {type(message)}\\\")\\n\\n\\nNotImplementedError: Unsupported message type: <class 'list'>\"\nE\n======================================================================\nERROR: test_query1 (__main__.TestLangchainAgent.test_query1)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"C:\\Users\\erdem\\Documents\\code\\LLMFinance\\tests\\test1.py\", line 134, in test_query1\n    response = get_llm_response(\n               ^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erdem\\Documents\\code\\LLMFinance\\utils\\entry.py\", line 49, in get_llm_response\n    return get_llm_response_langchain_llm_compiler(query, top_k, token_chunk_size, custom_prompt_template)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erdem\\Documents\\code\\LLMFinance\\utils\\langchain_agent_setups.py\", line 638, in get_llm_response_langchain_llm_compiler\n    for step in steps:\n  File \"C:\\Users\\erdem\\AppData\\Roaming\\Python\\Python311\\site-packages\\langgraph\\pregel\\__init__.py\", line 910, in stream   \n    while loop.tick(\n          ^^^^^^^^^^\n  File \"C:\\Users\\erdem\\AppData\\Roaming\\Python\\Python311\\site-packages\\langgraph\\pregel\\loop.py\", line 178, in tick\n    apply_writes(\n  File \"C:\\Users\\erdem\\AppData\\Roaming\\Python\\Python311\\site-packages\\langgraph\\pregel\\algo.py\", line 195, in apply_writes \n    updated = channels[chan].update(vals)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erdem\\AppData\\Roaming\\Python\\Python311\\site-packages\\langgraph\\channels\\binop.py\", line 104, in update    \n    self.value = self.operator(self.value, value)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erdem\\AppData\\Roaming\\Python\\Python311\\site-packages\\langgraph\\graph\\message.py\", line 70, in add_messages    right = [message_chunk_to_message(m) for m in convert_to_messages(right)]\n                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erdem\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\messages\\utils.py\", line 304, in convert_to_messages\n    return [_convert_to_message(m) for m in messages]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erdem\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\messages\\utils.py\", line 304, in <listcomp>\n    return [_convert_to_message(m) for m in messages]\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erdem\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\messages\\utils.py\", line 283, in _convert_to_message\n    raise NotImplementedError(f\"Unsupported message type: {type(message)}\")\nNotImplementedError: Unsupported message type: <class 'list'>\nDescription\nI am trying to get LLM compiler work\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.19045\nPython Version:  3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.2.32\nlangchain: 0.2.14\nlangchain_community: 0.0.38\nlangsmith: 0.1.93\nlangchain_chroma: 0.1.3\nlangchain_cli: 0.0.30\nlangchain_experimental: 0.0.64\nlangchain_openai: 0.1.20\nlangchain_text_splitters: 0.2.2\nlangchainhub: 0.1.21\nlanggraph: 0.2.3\nlangserve: 0.2.2\n\nOther Dependencies\n\naiohttp: 3.9.3\naiosqlite: Installed. No version info available.\naleph-alpha-client: Installed. No version info available.\nanthropic: Installed. No version info available.\narxiv: Installed. No version info available.\nassemblyai: Installed. No version info available.\nasync-timeout: 4.0.3\natlassian-python-api: Installed. No version info available.\nazure-ai-documentintelligence: Installed. No version info available.\nazure-identity: Installed. No version info available.\nazure-search-documents: Installed. No version info available.\nbeautifulsoup4: 4.12.3\nbibtexparser: Installed. No version info available.\ncassio: Installed. No version info available.\nchardet: 4.0.0\nchromadb: 0.5.3\ncloudpickle: 2.2.1\ncohere: Installed. No version info available.\ndatabricks-vectorsearch: Installed. No version info available.\ndataclasses-json: 0.6.7\ndatasets: 2.21.0\ndgml-utils: Installed. No version info available.\nelasticsearch: Installed. No version info available.\nesprima: Installed. No version info available.\nfaiss-cpu: Installed. No version info available.\nfastapi: 0.112.1\nfeedparser: Installed. No version info available.\nfireworks-ai: Installed. No version info available.\nfriendli-client: Installed. No version info available.\ngeopandas: Installed. No version info available.\ngitpython: 3.1.43\ngoogle-cloud-documentai: Installed. No version info available.\ngql: Installed. No version info available.\ngradientai: Installed. No version info available.\nhdbcli: Installed. No version info available.\nhologres-vector: Installed. No version info available.\nhtml2text: Installed. No version info available.\nhttpx: 0.27.0\nhttpx-sse: Installed. No version info available.\njavelin-sdk: Installed. No version info available.\njinja2: 3.1.3\njq: Installed. No version info available.\njsonpatch: 1.33\njsonschema: 4.19.2\nlanggraph-checkpoint: 1.0.3\nlangserve[all]: Installed. No version info available.\nlibcst: 1.4.0\nlxml: 4.9.3\nmarkdownify: Installed. No version info available.\nmotor: Installed. No version info available.\nmsal: Installed. No version info available.\nmwparserfromhell: Installed. No version info available.\nmwxml: Installed. No version info available.\nnewspaper3k: Installed. No version info available.\nnumexpr: 2.8.7\nnumpy: 1.26.4\nnvidia-riva-client: Installed. No version info available.\noci: Installed. No version info available.\nopenai: 1.36.1\nopenapi-pydantic: Installed. No version info available.\noracle-ads: Installed. No version info available.\noracledb: Installed. No version info available.\norjson: 3.10.6\npackaging: 24.1\npandas: 2.1.4\npdfminer-six: Installed. No version info available.\npgvector: 0.2.5\npraw: Installed. No version info available.\npremai: Installed. No version info available.\npsychicapi: Installed. No version info available.\npy-trello: Installed. No version info available.\npydantic: 1.10.12\npyjwt: 2.4.0\npymupdf: Installed. No version info available.\npypdf: 4.3.0\npypdfium2: Installed. No version info available.\npyproject-toml: 0.0.10\npyspark: Installed. No version info available.\nPyYAML: 6.0.1\nrank-bm25: Installed. No version info available.\nrapidfuzz: Installed. No version info available.\nrapidocr-onnxruntime: Installed. No version info available.\nrdflib: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrspace_client: Installed. No version info available.\nscikit-learn: 1.2.2\nSQLAlchemy: 2.0.25\nsqlite-vss: Installed. No version info available.\nsse-starlette: 1.8.2\nstreamlit: 1.30.0\nsympy: 1.12\ntelethon: Installed. No version info available.\ntenacity: 8.4.2\ntidb-vector: Installed. No version info available.\ntiktoken: 0.7.0\ntimescale-vector: Installed. No version info available.\ntomlkit: 0.12.5\ntqdm: 4.66.4\ntree-sitter: Installed. No version info available.\ntree-sitter-languages: Installed. No version info available.\ntyper: 0.9.4\ntyper[all]: Installed. No version info available.\ntypes-requests: Installed. No version info available.\ntyping-extensions: 4.9.0\nupstash-redis: Installed. No version info available.\nuvicorn: 0.23.2\nvdms: Installed. No version info available.\nxata: Installed. No version info available.\nxmltodict: Installed. No version info available.\n", "created_at": "2024-08-24", "closed_at": "2024-09-04", "labels": [], "State": "closed", "Author": "erdult"}
{"issue_number": 1456, "issue_title": "langgraph up broken again.. ", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nlanggraph up\nError Message and Stack Trace (if applicable)\n\u279c  langgraph-pyproject git:(master) \u2717 langgraph up       \nStarting LangGraph API server...\nFor local dev, requires env var LANGSMITH_API_KEY with access to LangGraph Cloud closed beta.\nFor production use, requires a license key in env var LANGGRAPH_CLOUD_LICENSE_KEY.\n- Building...#0 building with \"base\" instance using docker-container driver\n\n#1 [langgraph-api internal] load build definition from Dockerfile\n#1 transferring dockerfile: 390B done\n#1 DONE 0.0s\n\n#2 [langgraph-api internal] load metadata for docker.io/langchain/langgraph-api:3.11\n#2 DONE 1.0s\n\n#3 [langgraph-api internal] load .dockerignore\n#3 transferring context: 2B done\n#3 DONE 0.0s\n\n#4 [langgraph-api 1/4] FROM docker.io/langchain/langgraph-api:3.11@sha256:b1b6f261959f2dce331ff39a94682ff0ebf241efd748a32701814bbca9686f14\n#4 resolve docker.io/langchain/langgraph-api:3.11@sha256:b1b6f261959f2dce331ff39a94682ff0ebf241efd748a32701814bbca9686f14 done\n#4 DONE 0.0s\n\n#5 [langgraph-api internal] load build context\n#5 transferring context: 13.16kB done\n#5 DONE 0.0s\n\n#6 [langgraph-api 3/4] RUN PYTHONDONTWRITEBYTECODE=1 pip install --no-cache-dir -c /api/constraints.txt -e /deps/*\n#6 CACHED\n\n#7 [langgraph-api 2/4] ADD . /deps/langgraph-pyproject\n#7 CACHED\n\n#8 [langgraph-api 4/4] WORKDIR /deps/langgraph-pyproject\n#8 CACHED\n\n#9 [langgraph-api] exporting to docker image format\n#9 exporting layers done\n#9 exporting manifest sha256:cb37ddeb855bb8bc557b687829160840dd889bbd03955af72ea9096924b8073f 0.0s done\n#9 exporting config sha256:f448a5cbf11c3bb2aab986be8af4ddc7ef90780cb371bc8b50bdc3f3c117563a done\n#9 sending tarball\n#9 sending tarball 2.1s done\n#9 ERROR: rpc error: code = Unknown desc = invalid diffID for layer 13: expected \"sha256:f36a6bedaaa9c1b8aefacc1bf0f2338b52531ad4734368f5951d33f1f38e0268\", got \"sha256:0616d1187d8b092a7495f6e70312b77cb153f861008477b8ead31cd95c254f4e\"\n\n#10 [langgraph-api] importing to docker\n#10 loading layer 2102b636cbbc 32.77kB / 241.94kB 1.8s done\n#10 loading layer f36a6bedaaa9 557.06kB / 83.64MB 1.8s done\n#10 ERROR: invalid diffID for layer 13: expected \"sha256:f36a6bedaaa9c1b8aefacc1bf0f2338b52531ad4734368f5951d33f1f38e0268\", got \"sha256:0616d1187d8b092a7495f6e70312b77cb153f861008477b8ead31cd95c254f4e\"\n------\n > [langgraph-api] exporting to docker image format:\n------\n------\n > [langgraph-api] importing to docker:\n------\nfailed to solve: rpc error: code = Unknown desc = invalid diffID for layer 13: expected \"sha256:f36a6bedaaa9c1b8aefacc1bf0f2338b52531ad4734368f5951d33f1f38e0268\", got \"sha256:0616d1187d8b092a7495f6e70312b77cb153f861008477b8ead31cd95c254f4e\"\nDescription\nHi Team\nThe command\n langgraph up\nstopped working, it was working a few hours ago, I dont think its due to any change i have made\nA similar thing happened a few day ago, They suggested I do\ndocker system prune\nHowever this time it did NOT resolve the issue. Also tried deleting all the docker images, but no luck\nAny help would be  great;y appreciated\nthanks\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP PREEMPT_DYNAMIC Sun Aug 11 15:32:50 UTC 2024\nPython Version:  3.12.4 (main, Jun  7 2024, 00:00:00) [GCC 14.1.1 20240607 (Red Hat 14.1.1-5)]\n\nPackage Information\n\nlangchain_core: 0.1.52\nlangchain_community: 0.0.38\nlangsmith: 0.1.77\nlangchain_cli: 0.0.25\nlangserve: 0.2.2\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlanggraph\n", "created_at": "2024-08-23", "closed_at": "2024-08-28", "labels": [], "State": "closed", "Author": "darthShana"}
{"issue_number": 1450, "issue_title": "In Customer Support Bot tutorial, the backup db file is created too early so dates aren't current.", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\n### Database file initialization\n\nimport os\nimport shutil\nimport sqlite3\n\nimport pandas as pd\nimport requests\n\ndb_url = \"https://storage.googleapis.com/benchmarks-artifacts/travel-db/travel2.sqlite\"\nlocal_file = \"travel2.sqlite\"\n# The backup lets us restart for each tutorial section\nbackup_file = \"travel2.backup.sqlite\"\noverwrite = False\nif overwrite or not os.path.exists(local_file):\n    response = requests.get(db_url)\n    response.raise_for_status()  # Ensure the request was successful\n    with open(local_file, \"wb\") as f:\n        f.write(response.content)\n    # Backup - we will use this to \"reset\" our DB in each section\n    shutil.copy(local_file, backup_file)\n# Convert the flights to present time for our tutorial\nconn = sqlite3.connect(local_file)\ncursor = conn.cursor()\n\ntables = pd.read_sql(\n    \"SELECT name FROM sqlite_master WHERE type='table';\", conn\n).name.tolist()\ntdf = {}\nfor t in tables:\n    tdf[t] = pd.read_sql(f\"SELECT * from {t}\", conn)\n\nexample_time = pd.to_datetime(\n    tdf[\"flights\"][\"actual_departure\"].replace(\"\\\\N\", pd.NaT)\n).max()\ncurrent_time = pd.to_datetime(\"now\").tz_localize(example_time.tz)\ntime_diff = current_time - example_time\n\ntdf[\"bookings\"][\"book_date\"] = (\n    pd.to_datetime(tdf[\"bookings\"][\"book_date\"].replace(\"\\\\N\", pd.NaT), utc=True)\n    + time_diff\n)\n\ndatetime_columns = [\n    \"scheduled_departure\",\n    \"scheduled_arrival\",\n    \"actual_departure\",\n    \"actual_arrival\",\n]\nfor column in datetime_columns:\n    tdf[\"flights\"][column] = (\n        pd.to_datetime(tdf[\"flights\"][column].replace(\"\\\\N\", pd.NaT)) + time_diff\n    )\n\nfor table_name, df in tdf.items():\n    df.to_sql(table_name, conn, if_exists=\"replace\", index=False)\ndel df\ndel tdf\nconn.commit()\nconn.close()\n\ndb = local_file  # We'll be using this local file as our DB in this tutorial\n\n### End of initialization\n\n### And here's what's before every test of the bot ###\n# Update with the backup file so we can restart from the original place in each section\nshutil.copy(backup_file, db)\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nIn the Customer Support Bot tutorial, when initializing the DB, the backup file is created before the dates in the DB are updated to be current.\nUnfortunately before every execution of the graph, the backup file is reloaded and the dates aren't re-updated, so the dates are back to being old and since the bot uses the current date as a reference to book a new flight, it can't find relevant entries in the DB.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.6.0: Mon Jul 29 21:13:00 PDT 2024; root:xnu-10063.141.2~1/RELEASE_X86_64\nPython Version:  3.12.3 (v3.12.3:f6650f9ad7, Apr  9 2024, 08:18:48) [Clang 13.0.0 (clang-1300.0.29.30)]\n\nPackage Information\n\nlangchain_core: 0.2.28\nlangchain: 0.2.12\nlangchain_community: 0.2.11\nlangsmith: 0.1.96\nlangchain_chroma: 0.1.2\nlangchain_google_vertexai: 1.0.8\nlangchain_text_splitters: 0.2.2\nlangchainhub: 0.1.21\nlanggraph: 0.2.0\nlangserve: 0.2.2\n", "created_at": "2024-08-23", "closed_at": "2024-08-27", "labels": [], "State": "closed", "Author": "florentremis"}
{"issue_number": 1449, "issue_title": "Passing Graph State to Tool No longer works", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\n@tool\ndef state_tool(x: int, state: Annotated[dict, InjectedState]) -> str:\n    '''Do something with state.'''\n    if len(state[\"messages\"]) > 2:\n        return state[\"foo\"] + str(x)\n    else:\n        return \"not enough messages\"\n\n@tool\ndef foo_tool(x: int, foo: Annotated[str, InjectedState(\"foo\")]) -> str:\n    '''Do something else with state.'''\n    return foo + str(x + 1)\n\ntools = [state_tool,foo_tool]\nError Message and Stack Trace (if applicable)\nInvalid argument provided to Gemini: 400 * GenerateContentRequest.tools[0].function_declarations[0].parameters.properties[state].properties: should be non-empty for OBJECT type\nDescription\nI am trying to use langchain-google-genai = \"^1.0.10\", langgraph = \"^0.2.12\", receiving error\ni am using from langchain_google_genai import ChatGoogleGenerativeAI and model gemini-1.5-flash\nSystem Info\npython = \">=3.9,<3.13\"\nuvicorn = \"^0.30.6\"\nlangchain-community = \"^0.2.12\"\nlangchain-text-splitters = \"^0.2.2\"\nlangchain-chroma = \"^0.1.3\"\ndistro = \"^1.9.0\"\nlanggraph = \"^0.2.12\"\naiosqlite = \"^0.20.0\"\nwatchfiles = \"^0.22.0\"\nfilelock = \"^3.15.4\"\npython-multipart = \"*\"\nlangchain-core = \"^0.2.34\"\nsentence-transformers = \"^3.0.1\"\npypdf = \"^4.3.0\"\ngunicorn = \"^23.0.0\"\npytz = \"^2024.1\"\ntzlocal = \"^5.2\"\ncolorama = \"^0.4.6\"\nlanggraph-checkpoint-sqlite = \"^1.0.0\"\ngrpcio = \"1.64.1\"\ntorch = \"2.2.2\"\ntiktoken = \"^0.7.0\"\nlangchain-google-genai = \"^1.0.10\"\npydantic-settings = \"^2.4.0\"\npandas = \"^2.2.2\"\nopenpyxl = \"^3.1.5\"\npydantic = \"^2.8.2\"", "created_at": "2024-08-23", "closed_at": "2024-08-30", "labels": [], "State": "closed", "Author": "HiraveBapu"}
{"issue_number": 1430, "issue_title": "Compiled graph aget_state() throws exception: 'NoneType' object has no attribute 'pending_writes'", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\ncompiled_graph = stategraph.compile(checkpointer=checkpointer, interrupt_before=[\"human_review_node\"])\n\nsnapshot = await compiled_graph.aget_state(config)\nError Message and Stack Trace (if applicable)\nexception: 'NoneType' object has no attribute 'pending_writes'\nDescription\naget_state throws exception\nasync def aget_state(self, config: RunnableConfig) -> StateSnapshot:\n    \"\"\"Get the current state of the graph.\"\"\"\n    if not self.checkpointer:\n        raise ValueError(\"No checkpointer set\")\n\n    saved = await self.checkpointer.aget_tuple(config)\n    checkpoint = saved.checkpoint if saved else empty_checkpoint()\n\n    config = saved.config if saved else config\n    async with AsyncChannelsManager(\n        self.channels, checkpoint, config, skip_context=True\n    ) as (channels, managed):\n        next_tasks = prepare_next_tasks(\n            checkpoint,\n            self.nodes,\n            channels,\n            managed,\n            config,\n            saved.metadata.get(\"step\", -1) + 1 if saved else -1,\n            for_execution=False,\n        )\n        return StateSnapshot(\n            read_channels(channels, self.stream_channels_asis),\n            tuple(t.name for t in next_tasks),\n            saved.config if saved else config,\n            saved.metadata if saved else None,\n            saved.checkpoint[\"ts\"] if saved else None,\n            saved.parent_config if saved else None,\n            tasks_w_writes(next_tasks, saved.pending_writes), # this line causing issue ?\n        )\n\nSystem Info\npython = \">=3.9,<3.13\"\nuvicorn = \"^0.30.3\"\npydantic = {extras = [\"email\"], version = \"^2.7.4\"}\nlangchain-community = \"^0.2.12\"\nlangchain-text-splitters = \"^0.2.1\"\nlangchain-chroma = \"^0.1.3\"\nlangchain-google-vertexai = \"^1.0.7\"\ndistro = \"^1.9.0\"\nlangchain-openai = \"^0.1.22\"\nlangchain-aws = \"^0.1.17\"\naiosqlite = \"^0.20.0\"\nwatchfiles = \"^0.22.0\"\nfilelock = \"^3.15.4\"\npython-multipart = \"*\"\nlangchain-core = \"^0.2.34\"\nsentence-transformers = \"^3.0.1\"\npypdf = \"^4.3.1\"\ngunicorn = \"^22.0.0\"\nlangchain-groq = \"^0.1.6\"\npydantic-settings = \"^2.3.4\"\nlangchain-mistralai = \"^0.1.12\"\nlanggraph-checkpoint-sqlite = \"^1.0.0\"\ngrpcio = \"^1.65.4\"\nlanggraph = \"^0.2.10\"", "created_at": "2024-08-22", "closed_at": "2024-08-22", "labels": [], "State": "closed", "Author": "HiraveBapu"}
{"issue_number": 1425, "issue_title": "How to add memory using langgrapg.checkpoint in langgraph_agentic_rag.ipynb file", "issue_body": "Issue with current documentation:\nI want to enhance the current flow by adding memory so that when a user asks about previous information, the system can respond accordingly.\nBelow is the agentic langgraph flow\n\nAny guidance on this would be greatly appreciated.\nIdea or request for content:\nNo response", "created_at": "2024-08-22", "closed_at": "2024-08-22", "labels": [], "State": "closed", "Author": "SujayC66"}
{"issue_number": 1406, "issue_title": "map-reduce InvalideUpdateError", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nasync def test1():\n    from langgraph.graph import StateGraph, START, END\n    from typing import TypedDict\n    from langgraph.checkpoint.memory import MemorySaver\n    from langgraph.constants import Send\n    from typing import Annotated\n    import operator\n\n    # The overall state of the graph\n    class OverallState(TypedDict):\n        question: str\n        answer: str\n        user_datas: Annotated[list[str], operator.add]\n\n    # This is what the node that generates the query will return\n    class QueryOutputState(TypedDict):\n        query: str\n        state: str\n        datas: list[str]\n\n    # This is what the node that retrieves the documents will return\n\n    class DataOutputState(TypedDict):\n        num: str\n        num_p: str\n        nums: Annotated[list[str], operator.add]\n\n    class DocumentOutputState(TypedDict):\n        docs: list[str]\n\n    # This is what the node that generates the final answer will take in\n    class GenerateInputState(OverallState, DocumentOutputState):\n        pass\n\n    # Node to generate query\n    def generate_query(state: OverallState) -> QueryOutputState:\n        # Replace this with real logic\n        return {\"query\": state[\"question\"][:2], \"datas\": [\"3\", \"4\"]}\n\n    def generate_query1(state: OverallState) -> QueryOutputState:\n        # Replace this with real logic\n        return {\"query\": state[\"question\"][:1], \"datas\": [\"1\", \"2\", \"3\"]}\n\n    def process_query(state: OverallState):\n        # Replace this with real logic\n        return {\"question\": state[\"question\"] + \"1\"}\n\n    def should_con(state: OverallState):\n        if len(state.get(\"query\", \"\")) > 4:\n            return \"generate_query\"\n        else:\n            return \"process_query\"\n\n    # Node to retrieve documents\n    def retrieve_documents(state: QueryOutputState) -> DocumentOutputState:\n        # Replace this with real logic\n        return {\"docs\": [state[\"query\"]] * 2}\n\n    def continue_to_jokes(state: QueryOutputState):\n        return [Send(\"num_pros\", {\"num\": s}) for s in state[\"datas\"]]\n\n    def num_pros(state: DataOutputState):\n        return {\"num_p\": state[\"num\"] + \"123\"}\n\n    def num_pros1(state: DataOutputState):\n        return {\"query\": state[\"num_p\"] + \"345\"}\n\n    # Node to generate answer\n    def generate(state: GenerateInputState) -> OverallState:\n        return {\"answer\": \"\\n\\n\".join(state[\"docs\"] + [state[\"question\"]])}\n\n    graph = StateGraph(OverallState)\n    graph.add_node(\"generate_query\", generate_query)\n    graph.add_node(\"generate_query1\", generate_query1)\n    graph.add_node(\"process_query\", process_query)\n    graph.add_node(\"retrieve_documents\", retrieve_documents)\n    graph.add_node(\"num_pros\", num_pros)\n    graph.add_node(\"num_pros1\", num_pros1)\n    graph.add_node(\"generate\", generate)\n\n    graph.add_conditional_edges(\n        START,\n        should_con,\n        {\n            \"generate_query\": \"generate_query\",\n            \"process_query\": \"process_query\",\n        },\n    )\n    graph.add_edge(\"process_query\", \"generate_query1\")\n\n    graph.add_conditional_edges(\"generate_query\", continue_to_jokes, [\"num_pros\"])\n    graph.add_conditional_edges(\"generate_query1\", continue_to_jokes, [\"num_pros\"])\n    graph.add_edge(\"num_pros\", \"num_pros1\")\n    graph.add_edge(\"num_pros1\", \"retrieve_documents\")\n    graph.add_edge(\"retrieve_documents\", \"generate\")\n    graph.add_edge(\"generate\", END)\n    memory = MemorySaver()\n    graph = graph.compile(checkpointer=memory)\n\n    thread_config = {\"configurable\": {\"thread_id\": \"1\"}}\n\n    # res = graph.invoke({\"question\": \"foo\"}, config=thread_config)\n\n    async for event in graph.astream_events(\n        {\"question\": \"foo\"}, config=thread_config, version=\"v2\"\n    ):\n        print(graph.get_state(config=thread_config))\n\nasyncio.run(test1())\nError Message and Stack Trace (if applicable)\nraise InvalidUpdateError(\nlanggraph.errors.InvalidUpdateError: Invalid update for channel num_p with values ['1123', '2123', '3123']\nDescription\nI want to simulate the operation of multiple nodes in map reduce, and then aggregate them. However, when updating the status of intermediate nodes, there is an issue where the list value is updated to str\nSystem Info\nlanggraph version is 0.2.3", "created_at": "2024-08-21", "closed_at": "2024-08-30", "labels": [], "State": "closed", "Author": "tianshangwuyun"}
{"issue_number": 1394, "issue_title": "Visualized graph has wrong conditional edges", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langgraph.graph import END, StateGraph\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langchain_core.runnables import RunnablePassthrough\nfrom typing import TypedDict\n\nclass State(TypedDict):\n    current_step: int\n    total_number_of_steps: int\n\ng = StateGraph(State)\ng.set_entry_point(\"entry\")\n\ng.add_node(\"entry\", RunnablePassthrough())\ng.add_edge(\"entry\", \"A\")\n\ng.add_node(\"A\", RunnablePassthrough())\ng.add_edge(\"A\", \"B\")\n\ng.add_node(\"B\", RunnablePassthrough())\ng.add_edge(\"B\", \"reduce_middle_step\")\n\ng.add_node(\"reduce_middle_step\", RunnablePassthrough())\ng.add_conditional_edges(\n    \"reduce_middle_step\",\n    lambda state: (\n        END if state[\"current_step\"] >= state[\"total_number_of_steps\"] else \"A\"\n    ),\n)\n\nmain_graph = g.compile(checkpointer=MemorySaver(), interrupt_after=[\"B\"])\n\n\nwith open(\"./main_graph.png\", \"wb\") as f:\n    f.write(main_graph.get_graph().draw_mermaid_png())\nError Message and Stack Trace (if applicable)\nThe graph diagram has unspecified connections\nDescription\nRun the code and check the image generated.\nIt has conditional edges from \"recude_middle_step\" to \"entry\", \"A\", and \"B\" where I only connected to \"A\".\n\nSystem Info\nnot required.", "created_at": "2024-08-19", "closed_at": "2024-08-20", "labels": [], "State": "closed", "Author": "minki-j"}
{"issue_number": 1386, "issue_title": "Get error returning list object as state update from nodes", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\n{'messages': [{'content': 'hi', 'role': 'user'}]}\n[{'step': -2}:checkpoint] State at the end of step {'step': -2}:\n{'messages': []}\n[0:tasks] Starting step 0 with 1 task:\n- __start__ -> {'messages': [{'content': 'hi', 'role': 'user'}]}\n[0:writes] Finished step 0 with writes to 1 channel:\n- messages -> [{'content': 'hi', 'role': 'user'}]\n[{'step': -2}:checkpoint] State at the end of step {'step': -2}:\n{'messages': [HumanMessage(content='hi', id='23d2de95-4eb3-4766-8cd3-2caad80dccbf')]}\n[1:tasks] Starting step 1 with 1 task:\n- agent -> {'is_last_step': False,\n 'messages': [HumanMessage(content='hi', id='23d2de95-4eb3-4766-8cd3-2caad80dccbf')]}\n[1:writes] Finished step 1 with writes to 1 channel:\n- messages -> [AIMessage(content=' Hello! How can I assist you today?', response_metadata={'finish_reason': 'stop', 'model_name': 'Qwen2-72B-Instruct'}, id='run-0ddfab40-31e5-4fab-b8b4-b26f1659d854')]\nError Message and Stack Trace (if applicable)\n[{'content': 'hi', 'role': 'user'}]\n[{'step': -2}:checkpoint] State at the end of step {'step': -2}:\n{'messages': []}\n[0:tasks] Starting step 0 with 1 task:\n- __start__ -> [{'content': 'hi', 'role': 'user'}]\nERROR:    Exception in ASGI application\nTraceback (most recent call last):\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 426, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\n    return await self.app(scope, receive, send)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/starlette/applications.py\", line 123, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\n    raise exc\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\n    await self.app(scope, receive, _send)\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n    await self.app(scope, receive, send)\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 65, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n    raise exc\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/starlette/routing.py\", line 756, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/starlette/routing.py\", line 776, in app\n    await route.handle(scope, receive, send)\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/starlette/routing.py\", line 297, in handle\n    await self.app(scope, receive, send)\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/starlette/routing.py\", line 77, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\n    raise exc\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/starlette/routing.py\", line 75, in app\n    await response(scope, receive, send)\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/starlette/responses.py\", line 258, in __call__\n    async with anyio.create_task_group() as task_group:\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 597, in __aexit__\n    raise exceptions[0]\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/starlette/responses.py\", line 261, in wrap\n    await func()\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/starlette/responses.py\", line 250, in stream_response\n    async for chunk in self.body_iterator:\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/langchain_openai_api_bridge/chat_completion/http_stream_response_adapter.py\", line 10, in to_str_stream\n    async for chunk in chunk_stream:\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/langchain_openai_api_bridge/core/utils/pydantic_async_iterator.py\", line 7, in ato_dict\n    async for obj in async_iter:\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/langchain_openai_api_bridge/chat_completion/langchain_stream_adapter.py\", line 26, in ato_chat_completion_chunk_stream\n    async for event in astream_event:\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 5290, in astream_events\n    async for item in self.bound.astream_events(\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1247, in astream_events\n    async for event in event_stream:\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/langchain_core/tracers/event_stream.py\", line 1005, in _astream_events_implementation_v2\n    await task\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/langchain_core/tracers/event_stream.py\", line 965, in consume_astream\n    async for _ in event_streamer.tap_output_aiter(run_id, stream):\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/langchain_core/tracers/event_stream.py\", line 181, in tap_output_aiter\n    first = await py_anext(output, default=sentinel)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/langchain_core/utils/aiter.py\", line 78, in anext_impl\n    return await __anext__(iterator)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/valdanito/workspace/original/langgraph/libs/langgraph/langgraph/pregel/__init__.py\", line 1277, in astream\n    _panic_or_proceed(done, inflight, loop.step, asyncio.TimeoutError)\n  File \"/Users/valdanito/workspace/original/langgraph/libs/langgraph/langgraph/pregel/__init__.py\", line 1456, in _panic_or_proceed\n    raise exc\n  File \"/Users/valdanito/workspace/original/langgraph/libs/langgraph/langgraph/pregel/executor.py\", line 123, in done\n    task.result()\n  File \"/Users/valdanito/workspace/original/langgraph/libs/langgraph/langgraph/pregel/retry.py\", line 72, in arun_with_retry\n    async for _ in task.proc.astream(task.input, task.config):\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 875, in astream\n    yield await self.ainvoke(input, config, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/valdanito/workspace/original/langgraph/libs/langgraph/langgraph/utils.py\", line 114, in ainvoke\n    ret = await self._acall_with_config(\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/valdanito/install/conda/miniconda3/envs/gradio/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1835, in _acall_with_config\n    output: Output = await asyncio.create_task(coro, context=context)  # type: ignore\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/valdanito/workspace/original/langgraph/libs/langgraph/langgraph/pregel/write.py\", line 127, in _awrite\n    values = await asyncio.gather(\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/valdanito/workspace/original/langgraph/libs/langgraph/langgraph/utils.py\", line 111, in ainvoke\n    return self.invoke(input, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/valdanito/workspace/original/langgraph/libs/langgraph/langgraph/utils.py\", line 102, in invoke\n    ret = context.run(self.func, input, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/valdanito/workspace/original/langgraph/libs/langgraph/langgraph/graph/state.py\", line 529, in _get_state_key\n    raise InvalidUpdateError(f\"Expected dict, got {input}\")\nlanggraph.errors.InvalidUpdateError: Expected dict, got [{'role': 'user', 'content': 'hi'}]\nDescription\nWhen I got the error langgraph.errors.InvalidUpdateError: Expected dict, got [{'role': 'user', 'content': 'hi'}], I checked the source code of langgraph.\nI made simple modifications to the code -> Valdanitooooo@c2d05ed, and my application runs well. I am not sure if my modifications will damage other features or if there are other better ways to fix this error. If my modifications are safe and constructive, please let me know and I will initiate a pull request.\nSystem Info\nSystem Information\n------------------\n> OS:  Darwin\n> OS Version:  Darwin Kernel Version 22.6.0: Sun Dec 17 22:18:09 PST 2023; root:xnu-8796.141.3.703.2~2/RELEASE_X86_64\n> Python Version:  3.11.9 (main, Apr 19 2024, 11:44:45) [Clang 14.0.6 ]\n\nPackage Information\n-------------------\n> langchain_core: 0.2.30\n> langchain: 0.2.9\n> langchain_community: 0.2.7\n> langsmith: 0.1.85\n> langchain_experimental: 0.0.62\n> langchain_openai: 0.1.17\n> langchain_openai_api_bridge: 0.11.1\n> langchain_text_splitters: 0.2.1\n> langchainhub: 0.1.20\n> langgraph: 0.2.4\n", "created_at": "2024-08-19", "closed_at": "2024-08-27", "labels": [], "State": "closed", "Author": "Valdanitooooo"}
{"issue_number": 1384, "issue_title": "checkpointer.setup() from AsyncPostgresSaver throws TypeError: `tuple indices must be integers or slices, not str`", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\n\nasync with AsyncConnectionPool(\n    conninfo=settings.DB_URI,\n    max_size=20,\n) as pool, pool.connection() as conn:\n    checkpointer = AsyncPostgresSaver(conn)\n    await checkpointer.setup() # fails here\nError Message and Stack Trace (if applicable)\n...langgraph/checkpoint/postgres/aio.py:72, in AsyncPostgresSaver.setup(self)\n     68 try:\n     69     results = await cur.execute(\n     70         \"SELECT v FROM checkpoint_migrations ORDER BY v DESC LIMIT 1\"\n     71     )\n---> 72     version = (await results.fetchone())[\"v\"] # await results.fetchone() returns (4,)\n     73 except UndefinedTable:\n     74     version = -1\n\nTypeError: tuple indices must be integers or slices, not str\nDescription\nname = \"langgraph\"\nversion = \"0.2.4\"\nname = \"langgraph-checkpoint-postgres\"\nversion = \"1.0.3\"\nname = \"langgraph-checkpoint\"\nversion = \"1.0.3\"\nname = \"langchain\"\nversion = \"0.2.14\"\nname = \"langchain-core\"\nversion = \"0.2.32\"\nSystem Info\nSee description for deps.\nMac, Python 3.11.6", "created_at": "2024-08-19", "closed_at": "2024-08-19", "labels": [], "State": "closed", "Author": "sataycat"}
{"issue_number": 1381, "issue_title": "langgraph up stopped working", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\n\u279c  langgraph-pyproject git:(master) \u2717 langgraph up\nStarting LangGraph API server...\nFor local dev, requires env var LANGSMITH_API_KEY with access to LangGraph Cloud closed beta.\nFor production use, requires a license key in env var LANGGRAPH_CLOUD_LICENSE_KEY.\n- Building\nError Message and Stack Trace (if applicable)\nno error produced\nDescription\nHi Team\nI was making pretty good progress on my app with langgraph \"backend\" and angular frontend\nHowever just now Im unable to bring up langgraph with\n\u279c  langgraph-pyproject git:(master) \u2717 langgraph up\nI get the following output\nStarting LangGraph API server...\nFor local dev, requires env var LANGSMITH_API_KEY with access to LangGraph Cloud closed beta.\nFor production use, requires a license key in env var LANGGRAPH_CLOUD_LICENSE_KEY.\n- building\nwith the spinner on building..\nIt has always given me the message about the license key, but it continued to work\nhave checked the usual suspects like internet connectivity and anthropic credit\nbut its not giving me any more to go on\nalso worth noting i do have a\nLANGSMITH_API_KEY\nwhich has worked\nI don't think i made any changes from the time it was working to now\ndocker is runnig\ndocker info\nClient: Docker Engine - Community\n Version:    27.1.2\n Context:    default\n Debug Mode: false\n Plugins:\n  buildx: Docker Buildx (Docker Inc.)\n    Version:  v0.15.1-desktop.1\n    Path:     /usr/lib/docker/cli-plugins/docker-buildx\n  compose: Docker Compose (Docker Inc.)\n    Version:  v2.28.1-desktop.1\n    Path:     /usr/lib/docker/cli-plugins/docker-compose\n  debug: Get a shell into any image or container (Docker Inc.)\n    Version:  0.0.32\n    Path:     /usr/lib/docker/cli-plugins/docker-debug\n  desktop: Docker Desktop commands (Alpha) (Docker Inc.)\n    Version:  v0.0.14\n    Path:     /usr/lib/docker/cli-plugins/docker-desktop\n  dev: Docker Dev Environments (Docker Inc.)\n    Version:  v0.1.2\n    Path:     /usr/lib/docker/cli-plugins/docker-dev\n  extension: Manages Docker extensions (Docker Inc.)\n    Version:  v0.2.25\n    Path:     /usr/lib/docker/cli-plugins/docker-extension\n  feedback: Provide feedback, right in your terminal! (Docker Inc.)\n    Version:  v1.0.5\n    Path:     /usr/lib/docker/cli-plugins/docker-feedback\n  init: Creates Docker-related starter files for your project (Docker Inc.)\n    Version:  v1.3.0\n    Path:     /usr/lib/docker/cli-plugins/docker-init\n  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)\n    Version:  0.6.0\n    Path:     /usr/lib/docker/cli-plugins/docker-sbom\n  scout: Docker Scout (Docker Inc.)\n    Version:  v1.10.0\n    Path:     /usr/lib/docker/cli-plugins/docker-scout\n\nServer:\n Containers: 73\n  Running: 2\n  Paused: 0\n  Stopped: 71\n Images: 7\n Server Version: 27.1.2\n Storage Driver: overlay2\n  Backing Filesystem: btrfs\n  Supports d_type: true\n  Using metacopy: false\n  Native Overlay Diff: true\n  userxattr: false\n Logging Driver: json-file\n Cgroup Driver: systemd\n Cgroup Version: 2\n Plugins:\n  Volume: local\n  Network: bridge host ipvlan macvlan null overlay\n  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog\n Swarm: inactive\n Runtimes: io.containerd.runc.v2 runc\n Default Runtime: runc\n Init Binary: docker-init\n containerd version: 8fc6bcff51318944179630522a095cc9dbf9f353\n runc version: v1.1.13-0-g58aa920\n init version: de40ad0\n Security Options:\n  seccomp\n   Profile: builtin\n  cgroupns\n Kernel Version: 6.10.4-200.fc40.x86_64\n Operating System: Fedora Linux 40 (Workstation Edition)\n OSType: linux\n Architecture: x86_64\n CPUs: 20\n Total Memory: 31.03GiB\n Name: fedora\n ID: 79a54786-4ba0-4893-a37b-04eb1a9dd4a3\n Docker Root Dir: /var/lib/docker\n Debug Mode: false\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n Live Restore Enabled: false\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP PREEMPT_DYNAMIC Sun Aug 11 15:32:50 UTC 2024\nPython Version:  3.12.4 (main, Jun  7 2024, 00:00:00) [GCC 14.1.1 20240607 (Red Hat 14.1.1-5)]\n\nPackage Information\n\nlangchain_core: 0.1.52\nlangchain_community: 0.0.38\nlangsmith: 0.1.77\nlangchain_cli: 0.0.25\nlangserve: 0.2.2\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlanggraph\n", "created_at": "2024-08-19", "closed_at": "2024-08-19", "labels": [], "State": "closed", "Author": "darthShana"}
{"issue_number": 1377, "issue_title": "KeyError encountered: '__start__", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nimport os\nimport streamlit as st\nfrom langchain.agents import Tool\nimport openai\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.tools import tool\n\n# Set environment variables\nos.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\"\nos.environ['LANGCHAIN_TRACING_V2'] = 'true'\nos.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\"\nos.environ['LANGCHAIN_PROJECT'] = \"agent scraper\"\nos.environ['LANGCHAIN_API_KEY'] = \"your_langchain_api_key\"\n\nfrom typing import TypedDict, Annotated\nfrom langchain_core.agents import AgentAction\nfrom langchain_core.messages import BaseMessage\nimport operator\n\nclass AgentState(TypedDict):\n    input: str\n    chat_history: list[BaseMessage]\n    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]\n\nfrom serpapi import GoogleSearch\n\n@tool(\"web_search\")\ndef web_search(name: str):\n    \"\"\"Finds general knowledge information using Google search.\"\"\"\n    serpapi_params = {\n        \"engine\": \"google\",\n        \"api_key\": \"your_serpapi_api_key\"\n    }\n    search = GoogleSearch({\n        **serpapi_params,\n        \"q\": name\n    })\n    results = search.get_dict()\n    organic_results = results.get(\"organic_results\", [])\n\n    contexts = []\n    for result in organic_results:\n        title = result.get(\"title\", \"No Title\")\n        snippet = result.get(\"snippet\", \"No Description\")\n        link = result.get(\"link\", \"No Link\")\n\n        context = f\"{title}\\n{snippet}\\n{link}\\n---\"\n        contexts.append(context)\n\n    final_context = \"\\n\".join(contexts)\n    return final_context\n\n@tool(\"final_answer\")\ndef final_answer(introduction: str, research_steps: str, main_body: str, conclusion: str, sources: str):\n    if type(research_steps) is list:\n        research_steps = \"\\n\".join([f\"- {r}\" for r in research_steps])\n    if type(sources) is list:\n        sources = \"\\n\".join([f\"- {s}\" for s in sources])\n    return \"\"\n\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.messages import ToolCall, ToolMessage\nfrom langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"gpt-4o\", temperature=0.2)\ntools = [web_search]\n\nfrom langgraph.graph import StateGraph, END\n\ngraph = StateGraph(AgentState)\ngraph.add_node(\"oracle\", lambda state: {\"next\": \"next_function_name\"})\n# Define other nodes and edges as per your graph logic\n\nprint(\"Initial state:\", {})\nError Message and Stack Trace (if applicable)\nInitial state: {'__start__': 'oracle', 'input': 'https://www.linkedin.com/in/anshul-bhide-26a96026/', 'chat_history': [], 'intermediate_steps': []}\nKeyError encountered: '__start__'\nTraceback (most recent call last):\n  File \"/home/runner/langchain-agent-profile-viewer/main.py\", line 340, in <module>\n    output=out[\"intermediate_steps\"][-1].tool_input\nNameError: name 'out' is not defined. Did you mean: 'oct'?\nDescription\nI am using Langchain to build an agent with several tools (linkedin_search, web_search, and final_answer).\nThe error occurs when invoking the compiled runnable graph using the invoke method.\nThe graph uses an initial state that includes the input, chat_history, and intermediate_steps keys. The start key is intended to trigger the start of the graph, but it seems the graph is not handling this key properly.\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #27~22.04.1-Ubuntu SMP Tue Jul 16 23:03:39 UTC 2024\nPython Version:  3.10.14 (main, Mar 19 2024, 21:46:16) [GCC 13.2.0]\n\nPackage Information\n\nlangchain_core: 0.2.9\nlangchain: 0.2.5\nlangchain_community: 0.2.12\nlangsmith: 0.1.81\nlangchain_cohere: 0.1.5\nlangchain_openai: 0.1.9\nlangchain_pinecone: 0.1.1\nlangchain_text_splitters: 0.2.1\nlangchainhub: 0.1.21\nlanggraph: 0.1.1\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlangserve\n", "created_at": "2024-08-18", "closed_at": "2024-08-24", "labels": [], "State": "closed", "Author": "anshulbhide"}
{"issue_number": 1372, "issue_title": "DOC: Example of parsing a Pydantic object str returned by ToolNode", "issue_body": "Issue with current documentation:\nI saw in the examples that ToolNode is meant to be used to handle errors and retrying for function calling. However I don't want to actually call a tool, I just want to return the pydantic object after successful validation. It seems that the object is sent back as a message string, and it's unclear to me how to parse it back into a Pydantic object before returning it.\nHere's an example below. The parse_obj function is the one that's unclear to me:\n\nfrom typing import Literal\n\nfrom IPython.display import Image, display\nfrom langchain.pydantic_v1 import BaseModel, conlist, Field\nfrom langchain_aws import ChatBedrockConverse\nfrom langgraph.graph import StateGraph, MessagesState\nfrom langgraph.prebuilt import ToolNode\n\nnum_weeks = 4\n\nclass Day(BaseModel):\n    day_of_week: str = Field(..., description=\"Day of the week\")\n    activities: list[str] = Field(..., description=\"List of 1-3 high-level workout descriptions or rest day activities\")\n\nclass Week(BaseModel):\n    monday: Day\n    tuesday: Day\n    wednesday: Day\n    thursday: Day\n    friday: Day\n    saturday: Day\n    sunday: Day\n\nclass WorkoutProgram(BaseModel):\n    \"\"\"Generate a workout program for an individual\"\"\"\n    weeks: conlist(Week, min_items=num_weeks, max_items=num_weeks)\n\n\ntool_node = ToolNode([WorkoutProgram], handle_tool_errors=True)\n\nmodel = ChatBedrockConverse(\n    model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n    temperature=0,\n    max_tokens=4096,\n)\nmodel_with_tools = model.bind_tools([WorkoutProgram], tool_choice=\"any\")\n\nclass ProgramState(MessagesState):\n    workout_program: WorkoutProgram | None = None\n\ndef should_retry(state: ProgramState) -> Literal[\"model\", \"parse_obj\"]:\n    \"\"\"Decide whether to send error to agent or tool response to parser\"\"\"\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    # Side question: Any better way to check for error?\n    if \"Error: ValidationError\" in last_message.content:\n        return \"model\"\n    return \"parse_obj\"\n\ndef parse_obj(state: ProgramState) -> Literal['__end__']:\n    \"\"\"Parse the object from the tool response\"\"\"\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    # ISSUE: This doesn't work \u2014 how to parse the string back to a BaseModel?\n    workout_program = WorkoutProgram.parse_obj(dict(eval(last_message.content)))\n    state[\"workout_program\"] = workout_program\n    return state\n\ndef call_model(state: ProgramState) -> ProgramState:\n    \"\"\"Call the model with the messages\"\"\"\n    messages = state[\"messages\"]\n    response = model_with_tools.invoke(messages)\n    state[\"messages\"] = [response]\n    return state\n\nworkflow = StateGraph(ProgramState)\n\n# Define the two nodes\nworkflow.add_node(\"model\", call_model)\nworkflow.add_node(\"tools\", tool_node)\nworkflow.add_node(\"parse_obj\", parse_obj)\n\nworkflow.add_edge(\"__start__\", \"model\")\nworkflow.add_edge(\"model\", \"tools\")\nworkflow.add_conditional_edges(\n    \"tools\",\n    should_retry # back to agent or to parse_obj\n)\nworkflow.add_edge(\"parse_obj\", \"__end__\")\n\napp = workflow.compile()\n\ndisplay(Image(app.get_graph().draw_mermaid_png()))\n\nresponse = app.invoke(\n    {\"messages\": [(\"human\", \"Create a workout program for a beginner with the following characteristics: height: 5 feet 10 inches, weight: 180 lbs, age: 30, goals: Lose weight and improve overall fitness.\")],\n    },\n    {\"recursion_limit\": 10},\n)\n\nworkout_program = response[\"workout_program\"]\nlast_message.content in parse_obj is a str that may look like, for example:\n\"weeks=[Week(monday=Day(day_of_week='Monday', activities=['30 mins cardio (e.g. brisk walking, jogging)', '20 mins strength training (e.g. bodyweight exercises)']), tuesday=Day(day_of_week='Tuesday', activities=['Rest day']), wednesday=Day(day_of_week='Wednesday', activities=['30 mins cardio (e.g. cycling, swimming)', '20 mins strength training (e.g. resistance bands)']), thursday=Day(day_of_week='Thursday', activities=['Rest day']), friday=Day(day_of_week='Friday', activities=['45 mins cardio (e.g. elliptical, rowing)', '15 mins core exercises']), saturday=Day(day_of_week='Saturday', activities=['Rest day']), sunday=Day(day_of_week='Sunday', activities=['60 mins outdoor activity (e.g. hiking, bike ride)'])), Week(monday=Day(day_of_week='Monday', activities=['40 mins cardio (e.g. jogging, swimming)', '25 mins strength training (e.g. bodyweight exercises, light weights)']), tuesday=Day(day_of_week='Tuesday', activities=['Rest day']), wednesday=Day(day_of_week='Wednesday', activities=['40 mins cardio (e.g. cycling, rowing)', '25 mins strength training (e.g. resistance bands, dumbbells)']), thursday=Day(day_of_week='Thursday', activities=['Rest day']), friday=Day(day_of_week='Friday', activities=['50 mins cardio (e.g. elliptical, stair climber)', '20 mins core exercises']), saturday=Day(day_of_week='Saturday', activities=['Rest day']), sunday=Day(day_of_week='Sunday', activities=['75 mins outdoor activity (e.g. hiking, bike ride)']))]\"\nIdea or request for content:\nCan a simple example be added to the examples section for combining retry logic (including error messages) with Pydantic object outputs?", "created_at": "2024-08-16", "closed_at": "2024-08-18", "labels": [], "State": "closed", "Author": "austinmw"}
{"issue_number": 1366, "issue_title": "The minimum supported Python version for this project", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nDescription\nThe pyproject.toml in the root directory specifies that this project requires a Python version greater than 3.10.\nBut the pyproject.toml files in the libs directory indicate that the project only requires a Python version greater than 3.9.\nWhich one is correct?\nI see Python 3.9 testing in the CI, so is it the root directory that's incorrect?", "created_at": "2024-08-16", "closed_at": "2024-08-16", "labels": [], "State": "closed", "Author": "gbaian10"}
{"issue_number": 1358, "issue_title": "PostgresSaver throw \"PostgresSaver.put() missing 1 required positional argument: 'new_versions'\"", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\ncheckpointer = PostgresSaver(conn)\n# checkpointer.setup()\n\n\nsearch = TavilySearchResults(max_results=2)\ntools = [search,]\nagent_executor = create_react_agent(\n    llm, tools, checkpointer=checkpointer, debug=False)\n\ncheckpointer = PostgresSaver(conn)\n\n# NOTE: you need to call .setup() the first time you're using your checkpointer\ncheckpointer.setup()\n\ngraph = create_react_agent(model, tools=tools, checkpointer=checkpointer)\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\nres = graph.invoke({\"messages\": [(\"human\", \"what's the weather in sf\")]}, config)\ncheckpoint = checkpointer.get(config)\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile /Users/echo/miniconda3/envs/text-genie-api/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1002, in Pregel.stream(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\n   1001 for step in range(start, stop):\n-> 1002     next_checkpoint, next_tasks = _prepare_next_tasks(\n   1003         checkpoint,\n   1004         processes,\n   1005         channels,\n   1006         managed,\n   1007         config,\n   1008         step,\n   1009         for_execution=True,\n   1010         manager=run_manager,\n   1011         get_next_version=(\n   1012             self.checkpointer.get_next_version\n   1013             if self.checkpointer\n   1014             else _increment\n   1015         ),\n   1016     )\n   1018     # assign pending writes to tasks\n\nFile /Users/echo/miniconda3/envs/text-genie-api/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2019, in _prepare_next_tasks(checkpoint, processes, channels, managed, config, step, for_execution, get_next_version, manager)\n   2018 for name, proc in processes.items():\n-> 2019     seen = checkpoint[\"versions_seen\"][name]\n   2020     # If any of the channels read by this process were updated\n...\n---> 58     result = self.fn(*self.args, **self.kwargs)\n     59 except BaseException as exc:\n     60     self.future.set_exception(exc)\n\nTypeError: PostgresSaver.put() missing 1 required positional argument: 'new_versions'\nOutput is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\nDescription\n\nAs above code, just test with the ipynb document.\nSystem Info\n\u2570$ python -m langchain_core.sys_info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.4.0: Fri Mar 15 00:12:41 PDT 2024; root:xnu-10063.101.17~1/RELEASE_ARM64_T8103\nPython Version:  3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 10:07:17) [Clang 14.0.6 ]\n\nPackage Information\n\nlangchain_core: 0.2.28\nlangchain: 0.2.12\nlangchain_community: 0.2.11\nlangsmith: 0.1.88\nlangchain_anthropic: 0.1.20\nlangchain_aws: 0.1.11\nlangchain_chroma: 0.1.2\nlangchain_cohere: 0.1.9\nlangchain_experimental: 0.0.62\nlangchain_fireworks: 0.1.5\nlangchain_google_genai: 1.0.7\nlangchain_google_vertexai: 1.0.6\nlangchain_groq: 0.1.6\nlangchain_huggingface: 0.0.3\nlangchain_mistralai: 0.1.10\nlangchain_openai: 0.1.20\nlangchain_text_splitters: 0.2.2\nlangchain_together: 0.1.4\nlangchainhub: 0.1.20\nlanggraph: 0.1.8\n\nPackages not installed (Not Necessarily a Problem)", "created_at": "2024-08-15", "closed_at": "2024-08-15", "labels": [], "State": "closed", "Author": "monster-echo"}
{"issue_number": 1348, "issue_title": "`langgraph up` with syntax errors creates dangling Docker images (and eats up available disk)", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom typing import Any, Dict, List, Optional\nfrom pydantic import BaseModel\nfrom langgraph.graph import StateGraph, START, END\n\n\nclass State(BaseModel):\n    string_default: Optional[str] = None\n    int_default: Optional[int] = None\n    float_default: Optional[float] = None\n    dict_default: Optional[Dict[str, Any]] = None\n    list_default: Optional[List[str]] = None\n\n\ndef node(state: State):\n    return {\n        'string_default': state.string_default,\n        'int_default': state.int_default,\n        'float_default': state.float_default,\n        'dict_default': state.dict_default,\n        'list_default': state.list_default,\n    }\n\n\ngraph_builder = StateGraph(State)\ngraph_builder.add_node(\"default\", node)\ngraph_builder.add_edge(START, \"syntax_error\")\ngraph_builder.add_edge(\"default\", END)\n\ngraph = graph_builder.compile()\nError Message and Stack Trace (if applicable)\nDoing this enough times will result in Docker errors like:\n\nfailed to solve: failed to copy files: userspace copy failed: write /var/lib/docker/overlay2/f89z6c11ya4bjzmal3lonj3tk/merged/deps/[repo]/.env.dev: no space left on device\nDescription\nInitial state\n> docker system df           \nTYPE            TOTAL     ACTIVE    SIZE      RECLAIMABLE\nImages          0         0         0B        0B\nContainers      0         0         0B        0B\nLocal Volumes   0         0         0B        0B\nBuild Cache     0         0         0B        0B\nAmend graph code to have one syntax error and langgraph up\n> docker system df\nTYPE            TOTAL     ACTIVE    SIZE      RECLAIMABLE\nImages          3         2         1.042GB   257.3MB (24%)\nContainers      2         0         5.002MB   5.002MB (100%)\nLocal Volumes   1         1         40.85MB   0B (0%)\nBuild Cache     21        0         98.84MB   98.84MB\nAmend graph code to have a different syntax error and langgraph\n> docker system df\ndocker system df          \nTYPE            TOTAL     ACTIVE    SIZE      RECLAIMABLE\nImages          4         2         1.143GB   685.5MB (59%)\nContainers      2         0         5.002MB   5.002MB (100%)\nLocal Volumes   1         1         40.85MB   0B (0%)\nBuild Cache     25        0         98.84MB   98.84MB\n\nDoing a verbose df reveals that dangling images are not being cleaned up:\ndocker system df --verbose\n Images space usage:\n\nREPOSITORY                        TAG       IMAGE ID       CREATED         SIZE      SHARED SIZE   UNIQUE SIZE   CONTAINERS\newalker-langgraph-langgraph-api   latest    ad138f07385a   4 minutes ago   686MB     584.1MB       101.4MB       1\n<none>                            <none>    bfe00bd0a94c   4 minutes ago   686MB     584.1MB       101.4MB       0\n<none>                            <none>    46d4c46c4981   6 minutes ago   686MB     584.1MB       101.4MB       0\nlangchain/langgraph-api           3.11      799a89355392   17 hours ago    257MB     257.3MB       0B            0\npostgres                          16        35042a754d27   6 days ago      453MB     97.11MB       356.3MB       1\n...\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.5.0: Wed May  1 20:17:33 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T6031\nPython Version:  3.12.3 (main, May 28 2024, 11:22:04) [Clang 15.0.0 (clang-1500.3.9.4)]\n\nPackage Information\n\nlangchain_core: 0.2.28\nlangchain: 0.2.12\nlangchain_community: 0.2.11\nlangsmith: 0.1.96\nlangchain_anthropic: 0.1.22\nlangchain_openai: 0.1.20\nlangchain_text_splitters: 0.2.2\nlanggraph: 0.2.2\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlangserve\n", "created_at": "2024-08-14", "closed_at": "2024-12-20", "labels": [], "State": "closed", "Author": "ewalker11"}
{"issue_number": 1341, "issue_title": "interrupt_before not work", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom typing import TypedDict\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph, START, END\n\n\nclass State(TypedDict):\n    input: str\n    user_feedback: str\n\n\ndef step_1(state):\n    print(\"---Step 1---\")\n    pass\n\n\ndef human_feedback(state):\n    print(\"---human_feedback---\")\n    pass\n\n\ndef step_3(state):\n    print(\"---Step 3---\")\n    pass\n\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"step_1\", step_1)\nbuilder.add_node(\"human_feedback\", human_feedback)\nbuilder.add_node(\"step_3\", step_3)\nbuilder.add_edge(START, \"step_1\")\nbuilder.add_edge(\"step_1\", \"human_feedback\")\nbuilder.add_edge(\"human_feedback\", \"step_3\")\nbuilder.add_edge(\"step_3\", END)\n\n# Set up memory\nmemory = MemorySaver()\n\n# Add\ngraph = builder.compile(checkpointer=memory, interrupt_before=[\"human_feedback\"])\n# Input\ninitial_input = {\"input\": \"hello world\"}\n\n# Thread\nthread = {\"configurable\": {\"thread_id\": \"1\"}}\n\n# Run the graph until the first interruption\n\nfor event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n    print(\"---111---\")\n    print(event)\n\nfor event in graph.stream(None, thread, stream_mode=\"values\"):\n    print(\"---222---\")\n    print(event)\nError Message and Stack Trace (if applicable)\nNo response\nDescription\noutput should be\n---111---\n{'input': 'hello world'}\n---Step 1---\n---222---\n---human_feedback---\n---Step 3---\n\nreal output\n---111---\n{'input': 'hello world'}\n---Step 1---\n---human_feedback---\n---Step 3---\n\nif we delete\nfor event in graph.stream(None, thread, stream_mode=\"values\"):\n    print(\"---222---\")\n    print(event)\n\noutput is\n---111---\n{'input': 'hello world'}\n---Step 1---\n\nwhich is work as except\nSystem Info\nlanggraph==0.2.3\nlanggraph-checkpoint==1.0.2\nlanggraph-checkpoint-sqlite==1.0.0", "created_at": "2024-08-14", "closed_at": "2024-08-14", "labels": [], "State": "closed", "Author": "FunnyWolf"}
{"issue_number": 1333, "issue_title": "State getting overwritten by Nones no matter what", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nState(TypedDict):\n    messages: Annotated[list, add_messages]\n    path: str\n    typ: str = 'defaulttype'\n    legalargs: set = {'arg1','name'}\n\ndef someNode(state: State):\n    ...\n    state['path'] = updateOperation\n    return State(**state)\n\nflow = StateGraph(State)\nflow.add_node(\n...\n\ngraph = flow.compile()\n\nstate = await graph.ainvoke({\"path\": 'mypath'})\n\nDefault state values are always overwritten with None {'legalargs':None,'typ':None}, I've tried using pydantic BaseModel, custom __init__ in State, nothing can stop this bizarre behavior!\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nDefault state values are always overwritten with None {'legalargs':None,'typ':None}, I've tried using pydantic BaseModel, custom init in State, nothing can stop this bizarre behavior!\nSystem Info\nAttributeError: module 'langgraph' has no attribute 'version'", "created_at": "2024-08-13", "closed_at": "2024-08-15", "labels": [], "State": "closed", "Author": "openSourcerer9000"}
{"issue_number": 1331, "issue_title": "The graph incorrectly obtained the input schema", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import MessagesState, StateGraph\n\n\ndef foo(state: MessagesState) -> MessagesState:\n    print(f\"foo: {state}\")\n    return {\"messages\": \"foo\"}\n\n\ndef error1(state, config: RunnableConfig) -> MessagesState:\n    # It read the first type_hint,\n    # thus incorrectly obtaining the type_hint of the `config` variable.\n    print(f\"error1: {state}\")\n    return {\"messages\": \"this is config\"}\n\n\ndef error2(state, config) -> MessagesState:\n    # It actually obtained the type_hint of the `return`,\n    # which just happened to be the same as the `state`.\n    print(f\"error2: {state}\")\n    return {\"messages\": \"this is return\"}\n\n\ngraph = StateGraph(MessagesState)\ngraph.add_node(foo)\ngraph.add_node(error1)\ngraph.add_node(error2)\n\ngraph.set_entry_point(\"foo\")\ngraph.add_edge(\"foo\", \"error1\")\ngraph.add_edge(\"error1\", \"error2\")\ngraph.set_finish_point(\"error2\")\n\ngraph = graph.compile()\ngraph.invoke({\"messages\": \"Hello\"})\nError Message and Stack Trace (if applicable)\nfoo: {'messages': [HumanMessage(content='Hello', id='d00c79c6-3c77-4a1b-8e68-806334f1dd36')]}\n\nerror1: {'tags': None, 'metadata': None, 'callbacks': None, 'run_name': None, 'max_concurrency': None, 'recursion_limit': None, 'configurable': None, 'run_id': None}\n\nerror2: {'messages': [HumanMessage(content='Hello', id='d00c79c6-3c77-4a1b-8e68-806334f1dd36'), HumanMessage(content='foo', id='6ac49f5c-e8be-4d93-a5e0-bcce19cf9ca3'), HumanMessage(content='this is config', id='91bc0d28-3095-4d27-a667-0bb0aab09123')]}\nDescription\nWhen using add_node, if the func of the action has an incomplete type_hint (e.g., only config type_hint or return type_hint), unexpected results may occur.\n\nThe reason is that when there is at least one type_hint, it always reads the first one type_hint, but this type_hint may not be the type_hint of the state variable.\nSystem Info\nlanggraph==0.2.3\nlangchain_core==0.2.30\nplatform==windows\npython_version==3.12.5", "created_at": "2024-08-13", "closed_at": "2024-08-22", "labels": [], "State": "closed", "Author": "gbaian10"}
{"issue_number": 1320, "issue_title": "LangGraph Checkpointer fails to track changes made to pydantic model object in the state when calling get_state", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom loguru import logger\nimport operator\nimport asyncio\nfrom langgraph.graph import END, StateGraph, START\nfrom langgraph.checkpoint.memory import MemorySaver\n\n# from pydantic import BaseModel, Field\nfrom langchain_core.pydantic_v1 import BaseModel, Field, Extra\nfrom typing import Annotated, Literal, List, Sequence, TypedDict, Tuple, Optional, Dict, Any\nfrom enum import Enum\n\nclass Pipeline(str, Enum):\n    P1 = \"p1\"\n    P2 = \"p2\"\n    \nclass StepStatus(str, Enum):\n    PENDING = \"pending\"\n    COMPLETED = \"completed\"\n    \nclass Step(str, Enum):\n    S1 = \"s1\"\n    S2 = \"s2\"\n    \nclass StepTracker(BaseModel):\n    name: str\n    status: StepStatus = Field(StepStatus.PENDING)\n    outputs: Optional[Dict[str, Any]] = Field(default_factory=dict)\n    step_desc: Optional[str] = Field(None)\n    \n\nclass PipelineTracker(BaseModel):\n    name: Pipeline\n    steps: Dict[Step, StepTracker] = Field(default_factory=dict, description=\"The steps in the pipeline.\")\n\n    def mark_step_completed(self, step: Step):\n        if step in self.steps:\n            self.steps[step].status = StepStatus.COMPLETED\n        else:\n            raise ValueError(f\"Step {step} not found in pipeline {self.name}\")\n    \n    def fill_desc_with_params(self, fill_params: Dict):        \n        for step in self.steps:\n            self.steps[step].step_desc = self.steps[step].step_desc.format(**fill_params)\n            \nclass Task(BaseModel):\n    task_type: Pipeline = Field(None, description=\"The type of pipeline task to be performed.\")\n    pipeline: Optional[PipelineTracker] = Field(None, description=\"The pipeline tracker for the task.\")\n    \n    class Config:\n        # extra = Extra.allow\n        # allow_mutation = True\n        extra = \"allow\"\n        frozen = False\n        \n    def __init__(self, **data):\n        super().__init__(**data)\n        if self.task_type == Pipeline.P1:\n            step_descs = {\n                Step.S1: \"Perform step 1: {param1}\",\n                Step.S2: \"Perform step 2: {param2}\"\n            }\n            self.pipeline = PipelineTracker(\n                name=Pipeline.P1,\n                steps={step: StepTracker(name=step, step_desc=desc) for step, desc in step_descs.items()}\n            )\n        else:\n            logger.warning(\"Task type not recognized or not set. No pipeline tracker created.\")\n\nasync def run_app(app, inputs, config):\n    async for output in app.astream(inputs, config):\n        # stream_mode=\"updates\" yields dictionaries with output keyed by node name\n        for key, value in output.items():\n            print(f\"Output from node '{key}':\")\n            for k, v in value.items():\n                print(f\"  {k}: {v}\")\n        print(\"\\n---\\n\")\n        \n        \n# The agent state is the input to each node in the graph\nclass AgentState(TypedDict):\n    task_list: Annotated[Sequence[Task], operator.add]\n    active_task: Task\n    \ndef node1_update(state):\n    # We create a new task and fill the description with parameters\n    active_task = Task(task_type=Pipeline.P1)\n    active_task.pipeline.fill_desc_with_params({\"param1\": \"Hello\", \"param2\": \"World\"})\n    return {\"task_list\": [active_task], \"active_task\": active_task}\n\ndef node2_update(state):\n    # we mark the first step as completed\n    active_task = state[\"active_task\"]\n    active_task.pipeline.mark_step_completed(Step.S1)\n    return {\"active_task\": active_task}\n\nif __name__ == \"__main__\":\n    # Create the graph\n    graph = StateGraph(AgentState)\n    graph.add_node(\"node1\", node1_update)\n    graph.add_node(\"node2\", node2_update)\n\n    graph.add_edge(START, \"node1\")\n    graph.add_edge(\"node1\", \"node2\")\n    graph.add_edge(\"node2\", END)\n\n    app = graph.compile(checkpointer=MemorySaver())\n\n    asyncio.run(run_app(app, {\"task_list\": [], \"active_task\": None}, RunnableConfig(configurable={\"thread_id\": 1})))\n    state = app.get_state({\"configurable\": {\"thread_id\": 1}})\n    \n    # Should say that step 1 is completed and step 2 is pending\n    # Should also show the description of step 1 as \"Perform step 1: Hello\" and the description of step 2 as \"Perform step 2: World\"\n    for step in state.values['active_task'].pipeline.steps:\n        print(f\"Step {step}: {state.values['active_task'].pipeline.steps[step].status}\")\n        print(f\"Step {step}: {state.values['active_task'].pipeline.steps[step].step_desc}\")\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nI find that when using a custom Pydantic model (I make sure to define it using langchain_core.pydantic_v1 import BaseModel) as part of the State schema, if the Pydantic model has a custom __init__ call that sets some default values that later get updated, the checkpointer fails to produce the updated values when calling app.get_state\nIn the example, we see that during the graph execution, the custom pydantic model (in this case a \"Task\") has certain fields updated (e.g. Step 1 was marked as complete, and descriptions of Step 1 and 2 are filled in and changed), and indeed langgraph seems to track those changes from one node to the next (as evidenced from printing the outputs of app.stream), however, when finally calling app.get_state, all those changes are gone - maybe for some reason it called the __init__ of the pydantic model again, and reset everything to the initial values?\nHere's the stdout from running the above code:\nOutput from node 'node1':\n  task_list: [Task(task_type=<Pipeline.P1: 'p1'>, pipeline=PipelineTracker(name=<Pipeline.P1: 'p1'>, steps={<Step.S1: 's1'>: StepTracker(name='s1', status=<StepStatus.PENDING: 'pending'>, outputs={}, step_desc='Perform step 1: Hello'), <Step.S2: 's2'>: StepTracker(name='s2', status=<StepStatus.PENDING: 'pending'>, outputs={}, step_desc='Perform step 2: World')}))]\n  active_task: task_type=<Pipeline.P1: 'p1'> pipeline=PipelineTracker(name=<Pipeline.P1: 'p1'>, steps={<Step.S1: 's1'>: StepTracker(name='s1', status=<StepStatus.PENDING: 'pending'>, outputs={}, step_desc='Perform step 1: Hello'), <Step.S2: 's2'>: StepTracker(name='s2', status=<StepStatus.PENDING: 'pending'>, outputs={}, step_desc='Perform step 2: World')})\n\n---\n\nOutput from node 'node2':\n  active_task: task_type=<Pipeline.P1: 'p1'> pipeline=PipelineTracker(name=<Pipeline.P1: 'p1'>, steps={<Step.S1: 's1'>: StepTracker(name='s1', status=<StepStatus.COMPLETED: 'completed'>, outputs={}, step_desc='Perform step 1: Hello'), <Step.S2: 's2'>: StepTracker(name='s2', status=<StepStatus.PENDING: 'pending'>, outputs={}, step_desc='Perform step 2: World')})\n\n---\n\nStep s1: pending\nStep s1: Perform step 1: {param1}\nStep s2: pending\nStep s2: Perform step 2: {param2}\n\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #127~20.04.1-Ubuntu SMP Thu Jul 11 15:36:12 UTC 2024\nPython Version:  3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0]\n\nPackage Information\n\nlangchain_core: 0.2.29\nlangchain: 0.2.7\nlangchain_community: 0.2.7\nlangsmith: 0.1.85\nlangchain_anthropic: 0.1.20\nlangchain_chroma: 0.1.2\nlangchain_experimental: 0.0.62\nlangchain_huggingface: 0.0.3\nlangchain_openai: 0.1.16\nlangchain_text_splitters: 0.2.2\nlangchainhub: 0.1.20\nlanggraph: 0.2.3\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.9.5\nanthropic: 0.31.0\nasync-timeout: 4.0.3\nchromadb: 0.5.4\ndataclasses-json: 0.6.7\ndefusedxml: 0.7.1\nfastapi: 0.111.1\nhuggingface-hub: 0.23.4\njsonpatch: 1.33\nlanggraph-checkpoint: 1.0.2\nnumpy: 1.26.4\nopenai: 1.35.13\norjson: 3.10.6\npackaging: 24.1\npydantic: 1.10.17\nPyYAML: 6.0.1\nrequests: 2.32.3\nsentence-transformers: 3.0.1\nSQLAlchemy: 2.0.31\ntenacity: 8.5.0\ntiktoken: 0.7.0\ntokenizers: 0.19.1\ntransformers: 4.43.3\ntypes-requests: 2.32.0.20240712\ntyping-extensions: 4.12.2\n", "created_at": "2024-08-12", "closed_at": "2024-08-12", "labels": [], "State": "closed", "Author": "fedshyvana"}
{"issue_number": 1319, "issue_title": "`langgraph up` doesn't find Docker on mac", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\n[Not applicable]\nError Message and Stack Trace (if applicable)\nNo response\nDescription\n> langgraph up\n\nStarting LangGraph API server...\nFor local dev, requires env var LANGSMITH_API_KEY with access to LangGraph Cloud closed beta.\nFor production use, requires a license key in env var LANGGRAPH_CLOUD_LICENSE_KEY.\nUsage: langgraph up [OPTIONS]\nTry 'langgraph up --help' for help.\n\nError: Docker not installed or not running\nSystem Info\nlangchain-core==0.2.29\nlangchain-openai==0.1.21\n\nmac\npython 3.12", "created_at": "2024-08-12", "closed_at": "2025-04-15", "labels": ["question"], "State": "closed", "Author": "n-sviridenko"}
{"issue_number": 1314, "issue_title": "DOC: filter_message code is wrong", "issue_body": "Issue with current documentation:\nsimple issue, but worth pointing out\nthe code here:\nhttps://langchain-ai.github.io/langgraph/how-tos/memory/manage-conversation-history/#filtering-messages\n    # This is very simple helper function which only ever uses the last two messages\n    return messages[-1:]\n\n\nactually returns the last message, not the last 2.\nlast 2 would be messages[-2:]\nIdea or request for content:\nNo response", "created_at": "2024-08-12", "closed_at": "2024-08-12", "labels": [], "State": "closed", "Author": "streamnsight"}
{"issue_number": 1310, "issue_title": "ToolNode doesn't support custom state classes", "issue_body": "Privileged issue\n\n I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.\n\nIssue Content\n\nI also realized I added a get method to the state class because ToolNode assumes your state is a TypedDict. This may also be another issue, but I haven't looked into it yet. In any case, I left the logging to show when it's accessing messages.\n\nFirst reported in langchain-ai/langgraph-studio#66", "created_at": "2024-08-11", "closed_at": "2024-09-06", "labels": [], "State": "closed", "Author": "nfcampos"}
{"issue_number": 1308, "issue_title": "langgraph-cli build fails on pip install  with \"No .egg-info directory found\"", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\n$ langgraph build -c langgraph.json -t latest\n\nlanggraph.json:\n\n{\n    \"dependencies\": [\n        \".\"\n    ],\n    \"graphs\": {\n        \"myagent\": \"./path/to/agent.py:graph\",\n    },\n    \"env\": \"./.env\",\n    \"python_version\": \"3.12\"\n}\n\n\nI have a requirements.txt file.\n\nI have a Dockerfile that builds OK with this requirements.txt.\nError Message and Stack Trace (if applicable)\n- Pulling...+ docker pull langchain/langgraph-api:3.12\n| Pulling...3.12: Pulling from langchain/langgraph-api\n4f4fb700ef54: Pulling fs layer\n262a5f25eec7: Pulling fs layer\nba32bf0f91fc: Pulling fs layer\n1dc5ee30d1f7: Pulling fs layer\n05f4007f5a88: Pulling fs layer\ncb8d7ef7003a: Pulling fs layer\n489bc8fee3cf: Pulling fs layer\n8dfcd0f4e008: Pulling fs layer\naf4ac4ecdac2: Pulling fs layer\n5bb783acb057: Pulling fs layer\n664fc557737b: Pulling fs layer\n01c312566e6a: Pulling fs layer\n/ Pulling...01c312566e6a: Download complete\n- Pulling...05f4007f5a88: Download complete\n8dfcd0f4e008: Download complete\n\\ Pulling...664fc557737b: Download complete\n| Pulling...4f4fb700ef54: Download complete\n/ Pulling...af4ac4ecdac2: Download complete\n- Pulling...489bc8fee3cf: Download complete\n/ Pulling...cb8d7ef7003a: Download complete\n- Pulling...ba32bf0f91fc: Download complete\n/ Pulling...1dc5ee30d1f7: Download complete\n/ Pulling...5bb783acb057: Download complete\n| Pulling...262a5f25eec7: Download complete\n- Pulling...Digest: sha256:6b7b13f0c83d50ef83da8329f913ace3e3044ec11c1068201abb26551f525927\nStatus: Downloaded newer image for langchain/langgraph-api:3.12\ndocker.io/langchain/langgraph-api:3.12\n+ docker build -f - -t latest /Users/user/developer/myproject <\nFROM langchain/langgraph-api:3.12\nADD . /deps/myproject\nRUN PYTHONDONTWRITEBYTECODE=1 pip install --no-cache-dir -c /api/constraints.txt -e /deps/*\nENV LANGSERVE_GRAPHS='{\"marketer\": \"/deps/myproject/path/to/myagents.py:graph\"}'\nWORKDIR /deps/myproject\n/ Building...#0 building with \"desktop-linux\" instance using docker driver\n\n#1 [internal] load build definition from Dockerfile\n#1 transferring dockerfile: 354B done\n#1 DONE 0.0s\n\n#2 [internal] load metadata for docker.io/langchain/langgraph-api:3.12\n#2 DONE 0.0s\n\n#3 [internal] load .dockerignore\n#3 transferring context: 181B done\n#3 DONE 0.0s\n\n#4 [1/4] FROM docker.io/langchain/langgraph-api:3.12@sha256:6b7b13f0c83d50ef83da8329f913ace3e3044ec11c1068201abb26551f525927\n#4 resolve docker.io/langchain/langgraph-api:3.12@sha256:6b7b13f0c83d50ef83da8329f913ace3e3044ec11c1068201abb26551f525927 done\n- Building...#4 DONE 0.1s\n\n#5 [internal] load build context\n#5 transferring context: 8.31MB 0.1s done\n#5 DONE 0.1s\n\n#6 [2/4] ADD . /deps/myproject\n#6 DONE 0.1s\n\\ Building...\n#7 [3/4] RUN PYTHONDONTWRITEBYTECODE=1 pip install --no-cache-dir -c /api/constraints.txt -e /deps/*\n\\ Building...#7 1.002 Obtaining file:///deps/myproject\n/ Building...#7 1.003   Preparing metadata (setup.py): started\n- Building...#7 1.315   Preparing metadata (setup.py): finished with status 'done'\n\\ Building...#7 1.400 ERROR: No .egg-info directory found in /tmp/pip-pip-egg-info-uxa0sfwa\n#7 ERROR: process \"/bin/sh -c PYTHONDONTWRITEBYTECODE=1 pip install --no-cache-dir -c /api/constraints.txt -e /deps/*\" did not complete successfully: exit code: 1\n| Building...#0 building with \"desktop-linux\" instance using docker driver\n\n#1 [internal] load build definition from Dockerfile\n#1 transferring dockerfile: 354B done\n#1 DONE 0.0s\n\n#2 [internal] load metadata for docker.io/langchain/langgraph-api:3.12\n#2 DONE 0.0s\n\n#3 [internal] load .dockerignore\n#3 transferring context: 181B done\n#3 DONE 0.0s\n\n#4 [1/4] FROM docker.io/langchain/langgraph-api:3.12@sha256:6b7b13f0c83d50ef83da8329f913ace3e3044ec11c1068201abb26551f525927\n#4 resolve docker.io/langchain/langgraph-api:3.12@sha256:6b7b13f0c83d50ef83da8329f913ace3e3044ec11c1068201abb26551f525927 done\n#4 DONE 0.1s\n\n#5 [internal] load build context\n#5 transferring context: 8.31MB 0.1s done\n#5 DONE 0.1s\n\n#6 [2/4] ADD . /deps/myproject\n#6 DONE 0.1s\n\n#7 [3/4] RUN PYTHONDONTWRITEBYTECODE=1 pip install --no-cache-dir -c /api/constraints.txt -e /deps/*\n#7 1.002 Obtaining file:///deps/myproject\n#7 1.003   Preparing metadata (setup.py): started\n#7 1.315   Preparing metadata (setup.py): finished with status 'done'\n#7 1.400 ERROR: No .egg-info directory found in /tmp/pip-pip-egg-info-uxa0sfwa\n#7 ERROR: process \"/bin/sh -c PYTHONDONTWRITEBYTECODE=1 pip install --no-cache-dir -c /api/constraints.txt -e /deps/*\" did not complete successfully: exit code: 1\n------\n > [3/4] RUN PYTHONDONTWRITEBYTECODE=1 pip install --no-cache-dir -c /api/constraints.txt -e /deps/*:\n1.002 Obtaining file:///deps/myproject\n1.003   Preparing metadata (setup.py): started\n1.315   Preparing metadata (setup.py): finished with status 'done'\n1.400 ERROR: No .egg-info directory found in /tmp/pip-pip-egg-info-uxa0sfwa\n------\nDockerfile:7\n--------------------\n   5 |     ADD . /deps/myproject\n   6 |     \n   7 | >>> RUN PYTHONDONTWRITEBYTECODE=1 pip install --no-cache-dir -c /api/constraints.txt -e /deps/*\n   8 |     \n   9 |     ENV LANGSERVE_GRAPHS='{\"myagent\": \"/deps/path/to/agent.py:graph\"}'\n--------------------\nERROR: failed to solve: process \"/bin/sh -c PYTHONDONTWRITEBYTECODE=1 pip install --no-cache-dir -c /api/constraints.txt -e /deps/*\" did not complete successfully: exit code: 1\n\nView build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/ryj0fu6muwqg9obxkd91iksy6\n------\n > [3/4] RUN PYTHONDONTWRITEBYTECODE=1 pip install --no-cache-dir -c /api/constraints.txt -e /deps/*:\n1.002 Obtaining file:///deps/myproject\n1.003   Preparing metadata (setup.py): started\n1.315   Preparing metadata (setup.py): finished with status 'done'\n1.400 ERROR: No .egg-info directory found in /tmp/pip-pip-egg-info-uxa0sfwa\n------\nDockerfile:7\n--------------------\n   5 |     ADD . /deps/myproject\n   6 |     \n   7 | >>> RUN PYTHONDONTWRITEBYTECODE=1 pip install --no-cache-dir -c /api/constraints.txt -e /deps/*\n   8 |     \n   9 |     ENV LANGSERVE_GRAPHS='{\"myagent\": \"/deps/path/to/agent.py:graph\"}'\n--------------------\nERROR: failed to solve: process \"/bin/sh -c PYTHONDONTWRITEBYTECODE=1 pip install --no-cache-dir -c /api/constraints.txt -e /deps/*\" did not complete successfully: exit code: 1\n\nView build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/ryj0fu6muwqg9obxkd91iksy6\nDescription\nI'm trying to deploy my agent to LangGraph Cloud to test that service. My agent and code has been working great via Docker.\nThe error I'm getting doesn't really help me solve it.\nSystem Info\nlangchain==0.2.12\nlangchain-anthropic==0.1.22\nlangchain-cli==0.0.28\nlangchain-community==0.2.11\nlangchain-core==0.2.29\nlangchain-openai==0.1.21rc2\nlangchain-pinecone==0.1.3\nlangchain-text-splitters==0.2.2\nlangchainhub==0.1.20\nPython 3.12.4\nmacOS 14.5 Sonoma", "created_at": "2024-08-11", "closed_at": "2024-08-12", "labels": [], "State": "closed", "Author": "godojoai"}
{"issue_number": 1307, "issue_title": "Using openai_function_agent in Langgraph returned intermediate_steps, which caused the agent to answer incorrectly.", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nclass AgentState(TypedDict):\n    # The input string\n    input: str\n    # The list of previous messages in the conversation\n    chat_history: list[BaseMessage]\n    # chat_history: Annotated[list[BaseMessage], operator.add]\n    # The outcome of a given call to the agent\n    # Needs `None` as a valid type, since this is what this will start as\n    agent_outcome: Union[AgentAction, AgentFinish, None]\n    # List of actions and corresponding observations\n    # Here we annotate this with `operator.add` to indicate that operations to\n    # this state should be ADDED to the existing values (not overwrite it)\n    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]\n    # intermediate_steps: list[tuple[AgentAction, str]]\n    # The result of the RAG - extra information for the model if there is an answer from the FAQS step to the user question.\n    rag_result: str\n\n    #* External parameters from the request\n    execRAG: bool\n    agentPrompt: str\n\n# Define the agent Node\ndef run_agent(data: AgentState):\n    if not data[\"chat_history\"]:\n        data[\"chat_history\"] = []\n\n    conversation_history = data[\"chat_history\"]\n    agent_system_prompt = data[\"agentPrompt\"]\n    \n    final_llm_node = agent_llm.with_config(tags=[\"final_node\"])\n    final_agent_prompt = build_agent_prompt(\"\"\"{}\"\"\", agent_system_prompt, BRAND_NAME, BRAND_CODE)\n    openai_functions_agent_prompt = ChatPromptTemplate.from_messages(\n        [\n            SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=final_agent_prompt)),\n            *([] if data[\"rag_result\"] is None else [\n                    SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template=automatic_faq_prompt.format(retrieval_answer=data[\"rag_result\"], question=data[\"input\"])))\n            ]),\n            MessagesPlaceholder(\"chat_history\", optional=True),\n            HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n            MessagesPlaceholder(\"agent_scratchpad\"),\n        ]\n    )\n\n    openai_functions_agent = create_openai_functions_agent(final_llm_node, tools, openai_functions_agent_prompt)\n    agent_outcome = openai_functions_agent.invoke({\"input\": data[\"input\"], \"intermediate_steps\": data[\"intermediate_steps\"], \"chat_history\": data[\"chat_history\"]})\n    \n\n    if agent_outcome.type == \"AgentFinish\":\n        conversation_history = conversation_history + [HumanMessage(content=data[\"input\"])] + agent_outcome.messages\n\n\n    return {\"agent_outcome\": agent_outcome, \"chat_history\": conversation_history}\n\n# Define the function to execute tools\ndef execute_tools(data: AgentState):\n    # This a helper class we have that is useful for running tools\n    # It takes in an agent action and calls that tool and returns the result\n    tool_executor = ToolExecutor(tools)\n\n    # Get the most recent agent_outcome - this is the key added in the `agent` above\n    agent_action = data[\"agent_outcome\"]\n    output = tool_executor.invoke(agent_action)\n\n    return {\n        \"intermediate_steps\": [(agent_action, str(output))],\n    }\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nI am encountering an issue while building an agent using Langgraph with the create_openai_functions_agent function. The problem arises with intermediate_steps, which seems to cause the agent to repeat its previous responses to user queries following a tool invocation and reinitialization of intermediate_steps.\nHere is how my graph looks like:\n\nI've attached an image that illustrates the scenario:\n\nAs shown in the image, the user initially asks about their order and receives a relevant response. However, when the user subsequently asks an unrelated question, the agent erroneously repeats the response to the first question. It's important to note that the model possesses the correct information to respond to the second question but seems to revert to using intermediate_steps for its response.\nTo diagnose whether the issue was specifically related to intermediate_steps, I replicated the scenario in the Langsmith playground by disabling intermediate_steps. This change led to the agent providing the correct answer, indicating that the issue indeed pertains to how intermediate_steps are implemented or invoked.\nHere are some images from Langsmith that shown the issue, and when I remove the intermediate_steps the agent answer correctly:\nWithout intermediate_steps - agent answer correctly\n\nWith intermediate_steps - Agent answer wrong!\n\nQuestions:\n\nIs there a way to disable intermediate_steps in Langgraph using create_openai_functions_agent?\nCould there be an error in how I have implemented intermediate_steps?\nthere is any other solution for this case?\nI have been stuck on this issue for several days and am unsure how to resolve it. Any guidance or suggestions would be greatly appreciated.\n\nIf someone wants to schedule a Zoom appointment with me for further assistance, I would be happy to do so :)\nSystem Info\naiohttp==3.9.1\naiosignal==1.3.1\naiostream==0.5.2\naniso8601==9.0.1\nanyio==4.3.0\nargilla==0.0.1\nasttokens==2.4.1\nasync-timeout==4.0.2\nattrs==22.2.0\nbackoff==2.2.1\nbeautifulsoup4==4.12.3\nbidict==0.23.1\nboto3==1.34.117\nbotocore==1.34.117\nbuild==1.2.1\nCacheControl==0.14.0\ncachetools==5.3.2\ncertifi==2022.12.7\ncffi==1.16.0\ncharset-normalizer==3.1.0\ncleo==2.1.0\nclick==8.1.3\ncohere==5.5.4\ncolorama==0.4.6\ncomm==0.2.2\ncontourpy==1.2.1\ncrashtest==0.4.1\ncycler==0.12.1\ndataclasses-json==0.5.7\ndebugpy==1.8.2\ndecorator==5.1.1\nDeprecated==1.2.14\ndistlib==0.3.8\ndistro==1.9.0\ndnspython==2.3.0\ndocopt==0.6.2\ndulwich==0.21.7\nelastic-transport==8.13.1\nelasticsearch==8.13.2\net-xmlfile==1.1.0\neventlet==0.33.3\nexecuting==2.0.1\nfastapi==0.109.1\nfastavro==1.9.4\nfastjsonschema==2.19.1\nfilelock==3.13.3\nfonttools==4.53.1\nfrozenlist==1.3.3\nfsspec==2024.3.1\ngevent==22.10.2\ngevent-websocket==0.10.1\ngreenlet==2.0.2\ngroq==0.9.0\nh11==0.14.0\nhttpcore==1.0.5\nhttpx==0.27.0\nhttpx-sse==0.4.0\nhuggingface-hub==0.23.2\nidna==3.4\nimportlib-metadata==6.11.0\ninstaller==0.7.0\nipykernel==6.29.5\nipython==8.26.0\nitsdangerous==2.1.2\njaraco.classes==3.4.0\njedi==0.19.1\nJinja2==3.1.2\njmespath==1.0.1\njoblib==1.3.2\njsonpatch==1.33\njsonpointer==2.4\njupyter_client==8.6.2\njupyter_core==5.7.2\nkeyring==24.3.1\nkiwisolver==1.4.5\nlangchain==0.2.1\nlangchain-cohere==0.1.5\nlangchain-community==0.2.1\nlangchain-core==0.2.18\nlangchain-groq==0.1.5\nlangchain-openai==0.1.8\nlangchain-pinecone==0.1.1\nlangchain-text-splitters==0.2.0\nlangchainhub==0.1.20\nlanggraph==0.1.8\nlangsmith==0.1.85\nLevenshtein==0.25.1\nllama-index==0.9.8.post1\nloguru==0.7.0\nlxml==5.2.0\nMarkdown==3.6\nMarkupSafe==2.1.2\nmarshmallow==3.20.2\nmarshmallow-enum==1.5.1\nmatplotlib==3.9.1\nmatplotlib-inline==0.1.7\nmore-itertools==10.2.0\nmotor==3.5.1\nmsg-parser==1.2.0\nmsgpack==1.0.8\nmultidict==6.0.4\nmypy-extensions==1.0.0\nnest-asyncio==1.6.0\nnltk==3.8.1\nnumpy==1.24.2\nolefile==0.47\nopenai==1.30.5\nopenapi-schema-pydantic==1.2.4\nopenpyxl==3.1.2\nopentelemetry-api==1.25.0\nopentelemetry-sdk==1.25.0\nopentelemetry-semantic-conventions==0.46b0\norjson==3.10.0\npackaging==23.2\npandas==2.2.1\nparso==0.8.4\npexpect==4.9.0\npillow==10.3.0\npinecone-client==3.2.2\npipreqs==0.4.12\npkginfo==1.10.0\nplatformdirs==4.2.0\npoetry==1.8.2\npoetry-core==1.9.0\npoetry-plugin-export==1.7.1\nprompt_toolkit==3.0.47\npsutil==6.0.0\nptyprocess==0.7.0\npure-eval==0.2.2\npycparser==2.22\npydantic==1.10.7\nPygments==2.18.0\npymongo==4.8.0\npyodbc==5.0.1\npypandoc==1.13\npyparsing==3.1.2\npypdf==3.8.1\npyproject_hooks==1.0.0\npython-dateutil==2.8.2\npython-docx==1.1.0\npython-dotenv==1.0.1\npython-engineio==4.9.0\npython-Levenshtein==0.25.1\npython-magic==0.4.27\npython-pptx==0.6.23\npython-socketio==5.11.2\npytz==2023.3\npywin32==306\npywin32-ctypes==0.2.2\nPyYAML==6.0\npyzmq==26.0.3\nrapidfuzz==3.8.1\nredis==5.0.1\nregex==2023.3.23\nrequests==2.31.0\nrequests-toolbelt==1.0.0\ns3transfer==0.10.1\nshellingham==1.5.4\nsimple-websocket==1.0.0\nsix==1.16.0\nsniffio==1.3.1\nsoupsieve==2.5\nSQLAlchemy==2.0.23\nstack-data==0.6.3\nstarlette==0.35.1\ntenacity==8.2.2\ntiktoken==0.7.0\ntokenizers==0.15.2\ntomlkit==0.12.4\ntornado==6.4.1\ntqdm==4.65.0\ntraitlets==5.14.3\ntrove-classifiers==2024.3.25\ntypes-requests==2.31.0.6\ntypes-urllib3==1.26.25.14\ntyping-inspect==0.8.0\ntyping_extensions==4.9.0\ntzdata==2024.1\nUnidecode==1.3.8\nunstructured==0.6.2\nurllib3==1.26.15\nuvicorn==0.27.0.post1\nvirtualenv==20.25.1\nwcwidth==0.2.13\nWerkzeug==2.2.3\nwikipedia==1.4.0\nwin32-setctime==1.1.0\nwrapt==1.16.0\nwsproto==1.2.0\nXlsxWriter==3.2.0\nyarg==0.1.9\nyarl==1.8.2\nzipp==3.18.1\nzope.event==5.0\nzope.interface==6.2\nPlatform: Windows\nPython version: Python 3.11.2", "created_at": "2024-08-11", "closed_at": "2024-08-21", "labels": [], "State": "closed", "Author": "lironezra"}
{"issue_number": 1304, "issue_title": "OpenAI works as expected but AzureOpenAI does not (ref multi-agent examples)", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\n(from https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb)\n\nfor s in research_chain.stream(\n    \"when is Taylor Swift's next tour?\", {\"recursion_limit\": 100}\n):\n    if \"__end__\" not in s:\n        print(s)\n        print(\"---\")\nError Message and Stack Trace (if applicable)\n{\n\t\"name\": \"APIConnectionError\",\n\t\"message\": \"Connection error.\",\n\t\"stack\": \"---------------------------------------------------------------------------\nUnsupportedProtocol                       Traceback (most recent call last)\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_transports\\\\default.py:69, in map_httpcore_exceptions()\n     68 try:\n---> 69     yield\n     70 except Exception as exc:\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_transports\\\\default.py:233, in HTTPTransport.handle_request(self, request)\n    232 with map_httpcore_exceptions():\n--> 233     resp = self._pool.handle_request(req)\n    235 assert isinstance(resp.stream, typing.Iterable)\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpcore\\\\_sync\\\\connection_pool.py:167, in ConnectionPool.handle_request(self, request)\n    166 if scheme == \\\"\\\":\n--> 167     raise UnsupportedProtocol(\n    168         \\\"Request URL is missing an 'http://' or 'https://' protocol.\\\"\n    169     )\n    170 if scheme not in (\\\"http\\\", \\\"https\\\", \\\"ws\\\", \\\"wss\\\"):\n\nUnsupportedProtocol: Request URL is missing an 'http://' or 'https://' protocol.\n\nThe above exception was the direct cause of the following exception:\n\nUnsupportedProtocol                       Traceback (most recent call last)\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\openai\\\\_base_client.py:978, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    977 try:\n--> 978     response = self._client.send(\n    979         request,\n    980         stream=stream or self._should_stream_response_body(request=request),\n    981         **kwargs,\n    982     )\n    983 except httpx.TimeoutException as err:\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_client.py:914, in Client.send(self, request, stream, auth, follow_redirects)\n    912 auth = self._build_request_auth(request, auth)\n--> 914 response = self._send_handling_auth(\n    915     request,\n    916     auth=auth,\n    917     follow_redirects=follow_redirects,\n    918     history=[],\n    919 )\n    920 try:\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_client.py:942, in Client._send_handling_auth(self, request, auth, follow_redirects, history)\n    941 while True:\n--> 942     response = self._send_handling_redirects(\n    943         request,\n    944         follow_redirects=follow_redirects,\n    945         history=history,\n    946     )\n    947     try:\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_client.py:979, in Client._send_handling_redirects(self, request, follow_redirects, history)\n    977     hook(request)\n--> 979 response = self._send_single_request(request)\n    980 try:\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_client.py:1015, in Client._send_single_request(self, request)\n   1014 with request_context(request=request):\n-> 1015     response = transport.handle_request(request)\n   1017 assert isinstance(response.stream, SyncByteStream)\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_transports\\\\default.py:232, in HTTPTransport.handle_request(self, request)\n    220 req = httpcore.Request(\n    221     method=request.method,\n    222     url=httpcore.URL(\n   (...)\n    230     extensions=request.extensions,\n    231 )\n--> 232 with map_httpcore_exceptions():\n    233     resp = self._pool.handle_request(req)\n\nFile ~\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\contextlib.py:158, in _GeneratorContextManager.__exit__(self, typ, value, traceback)\n    157 try:\n--> 158     self.gen.throw(value)\n    159 except StopIteration as exc:\n    160     # Suppress StopIteration *unless* it's the same exception that\n    161     # was passed to throw().  This prevents a StopIteration\n    162     # raised inside the \\\"with\\\" statement from being suppressed.\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_transports\\\\default.py:86, in map_httpcore_exceptions()\n     85 message = str(exc)\n---> 86 raise mapped_exc(message) from exc\n\nUnsupportedProtocol: Request URL is missing an 'http://' or 'https://' protocol.\n\nDuring handling of the above exception, another exception occurred:\n\nUnsupportedProtocol                       Traceback (most recent call last)\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_transports\\\\default.py:69, in map_httpcore_exceptions()\n     68 try:\n---> 69     yield\n     70 except Exception as exc:\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_transports\\\\default.py:233, in HTTPTransport.handle_request(self, request)\n    232 with map_httpcore_exceptions():\n--> 233     resp = self._pool.handle_request(req)\n    235 assert isinstance(resp.stream, typing.Iterable)\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpcore\\\\_sync\\\\connection_pool.py:167, in ConnectionPool.handle_request(self, request)\n    166 if scheme == \\\"\\\":\n--> 167     raise UnsupportedProtocol(\n    168         \\\"Request URL is missing an 'http://' or 'https://' protocol.\\\"\n    169     )\n    170 if scheme not in (\\\"http\\\", \\\"https\\\", \\\"ws\\\", \\\"wss\\\"):\n\nUnsupportedProtocol: Request URL is missing an 'http://' or 'https://' protocol.\n\nThe above exception was the direct cause of the following exception:\n\nUnsupportedProtocol                       Traceback (most recent call last)\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\openai\\\\_base_client.py:978, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    977 try:\n--> 978     response = self._client.send(\n    979         request,\n    980         stream=stream or self._should_stream_response_body(request=request),\n    981         **kwargs,\n    982     )\n    983 except httpx.TimeoutException as err:\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_client.py:914, in Client.send(self, request, stream, auth, follow_redirects)\n    912 auth = self._build_request_auth(request, auth)\n--> 914 response = self._send_handling_auth(\n    915     request,\n    916     auth=auth,\n    917     follow_redirects=follow_redirects,\n    918     history=[],\n    919 )\n    920 try:\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_client.py:942, in Client._send_handling_auth(self, request, auth, follow_redirects, history)\n    941 while True:\n--> 942     response = self._send_handling_redirects(\n    943         request,\n    944         follow_redirects=follow_redirects,\n    945         history=history,\n    946     )\n    947     try:\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_client.py:979, in Client._send_handling_redirects(self, request, follow_redirects, history)\n    977     hook(request)\n--> 979 response = self._send_single_request(request)\n    980 try:\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_client.py:1015, in Client._send_single_request(self, request)\n   1014 with request_context(request=request):\n-> 1015     response = transport.handle_request(request)\n   1017 assert isinstance(response.stream, SyncByteStream)\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_transports\\\\default.py:232, in HTTPTransport.handle_request(self, request)\n    220 req = httpcore.Request(\n    221     method=request.method,\n    222     url=httpcore.URL(\n   (...)\n    230     extensions=request.extensions,\n    231 )\n--> 232 with map_httpcore_exceptions():\n    233     resp = self._pool.handle_request(req)\n\nFile ~\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\contextlib.py:158, in _GeneratorContextManager.__exit__(self, typ, value, traceback)\n    157 try:\n--> 158     self.gen.throw(value)\n    159 except StopIteration as exc:\n    160     # Suppress StopIteration *unless* it's the same exception that\n    161     # was passed to throw().  This prevents a StopIteration\n    162     # raised inside the \\\"with\\\" statement from being suppressed.\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_transports\\\\default.py:86, in map_httpcore_exceptions()\n     85 message = str(exc)\n---> 86 raise mapped_exc(message) from exc\n\nUnsupportedProtocol: Request URL is missing an 'http://' or 'https://' protocol.\n\nDuring handling of the above exception, another exception occurred:\n\nUnsupportedProtocol                       Traceback (most recent call last)\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_transports\\\\default.py:69, in map_httpcore_exceptions()\n     68 try:\n---> 69     yield\n     70 except Exception as exc:\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_transports\\\\default.py:233, in HTTPTransport.handle_request(self, request)\n    232 with map_httpcore_exceptions():\n--> 233     resp = self._pool.handle_request(req)\n    235 assert isinstance(resp.stream, typing.Iterable)\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpcore\\\\_sync\\\\connection_pool.py:167, in ConnectionPool.handle_request(self, request)\n    166 if scheme == \\\"\\\":\n--> 167     raise UnsupportedProtocol(\n    168         \\\"Request URL is missing an 'http://' or 'https://' protocol.\\\"\n    169     )\n    170 if scheme not in (\\\"http\\\", \\\"https\\\", \\\"ws\\\", \\\"wss\\\"):\n\nUnsupportedProtocol: Request URL is missing an 'http://' or 'https://' protocol.\n\nThe above exception was the direct cause of the following exception:\n\nUnsupportedProtocol                       Traceback (most recent call last)\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\openai\\\\_base_client.py:978, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    977 try:\n--> 978     response = self._client.send(\n    979         request,\n    980         stream=stream or self._should_stream_response_body(request=request),\n    981         **kwargs,\n    982     )\n    983 except httpx.TimeoutException as err:\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_client.py:914, in Client.send(self, request, stream, auth, follow_redirects)\n    912 auth = self._build_request_auth(request, auth)\n--> 914 response = self._send_handling_auth(\n    915     request,\n    916     auth=auth,\n    917     follow_redirects=follow_redirects,\n    918     history=[],\n    919 )\n    920 try:\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_client.py:942, in Client._send_handling_auth(self, request, auth, follow_redirects, history)\n    941 while True:\n--> 942     response = self._send_handling_redirects(\n    943         request,\n    944         follow_redirects=follow_redirects,\n    945         history=history,\n    946     )\n    947     try:\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_client.py:979, in Client._send_handling_redirects(self, request, follow_redirects, history)\n    977     hook(request)\n--> 979 response = self._send_single_request(request)\n    980 try:\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_client.py:1015, in Client._send_single_request(self, request)\n   1014 with request_context(request=request):\n-> 1015     response = transport.handle_request(request)\n   1017 assert isinstance(response.stream, SyncByteStream)\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_transports\\\\default.py:232, in HTTPTransport.handle_request(self, request)\n    220 req = httpcore.Request(\n    221     method=request.method,\n    222     url=httpcore.URL(\n   (...)\n    230     extensions=request.extensions,\n    231 )\n--> 232 with map_httpcore_exceptions():\n    233     resp = self._pool.handle_request(req)\n\nFile ~\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\contextlib.py:158, in _GeneratorContextManager.__exit__(self, typ, value, traceback)\n    157 try:\n--> 158     self.gen.throw(value)\n    159 except StopIteration as exc:\n    160     # Suppress StopIteration *unless* it's the same exception that\n    161     # was passed to throw().  This prevents a StopIteration\n    162     # raised inside the \\\"with\\\" statement from being suppressed.\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\httpx\\\\_transports\\\\default.py:86, in map_httpcore_exceptions()\n     85 message = str(exc)\n---> 86 raise mapped_exc(message) from exc\n\nUnsupportedProtocol: Request URL is missing an 'http://' or 'https://' protocol.\n\nThe above exception was the direct cause of the following exception:\n\nAPIConnectionError                        Traceback (most recent call last)\nCell In[8], line 1\n----> 1 for s in research_chain.stream(\n      2     \\\"when is Taylor Swift's next tour?\\\", {\\\"recursion_limit\\\": 100}\n      3 ):\n      4     if \\\"__end__\\\" not in s:\n      5         print(s)\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\langchain_core\\\\runnables\\\\base.py:3253, in RunnableSequence.stream(self, input, config, **kwargs)\n   3247 def stream(\n   3248     self,\n   3249     input: Input,\n   3250     config: Optional[RunnableConfig] = None,\n   3251     **kwargs: Optional[Any],\n   3252 ) -> Iterator[Output]:\n-> 3253     yield from self.transform(iter([input]), config, **kwargs)\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\langchain_core\\\\runnables\\\\base.py:3240, in RunnableSequence.transform(self, input, config, **kwargs)\n   3234 def transform(\n   3235     self,\n   3236     input: Iterator[Input],\n   3237     config: Optional[RunnableConfig] = None,\n   3238     **kwargs: Optional[Any],\n   3239 ) -> Iterator[Output]:\n-> 3240     yield from self._transform_stream_with_config(\n   3241         input,\n   3242         self._transform,\n   3243         patch_config(config, run_name=(config or {}).get(\\\"run_name\\\") or self.name),\n   3244         **kwargs,\n   3245     )\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\langchain_core\\\\runnables\\\\base.py:2053, in Runnable._transform_stream_with_config(self, input, transformer, config, run_type, **kwargs)\n   2051 try:\n   2052     while True:\n-> 2053         chunk: Output = context.run(next, iterator)  # type: ignore\n   2054         yield chunk\n   2055         if final_output_supported:\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\langchain_core\\\\runnables\\\\base.py:3202, in RunnableSequence._transform(self, input, run_manager, config, **kwargs)\n   3199     else:\n   3200         final_pipeline = step.transform(final_pipeline, config)\n-> 3202 for output in final_pipeline:\n   3203     yield output\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\langchain_core\\\\runnables\\\\base.py:1289, in Runnable.transform(self, input, config, **kwargs)\n   1286             final = ichunk\n   1288 if got_first_val:\n-> 1289     yield from self.stream(final, config, **kwargs)\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\langgraph\\\\pregel\\\\__init__.py:966, in Pregel.stream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug)\n    963         del fut, task\n    965 # panic on failure or timeout\n--> 966 _panic_or_proceed(done, inflight, loop.step)\n    967 # don't keep futures around in memory longer than needed\n    968 del done, inflight, futures\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\langgraph\\\\pregel\\\\__init__.py:1367, in _panic_or_proceed(done, inflight, step, timeout_exc_cls)\n   1365             inflight.pop().cancel()\n   1366         # raise the exception\n-> 1367         raise exc\n   1369 if inflight:\n   1370     # if we got here means we timed out\n   1371     while inflight:\n   1372         # cancel all pending tasks\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\langgraph\\\\pregel\\\\executor.py:60, in BackgroundExecutor.done(self, task)\n     58 def done(self, task: concurrent.futures.Future) -> None:\n     59     try:\n---> 60         task.result()\n     61     except GraphInterrupt:\n     62         # This exception is an interruption signal, not an error\n     63         # so we don't want to re-raise it on exit\n     64         self.tasks.pop(task)\n\nFile ~\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\concurrent\\\\futures\\\\_base.py:449, in Future.result(self, timeout)\n    447     raise CancelledError()\n    448 elif self._state == FINISHED:\n--> 449     return self.__get_result()\n    451 self._condition.wait(timeout)\n    453 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\nFile ~\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\concurrent\\\\futures\\\\_base.py:401, in Future.__get_result(self)\n    399 if self._exception:\n    400     try:\n--> 401         raise self._exception\n    402     finally:\n    403         # Break a reference cycle with the exception in self._exception\n    404         self = None\n\nFile ~\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\concurrent\\\\futures\\\\thread.py:58, in _WorkItem.run(self)\n     55     return\n     57 try:\n---> 58     result = self.fn(*self.args, **self.kwargs)\n     59 except BaseException as exc:\n     60     self.future.set_exception(exc)\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\langgraph\\\\pregel\\\\retry.py:25, in run_with_retry(task, retry_policy)\n     23 task.writes.clear()\n     24 # run the task\n---> 25 task.proc.invoke(task.input, task.config)\n     26 # if successful, end\n     27 break\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\langchain_core\\\\runnables\\\\base.py:2875, in RunnableSequence.invoke(self, input, config, **kwargs)\n   2873             input = step.invoke(input, config, **kwargs)\n   2874         else:\n-> 2875             input = step.invoke(input, config)\n   2876 # finish the root run\n   2877 except BaseException as e:\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\langchain_core\\\\runnables\\\\base.py:5060, in RunnableBindingBase.invoke(self, input, config, **kwargs)\n   5054 def invoke(\n   5055     self,\n   5056     input: Input,\n   5057     config: Optional[RunnableConfig] = None,\n   5058     **kwargs: Optional[Any],\n   5059 ) -> Output:\n-> 5060     return self.bound.invoke(\n   5061         input,\n   5062         self._merge_configs(config),\n   5063         **{**self.kwargs, **kwargs},\n   5064     )\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\langchain_core\\\\language_models\\\\chat_models.py:274, in BaseChatModel.invoke(self, input, config, stop, **kwargs)\n    263 def invoke(\n    264     self,\n    265     input: LanguageModelInput,\n   (...)\n    269     **kwargs: Any,\n    270 ) -> BaseMessage:\n    271     config = ensure_config(config)\n    272     return cast(\n    273         ChatGeneration,\n--> 274         self.generate_prompt(\n    275             [self._convert_input(input)],\n    276             stop=stop,\n    277             callbacks=config.get(\\\"callbacks\\\"),\n    278             tags=config.get(\\\"tags\\\"),\n    279             metadata=config.get(\\\"metadata\\\"),\n    280             run_name=config.get(\\\"run_name\\\"),\n    281             run_id=config.pop(\\\"run_id\\\", None),\n    282             **kwargs,\n    283         ).generations[0][0],\n    284     ).message\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\langchain_core\\\\language_models\\\\chat_models.py:714, in BaseChatModel.generate_prompt(self, prompts, stop, callbacks, **kwargs)\n    706 def generate_prompt(\n    707     self,\n    708     prompts: List[PromptValue],\n   (...)\n    711     **kwargs: Any,\n    712 ) -> LLMResult:\n    713     prompt_messages = [p.to_messages() for p in prompts]\n--> 714     return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\langchain_core\\\\language_models\\\\chat_models.py:571, in BaseChatModel.generate(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\n    569         if run_managers:\n    570             run_managers[i].on_llm_error(e, response=LLMResult(generations=[]))\n--> 571         raise e\n    572 flattened_outputs = [\n    573     LLMResult(generations=[res.generations], llm_output=res.llm_output)  # type: ignore[list-item]\n    574     for res in results\n    575 ]\n    576 llm_output = self._combine_llm_outputs([res.llm_output for res in results])\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\langchain_core\\\\language_models\\\\chat_models.py:561, in BaseChatModel.generate(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\n    558 for i, m in enumerate(messages):\n    559     try:\n    560         results.append(\n--> 561             self._generate_with_cache(\n    562                 m,\n    563                 stop=stop,\n    564                 run_manager=run_managers[i] if run_managers else None,\n    565                 **kwargs,\n    566             )\n    567         )\n    568     except BaseException as e:\n    569         if run_managers:\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\langchain_core\\\\language_models\\\\chat_models.py:793, in BaseChatModel._generate_with_cache(self, messages, stop, run_manager, **kwargs)\n    791 else:\n    792     if inspect.signature(self._generate).parameters.get(\\\"run_manager\\\"):\n--> 793         result = self._generate(\n    794             messages, stop=stop, run_manager=run_manager, **kwargs\n    795         )\n    796     else:\n    797         result = self._generate(messages, stop=stop, **kwargs)\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\langchain_openai\\\\chat_models\\\\base.py:601, in BaseChatOpenAI._generate(self, messages, stop, run_manager, **kwargs)\n    599     generation_info = {\\\"headers\\\": dict(raw_response.headers)}\n    600 else:\n--> 601     response = self.client.create(**payload)\n    602     generation_info = None\n    603 return self._create_chat_result(response, generation_info)\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\openai\\\\_utils\\\\_utils.py:277, in required_args.<locals>.inner.<locals>.wrapper(*args, **kwargs)\n    275             msg = f\\\"Missing required argument: {quote(missing[0])}\\\"\n    276     raise TypeError(msg)\n--> 277 return func(*args, **kwargs)\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\openai\\\\resources\\\\chat\\\\completions.py:643, in Completions.create(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\n    609 @required_args([\\\"messages\\\", \\\"model\\\"], [\\\"messages\\\", \\\"model\\\", \\\"stream\\\"])\n    610 def create(\n    611     self,\n   (...)\n    641     timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,\n    642 ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n--> 643     return self._post(\n    644         \\\"/chat/completions\\\",\n    645         body=maybe_transform(\n    646             {\n    647                 \\\"messages\\\": messages,\n    648                 \\\"model\\\": model,\n    649                 \\\"frequency_penalty\\\": frequency_penalty,\n    650                 \\\"function_call\\\": function_call,\n    651                 \\\"functions\\\": functions,\n    652                 \\\"logit_bias\\\": logit_bias,\n    653                 \\\"logprobs\\\": logprobs,\n    654                 \\\"max_tokens\\\": max_tokens,\n    655                 \\\"n\\\": n,\n    656                 \\\"parallel_tool_calls\\\": parallel_tool_calls,\n    657                 \\\"presence_penalty\\\": presence_penalty,\n    658                 \\\"response_format\\\": response_format,\n    659                 \\\"seed\\\": seed,\n    660                 \\\"service_tier\\\": service_tier,\n    661                 \\\"stop\\\": stop,\n    662                 \\\"stream\\\": stream,\n    663                 \\\"stream_options\\\": stream_options,\n    664                 \\\"temperature\\\": temperature,\n    665                 \\\"tool_choice\\\": tool_choice,\n    666                 \\\"tools\\\": tools,\n    667                 \\\"top_logprobs\\\": top_logprobs,\n    668                 \\\"top_p\\\": top_p,\n    669                 \\\"user\\\": user,\n    670             },\n    671             completion_create_params.CompletionCreateParams,\n    672         ),\n    673         options=make_request_options(\n    674             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n    675         ),\n    676         cast_to=ChatCompletion,\n    677         stream=stream or False,\n    678         stream_cls=Stream[ChatCompletionChunk],\n    679     )\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\openai\\\\_base_client.py:1266, in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)\n   1252 def post(\n   1253     self,\n   1254     path: str,\n   (...)\n   1261     stream_cls: type[_StreamT] | None = None,\n   1262 ) -> ResponseT | _StreamT:\n   1263     opts = FinalRequestOptions.construct(\n   1264         method=\\\"post\\\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1265     )\n-> 1266     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\openai\\\\_base_client.py:942, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    933 def request(\n    934     self,\n    935     cast_to: Type[ResponseT],\n   (...)\n    940     stream_cls: type[_StreamT] | None = None,\n    941 ) -> ResponseT | _StreamT:\n--> 942     return self._request(\n    943         cast_to=cast_to,\n    944         options=options,\n    945         stream=stream,\n    946         stream_cls=stream_cls,\n    947         remaining_retries=remaining_retries,\n    948     )\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\openai\\\\_base_client.py:1002, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    999 log.debug(\\\"Encountered Exception\\\", exc_info=True)\n   1001 if retries > 0:\n-> 1002     return self._retry_request(\n   1003         input_options,\n   1004         cast_to,\n   1005         retries,\n   1006         stream=stream,\n   1007         stream_cls=stream_cls,\n   1008         response_headers=None,\n   1009     )\n   1011 log.debug(\\\"Raising connection error\\\")\n   1012 raise APIConnectionError(request=request) from err\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\openai\\\\_base_client.py:1079, in SyncAPIClient._retry_request(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\n   1075 # In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\n   1076 # different thread if necessary.\n   1077 time.sleep(timeout)\n-> 1079 return self._request(\n   1080     options=options,\n   1081     cast_to=cast_to,\n   1082     remaining_retries=remaining,\n   1083     stream=stream,\n   1084     stream_cls=stream_cls,\n   1085 )\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\openai\\\\_base_client.py:1002, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    999 log.debug(\\\"Encountered Exception\\\", exc_info=True)\n   1001 if retries > 0:\n-> 1002     return self._retry_request(\n   1003         input_options,\n   1004         cast_to,\n   1005         retries,\n   1006         stream=stream,\n   1007         stream_cls=stream_cls,\n   1008         response_headers=None,\n   1009     )\n   1011 log.debug(\\\"Raising connection error\\\")\n   1012 raise APIConnectionError(request=request) from err\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\openai\\\\_base_client.py:1079, in SyncAPIClient._retry_request(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\n   1075 # In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\n   1076 # different thread if necessary.\n   1077 time.sleep(timeout)\n-> 1079 return self._request(\n   1080     options=options,\n   1081     cast_to=cast_to,\n   1082     remaining_retries=remaining,\n   1083     stream=stream,\n   1084     stream_cls=stream_cls,\n   1085 )\n\nFile c:\\\\Users\\\\James\\\\Documents\\\\Cap\\\\penbot\\\\venv\\\\Lib\\\\site-packages\\\\openai\\\\_base_client.py:1012, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)\n   1002         return self._retry_request(\n   1003             input_options,\n   1004             cast_to,\n   (...)\n   1008             response_headers=None,\n   1009         )\n   1011     log.debug(\\\"Raising connection error\\\")\n-> 1012     raise APIConnectionError(request=request) from err\n   1014 log.debug(\n   1015     'HTTP Response: %s %s \\\"%i %s\\\" %s',\n   1016     request.method,\n   (...)\n   1020     response.headers,\n   1021 )\n   1022 log.debug(\\\"request_id: %s\\\", response.headers.get(\\\"x-request-id\\\"))\n\nAPIConnectionError: Connection error.\"\n}\nDescription\nWhen running the multi agent example notebooks (https://github.com/langchain-ai/langgraph/tree/main/examples/multi_agent) they work as expected with my openai key. If I instead change OpenAI/ChatOpenAI to AzureOpenAI/AzureChatOpenAI and provide the key, endpoint, and version I get the connection error listed above (no other changes). I have tried several api versions but no change.\nI'm using these azure credentials in other applications with langchain agents but this is the first graph application I am building. If I provide invalid credentials I get the related error messages so I do not think that is the input. Is there something obvious I am doing wrong?\nSystem Info\nlangchain==0.2.12\nlangchain-community==0.2.11\nlangchain-core==0.2.28\nlangchain-experimental==0.0.64\nlangchain-openai==0.1.20\nlangchain-text-splitters==0.2.2\nlangchainhub==0.1.20\nwindows\nPython 3.12.4", "created_at": "2024-08-09", "closed_at": "2024-08-12", "labels": [], "State": "closed", "Author": "gitjgoud"}
{"issue_number": 1302, "issue_title": "LangGraph library not compatible with checkpointer base", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nasync for event in graph.astream(initial_state, debug=True):\n        if \"__end__\" not in event:\n            logging.info(f\"Event received: {json.dumps(event, default=str)}\")\n            yield event\nError Message and Stack Trace (if applicable)\nFile \"/usr/local/lib/python3.10/dist-packages/langgraph/graph/__init__.py\", line 1, in <module>\n    from langgraph.graph.graph import END, Graph\n  File \"/usr/local/lib/python3.10/dist-packages/langgraph/graph/graph.py\", line 26, in <module>\n    from langgraph.channels.ephemeral_value import EphemeralValue\n  File \"/usr/local/lib/python3.10/dist-packages/langgraph/channels/__init__.py\", line 1, in <module>\n    from langgraph.channels.binop import BinaryOperatorAggregate\n  File \"/usr/local/lib/python3.10/dist-packages/langgraph/channels/binop.py\", line 6, in <module>\n    from langgraph.channels.base import BaseChannel, EmptyChannelError, Value\n  File \"/usr/local/lib/python3.10/dist-packages/langgraph/channels/base.py\", line 17, in <module>\n    from langgraph.checkpoint.base import Checkpoint\n  File \"/usr/local/lib/python3.10/dist-packages/langgraph/checkpoint/__init__.py\", line 1, in <module>\n    from langgraph.checkpoint.base import (\nImportError: cannot import name 'CheckpointAt' from 'langgraph.checkpoint.base' (/usr/local/lib/python3.10/dist-packages/langgraph/checkpoint/base/__init__.py)\nDescription\nIm not sure hyw this is not working\nSystem Info\nlanggraph==0.2.0\nlanggraph-checkpoint==1.0.2\nlanggraph-checkpoint-postgres==1.0.3\nlanggraph-checkpoint-sqlite==1.0.0", "created_at": "2024-08-09", "closed_at": "2024-08-24", "labels": [], "State": "closed", "Author": "SellCXHarsha"}
{"issue_number": 1293, "issue_title": "Encountering RuntimeWarning with AsyncPostgresSaver checkpointer setup in LangGraph", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nasync def get_app(checkpointer: AsyncPostgresSaver) -> CompiledStateGraph:\n    app = workflow.compile(checkpointer=checkpointer)\n    return app\n\n\nasync def main() -> None:\n    connection_kwargs ={\n        \"autocommit\": True,\n        \"prepare_threshold\": 0,\n        \"row_factory\": dict_row,\n    }\n\n    async with await AsyncConnection.connect(DATABASE_URL, **connection_kwargs) as conn:\n        checkpointer = AsyncPostgresSaver(conn)\n\n        await checkpointer.setup()\n\n        app = await get_app(checkpointer)\n        await run(app)\n\nasyncio.run(main())\nError Message and Stack Trace (if applicable)\n*/venv/lib/python3.12/site-packages/langgraph/checkpoint/postgres/aio.py:69: RuntimeWarning: coroutine 'AsyncCursor.fetchone' was never awaited\n  version = (\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\nTraceback (most recent call last):\n  File \"*/bin/launchable/script\", line 348, in <module>\n    asyncio.run(main())\n  File \"*/.pyenv/versions/3.12.3/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"*/.pyenv/versions/3.12.3/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"*/.pyenv/versions/3.12.3/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"*/bin/launchable/script\", line 343, in main\n    await checkpointer.setup()\n  File \"*/venv/lib/python3.12/site-packages/langgraph/checkpoint/postgres/aio.py\", line 69, in setup\n    version = (\n              ^\nTypeError: 'coroutine' object is not subscriptable\nDescription\nI'm trying to use the new Postgre checkpointer and do the initial setup but I always get the same error.\nEven using with a connection pool or a connection string.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.5.0: Wed May  1 20:14:59 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T8122\nPython Version:  3.12.3 (main, Jun  5 2024, 17:17:50) [Clang 15.0.0 (clang-1500.3.9.4)]\n\nPackage Information\n\nlangchain_core: 0.2.29\nlangchain: 0.2.12\nlangchain_community: 0.2.11\nlangsmith: 0.1.98\nlangchain_milvus: 0.1.4\nlangchain_openai: 0.1.20\nlangchain_text_splitters: 0.2.2\nlanggraph: 0.2.3\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.10.1\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.7\njsonpatch: 1.33\nlanggraph-checkpoint: 1.0.2\nnumpy: 1.26.4\nopenai: 1.40.1\norjson: 3.10.6\npackaging: 23.2\npydantic: 2.8.2\npymilvus: 2.4.4\nPyYAML: 6.0.2\nrequests: 2.32.3\nscipy: 1.14.0\nSQLAlchemy: 2.0.32\ntenacity: 8.5.0\ntiktoken: 0.7.0\ntyping-extensions: 4.12.2\n", "created_at": "2024-08-09", "closed_at": "2024-08-09", "labels": [], "State": "closed", "Author": "merlin-croain"}
{"issue_number": 1292, "issue_title": "Encountering OperationalError with Postgres Checkpointer in LangGraph", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nasync with AsyncPostgresSaver.from_conn_string(os.environ.get(\"POSTGRES_CONN_STRING\")) as checkpointer:\n            return workflow.compile(\n                checkpointer=checkpointer,\n                interrupt_before=[AgentRole.USER_INPUT],\n            )\nError Message and Stack Trace (if applicable)\nFile \"<local>.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 114<local><local>.venv/lib/python3.11/site-packages/langgraph/pregel/loop.py\", line 503, in __aenter__\n    await self.checkpointer.aget_tuple(self.config)\n  File \"<local>.venv/lib/python3.11/site-packages/langgraph/checkpoint/postgres/aio.py\", line 165, in aget_tuple\n    async with self._cursor() as cur:\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.9/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"<local>.venv/lib/python3.11/site-packages/langgraph/checkpoint/postgres/aio.py\", line 309, in _cursor\n    async with self.lock, self.conn.cursor(binary=True) as cur:\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<local>.venv/lib/python3.11/site-packages/psycopg/connection_async.py\", line 229, in cursor\n    self._check_connection_ok()\n  File \"<local>.venv/lib/python3.11/site-packages/psycopg/_connection_base.py\", line 524, in _check_connection_ok\n    raise e.OperationalError(\"the connection is closed\")\npsycopg.OperationalError: the connection is closed\nDescription\nI\u2019m encountering an OperationalError when using the Postgres checkpointer in LangGraph. The error occurs during the cursor operation, indicating that the connection to the database is unexpectedly closed. No issue in database connection.\nSystem Info\nlanggraph==0.2.3\nlanggraph-checkpoint==1.0.2\nlanggraph-checkpoint-postgres==1.0.2", "created_at": "2024-08-09", "closed_at": "2024-08-09", "labels": [], "State": "closed", "Author": "gopinathan-pq"}
{"issue_number": 1274, "issue_title": "import errror from the statement \"from langgraph.checkpoint.sqlite import SqliteSaver \"", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langgraph.checkpoint.sqlite import SqliteSaver\n\nmemory = SqliteSaver.from_conn_string(\":memory:\")\nError Message and Stack Trace (if applicable)\nModuleNotFoundError: No module named 'langgraph.checkpoint.sqlite'\nDescription\ni am trying to follow the quickstart guide on langraph and using langgraph v0.2.0. The statement worked for lower version of langgraph but it's not working for the latest version\nSystem Info\npip install langgraph==0.2.2\nThe statement worked for pip install langgraph==0.1.17 but not for >=0.2.0", "created_at": "2024-08-08", "closed_at": "2024-08-08", "labels": [], "State": "closed", "Author": "pgaods"}
{"issue_number": 1266, "issue_title": "Time between nodes semi-randomly very long on DataBricks cloud server", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nimport time\nfrom typing import TypedDict, Optional\n\nfrom functools import wraps\n\nfrom langgraph.graph import StateGraph, END\n\n\nclass GraphState(TypedDict):\n    query: Optional[str] = None\n    temp_response: Optional[str] = None\n    final_response: Optional[str] = None\n\nstart_time = time.time()\n\n\ndef node_timing(f):\n    @wraps(f)\n    def wrap(*args, **kw):\n        ts = time.time()\n\n        result = f(*args, **kw)\n\n        te = time.time()\n\n        print(f\"in {ts - start_time:.3f} out {te - start_time:.3f}\")\n\n        return result\n\n    return wrap\n\n\nclass Flow:\n\n    @node_timing\n    def start_node(self, state):\n        time.sleep(1)\n        return {\"temp_response\": \"yes\"}\n\n    @node_timing\n    def node_1(self, state):\n        time.sleep(1)\n        return {\"temp_response\": \"no\"}\n\n    @node_timing\n    def node_2(self, state):\n        time.sleep(1)\n        return {\"temp_response\": \"no\"}\n\n    @node_timing\n    def node_3(self, state):\n        time.sleep(1)\n        return {\"temp_response\": \"no\"}\n\n    @node_timing\n    def node_4(self, state):\n        time.sleep(1)\n        return {\"temp_response\": \"no\"}\n\n    @node_timing\n    def end_node(self, state):\n        time.sleep(1)\n        return {\"final_response\": \"the graph is now finished\"}\n\n    @node_timing\n    def get_workflow(self):\n        workflow = StateGraph(GraphState)\n\n        workflow.add_node(\"start_node\", self.start_node)\n        workflow.add_node(\"node_1\", self.start_node)\n        workflow.add_node(\"node_2\", self.start_node)\n        workflow.add_node(\"node_3\", self.start_node)\n        workflow.add_node(\"node_4\", self.start_node)\n        workflow.add_node(\"end_node\", self.start_node)\n\n        workflow.set_entry_point(\"start_node\")\n        workflow.add_edge(\"start_node\", \"node_1\")\n        workflow.add_edge(\"node_1\", \"node_2\")\n        workflow.add_edge(\"node_2\", \"node_3\")\n        workflow.add_edge(\"node_3\", \"node_4\")\n        workflow.add_edge(\"node_4\", \"end_node\")\n        workflow.add_edge(\"end_node\", END)\n\n        self.app_workflow = workflow.compile()\n\n    @node_timing\n    def get_answer(self, query):\n\n        output = self.app_workflow.invoke({\"query\": query})\n\n        return output\n\nf = Flow()\nf.get_workflow()\n\nf.get_answer(\"why\")\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nThis is a tricky issue to reproduce because it only happens on certain systems and greatly depends on how the python process is started.\nIn the example provided, I look at the time between one node finishing, and the next node starting. On my local machine, and inside a DataBricks notebook, this is between 5 and 50 milliseconds, but when I run it on our DataBricks server directly, this is between 400 and 700 milliseconds. In a single run it is very consistently the same time, but it differs between runs.\nThis delay happens between each node, so for a flow with a useful number of nodes, it adds up to a 10 seconds delay which is prohibitively slow in a chat application.\nI am not able to root-cause the issue because I do not understand what LangGraph is doing between nodes, especially for such a simple flow as you see in this example. I do see a lot of asynchronous functions so I suspect there is some kind of race condition or wait that takes this time to resolve.\nAll timing experiments were done on a dedicated cloud compute cluster with no other processes running.\nSystem Info\nDataBricks Standard_DS3_v2 computer cluster (14 GB Memory, 4 cores)\nDatabRicks runtime version: 14.3 LTS ML (Apache Spark 3.5.0, Scala 2.12)\nOS: Linux #76-20-01-1-Ubuntu SMP\nPython: 3.10.12\nlangchain_core: 0.2.28\nlangchain: 0.0.348\nlangsmith: 0.1.98\nlanggraph: 0.2.0", "created_at": "2024-08-07", "closed_at": null, "labels": [], "State": "open", "Author": "vdhelm"}
{"issue_number": 1262, "issue_title": "SQLiteSaver fails to pass type checking at graph compilation", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom typing import List\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import END, StateGraph, START\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.checkpoint.sqlite import SqliteSaver\n\nworkflow = StateGraph(dict)\n\n# this works:\nworkflow.compile(checkpointer=MemorySaver())\n\n# this doesn't work:\nworkflow.compile(checkpointer=SqliteSaver.from_conn_string(\":memory:\"))\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\nCell In[28], line 1\n----> 1 workflow.compile(checkpointer=SqliteSaver.from_conn_string(\":memory:\"))\n\nFile ~/workspace/code/me/notebooks/.venv/lib/python3.11/site-packages/langgraph/graph/state.py:431, in StateGraph.compile(self, checkpointer, interrupt_before, interrupt_after, debug)\n    411 output_channels = (\n    412     \"__root__\"\n    413     if len(self.schemas[self.output]) == 1\n   (...)\n    419     ]\n    420 )\n    421 stream_channels = (\n    422     \"__root__\"\n    423     if len(self.channels) == 1 and \"__root__\" in self.channels\n   (...)\n    428     ]\n    429 )\n--> 431 compiled = CompiledStateGraph(\n    432     builder=self,\n    433     config_type=self.config_schema,\n    434     nodes={},\n    435     channels={**self.channels, START: EphemeralValue(self.input)},\n    436     input_channels=START,\n    437     stream_mode=\"updates\",\n    438     output_channels=output_channels,\n    439     stream_channels=stream_channels,\n    440     checkpointer=checkpointer,\n    441     interrupt_before_nodes=interrupt_before,\n    442     interrupt_after_nodes=interrupt_after,\n    443     auto_validate=False,\n    444     debug=debug,\n    445 )\n    447 compiled.attach_node(START, None)\n    448 for key, node in self.nodes.items():\n\nFile ~/workspace/code/me/notebooks/.venv/lib/python3.11/site-packages/pydantic/v1/main.py:341, in BaseModel.__init__(__pydantic_self__, **data)\n    339 values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)\n    340 if validation_error:\n--> 341     raise validation_error\n    342 try:\n    343     object_setattr(__pydantic_self__, '__dict__', values)\n\nValidationError: 1 validation error for CompiledStateGraph\ncheckpointer\n  instance of BaseCheckpointSaver expected (type=type_error.arbitrary_type; expected_arbitrary_type=BaseCheckpointSaver)\nDescription\nlangchain==0.2.9\nlangchain-community==0.2.7\nlangchain-core==0.2.28\nlangchain-experimental==0.0.62\nlangchain-text-splitters==0.2.2\nlangdetect==1.0.9\nlanggraph==0.2.1\nlanggraph-checkpoint==1.0.2\nlanggraph-checkpoint-sqlite==1.0.0\nlangsmith==0.1.85\nSystem Info\nlangchain==0.2.9\nlangchain-community==0.2.7\nlangchain-core==0.2.28\nlangchain-experimental==0.0.62\nlangchain-text-splitters==0.2.2\nlangdetect==1.0.9\nlanggraph==0.2.1\nlanggraph-checkpoint==1.0.2\nlanggraph-checkpoint-sqlite==1.0.0\nlangsmith==0.1.85\nplatform: linux\npython: 3.9.18\nSystem Information\n\nOS:  Linux\nOS Version:  #15-Ubuntu SMP PREEMPT_DYNAMIC Tue Jan  9 17:03:36 UTC 2024\nPython Version:  3.9.18 (main, May 16 2024, 00:00:00)\n[GCC 11.4.1 20231218 (Red Hat 11.4.1-3.0.1)]\n\nPackage Information\n\nlangchain_core: 0.2.29\nlangchain: 0.2.7\nlangchain_community: 0.2.7\nlangsmith: 0.1.90\nlangchain_text_splitters: 0.2.2\nlanggraph: 0.2.1\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.9.5\nasync-timeout: 4.0.3\ndataclasses-json: 0.6.7\njsonpatch: 1.33\nlanggraph-checkpoint: 1.0.2\nnumpy: 1.26.4\norjson: 3.10.6\npackaging: 24.1\npydantic: 2.8.2\nPyYAML: 6.0.1\nrequests: 2.32.3\nSQLAlchemy: 2.0.31\ntenacity: 8.5.0\ntyping-extensions: 4.12.2\n[root@colima src]#\n", "created_at": "2024-08-07", "closed_at": "2024-08-07", "labels": [], "State": "closed", "Author": "streamnsight"}
{"issue_number": 1253, "issue_title": "SqliteSaver and AsyncSqliteSaver missing in 0.2.1", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nError Message and Stack Trace (if applicable)\nerror shows, both of these packages not available\nDescription\ncompletely uninstalled previous version of langgraph, 0.2.19\nreinstalled / upgraded to 0.2.21\nhere are other packages:\nlangchain 0.2.12\nlangchain-chroma 0.1.2\nlangchain-community 0.2.11\nlangchain-core 0.2.28\nlangchain-google-vertexai 1.0.8\nlangchain-huggingface 0.0.3\nlangchain-openai 0.1.20\nlangchain-text-splitters 0.2.2\nlanggraph 0.2.1\nlanggraph-checkpoint 1.0.2\nboth of AsyncSqliteSaver and SqliteSaver imports are not available\nSystem Info\nlangchain 0.2.12\nlangchain-chroma 0.1.2\nlangchain-community 0.2.11\nlangchain-core 0.2.28\nlangchain-google-vertexai 1.0.8\nlangchain-huggingface 0.0.3\nlangchain-openai 0.1.20\nlangchain-text-splitters 0.2.2\nlanggraph 0.2.1\nlanggraph-checkpoint 1.0.2", "created_at": "2024-08-07", "closed_at": "2024-08-07", "labels": [], "State": "closed", "Author": "HiraveBapu"}
{"issue_number": 1252, "issue_title": "Graph display functionality not working when subgraph is nested", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom operator import add\nfrom typing import List, TypedDict, Optional, Annotated, Dict\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph, START, END\n\n\n# The structure of the logs\nclass Logs(TypedDict):\n    id: str\n    question: str\n    docs: Optional[List]\n    answer: str\n    grade: Optional[int]\n    grader: Optional[str]\n    feedback: Optional[str]\n\n\n# Failure Analysis Sub-graph\nclass FailureAnalysisState(TypedDict):\n    docs: List[Logs]\n    failures: List[Logs]\n    fa_summary: str\n\n\ndef get_failures(state):\n    docs = state[\"docs\"]\n    failures = [doc for doc in docs if \"grade\" in doc]\n    return {\"failures\": failures}\n\n\ndef generate_summary(state):\n    failures = state[\"failures\"]\n    # Add fxn: fa_summary = summarize(failures)\n    fa_summary = \"Poor quality retrieval of Chroma documentation.\"\n    return {\"fa_summary\": fa_summary}\n\n\nfa_builder = StateGraph(FailureAnalysisState)\nfa_builder.add_node(\"get_failures\", get_failures)\nfa_builder.add_node(\"generate_summary\", generate_summary)\nfa_builder.add_edge(START, \"get_failures\")\nfa_builder.add_edge(\"get_failures\", \"generate_summary\")\nfa_builder.add_edge(\"generate_summary\", END)\n\n\n# Summarization subgraph\nclass QuestionSummarizationState(TypedDict):\n    docs: List[Logs]\n    qs_summary: str\n    report: str\n\n\ndef generate_summary(state):\n    docs = state[\"docs\"]\n    # Add fxn: summary = summarize(docs)\n    summary = \"Questions focused on usage of ChatOllama and Chroma vector store.\"\n    return {\"qs_summary\": summary}\n\n\ndef send_to_slack(state):\n    qs_summary = state[\"qs_summary\"]\n    # Add fxn: report = report_generation(qs_summary)\n    report = \"foo bar baz\"\n    return {\"report\": report}\n\n\ndef format_report_for_slack(state):\n    report = state[\"report\"]\n    # Add fxn: formatted_report = report_format(report)\n    formatted_report = \"foo bar\"\n    return {\"report\": formatted_report}\n\n\nqs_builder = StateGraph(QuestionSummarizationState)\nqs_builder.add_node(\"generate_summary\", generate_summary)\nqs_builder.add_node(\"send_to_slack\", send_to_slack)\nqs_builder.add_node(\"format_report_for_slack\", format_report_for_slack)\nqs_builder.add_edge(START, \"generate_summary\")\nqs_builder.add_edge(\"generate_summary\", \"send_to_slack\")\nqs_builder.add_edge(\"send_to_slack\", \"format_report_for_slack\")\nqs_builder.add_edge(\"format_report_for_slack\", END)\n\n# Dummy logs\nquestion_answer = Logs(\n    id=\"1\",\n    question=\"How can I import ChatOllama?\",\n    answer=\"To import ChatOllama, use: 'from langchain_community.chat_models import ChatOllama.'\",\n)\n\nquestion_answer_feedback = Logs(\n    id=\"2\",\n    question=\"How can I use Chroma vector store?\",\n    answer=\"To use Chroma, define: rag_chain = create_retrieval_chain(retriever, question_answer_chain).\",\n    grade=0,\n    grader=\"Document Relevance Recall\",\n    feedback=\"The retrieved documents discuss vector stores in general, but not Chroma specifically\",\n)\n\n\n# Entry Graph\nclass EntryGraphState(TypedDict):\n    raw_logs: Annotated[List[Dict], add]\n    docs: Annotated[List[Logs], add]  # This will be used in sub-graphs\n    fa_summary: str  # This will be generated in the FA sub-graph\n    report: str  # This will be generated in the QS sub-graph\n\n\ndef convert_logs_to_docs(state):\n    # Get logs\n    raw_logs = state[\"raw_logs\"]\n    docs = [question_answer, question_answer_feedback]\n    return {\"docs\": docs}\n\n\nentry_builder = StateGraph(EntryGraphState)\nentry_builder.add_node(\"convert_logs_to_docs\", convert_logs_to_docs)\nentry_builder.add_node(\"question_summarization\", qs_builder.compile())\nentry_builder.add_node(\"failure_analysis\", fa_builder.compile())\n\nentry_builder.add_edge(START, \"convert_logs_to_docs\")\nentry_builder.add_edge(\"convert_logs_to_docs\", \"failure_analysis\")\nentry_builder.add_edge(\"convert_logs_to_docs\", \"question_summarization\")\nentry_builder.add_edge(\"failure_analysis\", END)\nentry_builder.add_edge(\"question_summarization\", END)\n\ngraph = entry_builder.compile()\n\nfrom IPython.display import Image, display\n\n# Setting xray to 1 will show the internal structure of the nested graph\ndisplay(Image(graph.get_graph(xray=1).draw_mermaid_png()))\nError Message and Stack Trace (if applicable)\nTypeError                                 Traceback (most recent call last)\nCell In[5], line 49\n     46 from IPython.display import Image, display\n     48 # Setting xray to 1 will show the internal structure of the nested graph\n---> 49 display(Image(graph.get_graph(xray=1).draw_mermaid_png()))\n\nFile /opt/anaconda3/envs/py310/lib/python3.10/site-packages/langgraph/graph/graph.py:457, in CompiledGraph.get_graph(self, config, xray)\n    455 subgraph.trim_last_node()\n    456 if len(subgraph.nodes) > 1:\n--> 457     end_nodes[key], start_nodes[key] = graph.extend(\n    458         subgraph, prefix=key\n    459     )\n    460 else:\n    461     n = graph.add_node(node, key)\n\nFile /opt/anaconda3/envs/py310/lib/python3.10/site-packages/langgraph/utils.py:135, in DrawableGraph.extend(self, graph, prefix)\n    132     super().extend(graph)\n    133     return graph.first_node(), graph.last_node()\n--> 135 new_nodes = {\n    136     f\"{prefix}:{k}\": Node(f\"{prefix}:{k}\", v.data)\n    137     for k, v in graph.nodes.items()\n    138 }\n    139 new_edges = [\n    140     Edge(\n...\n    146     for edge in graph.edges\n    147 ]\n    148 self.nodes.update(new_nodes)\n\nTypeError: Node.__new__() missing 2 required positional arguments: 'data' and 'metadata'\nDescription\nI'm trying to get LangGraph to return an image of the graph with a subgraph within, but it throws the error displayed above. The code is as seen in the cookbook example on the website. The subgraph works fine, it is just the image displayer that fails.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.4.0: Wed Feb 21 21:44:54 PST 2024; root:xnu-10063.101.15~2/RELEASE_ARM64_T6030\nPython Version:  3.10.14 (main, May  6 2024, 14:42:37) [Clang 14.0.6 ]\n\nPackage Information\n\nlangchain_core: 0.2.23\nlangchain: 0.2.3\nlangchain_community: 0.2.4\nlangsmith: 0.1.75\nlangchain_chroma: 0.1.2\nlangchain_experimental: 0.0.60\nlangchain_ibm: 0.1.10\nlangchain_ollama: 0.1.0\nlangchain_openai: 0.1.8\nlangchain_text_splitters: 0.2.1\nlangchainhub: 0.1.20\nlanggraph: 0.0.66\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlangserve\n", "created_at": "2024-08-07", "closed_at": "2024-08-07", "labels": [], "State": "closed", "Author": "at421"}
{"issue_number": 1251, "issue_title": "ToolNode is not seting  the `status` field of the ToolMessage", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nimport requests\nfrom langgraph.prebuilt import ToolNode\nfrom langchain_core.messages import AIMessage\n\ndef binance(symbol: str):\n    \"\"\"This tool get the symbol information from binance\"\"\"\n    raise Exception(\"Network crashed !!!\")\n\n\nToolNode([binance]).invoke({\"messages\": [\nAIMessage(content=[{'type': 'tool_use', 'name': 'binance', 'input': {'symbol': 'BTCBRL'}, 'id': 'tooluse_82v7QpgYQv6uL2stXaFaQw'}], response_metadata={'ResponseMetadata': {'RequestId': '2bad89c2-9451-41ce-a360-221509ee434f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 07 Aug 2024 13:03:07 GMT', 'content-type': 'application/json', 'content-length': '275', 'connection': 'keep-alive', 'x-amzn-requestid': '2bad89c2-9451-41ce-a360-221509ee434f'}, 'RetryAttempts': 0}, 'stopReason': 'tool_use', 'metrics': {'latencyMs': 748}}, id='run-808b9eb2-887d-43ab-addd-b77d37ecb026-0', tool_calls=[{'name': 'binance', 'args': {'symbol': 'BTCBRL'}, 'id': 'tooluse_82v7QpgYQv6uL2stXaFaQw', 'type': 'tool_call'}], usage_metadata={'input_tokens': 337, 'output_tokens': 56, 'total_tokens': 393})\n    ]})\nError Message and Stack Trace (if applicable)\n{'messages': [ToolMessage(content=\"Error: Exception('Network crashed !!!')\\n Please fix your mistakes.\", name='binance', tool_call_id='tooluse_82v7QpgYQv6uL2stXaFaQw')]}\nDescription\nWhen a tool throws an exception, currently  ToolNode doesn't set the status field of the ToolMessage\nSystem Info\nplatform: linux\npoetry show:\naiohappyeyeballs         2.3.5                                                                               Happy Eyeballs for asyncio\naiohttp                  3.10.1                                                                              Async http client/server framework (asyncio)\naiosignal                1.3.1                                                                               aiosignal: a list of registered asynchronous callbacks\nannotated-types          0.7.0                                                                               Reusable constraint types to use with typing.Annotated\nanthropic                0.32.0                                                                              The official Python library for the anthropic API\nanyio                    4.4.0                                                                               High level compatibility layer for multiple asynchronous event loop implementations\nasgiref                  3.8.1                                                                               ASGI specs, helper code, and adapters\nasttokens                2.4.1                                                                               Annotate AST trees with source code positions\nattrs                    24.2.0                                                                              Classes Without Boilerplate\nawswrangler              3.9.0                                                                               Pandas on AWS.\nbackoff                  2.2.1                                                                               Function decoration for backoff and retry\nblinker                  1.8.2                                                                               Fast, simple object-to-object and broadcast signaling\nboto3                    1.34.155                                                                            The AWS SDK for Python\nbotocore                 1.34.155                                                                            Low-level, data-driven core of boto 3.\ncertifi                  2024.7.4                                                                            Python package for providing Mozilla's CA Bundle.\ncharset-normalizer       3.3.2                                                                               The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet.\nclick                    8.1.7                                                                               Composable command line interface toolkit\ncohere                   5.6.2\ncomm                     0.2.2                                                                               Jupyter Python Comm implementation, for usage in ipykernel, xeus-python etc.\ndataclasses-json         0.6.7                                                                               Easily serialize dataclasses to and from JSON.\ndebugpy                  1.8.5                                                                               An implementation of the Debug Adapter Protocol for Python\ndecorator                5.1.1                                                                               Decorators for Humans\ndistro                   1.9.0                                                                               Distro - an OS platform information API\net-xmlfile               1.1.0                                                                               An implementation of lxml.xmlfile for the standard library\nexecuting                2.0.1                                                                               Get the currently executing AST node of a frame, and other information\nfastavro                 1.9.5                                                                               Fast read/write of AVRO files\nfilelock                 3.15.4                                                                              A platform independent file lock.\nflask                    3.0.3                                                                               A simple framework for building complex web applications.\nfrozenlist               1.4.1                                                                               A list-like structure which implements collections.abc.MutableSequence\nfsspec                   2024.6.1                                                                            File-system specification\ngql                      3.5.0                                                                               GraphQL client for Python\ngraphql-core             3.2.3                                                                               GraphQL implementation for Python, a port of GraphQL.js, the JavaScript reference implementation for GraphQL.\ngreenlet                 3.0.3                                                                               Lightweight in-process concurrent programming\nh11                      0.14.0                                                                              A pure-Python, bring-your-own-I/O implementation of HTTP/1.1\nhttpcore                 1.0.5                                                                               A minimal low-level HTTP client.\nhttpx                    0.27.0                                                                              The next generation HTTP client.\nhttpx-sse                0.4.0                                                                               Consume Server-Sent Event (SSE) messages with HTTPX.\nhuggingface-hub          0.24.5                                                                              Client library to download and publish models, datasets and other repos on the huggingface.co hub\nidna                     3.7                                                                                 Internationalized Domain Names in Applications (IDNA)\nipykernel                6.29.5                                                                              IPython Kernel for Jupyter\nipython                  8.26.0                                                                              IPython: Productive Interactive Computing\nitsdangerous             2.2.0                                                                               Safely pass data to untrusted environments and back.\njedi                     0.19.1                                                                              An autocompletion tool for Python that can be used for text editors.\njinja2                   3.1.4                                                                               A very fast and expressive template engine.\njiter                    0.5.0                                                                               Fast iterable JSON parser.\njmespath                 1.0.1                                                                               JSON Matching Expressions\njsonpatch                1.33                                                                                Apply JSON-Patches (RFC 6902)\njsonpointer              3.0.0                                                                               Identify specific nodes in a JSON document (RFC 6901)\njupyter-client           8.6.2                                                                               Jupyter protocol implementation and client libraries\njupyter-core             5.7.2                                                                               Jupyter core package. A base package on which Jupyter projects rely.\nlangchain                0.2.12                                                                              Building applications with LLMs through composability\nlangchain-aws            0.1.15 ../../../../../../../mnt/secondary/thiago/github/langchain-aws-fork/libs/aws An integration package connecting AWS and LangChain\nlangchain-cohere         0.1.9                                                                               An integration package connecting Cohere and LangChain\nlangchain-community      0.2.11                                                                              Community contributed LangChain integrations.\nlangchain-core           0.2.28                                                                              Building applications with LLMs through composability\nlangchain-experimental   0.0.64                                                                              Building applications with LLMs through composability\nlangchain-openai         0.1.20                                                                              An integration package connecting OpenAI and LangChain\nlangchain-postgres       0.0.9                                                                               An integration package connecting Postgres and LangChain\nlangchain-text-splitters 0.2.2                                                                               LangChain text splitting utilities\nlangchainhub             0.1.20                                                                              The LangChain Hub API client\nlanggraph                0.2.1                                                                               Building stateful, multi-actor applications with LLMs\nlanggraph-checkpoint     1.0.2                                                                               Library with base interfaces for LangGraph checkpoint savers.\nlangsmith                0.1.98                                                                              Client library to connect to the LangSmith LLM Tracing and Evaluation Platform.\nmarkupsafe               2.1.5                                                                               Safely add untrusted strings to HTML/XML markup.\nmarshmallow              3.21.3                                                                              A lightweight library for converting complex datatypes to and from native Python datatypes.\nmatplotlib-inline        0.1.7                                                                               Inline Matplotlib backend for Jupyter\nmultidict                6.0.5                                                                               multidict implementation\nmypy-extensions          1.0.0                                                                               Type system extensions for programs checked with the mypy type checker.\nnest-asyncio             1.6.0                                                                               Patch asyncio to allow nested event loops\nnumpy                    1.26.4                                                                              Fundamental package for array computing in Python\nopenai                   1.40.0                                                                              The official Python library for the openai API\nopenpyxl                 3.1.5                                                                               A Python library to read/write Excel 2010 xlsx/xlsm files\norjson                   3.10.6                                                                              Fast, correct Python JSON library supporting dataclasses, datetimes, and numpy\npackaging                24.1                                                                                Core utilities for Python packages\npandas                   2.2.2                                                                               Powerful data structures for data analysis, time series, and statistics\nparameterized            0.9.0                                                                               Parameterized testing with any Python test framework\nparso                    0.8.4                                                                               A Python Parser\npexpect                  4.9.0                                                                               Pexpect allows easy control of interactive console applications.\npgvector                 0.2.5                                                                               pgvector support for Python\nplatformdirs             4.2.2                                                                               A small Python package for determining appropriate platform-specific dirs, e.g. a user data dir.\nprompt-toolkit           3.0.47                                                                              Library for building powerful interactive command lines in Python\npsutil                   6.0.0                                                                               Cross-platform lib for process and system monitoring in Python.\npsycopg                  3.2.1                                                                               PostgreSQL database adapter for Python\npsycopg-pool             3.2.2                                                                               Connection Pool for Psycopg\npsycopg2-binary          2.9.9                                                                               psycopg2 - Python-PostgreSQL Database Adapter\nptyprocess               0.7.0                                                                               Run a subprocess in a pseudo terminal\npure-eval                0.2.3                                                                               Safely evaluate AST nodes without side effects\npyarrow                  17.0.0                                                                              Python library for Apache Arrow\npydantic                 2.8.2                                                                               Data validation using Python type hints\npydantic-core            2.20.1                                                                              Core functionality for Pydantic validation and serialization\npygments                 2.18.0                                                                              Pygments is a syntax highlighting package written in Python.\npython-dateutil          2.9.0.post0                                                                         Extensions to the standard Python datetime module\npython-dotenv            1.0.1                                                                               Read key-value pairs from a .env file and set them as environment variables\npython-json-logger       2.0.7                                                                               A python library adding a json log formatter\npytz                     2024.1                                                                              World timezone definitions, modern and historical\npywa                     1.24.0                                                                              Python wrapper for the WhatsApp Cloud API\npyyaml                   6.0.2                                                                               YAML parser and emitter for Python\npyzmq                    26.1.0                                                                              Python bindings for 0MQ\nredis                    5.0.8                                                                               Python client for Redis database and key-value store\nregex                    2024.7.24                                                                           Alternative regular expression module, to replace re.\nrequests                 2.32.3                                                                              Python HTTP for Humans.\nrequests-aws4auth        1.3.1                                                                               AWS4 authentication for Requests\nrequests-toolbelt        1.0.0                                                                               A utility belt for advanced users of python-requests\ns3transfer               0.10.2                                                                              An Amazon S3 Transfer Manager\nsetuptools               72.1.0                                                                              Easily download, build, install, upgrade, and uninstall Python packages\nsix                      1.16.0                                                                              Python 2 and 3 compatibility utilities\nsniffio                  1.3.1                                                                               Sniff out which async library your code is running under\nsqlalchemy               2.0.32                                                                              Database Abstraction Library\nstack-data               0.6.3                                                                               Extract data from python stack frames and tracebacks for informative displays\ntabulate                 0.9.0                                                                               Pretty-print tabular data\ntavily-python            0.3.5                                                                               Python wrapper for the Tavily API\ntenacity                 8.5.0                                                                               Retry code until it succeeds\ntiktoken                 0.7.0                                                                               tiktoken is a fast BPE tokeniser for use with OpenAI's models\ntokenizers               0.19.1\ntornado                  6.4.1                                                                               Tornado is a Python web framework and asynchronous networking library, originally developed at FriendFeed.\ntqdm                     4.66.5                                                                              Fast, Extensible Progress Meter\ntraitlets                5.14.3                                                                              Traitlets Python configuration system\ntypes-requests           2.32.0.20240712                                                                     Typing stubs for requests\ntyping-extensions        4.12.2                                                                              Backported and Experimental Type Hints for Python 3.8+\ntyping-inspect           0.9.0                                                                               Runtime inspection utilities for typing module.\ntzdata                   2024.1                                                                              Provider of IANA time zone data\nurllib3                  2.2.2                                                                               HTTP library with thread-safe connection pooling, file post, and more.\nwcwidth                  0.2.13                                                                              Measures the displayed width of unicode strings in a terminal\nwebsockets               11.0.3                                                                              An implementation of the WebSocket Protocol (RFC 6455 & 7692)\nwerkzeug                 3.0.3                                                                               The comprehensive WSGI web application library.\nyarl                     1.9.4                                                                               Yet another URL library", "created_at": "2024-08-07", "closed_at": "2024-10-24", "labels": [], "State": "closed", "Author": "thiagotps"}
{"issue_number": 1248, "issue_title": "MemorySaver is missing", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langgraph.checkpoint.memory import MemorySaver\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nthe MemorySaver is missing\nSystem Info\nlanggraph 0.2.0", "created_at": "2024-08-07", "closed_at": "2024-08-07", "labels": [], "State": "closed", "Author": "cjdxhjj"}
{"issue_number": 1241, "issue_title": "DOC: Passing State in Tools is no longer supported", "issue_body": "Issue with current documentation:\nhttps://langchain-ai.github.io/langgraph/how-tos/pass-run-time-values-to-tools/\nthis no longer works.\nI wanted to point it out but also I'd like to know what is the alternative.\nRight now I have been passing config to tools, but this is not ideal.\nIs there a new way of passing state to tools? Thanks\nIdea or request for content:\nNo response", "created_at": "2024-08-06", "closed_at": "2024-08-07", "labels": [], "State": "closed", "Author": "streamnsight"}
{"issue_number": 1225, "issue_title": "Bug: create_react_agent doesn't actually take in a ToolExecutor", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom typing import Literal\n\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt.tool_executor import ToolExecutor\nfrom langchain_core.tools import tool\nfrom langgraph.prebuilt import create_react_agent\n\nopenai_api_key = \"sk-...\"\n\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0, api_key=openai_api_key)\n\n\n@tool\ndef get_weather(city: Literal[\"nyc\", \"sf\"]):\n    \"\"\"Use this to get weather information.\"\"\"\n    if city == \"nyc\":\n        return \"It might be cloudy in nyc\"\n    elif city == \"sf\":\n        return \"It's always sunny in sf\"\n    else:\n        raise AssertionError(\"Unknown city\")\n\n\n# this works fine\ntools = [get_weather]\n# this does not\ntools = ToolExecutor([get_weather])\n\n\n# Define the graph\n\n\ngraph = create_react_agent(\n    model,\n    tools=tools,\n)\n\n\ndef print_stream(stream):\n    for s in stream:\n        message = s[\"messages\"][-1]\n        if isinstance(message, tuple):\n            print(message)\n        else:\n            message.pretty_print()\n\n\nmessage = \"What is the weather in SF and new york?\"\n\n\ninputs = {\n    \"messages\": [\n        (\"user\", message),\n    ]\n}\nprint_stream(graph.stream(inputs, stream_mode=\"values\"))\nError Message and Stack Trace (if applicable)\ngraph = create_react_agent(\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/sevin/lib/python3.12/site-packages/langgraph/_api/deprecation.py\", line 54, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/sevin/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py\", line 611, in create_react_agent\n    workflow.add_node(\"tools\", ToolNode(tools))\n                               ^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/sevin/lib/python3.12/site-packages/langgraph/prebuilt/tool_node.py\", line 79, in __init__\n    for tool_ in tools:\nTypeError: 'ToolExecutor' object is not iterable\nDescription\nIn the type hints for create_react_agent, it says that tools can be passed in as a ToolExecutor, but in practice this does not work.\nSystem Info\nlangchain==0.2.1\nlangchain-anthropic==0.1.16\nlangchain-community==0.2.1\nlangchain-core==0.2.23\nlangchain-elasticsearch==0.2.1\nlangchain-mistralai==0.1.8\nlangchain-openai==0.1.8\nlangchain-text-splitters==0.2.0\nlangchainhub==0.1.17", "created_at": "2024-08-05", "closed_at": "2024-08-07", "labels": [], "State": "closed", "Author": "mschoenb97IL"}
{"issue_number": 1603, "issue_title": "Issue with Pydantic", "issue_body": "1/ a graph state key is a Pydantic model\nclass Analyst(BaseModel):\ndescription: str = Field(\n2/ i edit it in Studio\n3/ edited state is no long a Pydantic model (converted to dict after edit in Studio, IIUC)\n4/ anything in code that assumes Pydantic model will then break w/ the edited state\ne.g., accessing one attribute -\nanalyst.description\nerror -\nAttributeError: 'dict' object has no attribute 'description'", "created_at": "2024-08-05", "closed_at": null, "labels": ["bug", "maintainer"], "State": "open", "Author": "hwchase17"}
{"issue_number": 1222, "issue_title": "How to interrupt a subgraph, and insert a message, and then rerun graph?", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nimport uuid\nfrom typing import Annotated, Literal, TypedDict\n\nfrom dotenv import load_dotenv\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.checkpoint import BaseCheckpointSaver, MemorySaver\nfrom langgraph.graph import END, START, StateGraph, add_messages\nfrom langgraph.graph.state import CompiledStateGraph\n\nload_dotenv()\n\n\nclass State(TypedDict, total=False):\n    messages: Annotated[list, add_messages]\n    reply: str\n\n\ndef print_node(state: State, config: RunnableConfig) -> State:\n    curr_node = config[\"metadata\"][\"langgraph_node\"]\n    print(f\"This Node is '{curr_node}'\")\n    if curr_node == \"1_A\":\n        return {\"messages\": [(\"ai\", \"Do you want to go to B or C?\")]}\n    return {\"messages\": [(\"ai\", curr_node)]}\n\n\ndef go_to_b_or_c(state: State) -> Literal[\"B\", \"C\"]:\n    reply = state[\"reply\"]\n    if reply == \"B\":\n        return \"B\"\n    if reply == \"C\":\n        return \"C\"\n    raise ValueError(\"Invalid input\")\n\n\ndef create_sub_graph(\n    i: int, checkpointer: BaseCheckpointSaver | None = None\n) -> CompiledStateGraph:\n    A = f\"{i}_A\"\n    REPLY = f\"{i}_reply\"\n    B = f\"{i}_B\"\n    C = f\"{i}_C\"\n\n    builder = StateGraph(State)\n\n    builder.add_node(A, print_node)\n    builder.add_node(REPLY, lambda state: {\"reply\": state[\"messages\"][-1].content})\n    builder.add_node(B, print_node)\n    builder.add_node(C, print_node)\n\n    builder.add_edge(START, A)\n    builder.add_edge(A, REPLY)\n    builder.add_conditional_edges(REPLY, go_to_b_or_c, {\"B\": B, \"C\": C})\n    builder.add_edge(B, END)\n    builder.add_edge(C, END)\n\n    return builder.compile(interrupt_before=[REPLY], checkpointer=checkpointer)\n\n\ndef run_graph(graph: CompiledStateGraph) -> None:\n    config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n    for event in graph.stream({\"messages\": []}, config=config, stream_mode=\"values\"):\n        print(event)\n        \n    print(f\"{'-' * 20} interrupt {'-' * 20}\")\n    graph.update_state(config, {\"messages\": [(\"human\", \"B\")]})\n    \n    for event in graph.stream(None, config=config, stream_mode=\"values\"):\n        print(event)\n\n\nif __name__ == \"__main__\":\n    # check subgraph can run\n    run_graph(create_sub_graph(1, MemorySaver()))\n\n    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n\n    main_builder = StateGraph(State)\n    main_builder.add_node(\"entry\", lambda _: {\"messages\": [(\"ai\", \"main_entry\")]})\n    main_builder.add_node(\"sub_app_1\", create_sub_graph(1))\n    main_builder.add_node(\"exit\", lambda _: {\"messages\": [(\"ai\", \"main_exit\")]})\n\n    main_builder.add_edge(START, \"entry\")\n    main_builder.add_edge(\"entry\", \"sub_app_1\")\n    main_builder.add_edge(\"sub_app_1\", \"exit\")\n    main_builder.add_edge(\"exit\", END)\n\n    app = main_builder.compile(checkpointer=MemorySaver())\n    # app.get_graph(xray=1).draw_mermaid_png(output_file_path=\"example.png\")\n    run_graph(app)\nStdout\n{'messages': []}\nThis Node is '1_A'\n{'messages': [AIMessage(content='Do you want to go to B or C?', id='6dc577cb-2eed-4bb9-8423-bacebe1330a5')]}\n-------------------- interrupt --------------------\n{'messages': [AIMessage(content='Do you want to go to B or C?', id='6dc577cb-2eed-4bb9-8423-bacebe1330a5'), HumanMessage(content='B', id='b6a7b5a1-a146-47a9-b282-1cd133dd8dfb')], 'reply': 'B'}\nThis Node is '1_B'\n{'messages': [AIMessage(content='Do you want to go to B or C?', id='6dc577cb-2eed-4bb9-8423-bacebe1330a5'), HumanMessage(content='B', id='b6a7b5a1-a146-47a9-b282-1cd133dd8dfb'), AIMessage(content='1_B', id='49b28ebc-e7b3-4d84-ae48-99a7b536fad7')], 'reply': 'B'}\n\n==================================================\n\n{'messages': []}\n{'messages': [AIMessage(content='main_entry', id='380f46da-651d-47e0-a19e-48d298914ab8')]}\nThis Node is '1_A'\n-------------------- interrupt --------------------\nThis Node is '1_A'\n\nDescription\nI have already read How to create subgraphs .\nBut I don't know how to correctly use interrupt.\n\n\u2b07\ufe0f interrupt before\n\n\u2b07\ufe0f interrupt after\n\nIs this the expected behavior?\nI'm not sure how the subgraph should operate correctly. Does it need to insert the same or a different checkpointer?\nOr will it automatically use the parent's checkpointer?\nWhich checkpointer do I need to insert new information into?\nSystem Info\nlanggraph==0.1.19\nlangchain==0.2.12\nlangchain-core==0.2.28\nplatform==linux\npython-version==3.10.12", "created_at": "2024-08-05", "closed_at": "2024-09-04", "labels": [], "State": "closed", "Author": "gbaian10"}
{"issue_number": 1190, "issue_title": "Agent always calling tool even if not needed", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nresult =app.invoke(\n   {\"messages\": [HumanMessage(content=\"what is your name?\")]}, stream_mode=False)\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nThe following prompt that the model receives: \"Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\\n\\nRespond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}. Do not use variables.\" forces the agent to use a tool, even if I just send hello to the agent. It should say \"if you require to use a tool\" or something to make it optional\nSystem Info\nlangchain==0.2.11\nlangchain-community==0.2.10\nlangchain-core==0.2.24\nlangchain-experimental==0.0.63\nlangchain-ollama==0.1.0\nlangchain-text-splitters==0.2.2\nlangchainhub==0.1.20\nWindows\npython 3.11.7", "created_at": "2024-08-01", "closed_at": "2024-08-01", "labels": [], "State": "closed", "Author": "tomaszbk"}
{"issue_number": 1182, "issue_title": "Missing checkpoint package in LangGraph v0.1.18", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langgraph.checkpoint.memory import MemorySaver\nError Message and Stack Trace (if applicable)\nerror: \n\nfrom langgraph.checkpoint.memory import MemorySaver\nModuleNotFoundError: No module named 'langgraph.checkpoint'\nDescription\nMissing checkpoint package in LangGraph v0.1.18\nDescription\nIn the latest version of LangGraph (v0.1.18), the checkpoint package appears to be missing. This package was present in previous versions and is crucial for certain functionalities.\nExpected Behavior\nThe checkpoint package should be available in the latest version of LangGraph, as it was in previous versions.\nActual Behavior\nThe checkpoint package is not present in LangGraph v0.1.18.\nSteps to Reproduce\nInstall LangGraph v0.1.18\nAttempt to import or use the checkpoint package\nObserve that the package is not available\nSystem Info\npip freeze | grep langgraph\nlanggraph==0.1.18", "created_at": "2024-08-01", "closed_at": "2024-08-01", "labels": [], "State": "closed", "Author": "paradox1612"}
{"issue_number": 1161, "issue_title": "[ BUG ] Image not being sent to the model using langgraph and gpt-4o", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\n** Here is not all the code, only the main parts. **\n\nself.llm = ChatOpenAI(temperature=0, model=\"gpt-4o\").bind_tools(self.tools)\n\ndef should_continue(self, state: MessagesState) -> Literal[\"tools\", '__end__']:\n        messages = state['messages']\n        last_message = messages[-1]\n        if last_message.tool_calls:\n            return \"tools\"\n        return END\n\n\ndef call_model(self, state: MessagesState):\n    messages = state['messages']\n    agent_scratchpad = state.get(\"agent_scratchpad\", [])\n    prompt = self.prompt.format(\n        chat_history=messages,\n        input=messages[-1].content,\n        agent_scratchpad=agent_scratchpad\n    )\n    response = self.llm.invoke(prompt)\n    return {\"messages\": [response]}\n\n\nworkflow = StateGraph(MessagesState)\ntool_node = ToolNode(tools)\n\nworkflow.add_node(\"agent\", self.call_model)\nworkflow.add_node(\"tools\", tool_node)\nworkflow.set_entry_point(\"agent\")\nworkflow.add_conditional_edges(\n    \"agent\",\n    self.should_continue,\n)\nworkflow.add_edge(\"tools\", 'agent')\n\npool = ConnectionPool(\n    conninfo=f\"{os.getenv('DATABASE_URL')}\",\n    max_size=50,\n)\n\ncheckpointer = PostgresSaver(sync_connection=pool)\ncheckpointer.create_tables(pool)\n\ngraph = workflow.compile(checkpointer=checkpointer)\n\ngraph.invoke({\n        \"messages\": [HumanMessage(\n            content=[\n                {\"type\": \"text\", \"text\": \"Describe the image below.\"},\n                {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://s3-prod.cogmo.com.br/shared/cat.jpg\"}},\n            ]\n        )]\n    },\n    config={\n        \"configurable\": {\n            \"thread_id\": session_id,\n            \"recursion_limit\": 50,\n        }\n    }\n)\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nWhen using langgraph to send an image to the model, the image is not being sent correctly. The model responds with the message:\n\"I currently don't have the capability to view or describe images. However, if you provide me with some details about the image, I'd be happy to help you with any information or tasks related to it!\"\nThrough langsmith, it is possible to see that the image is being sent, but the model does not respond based on it.\n\nSystem Info\nPlataform: Windows 11\nlangchain==0.2.11\nlanggraph==0.1.14\nlangsmith=0.1.93", "created_at": "2024-07-29", "closed_at": "2024-08-07", "labels": [], "State": "closed", "Author": "HELIOPOTELICKI"}
{"issue_number": 1156, "issue_title": "DOC: Missing put_writes and aput_writes Method in \"Create a Custom Checkpointer Using MongoDB\"", "issue_body": "Issue with current documentation:\nIssue identical to the one for Redis: #1134\nThere is no instructions nor mentions for the put_writes and put_writes method. If you follow the tutorial, you get this error: NotImplementedError: This method was added in langgraph 0.1.7. Please update your checkpoint saver to implement it.\nLink to the doc page: https://langchain-ai.github.io/langgraph/how-tos/persistence_mongodb/\nIdea or request for content:\nHere are implementations of put_writes and async aput_writes:\n    def put_writes(\n        self,\n        config: RunnableConfig,\n        writes: Sequence[Tuple[str, Any]],\n        task_id: str,\n    ) -> None:\n        \"\"\"Save a list of writes to the database.\n\n        Args:\n            config (RunnableConfig): The MongoDB configuration.\n            writes (Sequence[Tuple[str, Any]]): A sequence of tuples representing the writes to be saved.\n            task_id (str): The task ID associated with the writes.\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: If the MongoDBSaver is not initialized with a MongoClient.\n        \"\"\"\n        if isinstance(self.client, AsyncIOMotorClient):\n            raise ValueError(\n                \"MongoDBSaver is not initialized with a MongoClient\"\n            )\n        self.client[self.db_name][self.collection_name].insert_many(\n            [\n                {\n                    \"thread_id\": config[\"configurable\"][\"thread_id\"],\n                    \"thread_ts\": config[\"configurable\"][\"thread_ts\"],\n                    \"task_id\": task_id,\n                    \"idx\": idx,\n                    \"channel\": channel,\n                    \"value\": self.serde.dumps(value),\n                }\n                for idx, (channel, value) in enumerate(writes)\n            ]\n        )\n    async def aput_writes(\n        self,\n        config: RunnableConfig,\n        writes: Sequence[Tuple[str, Any]],\n        task_id: str,\n    ) -> None:\n        \"\"\"Save a list of writes to the database, asynchronously.\n\n        Args:\n            config (RunnableConfig): The configuration for the MongoDB connection.\n            writes (Sequence[Tuple[str, Any]]): The list of writes to be saved.\n            task_id (str): The ID of the task.\n\n        Raises:\n            ValueError: If the MongoDBSaver is not initialized with an AsyncIOMotorClient.\n\n        Returns:\n            None\n        \"\"\"\n        if isinstance(self.client, MongoClient):\n            raise ValueError(\n                \"MongoDBSaver is not initialized with an AsyncIOMotorClient\"\n            )\n        await self.client[self.db_name][self.collection_name].insert_many(\n            [\n                {\n                    \"thread_id\": config[\"configurable\"][\"thread_id\"],\n                    \"thread_ts\": config[\"configurable\"][\"thread_ts\"],\n                    \"task_id\": task_id,\n                    \"idx\": idx,\n                    \"channel\": channel,\n                    \"value\": self.serde.dumps(value),\n                }\n                for idx, (channel, value) in enumerate(writes)\n            ]\n        )\nIt was tested and is working properly", "created_at": "2024-07-28", "closed_at": "2024-08-13", "labels": [], "State": "closed", "Author": "samitaaissat"}
{"issue_number": 1153, "issue_title": "Ollama tool calls not working via openai proxy only when using langgraph", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom typing import Literal\n\nfrom langchain_core.messages import HumanMessage\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.checkpoint import MemorySaver\nfrom langgraph.graph import END, MessagesState, StateGraph\nfrom langgraph.prebuilt import ToolNode\nimport asyncio\n\n\n@tool\ndef search(query: str):\n    \"\"\"Call to surf the web.\"\"\"\n    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\n        return \"It's 60 degrees and foggy.\"\n    return \"It's 90 degrees and sunny.\"\n\n\ntools = [search]\n\ntool_node = ToolNode(tools)\n\nmodel = ChatOpenAI(\n    model=\"llama3.1\", base_url=\"http://localhost:11434/v1\", temperature=0\n).bind_tools(tools)\n\n\ndef should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if last_message.tool_calls:\n        return \"tools\"\n    return END\n\n\nasync def call_model(state: MessagesState, config):\n    messages = state[\"messages\"]\n    response = await model.ainvoke(messages, config)\n    return {\"messages\": [response]}\n\n\nworkflow = StateGraph(MessagesState)\n\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"tools\", tool_node)\n\nworkflow.set_entry_point(\"agent\")\n\nworkflow.add_conditional_edges(\n    \"agent\",\n    should_continue,\n)\n\nworkflow.add_edge(\"tools\", \"agent\")\n\ncheckpointer = MemorySaver()\n\napp = workflow.compile(checkpointer=checkpointer)\n\nasync def test():\n    async for event in app.astream_events(\n        {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]},\n        version=\"v1\",\n        config={\"configurable\": {\"thread_id\": 42}},\n    ):\n        print(event)\n\n\nasyncio.run(test())\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nI want to invoke a tool-calling compatible Ollama model through ChatOpenAI proxy. However, using the code above, the model does not properly tool call:\n{'event': 'on_chain_end', 'data': {'output': {'messages': [HumanMessage(content='what is the weather in sf', id='f7017ae4-b2d0-49e3-b939-69738686368b'), AIMessage(content='{\"name\": \"search\", \"parameters\": {\"query\": \"sf weather\"}}', response_metadata={'finish_reason': 'stop', 'model_name': 'llama3.1', 'system_fingerprint': 'fp_ollama'}, id='run-6a214185-27ba-4505-9cc2-574f20d04909')]}}, 'run_id': 'b2aeba64-38b0-447a-b7de-eefff49e3555', 'name': 'LangGraph', 'tags': [], 'metadata': {'thread_id': 43}, 'parent_ids': []}\nHowever, the behaviour is different when using just langchain:\nimport asyncio\n\nimport openai\nfrom langchain_core.messages import HumanMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\n\n@tool\ndef search(query: str):\n    \"\"\"Call to surf the web.\"\"\"\n    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\n        return \"It's 60 degrees and foggy.\"\n    return \"It's 90 degrees and sunny.\"\n\n\nmodel = ChatOpenAI(model=\"llama3.1\", base_url=\"http://localhost:11434/v1\", temperature=0)\n\nmodel_with_tools = model.bind_tools([search])\n\nasync def test():\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            MessagesPlaceholder(variable_name=\"messages\"),\n        ]\n    )\n    chain = prompt | model_with_tools\n    response = await chain.ainvoke(\n        {\"messages\": [HumanMessage(content=\"What is the weather like in sf\")]}\n    )\n    return response\n\n\nresponse = asyncio.run(test())\nprint(response)\nThis way the model correctly utilise a tool call:\ncontent='' additional_kwargs={'tool_calls': [{'id': 'call_xncx3ycn', 'function': {'arguments': '{\"query\":\"sf weather\"}', 'name': 'search'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 148, 'total_tokens': 165}, 'model_name': 'llama3.1', 'system_fingerprint': 'fp_ollama', 'finish_reason': 'stop', 'logprobs': None} id='run-63a9efd2-6619-448b-9a89-476f45cfb5c8-0' tool_calls=[{'name': 'search', 'args': {'query': 'sf weather'}, 'id': 'call_xncx3ycn', 'type': 'tool_call'}] usage_metadata={'input_tokens': 148, 'output_tokens': 17, 'total_tokens': 165}\nSystem Info\nlangchain==0.2.7\nlangchain-anthropic==0.1.20\nlangchain-cohere==0.1.5\nlangchain-community==0.2.7\nlangchain-core==0.2.21\nlangchain-google-genai==1.0.5\nlangchain-ollama==0.1.0\nlangchain-openai==0.1.17\nlangchain-qdrant==0.1.1\nlangchain-text-splitters==0.2.0\nlangchain-weaviate==0.0.1.post1\nplatform: mac silicon\npython version: Python 3.12.2", "created_at": "2024-07-27", "closed_at": null, "labels": [], "State": "open", "Author": "StreetLamb"}
{"issue_number": 1144, "issue_title": "subflow can't be resume by main flow", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nimport random\nfrom typing import TypedDict, Sequence, Annotated, Dict, Callable, Any\nfrom langchain_core.messages import BaseMessage, AIMessage, HumanMessage\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.graph.message import add_messages\n\n\nclass StateA(TypedDict):\n    name: str\n    messages: Annotated[Sequence[BaseMessage], add_messages]\n\nclass StateB(TypedDict):\n    name: str\n    age: int\n    messages: Annotated[Sequence[BaseMessage], add_messages]\n\ndef a_llm(state: StateA) -> Dict[str, Any]:\n    print('a_llm')\n    print(state)\n    return {'messages': [AIMessage(content='a_llm')]}\n\ndef b_llm(state: StateB) -> Dict[str, Any]:\n    print('b_llm')\n    print(state)\n    return {'messages': [AIMessage(content='b_llm')]}\n\ndef a_a(state: StateA) -> Dict[str, Any]:\n    print('a_a')\n    print(state)\n    return {'messages': [AIMessage(content='a_a')]}\n\ndef b_a(state: StateB) -> Dict[str, Any]:\n    print('b_a')\n    print(state)\n    return {'messages': [AIMessage(content='b_a')]}\n\ndef a_b(state: StateB) -> Dict[str, Any]:\n    print('a_b')\n    print(state)\n    return {'messages': [AIMessage(content='a_b')]}\n\ndef human_b(state: StateB) -> Dict[str, Any]:\n    print('human_b, continue ? A: continue B stop')\n    print(state)\n    return {'messages': [AIMessage(content='continue ? A: continue B stop')]}\n\ndef judge(state: StateB) -> str:\n    i = random.randint(1, 10)\n    print(f'judge {i}')\n    print(state)\n    if i > 6:\n        return 'c_b'\n    else:\n        return END\n\ndef h_j(state: StateB) -> str:\n    msg = state.get('messages')[-1].content\n    if msg == 'A':\n        return 'b_b'\n    else:\n        return END\n\ndef b_b(state: StateB) -> Dict[str, Any]:\n    print('b_b')\n    print(state)\n    return {'messages': [AIMessage(content='b_b')]}\n\ndef c_b(state: StateB) -> Dict[str, Any]:\n    print('c_b')\n    print(state)\n    return {'messages': [AIMessage(content='c_b')]}\nsf = StateGraph(StateB)\nsf.add_node('b_llm', b_llm)\nsf.add_node('a_b', a_b)\nsf.add_node('human_b', human_b)\nsf.add_node('b_b', b_b)\nsf.add_node('c_b', c_b)\nsf.add_edge('b_llm', 'a_b')\nsf.add_edge('a_b', 'human_b')\nsf.add_conditional_edges('human_b', h_j)\nsf.add_conditional_edges('b_b', judge)\nsf.add_edge('c_b', END)\nsf.set_entry_point('b_llm')\n\nmemory = MemorySaver()\n\ngraph = StateGraph(StateA)\ngraph.add_node('a_llm', a_llm)\ngraph.add_node('a_a', a_a)\ngraph.add_node('b_s', sf.compile(checkpointer=memory, interrupt_before=['human_b']))\ngraph.add_node('b_a', b_a)\ngraph.add_edge('a_llm', 'a_a')\ngraph.add_edge('a_a', 'b_s')\ngraph.add_edge('b_s', 'b_a')\ngraph.set_entry_point('a_llm')\n\n\napp = graph.compile(checkpointer=memory)\ncfg = {\"configurable\": {\"thread_id\": \"thread-1\"}}\nres = app.invoke(StateA(name='xxx', messages=[]), cfg)\nprint(res)\nsnapshot = app.get_state(cfg)\napp.update_state(cfg, {'messages': [HumanMessage(content='A')]})\nr = app.invoke(None, config=cfg, stream_mode='values')\nprint(r)\nError Message and Stack Trace (if applicable)\na_llm\n{'name': 'xxx', 'messages': []}\na_a\n{'name': 'xxx', 'messages': [AIMessage(content='a_llm', id='914842fa-df73-4495-b2b0-1ebbceb67e8e')]}\nb_llm\n{'name': 'xxx', 'age': None, 'messages': [AIMessage(content='a_llm', id='914842fa-df73-4495-b2b0-1ebbceb67e8e'), AIMessage(content='a_a', id='59c026c4-dd08-4ca4-8187-038f31e89854')]}\na_b\n{'name': 'xxx', 'age': None, 'messages': [AIMessage(content='a_llm', id='914842fa-df73-4495-b2b0-1ebbceb67e8e'), AIMessage(content='a_a', id='59c026c4-dd08-4ca4-8187-038f31e89854'), AIMessage(content='b_llm', id='47006267-1337-4b21-b4ed-c04ec28bcaa0')]}\n{'name': 'xxx', 'messages': [AIMessage(content='a_llm', id='914842fa-df73-4495-b2b0-1ebbceb67e8e'), AIMessage(content='a_a', id='59c026c4-dd08-4ca4-8187-038f31e89854')]}\nb_llm\n{'name': 'xxx', 'age': None, 'messages': [AIMessage(content='a_llm', id='914842fa-df73-4495-b2b0-1ebbceb67e8e'), AIMessage(content='a_a', id='59c026c4-dd08-4ca4-8187-038f31e89854'), HumanMessage(content='A', id='fb15b763-a436-4b7a-afda-b503e7127d26')]}\na_b\n{'name': 'xxx', 'age': None, 'messages': [AIMessage(content='a_llm', id='914842fa-df73-4495-b2b0-1ebbceb67e8e'), AIMessage(content='a_a', id='59c026c4-dd08-4ca4-8187-038f31e89854'), HumanMessage(content='A', id='fb15b763-a436-4b7a-afda-b503e7127d26'), AIMessage(content='b_llm', id='6d5919fe-9730-4442-b5a9-7cc6e0752ae8')]}\nNone\nDescription\nmy sub flow has some human node to collect manual feedback, when i add the message to main flow, the subflow can't resume.\ni can't invoke the subflow invoke to resume it because i dosn't know the graph step to which node, if i hava many subflow, i have to\nrecord it interrupt at which node and put every subflow at global variable\nSystem Info\npython 3.12 langraph 0.1.14", "created_at": "2024-07-26", "closed_at": "2024-09-04", "labels": [], "State": "closed", "Author": "cjdxhjj"}
{"issue_number": 1142, "issue_title": "AttributeError: 'Collection' object has no attribute 'model_fields'", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\n\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n]\n\ndocs = [WebBaseLoader(url).load() for url in urls]\ndocs_list = [item for sublist in docs for item in sublist]\n\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=250, chunk_overlap=0\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Add to vectorDB\nvectorstore = Chroma.from_documents(\n    documents=doc_splits,\n    collection_name=\"rag-chroma\",\n    embedding=OpenAIEmbeddings(),\n)\nretriever = vectorstore.as_retriever()\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[2], line 21\n     18 doc_splits = text_splitter.split_documents(docs_list)\n     20 # Add to vectorDB\n---> 21 vectorstore = Chroma.from_documents(\n     22     documents=doc_splits,\n     23     collection_name=\"rag-chroma\",\n     24     embedding=OpenAIEmbeddings(),\n     25 )\n     26 retriever = vectorstore.as_retriever()\n\nFile ~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_community\\vectorstores\\chroma.py:878, in Chroma.from_documents(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\n    876 texts = [doc.page_content for doc in documents]\n    877 metadatas = [doc.metadata for doc in documents]\n--> 878 return cls.from_texts(\n    879     texts=texts,\n    880     embedding=embedding,\n    881     metadatas=metadatas,\n    882     ids=ids,\n    883     collection_name=collection_name,\n    884     persist_directory=persist_directory,\n    885     client_settings=client_settings,\n    886     client=client,\n    887     collection_metadata=collection_metadata,\n...\n---> 99 if key in self.model_fields:\n    100     return getattr(self, key)\n    101 return None\n\nAttributeError: 'Collection' object has no attribute 'model_fields'\nDescription\nI just copy the Self-RAG document\nSystem Info\n! pip install -U langchain_community tiktoken langchain-openai langchainhub chromadb langchain langgraph", "created_at": "2024-07-26", "closed_at": "2024-07-26", "labels": [], "State": "closed", "Author": "WuYanZhao107"}
{"issue_number": 1134, "issue_title": "DOC: Missing put_writes and aput_writes Method in \"Create a Custom Checkpointer Using Redis\"", "issue_body": "Issue with current documentation:\nIn the documentation \"Create a Custom Checkpointer Using Redis\"  there is no implementation of the put_writes and aput_writes methods.\nThis is causing the following error: NotImplementedError: This method was added in langgraph 0.1.7. Please update your checkpoint saver to implement it.\nlink to referred doc: https://langchain-ai.github.io/langgraph/how-tos/persistence_redis/\nIdea or request for content:\nRequest implementation of put_writes and a_putwrites methods on persistence_redis.ipynb.\nhttps://github.com/langchain-ai/langgraph/blob/main/examples/persistence_redis.ipynb?short_path=e42963f\nBoth methods are implemented in persistence_postgres.ipynb for example:\nhttps://github.com/langchain-ai/langgraph/blob/main/examples/persistence_postgres.ipynb?short_path=5874aa8\ndef put_writes(\n        self,\n        config: RunnableConfig,\n        writes: Sequence[Tuple[str, Any]],\n        task_id: str,\n    ) -> None:\n        with self._get_sync_connection() as conn:\n            with conn.cursor() as cur:\n                cur.executemany(\n                    self.UPSERT_WRITES_QUERY,\n                    [\n                        (\n                            str(config[\"configurable\"][\"thread_id\"]),\n                            str(config[\"configurable\"][\"thread_ts\"]),\n                            task_id,\n                            idx,\n                            channel,\n                            self.serde.dumps(value),\n                        )\n                        for idx, (channel, value) in enumerate(writes)\n                    ],\n                )\n            conn.commit()\n\nasync def aput_writes(\n        self,\n        config: RunnableConfig,\n        writes: Sequence[Tuple[str, Any]],\n        task_id: str,\n    ) -> None:\n        async with self._get_async_connection() as conn:\n            async with conn.cursor() as cur:\n                await cur.executemany(\n                    self.UPSERT_WRITES_QUERY,\n                    [\n                        (\n                            str(config[\"configurable\"][\"thread_id\"]),\n                            str(config[\"configurable\"][\"thread_ts\"]),\n                            task_id,\n                            idx,\n                            channel,\n                            self.serde.dumps(value),\n                        )\n                        for idx, (channel, value) in enumerate(writes)\n                    ],\n                )\n            await conn.commit()\n", "created_at": "2024-07-25", "closed_at": "2024-08-09", "labels": [], "State": "closed", "Author": "GuilhermeKO"}
{"issue_number": 1122, "issue_title": "LlamaCpp with mistral fails with [TypeError: can only concatenate str (not \"NoneType\") to str] when function calling.", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nmodel_get_schema_sys=\"\"\"You are an SQL engineer and you have to choose, given the user questions and the list of tables present in your datababase,\nthe most relevant tables that will be used in the query. You must select ONLY the tables names provided to you and make sure the spelling \nis correct. Call the tool and return ONLY the names of this tables separate by a comma to the tool.\nMake sure the arguments are ONLY names from the available tables and double check the spelling.\n\"\"\"\nmodel_get_schema_prompt = ChatPromptTemplate.from_messages(\n    [(\"system\", model_get_schema_sys), (\"placeholder\", \"{messages}\")])\n\n\nget_schema_bound_tool = model_get_schema_prompt | llm.bind_tools(\n    [get_schema_tool],tool_choice=True)\n\n###LETS TEST IT\n\nbig_openai_message={'messages': [HumanMessage(content='Which artist has the most songs after Iron Maiden?', id='639046f6-f4d0-4f4c-95da-0926243df500'), AIMessage(content='', id='73f957a5-6623-4cde-be02-a4f87b72989d', tool_calls=[{'name': 'sql_db_list_tables', 'args': {}, 'id': 'tool_abcd123', 'type': 'tool_call'}]), ToolMessage(content='Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track', name='sql_db_list_tables', id='935a7225-2719-482c-a032-be0f754893df', tool_call_id='tool_abcd123')]}\n\n#Llamacpp+Mistral will NOT work with this\nget_schema_bound_tool.invoke(big_openai_message)\n\n#Llamacpp+Mistral WILL work with this. The message is just a mock message but it makes me think #is a formatting issue?\nget_schema_bound_tool.invoke({\"messages\": [(\"user\", \"What is the artist with most tracks? Tables available on the Database are Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\")]})\nError Message and Stack Trace (if applicable)\n4651 def invoke(\n   4652     self,\n   4653     input: Input,\n   4654     config: Optional[RunnableConfig] = None,\n   4655     **kwargs: Optional[Any],\n   4656 ) -> Output:\n-> 4657     return self.bound.invoke(\n   4658         input,\n   4659         self._merge_configs(config),\n...\n--> 939 raise rewrite_traceback_stack(source=source)\n\nFile <template>:2, in top-level template code()\n\nTypeError: can only concatenate str (not \"NoneType\") to str\nDescription\nI am trying to adapt your SQL langgraph Agent tutorial for a local model, using ChatLlamaCpp and Mistral (both support tool calling). It wouldn't work for that Type error so I troubled shot it until here. I added this prompt instead of just binding it right away. I dug out the state that is passed to the tool when using openAI model (in the code as big_openai_message) and try to test the tool with that but it fails. It does work with a mock message that I provided that is formatted differently. Any suggestions?\nSystem Info\nlanggraph==0.1.9", "created_at": "2024-07-24", "closed_at": "2025-01-15", "labels": [], "State": "closed", "Author": "GabHoo"}
{"issue_number": 1831, "issue_title": "sqlite checkpointer import error", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[46], line 1\n----> 1 from langgraph.checkpoint.sqlite import SqliteSaver\n      2 # SqliteSaver.config_specs\n\nFile ...\\Lib\\site-packages\\langgraph\\checkpoint\\sqlite\\__init__.py:9\n      5 from typing import Any, AsyncIterator, Dict, Iterator, Optional, Sequence, Tuple\n      7 from langchain_core.runnables import RunnableConfig\n----> 9 from langgraph.checkpoint.base import (\n     10     WRITES_IDX_MAP,\n     11     BaseCheckpointSaver,\n     12     ChannelVersions,\n     13     Checkpoint,\n     14     CheckpointMetadata,\n     15     CheckpointTuple,\n     16     SerializerProtocol,\n     17     get_checkpoint_id,\n     18 )\n     19 from langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer\n     20 from langgraph.checkpoint.serde.types import ChannelProtocol\n\nImportError: cannot import name 'WRITES_IDX_MAP' from 'langgraph.checkpoint.base'```\nDescription\nCheckpointing is broken-  something broke with internal import statements.\nThis documentation is also out of date: https://langchain-ai.github.io/langgraph/reference/checkpoints/#sqlitesaver\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.19045\nPython Version:  3.11.8 | packaged by conda-forge | (main, Feb 16 2024, 20:40:50) [MSC v.1937 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.2.34\nlangchain: 0.2.12\nlangchain_community: 0.2.11\nlangsmith: 0.1.127\nlangchain_cli: 0.0.29\nlangchain_ollama: 0.1.1\nlangchain_openai: 0.1.22\nlangchain_text_splitters: 0.2.2\nlanggraph: 0.2.3\nlangserve: 0.2.2\n\nOther Dependencies\n\naiohttp: 3.10.1\nasync-timeout: 4.0.3\ndataclasses-json: 0.6.7\nfastapi: 0.112.0\ngitpython: 3.1.43\nhttpx: 0.27.0\njsonpatch: 1.33\nlanggraph-checkpoint: 1.0.11\nlangserve[all]: Installed. No version info available.\nlibcst: 1.4.0\nnumpy: 1.26.4\nollama: 0.3.1\nopenai: 1.40.1\norjson: 3.10.6\npackaging: 24.1\npydantic: 2.8.2\npyproject-toml: 0.0.10\nPyYAML: 6.0.1\nrequests: 2.31.0\nSQLAlchemy: 2.0.32\nsse-starlette: 1.8.2\ntenacity: 8.5.0\ntiktoken: 0.7.0\ntomlkit: 0.12.5\ntyper[all]: Installed. No version info available.\ntyping-extensions: 4.11.0\nuvicorn: 0.23.2\n", "created_at": "2024-09-24", "closed_at": "2024-10-04", "labels": [], "State": "closed", "Author": "openSourcerer9000"}
{"issue_number": 1800, "issue_title": "When a graph containing `async checkpointer` is incorrectly used with `invoke` or `get_state`, the program will hang", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nimport asyncio\n\nfrom langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\nfrom langgraph.graph import MessagesState, StateGraph\n\nthread_config = {\"configurable\": {\"thread_id\": \"1\"}}\n\n\nasync def main() -> None:\n    async with AsyncSqliteSaver.from_conn_string(\":memory:\") as checkpointer:\n        workflow = StateGraph(MessagesState)\n        app = workflow.compile(checkpointer=checkpointer)\n\n        app.invoke({\"messages\": []}, thread_config)  # it will hang here\n\n        # await app.ainvoke({\"messages\": []}, thread_config)\n        # app.get_state(thread_config)  # it also hangs here\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nError Message and Stack Trace (if applicable)\nIn langgraph-checkpoint-sqlite == 1.0.1, the program will throw an exception, which is what I expected.\n  File \".../python3.12/site-packages/langgraph/pregel/__init__.py\", line 601, in get_state\n    saved = checkpointer.get_tuple(config)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".../python3.12/site-packages/langgraph/checkpoint/sqlite/aio.py\", line 37, in wrapper\n    raise NotImplementedError(\nNotImplementedError: The AsyncSqliteSaver does not support synchronous methods. Consider using the SqliteSaver instead.\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nSee https://langchain-ai.github.io/langgraph/reference/checkpoints/langgraph.checkpoint.sqlite.SqliteSaver for more information.\nBut in versions of langgraph-checkpoint-sqlite >= 1.0.2, the program will hang without reporting any errors.\nThe same issue will also occur with langgraph-checkpoint-postgres.\nDescription\nThe program can't continue running, but it doesn't report an error. If you don't realize it's because you're not using the async method, it might take you a lot of time to debug.\nIn my case, I was originally using MemorySaver(), and it worked fine, but when I switched to the asynchronous checkpointer, it suddenly stopped responding.\nThe issue is likely because the async checkpointer implements get_tuple.\nIf this situation is unavoidable, I hope at least a warning can be printed to alert users to switch methods.\nSystem Info\nlangchain-core==0.3.5\nlanggraph==0.2.23\nlanggraph-checkpoint-sqlite==1.0.3\nlanggraph-checkpoint-postgres==1.0.7\nplatform==linux\npython-version==3.12.6", "created_at": "2024-09-23", "closed_at": "2024-09-23", "labels": [], "State": "closed", "Author": "gbaian10"}
{"issue_number": 1798, "issue_title": "[Fix KeyError: 'documents' in web search function] langgraph_rag_agent_llama3_local.ipynb is not working for the web search", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nhttps://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb\n[Sep 22 2024 Comments: ] It looks like the sample documentation is correct. Just the github version is incorrect.\nhttps://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag_local\nExample Code\ndef web_search(state):\n    \"\"\"\n    Web search based based on the question\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): Appended web results to documents\n    \"\"\"\n\n    print(\"---WEB SEARCH---\")\n    question = state[\"question\"]\n\n    ##### HERE #####\n    documents = state[\"documents\"]\n    ################\n\n    # Web search\n    docs = web_search_tool.invoke({\"query\": question})\n    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n    web_results = Document(page_content=web_results)\n    if documents is not None:\n        documents.append(web_results)\n    else:\n        documents = [web_results]\n    return {\"documents\": documents, \"question\": question}\nError Message and Stack Trace (if applicable)\nfrom pprint import pprint\n\n# Compile\napp = workflow.compile()\ninputs = {\"question\": \"Who are the Bears expected to draft first in the NFL draft?\"}\nfor output in app.stream(inputs):\n    for key, value in output.items():\n        pprint(f\"Finished running: {key}:\")\npprint(value[\"generation\"])\n\n---ROUTE QUESTION---\nWho are the Bears expected to draft first in the NFL draft?\n{'datasource': 'web_search'}\nweb_search\n---ROUTE QUESTION TO WEB SEARCH---\n---WEB SEARCH---\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[16], line 6\n      4 app = workflow.compile()\n      5 inputs = {\"question\": \"Who are the Bears expected to draft first in the NFL draft?\"}\n----> 6 for output in app.stream(inputs):\n      7     for key, value in output.items():\n      8         pprint(f\"Finished running: {key}:\")\n\nFile ~/miniconda3/envs/multi/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1278, in Pregel.stream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\n   1267     # Similarly to Bulk Synchronous Parallel / Pregel model\n   1268     # computation proceeds in steps, while there are channel updates\n   1269     # channel updates from step N are only visible in step N+1\n   1270     # channels are guaranteed to be immutable for the duration of the step,\n   1271     # with channel updates applied only at the transition between steps\n   1272     while loop.tick(\n   1273         input_keys=self.input_channels,\n   1274         interrupt_before=interrupt_before_,\n   1275         interrupt_after=interrupt_after_,\n   1276         manager=run_manager,\n   1277     ):\n-> 1278         for _ in runner.tick(\n   1279             loop.tasks.values(),\n   1280             timeout=self.step_timeout,\n   1281             retry_policy=self.retry_policy,\n   1282             get_waiter=get_waiter,\n   1283         ):\n   1284             # emit output\n   1285             yield from output()\n   1286 # emit output\n\nFile ~/miniconda3/envs/multi/lib/python3.11/site-packages/langgraph/pregel/runner.py:52, in PregelRunner.tick(self, tasks, reraise, timeout, retry_policy, get_waiter)\n     50 t = tasks[0]\n     51 try:\n---> 52     run_with_retry(t, retry_policy)\n     53     self.commit(t, None)\n     54 except Exception as exc:\n\nFile ~/miniconda3/envs/multi/lib/python3.11/site-packages/langgraph/pregel/retry.py:29, in run_with_retry(task, retry_policy)\n     27 task.writes.clear()\n     28 # run the task\n---> 29 task.proc.invoke(task.input, config)\n     30 # if successful, end\n     31 break\n\nFile ~/miniconda3/envs/multi/lib/python3.11/site-packages/langgraph/utils/runnable.py:385, in RunnableSeq.invoke(self, input, config, **kwargs)\n    383 context.run(_set_config_context, config)\n    384 if i == 0:\n--> 385     input = context.run(step.invoke, input, config, **kwargs)\n    386 else:\n    387     input = context.run(step.invoke, input, config)\n\nFile ~/miniconda3/envs/multi/lib/python3.11/site-packages/langgraph/utils/runnable.py:167, in RunnableCallable.invoke(self, input, config, **kwargs)\n    165 else:\n    166     context.run(_set_config_context, config)\n--> 167     ret = context.run(self.func, input, **kwargs)\n    168 if isinstance(ret, Runnable) and self.recurse:\n    169     return ret.invoke(input, config)\n\nCell In[13], line 121, in web_search(state)\n    118 print(\"---WEB SEARCH---\")\n    120 question = state[\"question\"]\n--> 121 documents = state[\"documents\"]\n    123 # Web search\n    124 docs = web_search_tool.invoke({\"query\": question})\n\nKeyError: 'documents'\nDescription\nDescription\nI encountered a KeyError: 'documents' in the web search function of this notebook. This error occurs when trying to access the 'documents' key from the state dictionary, which may not always exist. Proper error handling is needed to make the function more robust.\nCurrent Behavior\nThe function assumes that state[\"documents\"] always exists, leading to a KeyError when it doesn't.\nExpected Behavior\nThe function should handle cases where state[\"documents\"] doesn't exist, initializing it as an empty list if necessary.\nProposed Solution\nModify the code to use state.get(\"documents\", []) instead of directly accessing state[\"documents\"]. This change will return an empty list if the 'documents' key doesn't exist, preventing the KeyError.\nCode Changes\n# Before\ndocuments = state[\"documents\"]\n\n# After\ndocuments = state.get(\"documents\", [])\n\n# Rest of the function\ndocs = web_search_tool.invoke({\"query\": question})\nweb_results = \"\\n\".join([d[\"content\"] for d in docs])\nweb_results = Document(page_content=web_results)\ndocuments.append(web_results)\n\nreturn {\"documents\": documents, \"question\": question}\nWould it be acceptable for me to create a Pull Request with these changes?\nSystem Info\n\u279c python -m langchain_core.sys_info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 22.3.0: Mon Jan 30 20:39:35 PST 2023; root:xnu-8792.81.3~2/RELEASE_ARM64_T8103\nPython Version:  3.11.9 (main, Apr 19 2024, 11:43:47) [Clang 14.0.6 ]\n\nPackage Information\n\nlangchain_core: 0.3.5\nlangchain: 0.3.0\nlangchain_community: 0.3.0\nlangsmith: 0.1.125\nlangchain_experimental: 0.3.0\nlangchain_nomic: 0.1.3\nlangchain_ollama: 0.2.0\nlangchain_text_splitters: 0.3.0\nlangchainhub: 0.1.21\nlanggraph: 0.2.23\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.10.5\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.7\nhttpx: 0.27.2\njsonpatch: 1.33\nlanggraph-checkpoint: 1.0.10\nnomic: 3.1.2\nnumpy: 1.26.4\nollama: 0.3.3\norjson: 3.10.7\npackaging: 24.1\npillow: 10.3.0\npydantic: 2.9.2\npydantic-settings: 2.5.2\nPyYAML: 6.0.2\nrequests: 2.32.3\nSQLAlchemy: 2.0.35\ntenacity: 8.5.0\ntypes-requests: 2.32.0.20240914\ntyping-extensions: 4.12.2\n(multi)\n", "created_at": "2024-09-22", "closed_at": "2024-09-25", "labels": [], "State": "closed", "Author": "inoue0426"}
{"issue_number": 1789, "issue_title": "DOC: Update `thread_ts` to `checkpoint_id`.", "issue_body": "Issue with current documentation:\nThe current tutorial documents, such as the Quick Start page, still use thread_ts as an example.\nHowever, as far as I know, it was renamed to checkpoint_id after version 0.2.0. Although the program still allows thread_ts as input, should the documents be updated to reflect the newer usage?\nThere might be multiple pages using thread_ts, and the ipynb outputs also need to be re-executed.\nAdditionally, it seems the documents don't specifically explain the purpose of checkpoint_ns. I'm curious about this field\u2014when would we need to use it?", "created_at": "2024-09-21", "closed_at": "2024-09-22", "labels": [], "State": "closed", "Author": "gbaian10"}
{"issue_number": 1781, "issue_title": "[Docs] Be consistent in START/END and \"__start__\"/\"__end__\"", "issue_body": "Privileged issue\n\n I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.\n\nIssue Content\nIt's the same thing but as a reader, it's confusing to stumble across both.\nVariables (START, END) aren't permitted in Literal[START] typing, meaning the technique of inferring conditional edges connectivity based on the literal response is not supported using START/END.\n(it would likely work, but mypy disallows it, and people enjoy mypy), ipso facto we should either exclusively use literal strings OR we should stop relying on Literal[] and prefer adding a path map (as a list, preferably).\nMy current bias is:\n\nUse Literal[\"end\", \"tools\", ...] when possible\nIf using Send(), etc., use a path_map=[\"end\", \"tools\"], etc.\n\nI could be convinced to only use (2) for the sake of consistency, though it feels a bit redundant in simple cases.\nI don't see a great use case for dictionary-type path maps (unless we want to say that's the way of communicating both the \"finish_reason\" and \"finish\" as a single variable)", "created_at": "2024-09-20", "closed_at": "2024-10-04", "labels": [], "State": "closed", "Author": "hinthornw"}
{"issue_number": 1777, "issue_title": "Custom tools(implemented in \"Subclass Basetool\" approach) cannot access to state via run_manager ", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nclass GooglePlacesTool(BaseTool):\n    \"\"\"Tool that queries the Google places API.\"\"\"\n\n    name: str = \"google_places\"\n    description: str = (\n        \"A tool that queries the Google Places API. \"\n        \"Useful for finding places based on a search query. \"\n        \"This tool can help validate addresses or discover locations \"\n        \"from ambiguous text inputs. \"\n        \"Input should be a search query string.\"\n    )\n    api_wrapper: GooglePlacesAPIWrapper = Field(default_factory=GooglePlacesAPIWrapper)  # type: ignore[arg-type]\n    args_schema: Type[BaseModel] = GooglePlacesSchema\n\n\n    def _run(\n        self,\n        query: str,\n        run_manager: Optional[CallbackManagerForToolRun] = None,\n    ) -> str:\n        logger.debug(f\"RunManager has state?: {hasattr(run_manager, 'state')}\")\n\n        # Run the original tool logic\n        result = self.api_wrapper.run(query)\n\n        return result\nError Message and Stack Trace (if applicable)\nRunManager has state?: False\nDescription\n\nI'm trying to access states in a custom tool implemented using the \"[Subclass Basetool](https://python.langchain.com/docs/how_to/custom_tools/#subclass-basetool)\" approach.\nI've confirmed that I can access state with the [\"@tool\"](https://python.langchain.com/docs/how_to/custom_tools/#tool-decorator) decorator method.\n\nHow can I access state in a custom tool using the \"Subclass Basetool\" approach?\nSystem Info\n$ pip freeze | grep langchain langchain==0.3.0 langchain-anthropic==0.1.23 langchain-cohere==0.2.4 langchain-community==0.3.0 langchain-core==0.3.0 langchain-experimental==0.0.65 langchain-google-community==2.0.0 langchain-openai==0.2.0 langchain-text-splitters==0.3.0 langchainhub==0.1.21", "created_at": "2024-09-20", "closed_at": null, "labels": [], "State": "open", "Author": "humbroll"}
{"issue_number": 1764, "issue_title": "DOC: <Please write a comprehensive title after the 'DOC: ' prefix>How to add prompt to every react_agent in every team?", "issue_body": "Issue with current documentation:\nhierarchical_agent_teams.ipynb\nlink:https://github.com/langchain-ai/langgraph/blob/main/docs/docs/tutorials/multi_agent/hierarchical_agent_teams.ipynb\nIdea or request for content:\nI had been working on this example code for a long time in the field of penetration testing (although it was constantly being changed by the authorities) and started to notice a problem. For example, now there is a scan team consisting of a supervisor and two agents, each equipped with a tool. A team is given a task to be completed in two steps, which requires the cooperation of two agents to complete step by step, but often the task will be stuck in the cycle of the first agent, even if the first part of the task has been completed, and many times the team will not realize the existence of the second tool during the execution (the definition is correct, a separate test can succeed). I would like to know how this is caused and how it can be improved. In the example code, only the supervisor has prompts.How would you add a prompt to each agent, perhaps to solve this problem? I would be grateful if you could answer!", "created_at": "2024-09-19", "closed_at": "2024-12-17", "labels": [], "State": "closed", "Author": "guapia233"}
{"issue_number": 1761, "issue_title": "Error with Astream_events and AsyncSqliteSaver after migrating to v0.3", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nimport sqlite3\nfrom langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\nfrom langgraph.graph import StateGraph\n\nfrom typing import Literal\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_core.runnables import ConfigurableField\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.prebuilt import ToolNode\n\n\n@tool\ndef get_weather(city: Literal[\"nyc\", \"sf\"]):\n    \"\"\"Use this to get weather information.\"\"\"\n    if city == \"nyc\":\n        return \"It might be cloudy in nyc\"\n    elif city == \"sf\":\n        return \"It's always sunny in sf\"\n    else:\n        raise AssertionError(\"Unknown city\")\n\n\ntools = [get_weather]\nmodel = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\nfinal_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n\nmodel = model.bind_tools(tools)\n# NOTE: this is where we're adding a tag that we'll can use later to filter the model stream events to only the model called in the final node.\n# This is not necessary if you call a single LLM but might be important in case you call multiple models within the node and want to filter events\n# from only one of them.\nfinal_model = final_model.with_config(tags=[\"final_node\"])\ntool_node = ToolNode(tools=tools)\nfrom typing import TypedDict, Annotated\n\nfrom langgraph.graph import END, StateGraph, START\nfrom langgraph.graph.message import MessagesState\nfrom langchain_core.messages import BaseMessage, SystemMessage, HumanMessage\n\n\ndef should_continue(state: MessagesState) -> Literal[\"tools\", \"final\"]:\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    # If the LLM makes a tool call, then we route to the \"tools\" node\n    if last_message.tool_calls:\n        return \"tools\"\n    # Otherwise, we stop (reply to the user)\n    return \"final\"\n\n\ndef call_model(state: MessagesState):\n    messages = state[\"messages\"]\n    response = model.invoke(messages)\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": [response]}\n\n\ndef call_final_model(state: MessagesState):\n    messages = state[\"messages\"]\n    last_ai_message = messages[-1]\n    response = final_model.invoke(\n        [\n            SystemMessage(\"Rewrite this in the voice of Al Roker\"),\n            HumanMessage(last_ai_message.content),\n        ]\n    )\n    # overwrite the last AI message from the agent\n    response.id = last_ai_message.id\n    return {\"messages\": [response]}\n\nworkflow = StateGraph(MessagesState)\n\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"tools\", tool_node)\n# add a separate final node\nworkflow.add_node(\"final\", call_final_model)\n\nworkflow.add_edge(START, \"agent\")\nworkflow.add_conditional_edges(\n    \"agent\",\n    should_continue,\n)\n\nworkflow.add_edge(\"tools\", \"agent\")\nworkflow.add_edge(\"final\", END)\n\n# Memory\nmemory = AsyncSqliteSaver.from_conn_string(\"test.db\")\napp = workflow.compile(checkpointer=memory)\n\n# Astream_events for token streaming\ninputs = {\"messages\": [(\"human\", \"what's the weather in nyc?\")]}\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\nasync for event in app.astream_events(inputs, config, version=\"v2\"):\n    kind = event[\"event\"]\n    tags = event.get(\"tags\", [])\n    # filter on the langgraph node name\n    if kind == \"on_chat_model_stream\" and event[\"metadata\"].get(\"langgraph_node\") == \"final\":\n        data = event[\"data\"]\n        if data[\"chunk\"].content:\n            # Empty content in the context of OpenAI or Anthropic usually means\n            # that the model is asking for a tool to be invoked.\n            # So we only print non-empty content\n            print(data[\"chunk\"].content, end=\"|\", flush=True)\n\n\n### Error Message and Stack Trace (if applicable)\n\n```shell\ntrace (sync version): https://smith.langchain.com/public/7742def4-35e5-4578-af7f-c1b418c53eee/r\n\nAttributeError                            Traceback (most recent call last)\nCell In[36], line 3\n      1 inputs = {\"messages\": [(\"human\", \"what's the weather in nyc?\")]}\n      2 config = {\"configurable\": {\"thread_id\": \"1\"}}\n----> 3 async for event in app.astream_events(inputs, config, version=\"v2\"):\n      4     kind = event[\"event\"]\n      5     tags = event.get(\"tags\", [])\n\nFile /opt/miniconda3/envs/yuichan/lib/python3.12/site-packages/langchain_core/runnables/base.py:1377, in Runnable.astream_events(self, input, config, version, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\n   1372     raise NotImplementedError(\n   1373         'Only versions \"v1\" and \"v2\" of the schema is currently supported.'\n   1374     )\n   1376 async with aclosing(event_stream):\n-> 1377     async for event in event_stream:\n   1378         yield event\n\nFile /opt/miniconda3/envs/yuichan/lib/python3.12/site-packages/langchain_core/tracers/event_stream.py:1006, in _astream_events_implementation_v2(runnable, input, config, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\n   1004 # Await it anyway, to run any cleanup code, and propagate any exceptions\n   1005 try:\n-> 1006     await task\n   1007 except asyncio.CancelledError:\n   1008     pass\n\nFile /opt/miniconda3/envs/yuichan/lib/python3.12/site-packages/langchain_core/tracers/event_stream.py:966, in _astream_events_implementation_v2.<locals>.consume_astream()\n...\n--> 741     self.checkpointer_get_next_version = checkpointer.get_next_version\n    742     self.checkpointer_put_writes = checkpointer.aput_writes\n    743 else:\n\nAttributeError: '_AsyncGeneratorContextManager' object has no attribute 'get_next_version'\n\n\n\n### Description\n\nI've been using AsyncSqliteSaver with astream_events in my langgraph app for a while. The recent upgrade to langchain v0.3 and langgraph v0.2 broke it. The message is not very descriptive but looks like a bug from the checkpointer class.\nThe code snippet above can reproduce the error.\n\n### System Info\n\nruntime\nlangchain_core_version: \"0.3.1\"\n\nlangchain_version: \"0.3.0\"\n\nlibrary: \"langsmith\"\n\nplatform: \"Linux-6.5.0-1021-aws-x86_64-with-glibc2.36\"\n\npy_implementation: \"CPython\"\n\nRENDER_GIT_COMMIT: \"16d83d0c987788b782d3c8bb27c8ce34eaba911f\"\n\nruntime: \"python\"\n\nruntime_version: \"3.12.2\"\n\nsdk: \"langsmith-py\"\n\nsdk_version: \"0.1.122\"\n", "created_at": "2024-09-18", "closed_at": "2024-09-18", "labels": [], "State": "closed", "Author": "mingxuan-he"}
{"issue_number": 1739, "issue_title": "TypeScript : CJS issue, Module '@langchain/langgraph-sdk' has no exported member 'Client'", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nCode (in any file within the package) :\nimport { Client } from \"@langchain/langgraph-sdk\";\n\ntsconfig.json :\n\n{\n  \"extends\": \"@tsconfig/node16/tsconfig.json\",\n  \"version\": \"4.4.2\",\n  \"compilerOptions\": {\n    \"declaration\": true,\n    \"declarationMap\": true,\n    \"sourceMap\": true,\n    \"rootDir\": \"./src\",\n    \"outDir\": \"./dist\",\n    \"moduleResolution\": \"Node16\"\n  },\n  \"include\": [\n    \"src/**/*.ts\"\n  ],\n  \"exclude\": [\n    \"node_modules\",\n    \"dist\"\n  ]\n}\n\npackage.json :\n\n{\n  \"name\": \"@repo/workflow\",\n  \"version\": \"0.0.0\",\n  \"private\": true,\n  \"scripts\": {\n    \"dev\": \"tsc --build ./tsconfig.json --watch\",\n    \"build\": \"tsc --build ./tsconfig.json\",\n    \"build.watch\": \"tsc --build -w --preserveWatchOutput\",\n    \"lint\": \"eslint .\"\n  },\n  \"dependencies\": {\n    \"@aws-sdk/client-s3\": \"^3.645.0\",\n    \"@langchain/langgraph-sdk\": \"^0.0.9\",\n    \"@prisma/client\": \"^5.19.1\",\n    \"@repo/db\": \"workspace:*\",\n    \"@repo/storage\": \"workspace:*\",\n    \"@temporalio/activity\": \"^1.11.1\",\n    \"@temporalio/workflow\": \"^1.11.1\",\n    \"axios\": \"^1.7.7\"\n  },\n  \"devDependencies\": {\n    \"@tsconfig/node16\": \"^1.0.4\",\n    \"@types/mime-types\": \"^2.1.4\",\n    \"@typescript-eslint/eslint-plugin\": \"^5.62.0\",\n    \"@typescript-eslint/parser\": \"^5.62.0\",\n    \"eslint\": \"^7.32.0\",\n    \"eslint-config-prettier\": \"^8.10.0\",\n    \"eslint-plugin-deprecation\": \"^1.5.0\",\n    \"typescript\": \"^4.9.5\"\n  },\n}\nError Message and Stack Trace (if applicable)\nError : Module '\"@langchain/langgraph-sdk\"' has no exported member 'Client'.ts(2305)\nDescription\nIn a TurboRepo internal package, I'm trying to use langgraph-sdk but it seems that there is an issue due to ECMA / CommonJS module compatibility.\nI provided my typescript configuration and package.json\nSystem Info\n{\n\"name\": \"@repo/workflow\",\n\"version\": \"0.0.0\",\n\"private\": true,\n\"scripts\": {\n\"dev\": \"tsc --build ./tsconfig.json --watch\",\n\"build\": \"tsc --build ./tsconfig.json\",\n\"build.watch\": \"tsc --build -w --preserveWatchOutput\",\n\"lint\": \"eslint .\"\n},\n\"dependencies\": {\n\"@aws-sdk/client-s3\": \"^3.645.0\",\n\"@langchain/langgraph-sdk\": \"^0.0.9\",\n\"@prisma/client\": \"^5.19.1\",\n\"@repo/db\": \"workspace:\",\n\"@repo/storage\": \"workspace:\",\n\"@temporalio/activity\": \"^1.11.1\",\n\"@temporalio/workflow\": \"^1.11.1\",\n\"axios\": \"^1.7.7\"\n},\n\"devDependencies\": {\n\"@tsconfig/node16\": \"^1.0.4\",\n\"@types/mime-types\": \"^2.1.4\",\n\"@typescript-eslint/eslint-plugin\": \"^5.62.0\",\n\"@typescript-eslint/parser\": \"^5.62.0\",\n\"eslint\": \"^7.32.0\",\n\"eslint-config-prettier\": \"^8.10.0\",\n\"eslint-plugin-deprecation\": \"^1.5.0\",\n\"typescript\": \"^4.9.5\"\n},\n}", "created_at": "2024-09-17", "closed_at": "2024-09-17", "labels": [], "State": "closed", "Author": "arthberman"}
{"issue_number": 1722, "issue_title": "[LATS example] Selection should choose best child at each tree level until reaching leaf", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nIn the LATS example implementation, we have the following which selects the next node from all of the tree:\nclass Node:\n    @property\n    def best_child(self):\n        \"\"\"Select the child with the highest UCT to search next.\"\"\"\n        if not self.children:\n            return None\n        all_nodes = self._get_all_children()\n        return max(all_nodes, key=lambda child: child.upper_confidence_bound())\n\n    def _get_all_children(self):\n        all_nodes = []\n        nodes = deque()\n        nodes.append(self)\n        while nodes:\n            node = nodes.popleft()\n            all_nodes.extend(node.children)\n            for n in node.children:\n                nodes.append(n)\n        return all_nodes\n\n...\n\ndef expand(state: TreeState, config: RunnableConfig) -> dict:\n    \"\"\"Starting from the \"best\" node in the tree, generate N candidates for the next step.\"\"\"\n    root = state[\"root\"]\n    best_candidate: Node = root.best_child if root.children else root\n    ...\nError Message and Stack Trace (if applicable)\nIn some scenarios, the current search algorithm implementation can get stuck in an infinite recursion (until max recursion reached) if the same node is selected and nothing happens in the expand (because it was already expanded).\nDescription\nSelection should choose the best child of each level until reaching a leaf. From the LATS paper: \"Starting from the root node, denoted as the initial state s0, a child node is selected at each tree level until a leaf node is reached.\"\nThe current implementation creates a pool of all nodes and chooses from the entire pool, which is not the same. In particular, it can also choose nodes which are not leaves, which can cause infinite recursion problems.\nSystem Info\nlangchain==0.3.0\nlangchain-core==0.3.0\nlangchain-openai==0.2.0\nlangchain-text-splitters==0.3.0", "created_at": "2024-09-15", "closed_at": "2024-09-16", "labels": [], "State": "closed", "Author": "eranhirs"}
{"issue_number": 1719, "issue_title": "Unable to run agent with fine-tuned gpt-4o-mini model", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\n# Configure logger for the module\nlogger = logging.getLogger(__name__)\n\n# Define embeddings models\nembeddings_model = OpenAIEmbeddings(model=EMBEDDINGS_MODEL_NAME, api_key=OPENAI_API_KEY)\n\n# Define language model\nllm = ChatOpenAI(model=LLM_MODEL_NAME, openai_api_key=OPENAI_API_KEY, temperature=LLM_TEMPERATURE, seed=SEED, stream_usage=True)\n\n# Statefully manage chat history\nstore = {}\n\n# Define text splitter\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=TEXT_SPLITTER_CHUNK_SIZE)\n\n# Define message trimmer\ntrimmer = trim_messages(\n    max_tokens=TRIM_MESSAGES_MAX_TOKENS,\n    strategy=\"last\",\n    token_counter=llm,\n    include_system=True,\n    allow_partial=False,\n    start_on=\"human\",\n)\n\n# Define message filter\nfilter_ = filter_messages(exclude_types=[\"tool\"])\n\n# Useful for processing data if it is scraped in markdown format\nEXCLUDE_CONTENT = list()\nif SCRAPED_DATA_PROCESSING[\"process_data\"]:\n    exclude_content_file_path = SCRAPED_DATA_PROCESSING[\"process_data_file_path\"]\n\n    # Read file\n    with open(exclude_content_file_path, 'r') as file:\n        lines = file.readlines()\n\n    EXCLUDE_CONTENT = [line.strip() for line in lines if line.strip()]\n    print(EXCLUDE_CONTENT)\n\n\ndef split_document(docs):\n    \"\"\"\n    Split the document into smaller chunks.\n    \"\"\"\n    try:\n        logger.debug(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} splitting document\")\n        all_splits = text_splitter.split_documents(docs)\n        logger.debug(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} successfully split document\")\n    except Exception as e:\n        all_splits = list()\n        logger.info(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} failed to split document\")\n        logger.exception(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} exception caused - {e}\")\n    return all_splits\n\n\ndef create_new_document(data):\n    \"\"\"\n    Custom document creater from scraped markdown data.\n    \"\"\"\n    page_content = \"\"\n    for i in data:\n        if i.page_content not in EXCLUDE_CONTENT:\n            page_content += \"\\n\\n\" + i.page_content\n            if \"link_urls\" in i.metadata.keys():\n                for l, m in zip(i.metadata[\"link_texts\"], i.metadata[\"link_urls\"]):\n                    page_content += \"\\n\" + l + \" \" + m + \"\\n\"\n    doc = Document(page_content=page_content.strip(), metadata={\"source\": data[0].metadata[\"source\"]})\n    return doc\n\n\ndef load_knowledge_base():\n    \"\"\"\n    Load the knowledge base.\n    \"\"\"\n    try:\n        logger.info(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} loading knowledge base\")\n        all_splits = []\n        for file in os.listdir(KNOWLEDGE_BASE_DIR):\n            logger.info(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} processing file {file}\")\n            file_path = os.path.join(KNOWLEDGE_BASE_DIR, file)\n            if file_path.endswith(\".pdf\"):\n                loader = PyPDFLoader(file_path)\n                docs = loader.load()\n                splits = split_document(docs)\n                all_splits.extend(splits)\n            elif file_path.endswith(\".csv\"):\n                loader = CSVLoader(file_path=file_path)\n                docs = loader.load()\n                all_splits.extend(docs)\n            elif file_path.endswith(\".md\"):\n                loader = UnstructuredMarkdownLoader(file_path=file_path, mode=\"elements\")\n                data = loader.load()\n                if len(data):\n                    doc = create_new_document(data)\n                    all_splits.append(doc)\n            logger.info(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} successfully processed file {file}\")\n        logger.info(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} successfully loaded knowledge base\")\n        logger.info(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} total number of knowledge chunks - {len(all_splits)}\")\n    except Exception as e:\n        all_splits = list()\n        logger.info(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} failed to load knowledge base\")\n        logger.exception(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} exception caused - {e}\")\n    return all_splits\n\n\ndef create_retriever():\n    \"\"\"\n    Create the retriever.\n    \"\"\"\n    try:\n        logger.info(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} creating retriever\")\n        docs = load_knowledge_base()\n\n        # Associate summaries with the documents\n        chain = (\n                {\"doc\": lambda x: x.page_content}\n                | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n                | llm\n                | StrOutputParser()\n        )\n        summaries = chain.batch(docs, {\"max_concurrency\": 5})\n\n        # The vectorstore to use to index the child chunks\n        vectorstore = Chroma(embedding_function=embeddings_model, persist_directory=VECTOR_DB_DIR)\n\n        # The storage layer for the parent documents\n        store = LocalFileStore(PARENT_DOCUMENTS_STORAGE)\n        id_key = \"doc_id\"\n\n        # The retriever (empty to start)\n        retriever = MultiVectorRetriever(\n            vectorstore=vectorstore,\n            byte_store=store,\n            id_key=id_key,\n        )\n        doc_ids = [str(uuid.uuid4()) for _ in docs]\n        summary_docs = [\n            Document(page_content=s, metadata={id_key: doc_ids[i]})\n            for i, s in enumerate(summaries)\n        ]\n\n        retriever.vectorstore.add_documents(summary_docs)\n        retriever.docstore.mset(list(zip(doc_ids, docs)))\n\n        for i, doc in enumerate(docs):\n            doc.metadata[id_key] = doc_ids[i]\n        retriever.vectorstore.add_documents(docs)\n\n        logger.info(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} successfully created retriever\")\n    except Exception as e:\n        retriever = None\n        logger.info(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} failed to create retriever\")\n        logger.exception(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} exception caused - {e}\")\n    return retriever\n\n\ndef load_retriever():\n    \"\"\"\n    Load the retriever.\n    \"\"\"\n    try:\n        logger.info(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} loading retriever\")\n        if os.path.exists(VECTOR_DB_DIR) and os.path.exists(PARENT_DOCUMENTS_STORAGE):\n            logger.info(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} retriever already exist\")\n\n            # The vectorstore which indexed the child chunks\n            vector_db = Chroma(embedding_function=embeddings_model, persist_directory=VECTOR_DB_DIR)\n\n            # The storage layer for the parent documents\n            store = LocalFileStore(PARENT_DOCUMENTS_STORAGE)\n            id_key = \"doc_id\"\n\n            # The retriever (empty to start)\n            retriever = MultiVectorRetriever(\n                vectorstore=vector_db,\n                byte_store=store,\n                id_key=id_key,\n            )\n        else:\n            logger.info(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} retriever does not exist\")\n            retriever = create_retriever()\n        logger.info(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} successfully loaded retriever\")\n    except Exception as e:\n        retriever = None\n        logger.info(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} failed to load retriever\")\n        logger.exception(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} exception caused - {e}\")\n    return retriever\n\n\ndef format_docs(docs):\n    \"\"\"\n    Format the documents.\n    \"\"\"\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n\ndef get_session_history(session_id: str) -> BaseChatMessageHistory:\n    \"\"\"\n    Get the chat message history for the session.\n    \"\"\"\n    if session_id not in store:\n        store[session_id] = ChatMessageHistory()\n    return store[session_id]\n\n\ndef create_follow_up_question_generator_tool():\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                PROMPT_FOR_FOLLOW_UP_QUESTION_GENERATION,\n            ),\n            MessagesPlaceholder(variable_name=\"messages\"),\n        ]\n    )\n\n    parser = StrOutputParser()\n    chain = prompt | llm | parser\n    follow_up_tool = chain.as_tool(\n        name=\"follow_up_question_generation\",\n        description=FOLLOW_UP_QUESTION_GENERATION_TOOL_DESCRIPTION\n    )\n    return follow_up_tool\n\n\ndef create_tool():\n    \"\"\"\n    Create the tool to retrieve information from vector db.\n    \"\"\"\n    # Create retrieval tool\n    retriever = load_retriever()\n    tool = create_retriever_tool(\n        retriever,\n        \"wine_business_information\",\n        RAG_RETRIEVER_TOOL_DESCRIPTION\n    )\n\n    # Create search tool\n    search = GoogleSerperAPIWrapper(serper_api_key=SERPER_API_KEY)\n    search_tool = Tool(\n        name=\"online_web_search\",\n        func=search.run,\n        description=SEARCH_TOOL_DESCRIPTION,\n    )\n\n    # Create follow up question generator tool\n    # follow_up_tool = create_follow_up_question_generator_tool()\n    tools = [tool, search_tool]\n    # tools = [tool, search_tool, follow_up_tool]\n    return tools\n\n\ndef find_consecutive_tool_messages_count(messages):\n    \"\"\"\n    Find the count of ToolMessage in the conversation.\n    \"\"\"\n    idx = 0\n    count = 0\n    while isinstance(messages[idx], ToolMessage):\n        count += 1\n        if idx < len(messages) - 1:\n            idx += 1\n        else:\n            break\n    return count\n\n\ndef find_occurrences_tool_messages(messages):\n    \"\"\"\n    Find the consecutive occurrences of ToolMessage in the conversation.\n    \"\"\"\n    occurrence_count = 0\n    in_tool_message_sequence = False\n\n    for message in messages:\n        if isinstance(message, ToolMessage):\n            if not in_tool_message_sequence:\n                occurrence_count += 1\n                in_tool_message_sequence = True\n        else:\n            in_tool_message_sequence = False\n\n    return occurrence_count\n\n\ndef keep_last_messages(messages):\n    \"\"\"\n    Keep the last messages in the conversation.\n    \"\"\"\n    updated_messages = list()\n    c = 0\n    for message in messages[::-1]:\n        if c < KEEP_MESSAGES_COUNT:\n            updated_messages.append(message)\n        else:\n            break\n        c += 1\n    return updated_messages[::-1]\n\n\n@chain\ndef custom_filter_messages(messages):\n    \"\"\"\n    Custom filter messages to remove ToolMessage from in between the conversation.\n    \"\"\"\n    messages = keep_last_messages(messages)\n    total_occurrences_tool_message = find_occurrences_tool_messages(messages)\n    updated_messages = list()\n\n    # Add system message if not present.\n    if not isinstance(messages[0], SystemMessage):\n        updated_messages.append(SystemMessage(INSTRUCTIONS))\n\n    current_occurrence_tool_message = 0\n    num_of_tool_messages = 0\n    skip_occurrences = False\n    for idx, message in enumerate(messages):\n        message.content = message.content.replace(\"\\n\", \" \")\n        if isinstance(message, HumanMessage):\n            updated_messages.append(message)\n        elif isinstance(message, AIMessage):\n            if len(message.content):\n                updated_messages.append(message)\n            else:\n                current_occurrence_tool_message += 1\n                if current_occurrence_tool_message < total_occurrences_tool_message:\n                    num_of_tool_messages = find_consecutive_tool_messages_count(messages[idx + 1:])\n                    skip_occurrences = True\n                    continue\n                else:\n                    updated_messages.append(message)\n                    num_of_tool_messages = find_consecutive_tool_messages_count(messages[idx + 1:])\n                    skip_occurrences = False\n        elif isinstance(message, ToolMessage):\n            if skip_occurrences:\n                if num_of_tool_messages > 0:\n                    num_of_tool_messages -= 1\n                    continue\n                else:\n                    skip_occurrences = False\n            else:\n                updated_messages.append(message)\n    return updated_messages\n\n\ndef create_agent():\n    \"\"\"\n    Create the agent to answer user's questions.\n    \"\"\"\n    # memory = SqliteSaver.from_conn_string(\"checkpoints.sqlite\")\n    memory = AsyncSqliteSaver.from_conn_string(\"checkpoints.sqlite\")\n    tools = create_tool()\n    messages_filter = custom_filter_messages | trimmer\n    print(llm)\n    agent_executor = create_react_agent(llm, tools, messages_modifier=messages_filter, checkpointer=memory, debug=False)\n    return agent_executor\n\nagent = create_agent()\n\nasync def generate_streaming_response_from_agent(question: str, session_id: str):\n    ignore_tool_output = False\n    async for event in agent.astream_events(\n        {\"messages\": [HumanMessage(content=question)]},\n        config={\"configurable\": {\"thread_id\": session_id}},\n        version=\"v1\",\n        debug=True\n    ):\n        kind = event[\"event\"]\n        if kind == \"on_tool_start\":\n            ignore_tool_output = True\n\n        elif kind == \"on_tool_end\":\n            ignore_tool_output = False\n\n        elif kind == \"on_chat_model_stream\":\n            if not ignore_tool_output:\n                content = event[\"data\"][\"chunk\"].content\n                if content:\n                    content = content.replace('\\n', '__NEWLINE__')\n                    yield f\"data: {content}\\n\\n\"\n\n        elif kind == \"on_chat_model_end\":\n            logger.info(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} Session ID: {session_id} and User message: {question}\")\n            logger.info(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} Session ID: {session_id} and Chatbot message: {event['data']['output']['generations'][0][0]['message'].content}\")\n            logger.info(f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')} Session ID: {session_id} and Usage metadata: {event['data']['output']['generations'][0][0]['message'].usage_metadata}\")\nError Message and Stack Trace (if applicable)\n[-1:checkpoint] State at the end of step -1:\n{'messages': []}\n[0:tasks] Starting step 0 with 1 task:\n- __start__ -> {'messages': [HumanMessage(content='hi')]}\n[0:writes] Finished step 0 with writes to 1 channel:\n- messages -> [HumanMessage(content='hi')]\n[0:checkpoint] State at the end of step 0:\n{'messages': [HumanMessage(content='hi', id='4409af73-5303-4fec-b5e3-5d55a8bcfc9f')]}\n[1:tasks] Starting step 1 with 1 task:\n- agent -> {'is_last_step': False,\n 'messages': [HumanMessage(content='hi', id='4409af73-5303-4fec-b5e3-5d55a8bcfc9f')]}\n[14/Sep/2024 10:19:11] \"GET /stream_reply_from_chatbot/?session_id=53159df5-7a6b-451b-bad4-0d27ab0f4f04&user_message=hi HTTP/1.1\" 200 233\nDescription\n\nI have created a react agent for RAG application. Initially I was using gpt-4o-mini model with my agent, and it worked perfectly fine.\nTo improve the accuracy of the model, I fine-tuned a gpt-4o-mini model, and replaced gpt-4o-mini model name with name of the fine-tuned model.\nNow, the agent is not able to generate the response.\nThere are no error logged, no exception but it is not able to generate the response.\n\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #128-Ubuntu SMP Fri Jul 5 09:28:59 UTC 2024\nPython Version:  3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 11.4.0]\n\nPackage Information\n\nlangchain_core: 0.3.0\nlangchain: 0.3.0\nlangchain_community: 0.3.0\nlangsmith: 0.1.120\nlangchain_chroma: 0.1.4\nlangchain_cohere: 0.3.0\nlangchain_experimental: 0.3.0\nlangchain_openai: 0.2.0\nlangchain_text_splitters: 0.3.0\nlanggraph: 0.2.21\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.10.5\nasync-timeout: Installed. No version info available.\nchromadb: 0.5.3\ncohere: 5.9.2\ndataclasses-json: 0.6.7\nfastapi: 0.114.2\nhttpx: 0.27.2\njsonpatch: 1.33\nlanggraph-checkpoint: 1.0.9\nnumpy: 2.1.1\nopenai: 1.45.0\norjson: 3.10.7\npackaging: 24.1\npandas: 2.2.2\npydantic: 2.9.1\npydantic-settings: 2.5.2\nPyYAML: 6.0.2\nrequests: 2.32.3\nSQLAlchemy: 2.0.34\ntabulate: 0.9.0\ntenacity: 9.0.0\ntiktoken: 0.7.0\ntyping-extensions: 4.12.2\n", "created_at": "2024-09-14", "closed_at": "2024-09-14", "labels": [], "State": "closed", "Author": "DhruvCMH"}
{"issue_number": 1713, "issue_title": "Notebook example of lats can not define Node", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nimport math\nfrom collections import deque\nfrom typing import Optional\n\nfrom langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage\n\n\nclass Node:\n    def __init__(\n        self,\n        messages: list[BaseMessage],\n        reflection: Reflection,\n        parent: Optional[Node] = None,\n    ):\n        self.messages = messages\n        self.parent = parent\n        self.children = []\n        self.value = 0\n        self.visits = 0\n        self.reflection = reflection\n        self.depth = parent.depth + 1 if parent is not None else 1\n        self._is_solved = reflection.found_solution if reflection else False\n        if self._is_solved:\n            self._mark_tree_as_solved()\n        self.backpropagate(reflection.normalized_score)\n\n    def __repr__(self) -> str:\n        return (\n            f\"<Node value={self.value}, visits={self.visits},\"\n            f\" solution={self.messages} reflection={self.reflection}/>\"\n        )\n\n    @property\n    def is_solved(self):\n        \"\"\"If any solutions exist, we can end the search.\"\"\"\n        return self._is_solved\n\n    @property\n    def is_terminal(self):\n        return not self.children\n\n    @property\n    def best_child(self):\n        \"\"\"Select the child with the highest UCT to search next.\"\"\"\n        if not self.children:\n            return None\n        all_nodes = self._get_all_children()\n        return max(all_nodes, key=lambda child: child.upper_confidence_bound())\n\n    @property\n    def best_child_score(self):\n        \"\"\"Return the child with the highest value.\"\"\"\n        if not self.children:\n            return None\n        return max(self.children, key=lambda child: int(child.is_solved) * child.value)\n\n    @property\n    def height(self) -> int:\n        \"\"\"Check for how far we've rolled out the tree.\"\"\"\n        if self.children:\n            return 1 + max([child.height for child in self.children])\n        return 1\n\n    def upper_confidence_bound(self, exploration_weight=1.0):\n        \"\"\"Return the UCT score. This helps balance exploration vs. exploitation of a branch.\"\"\"\n        if self.parent is None:\n            raise ValueError(\"Cannot obtain UCT from root node\")\n        if self.visits == 0:\n            return self.value\n        # Encourages exploitation of high-value trajectories\n        average_reward = self.value / self.visits\n        # Encourages exploration of less-visited trajectories\n        exploration_term = math.sqrt(math.log(self.parent.visits) / self.visits)\n        return average_reward + exploration_weight * exploration_term\n\n    def backpropagate(self, reward: float):\n        \"\"\"Update the score of this node and its parents.\"\"\"\n        node = self\n        while node:\n            node.visits += 1\n            node.value = (node.value * (node.visits - 1) + reward) / node.visits\n            node = node.parent\n\n    def get_messages(self, include_reflections: bool = True):\n        if include_reflections:\n            return self.messages + [self.reflection.as_message()]\n        return self.messages\n\n    def get_trajectory(self, include_reflections: bool = True) -> list[BaseMessage]:\n        \"\"\"Get messages representing this search branch.\"\"\"\n        messages = []\n        node = self\n        while node:\n            messages.extend(\n                node.get_messages(include_reflections=include_reflections)[::-1]\n            )\n            node = node.parent\n        # Reverse the final back-tracked trajectory to return in the correct order\n        return messages[::-1]  # root solution, reflection, child 1, ...\n\n    def _get_all_children(self):\n        all_nodes = []\n        nodes = deque()\n        nodes.append(self)\n        while nodes:\n            node = nodes.popleft()\n            all_nodes.extend(node.children)\n            for n in node.children:\n                nodes.append(n)\n        return all_nodes\n\n    def get_best_solution(self):\n        \"\"\"Return the best solution from within the current sub-tree.\"\"\"\n        all_nodes = [self] + self._get_all_children()\n        best_node = max(\n            all_nodes,\n            # We filter out all non-terminal, non-solution trajectories\n            key=lambda node: int(node.is_terminal and node.is_solved) * node.value,\n        )\n        return best_node\n\n    def _mark_tree_as_solved(self):\n        parent = self.parent\n        while parent:\n            parent._is_solved = True\n            parent = parent.parent\nError Message and Stack Trace (if applicable)\n      3 from typing import Optional\n      5 from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage\n----> 8 class Node:\n      9     def __init__(\n     10         self,\n     11         messages: list[BaseMessage],\n     12         reflection: Reflection,\n     13         parent: Optional[Node] = None,\n     14     ):\n     15         self.messages = messages\n\nCell In[8], line 13, in Node()\n      8 class Node:\n      9     def __init__(\n     10         self,\n     11         messages: list[BaseMessage],\n     12         reflection: Reflection,\n---> 13         parent: Optional[Node] = None,\n     14     ):\n     15         self.messages = messages\n     16         self.parent = parent\n\nNameError: name 'Node' is not defined\"\nDescription\nI'm trying to new a Node class.It threw an error just like this\nSystem Info\nlangchain==0.2.16\nlangchain-community==0.2.7\nlangchain-core==0.2.39\nlangchain-experimental==0.0.47\nlangchain-fireworks==0.1.7\nlangchain-openai==0.1.24\nlangchain-qianwen==0.1.17\nlangchain-text-splitters==0.2.1\nlinux\npython==3.10.12", "created_at": "2024-09-13", "closed_at": "2024-09-13", "labels": [], "State": "closed", "Author": "ZXTFINAL"}
{"issue_number": 1676, "issue_title": "In the subgraph, when there is a branch point to __end__(or __start__ point to ...), it will lose part of the edges", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nimport secrets\n\nfrom langgraph.graph import END, START, MessagesState, StateGraph\nfrom langgraph.graph.state import CompiledStateGraph\nfrom rich import get_console\n\n\ndef foo(_: MessagesState) -> None:\n    return\n\n\ndef branch() -> bool:\n    return secrets.choice([True, False])\n\n\ndef sub() -> CompiledStateGraph:\n    workflow = StateGraph(MessagesState)\n    for node in [\"A\", \"B\", \"Z\"]:\n        workflow.add_node(node, foo)\n\n    workflow.add_conditional_edges(START, branch, {True: \"A\", False: END})\n    workflow.add_edge(\"A\", \"B\")\n    workflow.add_conditional_edges(\"B\", branch, {True: \"Z\", False: END})\n    workflow.add_edge(\"Z\", END)\n    return workflow.compile()\n\n\ndef main() -> CompiledStateGraph:\n    workflow = StateGraph(MessagesState)\n    workflow.add_node(\"main_entry\", foo)\n    workflow.add_node(\"sub_1\", sub())\n    workflow.add_node(\"main_exit\", foo)\n\n    workflow.add_edge(START, \"main_entry\")\n    workflow.add_edge(\"main_entry\", \"sub_1\")\n    workflow.add_edge(\"sub_1\", \"main_exit\")\n    workflow.add_edge(\"main_exit\", END)\n    return workflow.compile()\n\n\nif __name__ == \"__main__\":\n    sub_graph = sub()\n    get_console().print(sub_graph.get_graph())\n    sub_graph.get_graph().draw_mermaid_png(output_file_path=\"_only_sub.png\")\n\n    graph = main()\n    get_console().print(graph.get_graph())\n    graph.get_graph(xray=1).draw_mermaid_png(output_file_path=\"_with_sub.png\")\nError Message and Stack Trace (if applicable)\nGraph(\n    nodes={\n        '__start__': Node(id='__start__', name='__start__', data=<class 'pydantic.v1.main.LangGraphInput'>, metadata=None),\n        'A': Node(id='A', name='A', data=A(func_accepts_config=False, afunc_accepts_config=False, recurse=True), metadata=None),\n        'B': Node(id='B', name='B', data=B(func_accepts_config=False, afunc_accepts_config=False, recurse=True), metadata=None),\n        'Z': Node(id='Z', name='Z', data=Z(func_accepts_config=False, afunc_accepts_config=False, recurse=True), metadata=None),\n        '__end__': Node(id='__end__', name='__end__', data=<class 'pydantic.v1.main.LangGraphOutput'>, metadata=None)\n    },\n    edges=[\n        Edge(source='A', target='B', data=None, conditional=False),\n        Edge(source='Z', target='__end__', data=None, conditional=False),\n        Edge(source='__start__', target='A', data=True, conditional=True),\n        Edge(source='__start__', target='__end__', data=False, conditional=True),\n        Edge(source='B', target='Z', data=True, conditional=True),\n        Edge(source='B', target='__end__', data=False, conditional=True)\n    ]\n)\nGraph(\n    nodes={\n        '__start__': Node(id='__start__', name='__start__', data=<class 'pydantic.v1.main.LangGraphInput'>, metadata=None),\n        'main_entry': Node(id='main_entry', name='main_entry', data=main_entry(func_accepts_config=False, afunc_accepts_config=False, recurse=True), metadata=None),\n        'sub_1': Node(id='sub_1', name='sub_1', data=<langgraph.graph.state.CompiledStateGraph object at 0x000001D7541179B0>, metadata=None),\n        'main_exit': Node(id='main_exit', name='main_exit', data=main_exit(func_accepts_config=False, afunc_accepts_config=False, recurse=True), metadata=None),\n        '__end__': Node(id='__end__', name='__end__', data=<class 'pydantic.v1.main.LangGraphOutput'>, metadata=None)\n    },\n    edges=[\n        Edge(source='__start__', target='main_entry', data=None, conditional=False),\n        Edge(source='main_entry', target='sub_1', data=None, conditional=False),\n        Edge(source='main_exit', target='__end__', data=None, conditional=False),\n        Edge(source='sub_1', target='main_exit', data=None, conditional=False)\n    ]\n)\nDescription\n\nWhen there is an edge pointing from start or towards end, and another edge points to a node, part of the edges will disappear.\nSystem Info\nlanggraph==0.2.19\nlangchain-core==0.2.38", "created_at": "2024-09-10", "closed_at": "2024-12-09", "labels": [], "State": "closed", "Author": "gbaian10"}
{"issue_number": 1669, "issue_title": "Using Phi3 with Langgraph, Multi agent framework", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nNa\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nNa\nSystem Info\nThis issue is for a: (mark with an x)\n\n[x ] bug report -> please search issues before submitting\n[] feature request\n documentation issue or request\n regression (a behavior that used to work and stopped in a new release)\nMinimal steps to reproduce\nUsing Phi 3 with Langgraph\n\nAny log messages given by the failure\nNA\nExpected/desired behavior\nMulti agent framework with routing\nOS and Version?\nWindows 10\nMention any other details that might be useful\nI am trying to use Phi 3 mini 128 instruct with Langgraph for a multi-agent framework where it can route requests accurately. However, Phi 3 is not able to route the queries properly to a specific tool. Can anyone help? Does Phi 3 work with Langgraph, and does it support function calling or tool calling?", "created_at": "2024-09-10", "closed_at": "2024-09-10", "labels": [], "State": "closed", "Author": "hunaidkhan2000"}
{"issue_number": 1668, "issue_title": "custom checkpointers notebooks under examples are not working. I get errors for all 3 custom examples", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langgraph.checkpoint.postgres import PostgresSaver\nError Message and Stack Trace (if applicable)\nImportError: no pq wrapper available.\nAttempts made:\n- couldn't import psycopg 'c' implementation: No module named 'psycopg_c'\n- couldn't import psycopg 'binary' implementation: No module named 'psycopg_binary'\n- couldn't import psycopg 'python' implementation: libpq library not found\nDescription\nI installed all the necessary packages as mentioned in the notebooks\npip install -U psycopg psycopg-pool langgraph langgraph-checkpoint-postgres\nSystem Info\nI created a conda environment (Windows) and installed all the necessary packages there.", "created_at": "2024-09-10", "closed_at": "2024-09-11", "labels": [], "State": "closed", "Author": "shivachittamuru"}
{"issue_number": 1662, "issue_title": "No longer possible to have default value in graph State >=0.2.15", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nimport operator\nfrom typing import Literal, Annotated, TypedDict, Optional, List\nfrom langchain_core.pydantic_v1 import BaseModel, Field, validator\nfrom langchain_core.chat_history import BaseChatMessageHistory\nfrom langchain_core.documents import Document\nfrom langchain_core.messages import BaseMessage, AIMessage\nfrom langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\nfrom langgraph.graph import END, START, StateGraph, MessagesState\n\nclass MyState(TypedDict):\n    query: str\n    rephrase_query: str = None\n    session_id: str\n    documents: Annotated[list[Document], operator.add]\n    messages: list[BaseMessage]\n    chat_tosummarize: str\n    chat_summary: str = \"lol\"\n    output_graph: Optional[AIMessage] = None\n    intent: Optional[BaseModel]\n    guardrails_triggered: Optional[bool] = False\n\n# Define the function that calls the model\ndef test(state: TypedDict):\n    print(state['chat_summary'])\n    print(state['guardrails_triggered'])\n    return {\"output_graph\": \"hello\"}\n\n# Define a new graph\nworkflow = StateGraph(MyState)\nworkflow.add_node(\"test\", test)\nworkflow.add_edge(START, \"test\")\nworkflow.add_edge(\"test\", END)\n\napp = workflow.compile()\n\napp.invoke({\"query\":\"hello\"})\nError Message and Stack Trace (if applicable)\nKeyError: 'chat_summary'\nDescription\nHello,\nFrom langgraph==0.2.15 it is no longer possible to set default value in state like in my example.\nI used to set my state like that and use default value from my state into my nodes.\nI believe this change in the 0.2.15 release note is the cause of the issue:\nRemove None default for missing keys when generating input for each node\nIt is an intended behavior ? Or there is an alternative way to do this ?\nThanks for your help :)\nSystem Info\nwindows/linux\nPython 3.11.9", "created_at": "2024-09-09", "closed_at": "2024-09-09", "labels": [], "State": "closed", "Author": "Freezaa9"}
{"issue_number": 1655, "issue_title": "Future return state=Finished KeyError", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langchain_openai import AzureChatOpenAI\nfrom langgraph.graph import StateGraph, END,START\n\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], operator.add]\n    next: str\n\ndef predict_name(names):\n'''PROMPT'''\n  return {\"message\":llm.invoke(input={names})}\n\ndef predict_address(address):\n'''PROMPT'''\n  return {\"message\":llm.invoke(input={address})}\n\ndef final_result(state:AgentState):\n  messages = state[\"messages\"]\n  return {\"messages\":messages}\n\nmembers = [\"predict_address\",\"predict_name]\n\nsupervisor_prompt = \" my prompt \"\n\noptions = [\"FINISH\"] + members\noptions = members\nagent_scratchpad = []\n\nfunction_def = {\n    \"name\": \"route\",\n    \"description\": \"Select the next role.\",\n    \"parameters\": {\n        \"title\": \"routeSchema\",\n        \"type\": \"object\",\n        \"properties\": {\n            \"next\": {\n                \"title\": \"Next\",\n                \"anyOf\": [\n                    {\"enum\": options},\n                ],\n            }\n        },\n        \"required\": [\"next\"],\n    },\n}\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", supervisor_prompt),\n        MessagesPlaceholder(variable_name=\"messages\"),\n        (\n            \"system\",\n            \"Given the conversation above, who should act next?\"\n            \" Or should we END? Select one of: {options}\",\n        ),\n    ]\n).partial(options=str(options), members=\",\".join(members),names=names, address=address)\n\nsupervisor_chain = (prompt | llm_model.bind_functions(functions=[function_def], function_call=\"route\") | JsonOutputFunctionsParser())\n\nworkflow = StateGraph(AgentState)\n\nworkflow.add_node(\"Supervisor\",supervisor_chain)\nworkflow.add_node(\"predict_address\",predict_address)\nworkflow.add_node(\"predict_name\",predict_name)\nworkflow.add_node(\"final_result\",final_result)\n\nconditional_map = {k: k for k in members}\nconditional_map[\"final_result\"] = \"final_result\"\nworkflow.add_conditional_edges(\"Supervisor\", lambda x: x[\"next\"], conditional_map)\nworkflow.add_edge(START, \"Supervisor\")\nfor member in members:\n    # We want our workers to ALWAYS \"report back\" to the supervisor when done\n    workflow.add_edge(\"Supervisor\",member)\n    workflow.add_edge(member,\"final_result\")\n    workflow.add_edge(\"Supervisor\",END)\n\napp = workflow.compile()\n\ngraph_input = {\"messages\":[\"\"]}\n\nresponse = app.invoke(graph_input)\nError Message and Stack Trace (if applicable)\nException has occurred: KeyError\n<Future at 0x15f32aa50 state=finished raised KeyError>\n  File \"/create_langgraph.py\", line 418, in CreateLanggraph\n    response = app.invoke(graph_input)\n               ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/kushagrasahu/Documents/projects/SLAM-AI/src/ai_comment_generation/defect_impact.py\", line 87, in predict_impact\n    config_output, hardware_output, final_impact = CreateLanggraph(names, address)\n                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyError: <Future at 0x15f32aa50 state=finished raised KeyError>\nDescription\nOccurring When calling the function CreateLanggraph from another file. No error when running I interpreter (Notebook)\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.5.0: Wed May  1 20:13:18 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T6030\nPython Version:  3.11.8 (v3.11.8:db85d51d3e, Feb  6 2024, 18:02:37) [Clang 13.0.0 (clang-1300.0.29.30)]\n\nPackage Information\n\nlangchain_core: 0.2.28\nlangchain: 0.2.12\nlangchain_community: 0.2.11\nlangsmith: 0.1.98\nlangchain_aws: 0.1.15\nlangchain_openai: 0.1.20\nlangchain_text_splitters: 0.2.2\nlanggraph: 0.2.3\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlangserve\n", "created_at": "2024-09-08", "closed_at": "2024-09-21", "labels": [], "State": "closed", "Author": "KushagraSahu20012000"}
{"issue_number": 1634, "issue_title": "AttributeError: 'CompiledStateGraph' object has no attribute 'copy", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nworkflow.add_conditional_edges(\n    \"tools\",\n    route_tool_results,\n    {\n        \"assistant\": \"assistant\",\n        \"update_transactions\": \"update_transactions\"\n    }\n)\nworkflow.add_edge(\"ask_human\", \"assistant\")\nworkflow.add_edge(\"update_transactions\", \"assistant\")\n\n# The checkpointer lets the graph persist its state\n# this is a complete memory for the entire graph.\nmemory = MemorySaver()\ngraph = workflow.compile(checkpointer=memory, interrupt_before=[\"ask_human\"],)\nError Message and Stack Trace (if applicable)\nlanggraph-api-1       | 2024-09-06T04:08:36.068536Z [error    ] Background run failed          [langgraph_api.queue] api_revision=a4cec24 api_variant=licensed run_created_at=2024-09-06T04:08:35.993513+00:00 run_ended_at=2024-09-06T04:08:36.068297 run_exec_ms=1 run_id=1ef6c05b-0427-6f80-894f-61d9743f0482 run_started_at=2024-09-06T04:08:36.066956+00:00\nlanggraph-api-1       | Traceback (most recent call last):\nlanggraph-api-1       |   File \"/api/langgraph_api/queue.py\", line 92, in worker\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/asyncio/tasks.py\", line 489, in wait_for\nlanggraph-api-1       |     return fut.result()\nlanggraph-api-1       |            ^^^^^^^^^^^^\nlanggraph-api-1       |   File \"/api/langgraph_api/shared/stream.py\", line 225, in consume\nlanggraph-api-1       |   File \"/api/langgraph_api/shared/stream.py\", line 213, in consume\nlanggraph-api-1       |   File \"/api/langgraph_api/shared/stream.py\", line 90, in astream_state\nlanggraph-api-1       |   File \"/api/langgraph_api/shared/graph.py\", line 87, in get_graph\nlanggraph-api-1       | AttributeError: 'CompiledStateGraph' object has no attribute 'copy'\nDescription\nHI Team\nI started getting the following error when doing\nthis.client.runs.stream(\n      this.thread.thread_id,\n      this.assistant.assistant_id,\n      { input, streamMode: 'messages' }\n    ))\nim running langgraph up\n\u279c  langgraph-pyproject git:(master) \u2717 langgraph up            \nStarting LangGraph API server...\nFor local dev, requires env var LANGSMITH_API_KEY with access to LangGraph Cloud closed beta.\nFor production use, requires a license key in env var LANGGRAPH_CLOUD_LICENSE_KEY.\nReady!       \n- API: http://localhost:8123\n- Docs: http://localhost:8123/docs\n- Debugger: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:8123\nlanggraph-api-1       | 2024-09-06T04:07:24.911340Z [info     ] Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit) [uvicorn.error] api_revision=a4cec24 api_variant=licensed color_message=Uvicorn running on %s://%s:%d (Press CTRL+C to quit)\nlanggraph-api-1       | 2024-09-06T04:07:25.657213Z [info     ] GET /ok 200 1ms                [langgraph_api.server] api_revision=a4cec24 api_variant=licensed latency_ms=1 method=GET path=/ok path_params={} route=None status=200\nlanggraph-api-1       | 2024-09-06T04:08:25.730401Z [info     ] GET /ok 200 0ms                [langgraph_api.server] api_revision=a4cec24 api_variant=licensed latency_ms=0 method=GET path=/ok path_params={} route=None status=200\nlanggraph-api-1       | 2024-09-06T04:08:28.330420Z [info     ] POST /assistants 200 2ms       [langgraph_api.server] api_revision=a4cec24 api_variant=licensed latency_ms=2 method=POST path=/assistants path_params={} route=/assistants status=200\nlanggraph-api-1       | 2024-09-06T04:08:28.336284Z [info     ] POST /threads 200 2ms          [langgraph_api.server] api_revision=a4cec24 api_variant=licensed latency_ms=2 method=POST path=/threads path_params={} route=/threads status=200\nlanggraph-api-1       | 2024-09-06T04:08:36.067077Z [info     ] Starting background run        [langgraph_api.queue] api_revision=a4cec24 api_variant=licensed run_created_at=2024-09-06T04:08:35.993513+00:00 run_id=1ef6c05b-0427-6f80-894f-61d9743f0482 run_queue_ms=73 run_started_at=2024-09-06T04:08:36.066956+00:00\nlanggraph-api-1       | 2024-09-06T04:08:36.068536Z [error    ] Background run failed          [langgraph_api.queue] api_revision=a4cec24 api_variant=licensed run_created_at=2024-09-06T04:08:35.993513+00:00 run_ended_at=2024-09-06T04:08:36.068297 run_exec_ms=1 run_id=1ef6c05b-0427-6f80-894f-61d9743f0482 run_started_at=2024-09-06T04:08:36.066956+00:00\nlanggraph-api-1       | Traceback (most recent call last):\nlanggraph-api-1       |   File \"/api/langgraph_api/queue.py\", line 92, in worker\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/asyncio/tasks.py\", line 489, in wait_for\nlanggraph-api-1       |     return fut.result()\nlanggraph-api-1       |            ^^^^^^^^^^^^\nlanggraph-api-1       |   File \"/api/langgraph_api/shared/stream.py\", line 225, in consume\nlanggraph-api-1       |   File \"/api/langgraph_api/shared/stream.py\", line 213, in consume\nlanggraph-api-1       |   File \"/api/langgraph_api/shared/stream.py\", line 90, in astream_state\nlanggraph-api-1       |   File \"/api/langgraph_api/shared/graph.py\", line 87, in get_graph\nlanggraph-api-1       | AttributeError: 'CompiledStateGraph' object has no attribute 'copy'\n\nHave tried all the usual suspects\ndeleting all my docker images\ndocker system prune\npoetry upgrade\nbut no luck\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP PREEMPT_DYNAMIC Mon Aug 19 14:09:30 UTC 2024\nPython Version:  3.12.5 (main, Aug  7 2024, 00:00:00) [GCC 14.2.1 20240801 (Red Hat 14.2.1-1)]\n\nPackage Information\n\nlangchain_core: 0.1.52\nlangchain_community: 0.0.38\nlangsmith: 0.1.77\nlangchain_cli: 0.0.25\nlangserve: 0.2.2\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlanggraph\n", "created_at": "2024-09-06", "closed_at": "2024-09-06", "labels": [], "State": "closed", "Author": "darthShana"}
{"issue_number": 1632, "issue_title": "Debug mode of create_react_agent does not work after upgrading to the latest version", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\ncreate_react_agent(\n    model=llm,\n    tools=[\n        *tools(),\n    ],\n    state_modifier=system_message,\n    debug=True,\n)\nError Message and Stack Trace (if applicable)\nagent = create_react_agent(\n    model=llm,\n    tools=[\n        *tools(),\n    ],\n    state_modifier=system_message,\n    debug=True,\n)\n\nagent.astream(input={\"messages\": messages})\nDescription\nNo debug log output\nSystem Info\nlanggraph==0.2.18", "created_at": "2024-09-06", "closed_at": "2024-09-06", "labels": [], "State": "closed", "Author": "Valdanitooooo"}
{"issue_number": 1618, "issue_title": "Send for subgraph has an error about checkpoint id", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langgraph.graph import START, StateGraph, END\nfrom langgraph.constants import Send\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langchain_core.runnables import RunnableLambda\n\nimport operator\nfrom collections.abc import Sequence\nfrom typing import Annotated, TypedDict, Union\n\nfrom langchain_core.messages import BaseMessage, AIMessage, HumanMessage\nimport time\n\n\nclass SupervisorState(TypedDict):\n    messages: Annotated[Sequence[Union[BaseMessage, dict]], operator.add]\n\n\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[Union[BaseMessage, dict]], operator.add]\n\n\ndef supervisor_graph(checkpointer):\n    def router(state: SupervisorState):\n        return [Send(\"return_bad\", {\"messages\": [AIMessage(content=\"bad\")]})]\n\n    supervisor = StateGraph(SupervisorState)\n    supervisor.add_edge(START, \"return_good\")\n\n    supervisor.add_node(\"return_good\", RunnableLambda(lambda x:{\"messages\": [AIMessage(content=\"good\")]}))\n    supervisor.add_conditional_edges(\"return_good\", router, [\"return_bad\"])\n\n    supervisor.add_node(\"return_bad\", agent_graph(checkpointer))\n    supervisor.add_edge(\"return_bad\", END)\n\n    return supervisor.compile(checkpointer)\n\n\ndef agent_graph(checkpointer):\n    agent = StateGraph(AgentState)\n    agent.add_edge(START, \"return_bad\")\n\n    agent.add_node(\"return_bad\", RunnableLambda(lambda x:{\"messages\": [AIMessage(content=\"bad\")]}))\n    agent.add_edge(\"return_bad\", END)\n\n    return agent.compile(checkpointer=checkpointer)\n\n\nmemory = MemorySaver()\ng = supervisor_graph(checkpointer=memory)\n\nthread_id = str(time.time())\nconfig = {\n\t\"configurable\": {\n\t\t\"thread_id\": thread_id,\n\t}\n}\ng.invoke(\n\t{\n\t\t\"inputs\": {\"language\": \"Korean\"},\n\t\t\"messages\": [HumanMessage(content=\"how do you linke this dress?\")]\n\t},\n\tconfig\n)\nError Message and Stack Trace (if applicable)\n[chain/start] [chain:LangGraph] Entering Chain run with input:\n[inputs]\n[chain/start] [chain:LangGraph > chain:__start__] Entering Chain run with input:\n[inputs]\n[chain/end] [chain:LangGraph > chain:__start__] [0ms] Exiting Chain run with output:\n[outputs]\n[chain/start] [chain:LangGraph > chain:return_good] Entering Chain run with input:\n[inputs]\n[chain/start] [chain:LangGraph > chain:return_good > chain:RunnableLambda] Entering Chain run with input:\n[inputs]\n[chain/end] [chain:LangGraph > chain:return_good > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n[outputs]\n[chain/start] [chain:LangGraph > chain:return_good > chain:ChannelWrite<return_good,messages>] Entering Chain run with input:\n[inputs]\n[chain/end] [chain:LangGraph > chain:return_good > chain:ChannelWrite<return_good,messages>] [0ms] Exiting Chain run with output:\n[outputs]\n[chain/start] [chain:LangGraph > chain:return_good > chain:router] Entering Chain run with input:\n[inputs]\n[chain/end] [chain:LangGraph > chain:return_good > chain:router] [0ms] Exiting Chain run with output:\n[outputs]\n[chain/start] [chain:LangGraph > chain:return_good > chain:ChannelWrite<return_bad>] Entering Chain run with input:\n[inputs]\n[chain/end] [chain:LangGraph > chain:return_good > chain:ChannelWrite<return_bad>] [0ms] Exiting Chain run with output:\n[outputs]\n[chain/end] [chain:LangGraph > chain:return_good] [3ms] Exiting Chain run with output:\n[outputs]\n[chain/start] [chain:LangGraph > chain:return_bad] Entering Chain run with input:\n[inputs]\n[chain/start] [chain:LangGraph > chain:return_bad > chain:LangGraph] Entering Chain run with input:\n[inputs]\n[chain/error] [chain:LangGraph > chain:return_bad > chain:LangGraph] [1ms] Chain run errored with error:\n\"KeyError('checkpoint_id')Traceback (most recent call last):\\n\\n\\n  File \\\"/usr/local/lib/python3.9/site-packages/langgraph/pregel/__init__.py\\\", line 908, in stream\\n    while loop.tick(\\n\\n\\n  File \\\"/usr/local/lib/python3.9/site-packages/langgraph/pregel/loop.py\\\", line 206, in tick\\n    self._first(input_keys=input_keys)\\n\\n\\n  File \\\"/usr/local/lib/python3.9/site-packages/langgraph/pregel/loop.py\\\", line 356, in _first\\n    self._put_checkpoint({\\\"source\\\": \\\"input\\\", \\\"writes\\\": self.input})\\n\\n\\n  File \\\"/usr/local/lib/python3.9/site-packages/langgraph/pregel/loop.py\\\", line 377, in _put_checkpoint\\n    id=self.config[\\\"configurable\\\"][\\\"checkpoint_id\\\"]\\n\\n\\nKeyError: 'checkpoint_id'\"\n[chain/error] [chain:LangGraph > chain:return_bad] [3ms] Chain run errored with error:\n\"KeyError('checkpoint_id')Traceback (most recent call last):\\n\\n\\n  File \\\"/usr/local/lib/python3.9/site-packages/langchain_core/runnables/base.py\\\", line 2876, in invoke\\n    input = context.run(step.invoke, input, config, **kwargs)\\n\\n\\n  File \\\"/usr/local/lib/python3.9/site-packages/langgraph/pregel/__init__.py\\\", line 1307, in invoke\\n    for chunk in self.stream(\\n\\n\\n  File \\\"/usr/local/lib/python3.9/site-packages/langgraph/pregel/__init__.py\\\", line 908, in stream\\n    while loop.tick(\\n\\n\\n  File \\\"/usr/local/lib/python3.9/site-packages/langgraph/pregel/loop.py\\\", line 206, in tick\\n    self._first(input_keys=input_keys)\\n\\n\\n  File \\\"/usr/local/lib/python3.9/site-packages/langgraph/pregel/loop.py\\\", line 356, in _first\\n    self._put_checkpoint({\\\"source\\\": \\\"input\\\", \\\"writes\\\": self.input})\\n\\n\\n  File \\\"/usr/local/lib/python3.9/site-packages/langgraph/pregel/loop.py\\\", line 377, in _put_checkpoint\\n    id=self.config[\\\"configurable\\\"][\\\"checkpoint_id\\\"]\\n\\n\\nKeyError: 'checkpoint_id'\"\n[chain/error] [chain:LangGraph] [17ms] Chain run errored with error:\n\"KeyError('checkpoint_id')Traceback (most recent call last):\\n\\n\\n  File \\\"/usr/local/lib/python3.9/site-packages/langgraph/pregel/__init__.py\\\", line 993, in stream\\n    _panic_or_proceed(all_futures, loop.step)\\n\\n\\n  File \\\"/usr/local/lib/python3.9/site-packages/langgraph/pregel/__init__.py\\\", line 1423, in _panic_or_proceed\\n    raise exc\\n\\n\\n  File \\\"/usr/local/lib/python3.9/site-packages/langgraph/pregel/executor.py\\\", line 60, in done\\n    task.result()\\n\\n\\n  File \\\"/usr/local/lib/python3.9/concurrent/futures/_base.py\\\", line 439, in result\\n    return self.__get_result()\\n\\n\\n  File \\\"/usr/local/lib/python3.9/concurrent/futures/_base.py\\\", line 391, in __get_result\\n    raise self._exception\\n\\n\\n  File \\\"/usr/local/lib/python3.9/concurrent/futures/thread.py\\\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n\\n\\n  File \\\"/usr/local/lib/python3.9/site-packages/opentelemetry/instrumentation/threading/__init__.py\\\", line 143, in wrapped_func\\n    return original_func(*func_args, **func_kwargs)\\n\\n\\n  File \\\"/usr/local/lib/python3.9/site-packages/langgraph/pregel/retry.py\\\", line 26, in run_with_retry\\n    task.proc.invoke(task.input, task.config)\\n\\n\\n  File \\\"/usr/local/lib/python3.9/site-packages/langchain_core/runnables/base.py\\\", line 2876, in invoke\\n    input = context.run(step.invoke, input, config, **kwargs)\\n\\n\\n  File \\\"/usr/local/lib/python3.9/site-packages/langgraph/pregel/__init__.py\\\", line 1307, in invoke\\n    for chunk in self.stream(\\n\\n\\n  File \\\"/usr/local/lib/python3.9/site-packages/langgraph/pregel/__init__.py\\\", line 908, in stream\\n    while loop.tick(\\n\\n\\n  File \\\"/usr/local/lib/python3.9/site-packages/langgraph/pregel/loop.py\\\", line 206, in tick\\n    self._first(input_keys=input_keys)\\n\\n\\n  File \\\"/usr/local/lib/python3.9/site-packages/langgraph/pregel/loop.py\\\", line 356, in _first\\n    self._put_checkpoint({\\\"source\\\": \\\"input\\\", \\\"writes\\\": self.input})\\n\\n\\n  File \\\"/usr/local/lib/python3.9/site-packages/langgraph/pregel/loop.py\\\", line 377, in _put_checkpoint\\n    id=self.config[\\\"configurable\\\"][\\\"checkpoint_id\\\"]\\n\\n\\nKeyError: 'checkpoint_id'\"\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[8], line 56\n     50 thread_id = str(time.time())\n     51 config = {\n     52 \t\"configurable\": {\n     53 \t\t\"thread_id\": thread_id,\n     54 \t}\n     55 }\n---> 56 g.invoke(\n     57 \t{\n     58 \t\t\"inputs\": {\"language\": \"Korean\"},\n     59 \t\t\"messages\": [HumanMessage(content=\"how do you linke this dress?\")]\n     60 \t},\n     61 \tconfig\n     62 )\n\nFile /usr/local/lib/python3.9/site-packages/langgraph/pregel/__init__.py:1307, in Pregel.invoke(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\n   1305 else:\n   1306     chunks = []\n-> 1307 for chunk in self.stream(\n   1308     input,\n   1309     config,\n   1310     stream_mode=stream_mode,\n   1311     output_keys=output_keys,\n   1312     interrupt_before=interrupt_before,\n   1313     interrupt_after=interrupt_after,\n   1314     debug=debug,\n   1315     **kwargs,\n   1316 ):\n   1317     if stream_mode == \"values\":\n   1318         latest = chunk\n\nFile /usr/local/lib/python3.9/site-packages/langgraph/pregel/__init__.py:993, in Pregel.stream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug)\n    990         break\n    992 # panic on failure or timeout\n--> 993 _panic_or_proceed(all_futures, loop.step)\n    994 # don't keep futures around in memory longer than needed\n    995 del done, inflight, futures\n\nFile /usr/local/lib/python3.9/site-packages/langgraph/pregel/__init__.py:1423, in _panic_or_proceed(futs, step, timeout_exc_cls)\n   1421             inflight.pop().cancel()\n   1422         # raise the exception\n-> 1423         raise exc\n   1425 if inflight:\n   1426     # if we got here means we timed out\n   1427     while inflight:\n   1428         # cancel all pending tasks\n\nFile /usr/local/lib/python3.9/site-packages/langgraph/pregel/executor.py:60, in BackgroundExecutor.done(self, task)\n     58 def done(self, task: concurrent.futures.Future) -> None:\n     59     try:\n---> 60         task.result()\n     61     except GraphInterrupt:\n     62         # This exception is an interruption signal, not an error\n     63         # so we don't want to re-raise it on exit\n     64         self.tasks.pop(task)\n\nFile /usr/local/lib/python3.9/concurrent/futures/_base.py:439, in Future.result(self, timeout)\n    437     raise CancelledError()\n    438 elif self._state == FINISHED:\n--> 439     return self.__get_result()\n    441 self._condition.wait(timeout)\n    443 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\nFile /usr/local/lib/python3.9/concurrent/futures/_base.py:391, in Future.__get_result(self)\n    389 if self._exception:\n    390     try:\n--> 391         raise self._exception\n    392     finally:\n    393         # Break a reference cycle with the exception in self._exception\n    394         self = None\n\nFile /usr/local/lib/python3.9/concurrent/futures/thread.py:58, in _WorkItem.run(self)\n     55     return\n     57 try:\n---> 58     result = self.fn(*self.args, **self.kwargs)\n     59 except BaseException as exc:\n     60     self.future.set_exception(exc)\n\nFile /usr/local/lib/python3.9/site-packages/opentelemetry/instrumentation/threading/__init__.py:143, in ThreadingInstrumentor.__wrap_thread_pool_submit.<locals>.wrapped_func(*func_args, **func_kwargs)\n    141 try:\n    142     token = context.attach(otel_context)\n--> 143     return original_func(*func_args, **func_kwargs)\n    144 finally:\n    145     context.detach(token)\n\nFile /usr/local/lib/python3.9/site-packages/langgraph/pregel/retry.py:26, in run_with_retry(task, retry_policy)\n     24 task.writes.clear()\n     25 # run the task\n---> 26 task.proc.invoke(task.input, task.config)\n     27 # if successful, end\n     28 break\n\nFile /usr/local/lib/python3.9/site-packages/langchain_core/runnables/base.py:2876, in RunnableSequence.invoke(self, input, config, **kwargs)\n   2874 context.run(_set_config_context, config)\n   2875 if i == 0:\n-> 2876     input = context.run(step.invoke, input, config, **kwargs)\n   2877 else:\n   2878     input = context.run(step.invoke, input, config)\n\nFile /usr/local/lib/python3.9/site-packages/langgraph/pregel/__init__.py:1307, in Pregel.invoke(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\n   1305 else:\n   1306     chunks = []\n-> 1307 for chunk in self.stream(\n   1308     input,\n   1309     config,\n   1310     stream_mode=stream_mode,\n   1311     output_keys=output_keys,\n   1312     interrupt_before=interrupt_before,\n   1313     interrupt_after=interrupt_after,\n   1314     debug=debug,\n   1315     **kwargs,\n   1316 ):\n   1317     if stream_mode == \"values\":\n   1318         latest = chunk\n\nFile /usr/local/lib/python3.9/site-packages/langgraph/pregel/__init__.py:908, in Pregel.stream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug)\n    877 (\n    878     debug,\n    879     stream_modes,\n   (...)\n    890     debug=debug,\n    891 )\n    893 with SyncPregelLoop(\n    894     input,\n    895     config=config,\n   (...)\n    906     # channels are guaranteed to be immutable for the duration of the step,\n    907     # with channel updates applied only at the transition between steps\n--> 908     while loop.tick(\n    909         input_keys=self.input_channels,\n    910         interrupt_before=interrupt_before,\n    911         interrupt_after=interrupt_after,\n    912         manager=run_manager,\n    913     ):\n    914         # debug flag\n    915         if debug:\n    916             print_step_checkpoint(\n    917                 loop.checkpoint_metadata,\n    918                 loop.channels,\n    919                 self.stream_channels_list,\n    920             )\n\nFile /usr/local/lib/python3.9/site-packages/langgraph/pregel/loop.py:206, in PregelLoop.tick(self, input_keys, interrupt_after, interrupt_before, manager)\n    203     raise RuntimeError(\"Cannot tick when status is no longer 'pending'\")\n    205 if self.input not in (INPUT_DONE, INPUT_RESUMING):\n--> 206     self._first(input_keys=input_keys)\n    207 elif all(task.writes for task in self.tasks):\n    208     writes = [w for t in self.tasks for w in t.writes]\n\nFile /usr/local/lib/python3.9/site-packages/langgraph/pregel/loop.py:356, in PregelLoop._first(self, input_keys)\n    349     assert not apply_writes(\n    350         self.checkpoint,\n    351         self.channels,\n    352         discard_tasks + [PregelTaskWrites(INPUT, input_writes, [])],\n    353         self.checkpointer_get_next_version,\n    354     ), \"Can't write to SharedValues in graph input\"\n    355     # save input checkpoint\n--> 356     self._put_checkpoint({\"source\": \"input\", \"writes\": self.input})\n    357 else:\n    358     raise EmptyInputError(f\"Received no input for {input_keys}\")\n\nFile /usr/local/lib/python3.9/site-packages/langgraph/pregel/loop.py:377, in PregelLoop._put_checkpoint(self, metadata)\n    366 if self._checkpointer_put_after_previous is not None:\n    367     # create new checkpoint\n    368     self.checkpoint_metadata = metadata\n    369     self.checkpoint = create_checkpoint(\n    370         self.checkpoint,\n    371         self.channels,\n    372         self.step,\n    373         # child graphs keep at most one checkpoint per parent checkpoint\n    374         # this is achieved by writing child checkpoints as progress is made\n    375         # (so that error recovery / resuming from interrupt don't lose work)\n    376         # but doing so always with an id equal to that of the parent checkpoint\n--> 377         id=self.config[\"configurable\"][\"checkpoint_id\"]\n    378         if self.is_nested\n    379         else None,\n    380     )\n    382     self.checkpoint_config = {\n    383         **self.checkpoint_config,\n    384         \"configurable\": {\n   (...)\n    389         },\n    390     }\n    392     channel_versions = self.checkpoint[\"channel_versions\"].copy()\n\nKeyError: 'checkpoint_id'\nDescription\nI am using Supervision framework and I think supervisor can call the agent two time at once. (for example when the question is \"what's the weather in NewYork and SF\" then Call the Document Agent(\"what's the weather of NewYork\") and DocumentAgent(\"what's the weather of SanFrancisco\")) So I choose the function Send but it is not working now. The above code completely reproduce this error. Why does this error raised? Can you fix it??\nAnd, I try to give random checkpoint id.\nand It helps for first time but when next second question I entered into, the chat history got something wrong. chat history lost some messages.\nThere are no documents about this checkpoint or checkpoin id and no examples of calling agent from supervisor by Send.\nI can not figure out anything else about this. Plz help me. I am looking forward to your rapid reaction and update and explanation. So appreciate.\nAnd lastly this is not directly related to this issues much but I wonder there are some state trash(=remains).\nIt means If I call the agent by Send and then I call the agent one more after that time, the state of agent is not initialized. I want to use the agent by Send for One-Time using but I guess it is not. It is your intent? Can I use One-Time agent that initialize everytime they are called or after returning the result to supervisor\nSystem Info\nlanggraph==0.2.12\nlanggraph-checkpoint==1.0.3", "created_at": "2024-09-05", "closed_at": "2024-09-05", "labels": [], "State": "closed", "Author": "sangmandu"}
{"issue_number": 1617, "issue_title": "DOC: <Please write a comprehensive title after the 'DOC: ' prefix>How can I add custom tools using BaseModel and BaseTool to the official sample code?", "issue_body": "Issue with current documentation:\nofficial sample code link:https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/hierarchical_agent_teams.ipynb\nIdea or request for content:\nThe custom tool is as follows (the image is highlighted in red because I removed some parts of the function for screenshots, so please ignore it) :\n\nDefine the agent node in langgraph as follows (wrong feeling):\n\nThe error is as follows:\nTypeError: Object of type 'ModelMetaclass' is not JSON serializable\nor\nTypeError: _run() missing 2 required positional arguments: 'self' and 'network_segment'", "created_at": "2024-09-05", "closed_at": "2024-09-05", "labels": [], "State": "closed", "Author": "guapia233"}
{"issue_number": 1587, "issue_title": "The first node doesn't accept inputs other than InputState, but other nodes can", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom typing import TypedDict\n\nfrom langgraph.graph import END, START, MessagesState, StateGraph\n\n\nclass OverallState(MessagesState):\n    pass\n\n\nclass InputState(TypedDict):\n    question: str\n\n\nclass OutputState(TypedDict):\n    answer: str\n\n\nclass PrivateState(TypedDict):\n    secret: str\n\n\ndef first(state):\n    # Without type hint, the node following `__start__` only accepts\n    # `InputState` as the initial input.\n    print(\"=====This is First=====\")\n    # At this point, the filter will be handled by `OverallState`, not `InputState`.\n    print(state)  # {\"messages\": []}\n    return {\"answer\": \"Foo\"}\n\n\ndef middle(state):\n    # Without a type hint, non the node following `__start__` can accept at least one of\n    # `OverallState`, `InputState`, or `OutputState` as input.\n    print(\"=====This is Middle=====\")\n    print(state)\n    # return {\"secret\": \"Bar\"}  # ok\n    return {\"question\": \"Bar\"}\n\n\ndef last(state: PrivateState):\n    # With a type hint, it will additionally accept the extra key as a private state input,\n    # even if the previous node may not include this input.\n    # It will still act as a filter in this function, filtering out inputs that do not match this hint.\n    print(\"=====This is Last=====\")\n    print(state)\n    # Output = {}; Because the output of the previous node doesn't include PrivateState.\n    return {\"messages\": [(\"human\", \"World!\")]}\n\n\ngraph_builder = StateGraph(OverallState, input=InputState, output=OutputState)\n\ngraph_builder.add_node(first)\ngraph_builder.add_node(middle)\ngraph_builder.add_node(last)\n\ngraph_builder.add_edge(START, first.__name__)\ngraph_builder.add_edge(first.__name__, middle.__name__)\ngraph_builder.add_edge(middle.__name__, last.__name__)\ngraph_builder.add_edge(last.__name__, END)\napp = graph_builder.compile()\n# print(\"output: \", app.invoke({\"messages\": [(\"human\", \"Hello\")]})) # it will raise an error\nprint(\"output: \", app.invoke({\"question\": \"What is your name?\"}))\nError Message and Stack Trace (if applicable)\nOnly the first node doesn't accept three states; the other nodes can accept three states and extended states.\n\nIs this a feature or a bug?\nBecause it is very inconsistent with the state output in the same function (using `OverallState` as a filter).\n\nDescription\nHere is my understanding of the states: all nodes can accept OverallState, InputState, and OutputState as inputs, even if you don't include type hints.\nIf your type hint is not one of the above three, it will be considered an additional acceptable input. However, the previous node output must still include at least one key from these four states.\nIf you don't include type hints and you input the three states (StateGraph(OverallState, input=InputState, output=OutputState)), the filter will select OverallState.\nIf you don't include type hints and you input two states (StateGraph(input=InputState, output=OutputState)), the filter will select InputState.\nSystem Info\nlanggraph==0.2.16\nlangchain-core==0.2.37\nplatform==linux\npython-version==3.12.5", "created_at": "2024-09-03", "closed_at": "2024-09-04", "labels": [], "State": "closed", "Author": "gbaian10"}
{"issue_number": 1570, "issue_title": "Import error WRITES_IDX_MAP langgraph.checkpoint", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n\nDB_NAME = \"agent_memory.sqlite\"\nmemory = AsyncSqliteSaver(aiosqlite.connect(DB_NAME))\nError Message and Stack Trace (if applicable)\npython3.11/site-packages/langgraph/checkpoint/sqlite/__init__.py\", line 9, in <module>\n    from langgraph.checkpoint.base import (\nImportError: cannot import name 'WRITES_IDX_MAP' from 'langgraph.checkpoint.base' (/opt/miniconda3/envs/agent-dev/lib/python3.11/site-packages/langgraph/checkpoint/base/__init__.py)\nDescription\nwhile importing AsyncSqliteSaver this error occurs\nSystem Info\nlangchain-community = \"^0.2.15\"\nlanggraph = \"^0.2.15\"\nlanggraph-checkpoint-sqlite = \"^1.0.1\"\nlangchain-core = \"^0.2.37\"", "created_at": "2024-09-01", "closed_at": "2024-09-01", "labels": [], "State": "closed", "Author": "HiraveBapu"}
{"issue_number": 1569, "issue_title": "Graph not getting continued from the interupt ", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\n@cv_app.post(\"/\")\nasync def create_cv_graph(request: InputResumeModel):\n    \"\"\"\n    Initializes the CV creation process by starting the graph.\n\n    Args:\n        request (InputResumeModel): The input resume model.\n\n    Returns:\n        The result of the graph invocation.\n    \"\"\"\n    # Generate a unique thread ID for this session\n    thread_id = generate_random_string(length=20)\n    logger.info(f\"Generated thread ID: {thread_id}\")\n\n    # Prepare the input data for the graph\n    input_data = {\"cv\": [json.loads(request.model_dump_json())]}\n    config = {\"configurable\": {\"thread_id\": thread_id}}\n\n    # Invoke the graph to start the CV creation process\n    return_value = graph.stream(\n        input=input_data,\n        config=config,\n        stream_mode=\"values\"\n    )\n\n    \n    return return_value\n\n@cv_app.get(\"/image\")\ndef get_image():\n    get_graph()\n\n@cv_app.post(\"/message\")\ndef get_response(request: ChatModel):\n    \"\"\"\n    Handles the continuation of the graph based on the current state.\n\n    Args:\n        request (ChatModel): Input request model.\n    \"\"\"\n    logger.info(f\"Request received for message API: {request}\")\n    \n    thread_id = request.thread\n    config = {\"configurable\": {\"thread_id\": thread_id}}\n    return_value = None\n\n    # Retrieve the current state\n    current_state = graph.get_state(config=config)[1][0]\n    message_input = {\"complete_messages\": [request.message]}\n    \n    # Determine the next action based on the current state\n    if current_state == \"edit_cv\":\n        return_value = graph.invoke(\n            input=message_input,\n            config=config,\n            stream_mode=\"stream\"\n        )\n        \n    elif current_state == \"get_details\":\n        message_input[\"get_details_messages\"] = [request.message]\n        return_value = graph.invoke(\n            input=message_input,\n            config=config,\n            stream_mode=\"values\"\n        )\n        \n\n    return return_value\n\n\ndef builder():\n    # Add nodes to the graph\n    graph = StateGraph(InputGraphState)\n    graph.add_node(\"create_cv\", create_cv_node)\n    graph.add_node(\"get_details\", get_details)\n    graph.add_node(\"render_cv\", render_cv)\n    graph.add_node(\"edit_cv\", edit_cv_node)\n    # graph.add_node(\"get_missing_in_resume\", get_missing_values)\n\n    # Add edges to the graph\n    graph.add_edge(START, \"create_cv\")\n    graph.add_conditional_edges(\n        \"create_cv\", check_exception,{\"END\":END,\"render_cv\":\"render_cv\"}\n    )\n    graph.add_conditional_edges(\n        \"render_cv\",\n        check_render_cv,{\"END\":END,\"edit_cv\":\"edit_cv\"}\n        )\n    graph.add_conditional_edges(\n        \"edit_cv\", check_missing_values,{\"get_details\":\"get_details\",\"END\":END,\"render_cv\":\"render_cv\"})\n    graph.add_conditional_edges(\n        \"get_details\", check_details,{\"edit_cv\":\"edit_cv\",\"END\":END}\n    )\n    with pool.connection() as conn:\n        checkpointer = PostgresSaver(conn)\n        #checkpointer = MemorySaver()\n        return graph.compile(\n            checkpointer=checkpointer, interrupt_before=[\"get_details\",\"edit_cv\"]\n            )\nError Message and Stack Trace (if applicable)\nNo error message\nDescription\nI have used postgres checkpoint\nwhen i call the /message api the graph is not getting continued from \"edit_cv\" its rather going to the start of \"create_cv\" there any thing in am missing out here\nSystem Info\nThese are the version of libraries i am using\nlanggraph==0.2.15\nlanggraph-checkpoint==1.0.8\nlanggraph-checkpoint-postgres==1.0.4", "created_at": "2024-09-01", "closed_at": "2024-09-02", "labels": [], "State": "closed", "Author": "vigneshmj1997"}
{"issue_number": 1568, "issue_title": "Problem with add_messages, message didn't get merged", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom typing import Annotated\n\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\nfrom langchain_openai import ChatOpenAI\n\nclass State(TypedDict):\n    # Messages have the type \"list\". The `add_messages` function\n    # in the annotation defines how this state key should be updated\n    # (in this case, it appends messages to the list, rather than overwriting them)\n    messages: Annotated[list, add_messages]\n\ndef chatbot(state: State):\n    model = ChatOpenAI(temperature=0.1, model_name=\"gpt-3.5-turbo\")\n    print(state)\n    return {\"messages\": [model.invoke(state[\"messages\"])]}\n\ngraph_builder = StateGraph(State)\ngraph_builder.add_node(\"chatbot\", chatbot)\ngraph_builder.add_edge(START, \"chatbot\")\ngraph_builder.add_edge(\"chatbot\", END)\ngraph = graph_builder.compile()\n\nwhile True:\n    user_input = input(\"User: \")\n    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n        print(\"Goodbye!\")\n        break\n    for event in graph.stream({\"messages\": (\"user\", user_input)}):\n        for value in event.values():\n            print(\"Assistant:\", value[\"messages\"][-1].content)\nError Message and Stack Trace (if applicable)\npython my_agent/agent.py\nUser: Do you know Calculus?\n{'messages': [HumanMessage(content='Do you know Calculus?', id='05425ee6-2c31-4c0b-8843-2ca88596e420')]}\nAssistant: Yes, I am familiar with Calculus. Calculus is a branch of mathematics that deals with rates of change and accumulation of quantities. It includes topics such as differentiation, integration, limits, and infinite series.\nUser: Can you make an example?\n{'messages': [HumanMessage(content='Can you make an example?', id='e7aee16b-f51d-4dd1-b7d3-ee606278369d')]}\nAssistant: Sure! Here is an example:\n\nSentence: \"The cat chased the mouse around the house.\"\n\nExample: The cat, a sleek and agile tabby with bright green eyes, darted after the small grey mouse as it scurried through the various rooms of the house, knocking over vases and skidding around corners in a frantic attempt to escape.\nDescription\nI follow the first example in quickstart here: https://langchain-ai.github.io/langgraph/tutorials/introduction/#setup\nand I try to print the state, and I realized that the message is being replaced rather than appended.\nBut on the website, it states: \"The messages key is annotated with the add_messages reducer function, which tells LangGraph to append new messages to the existing list, rather than overwriting it.\"\nSystem Info\nName: langgraph\nVersion: 0.2.15\nSummary: Building stateful, multi-actor applications with LLMs\nHome-page: https://www.github.com/langchain-ai/langgraph\nAuthor:\nAuthor-email:\nLicense: MIT\nLocation: /Users/yuxuanwu/opt/anaconda3/envs/langchain/lib/python3.11/site-packages\nRequires: langchain-core, langgraph-checkpoint\nRequired-by:", "created_at": "2024-09-01", "closed_at": "2024-09-01", "labels": [], "State": "closed", "Author": "CalendulaED"}
{"issue_number": 1565, "issue_title": "LLM Compiler Example - Replan action failing due to incorrect format", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfor step in chain.stream({\"messages\":\n    [\n        HumanMessage(\n            content=\"Find the current temperature in Tokyo, then, respond with a flashcard summarizing this information\"\n        )\n    ]}\n):\n    print(step)\nError Message and Stack Trace (if applicable)\n{\n\t\"name\": \"InvalidUpdateError\",\n\t\"message\": \"Expected dict, got [AIMessage(content='Thought: The search results provided do not contain the current temperature in Tokyo directly, making it impossible to respond accurately with a flashcard summarizing this information.'), SystemMessage(content='Context from last attempt: The information retrieved does not include the current temperature in Tokyo, which is necessary to create the flashcard summary as requested. A direct search for the current temperature in Tokyo or a weather report source that includes this information is needed.')]\",\n\t\"stack\": \"---------------------------------------------------------------------------\nInvalidUpdateError                        Traceback (most recent call last)\nCell In[24], line 1\n----> 1 for step in chain.stream({\\\"messages\\\":\n      2     [\n      3         HumanMessage(\n      4             content=\\\"Find the current temperature in Tokyo, then, respond with a flashcard summarizing this information\\\"\n      5         )\n      6     ]}\n      7 ):\n      8     print(step)\n\nFile ~/workspace/PyriStage/langgraph/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1020, in Pregel.stream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug)\n   1017         break\n   1019 # panic on failure or timeout\n-> 1020 _panic_or_proceed(all_futures, loop.step)\n   1021 # don't keep futures around in memory longer than needed\n   1022 del done, inflight, futures\n\nFile ~/workspace/PyriStage/langgraph/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1450, in _panic_or_proceed(futs, step, timeout_exc_cls)\n   1448             inflight.pop().cancel()\n   1449         # raise the exception\n-> 1450         raise exc\n   1452 if inflight:\n   1453     # if we got here means we timed out\n   1454     while inflight:\n   1455         # cancel all pending tasks\n\nFile ~/workspace/PyriStage/langgraph/.venv/lib/python3.12/site-packages/langgraph/pregel/executor.py:60, in BackgroundExecutor.done(self, task)\n     58 def done(self, task: concurrent.futures.Future) -> None:\n     59     try:\n---> 60         task.result()\n     61     except GraphInterrupt:\n     62         # This exception is an interruption signal, not an error\n     63         # so we don't want to re-raise it on exit\n     64         self.tasks.pop(task)\n\nFile /usr/local/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:449, in Future.result(self, timeout)\n    447     raise CancelledError()\n    448 elif self._state == FINISHED:\n--> 449     return self.__get_result()\n    451 self._condition.wait(timeout)\n    453 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\nFile /usr/local/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401, in Future.__get_result(self)\n    399 if self._exception:\n    400     try:\n--> 401         raise self._exception\n    402     finally:\n    403         # Break a reference cycle with the exception in self._exception\n    404         self = None\n\nFile /usr/local/Cellar/python@3.12/3.12.4/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/thread.py:58, in _WorkItem.run(self)\n     55     return\n     57 try:\n---> 58     result = self.fn(*self.args, **self.kwargs)\n     59 except BaseException as exc:\n     60     self.future.set_exception(exc)\n\nFile ~/workspace/PyriStage/langgraph/.venv/lib/python3.12/site-packages/langgraph/pregel/retry.py:26, in run_with_retry(task, retry_policy)\n     24 task.writes.clear()\n     25 # run the task\n---> 26 task.proc.invoke(task.input, task.config)\n     27 # if successful, end\n     28 break\n\nFile ~/workspace/PyriStage/langgraph/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:2878, in RunnableSequence.invoke(self, input, config, **kwargs)\n   2876             input = context.run(step.invoke, input, config, **kwargs)\n   2877         else:\n-> 2878             input = context.run(step.invoke, input, config)\n   2879 # finish the root run\n   2880 except BaseException as e:\n\nFile ~/workspace/PyriStage/langgraph/.venv/lib/python3.12/site-packages/langgraph/utils.py:93, in RunnableCallable.invoke(self, input, config, **kwargs)\n     91 kwargs = {**self.kwargs, **kwargs}\n     92 if self.trace:\n---> 93     ret = self._call_with_config(\n     94         self.func, input, merge_configs(self.config, config), **kwargs\n     95     )\n     96 else:\n     97     config = merge_configs(self.config, config)\n\nFile ~/workspace/PyriStage/langgraph/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:1785, in Runnable._call_with_config(self, func, input, config, run_type, **kwargs)\n   1781     context = copy_context()\n   1782     context.run(_set_config_context, child_config)\n   1783     output = cast(\n   1784         Output,\n-> 1785         context.run(\n   1786             call_func_with_variable_args,  # type: ignore[arg-type]\n   1787             func,  # type: ignore[arg-type]\n   1788             input,  # type: ignore[arg-type]\n   1789             config,\n   1790             run_manager,\n   1791             **kwargs,\n   1792         ),\n   1793     )\n   1794 except BaseException as e:\n   1795     run_manager.on_chain_error(e)\n\nFile ~/workspace/PyriStage/langgraph/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:397, in call_func_with_variable_args(func, input, config, run_manager, **kwargs)\n    395 if run_manager is not None and accepts_run_manager(func):\n    396     kwargs[\\\"run_manager\\\"] = run_manager\n--> 397 return func(input, **kwargs)\n\nFile ~/workspace/PyriStage/langgraph/.venv/lib/python3.12/site-packages/langgraph/pregel/write.py:98, in ChannelWrite._write(self, input, config)\n     93 # process entries into values\n     94 values = [\n     95     input if write.value is PASSTHROUGH else write.value for write in entries\n     96 ]\n     97 values = [\n---> 98     val if write.mapper is None else write.mapper.invoke(val, config)\n     99     for val, write in zip(values, entries)\n    100 ]\n    101 values = [\n    102     (write.channel, val)\n    103     for val, write in zip(values, entries)\n    104     if not write.skip_none or val is not None\n    105 ]\n    106 # write packets and values\n\nFile ~/workspace/PyriStage/langgraph/.venv/lib/python3.12/site-packages/langgraph/utils.py:102, in RunnableCallable.invoke(self, input, config, **kwargs)\n    100     if accepts_config(self.func):\n    101         kwargs[\\\"config\\\"] = config\n--> 102     ret = context.run(self.func, input, **kwargs)\n    103 if isinstance(ret, Runnable) and self.recurse:\n    104     return ret.invoke(input, config)\n\nFile ~/workspace/PyriStage/langgraph/.venv/lib/python3.12/site-packages/langgraph/graph/state.py:543, in CompiledStateGraph.attach_node.<locals>._get_state_key(input, config, key)\n    541     return value if value is not None else SKIP_WRITE\n    542 else:\n--> 543     raise InvalidUpdateError(f\\\"Expected dict, got {input}\\\")\n\nInvalidUpdateError: Expected dict, got [AIMessage(content='Thought: The search results provided do not contain the current temperature in Tokyo directly, making it impossible to respond accurately with a flashcard summarizing this information.'), SystemMessage(content='Context from last attempt: The information retrieved does not include the current temperature in Tokyo, which is necessary to create the flashcard summary as requested. A direct search for the current temperature in Tokyo or a weather report source that includes this information is needed.')]\"\n}\nDescription\n#1513 and #1459 are related issues.\nThe LLM Compiler example has a small issue which is erroring out of any Replan functionality. The above code is an example I have added to the Jupyter notebook, since none of the current examples require a replan. The _parse_joiner_output function should return a dict, but in the case of a replan it excludes the required {\"messages\": ... } formatting.\nI have fixed this issue locally, added a Replan example, and will link a PR shortly in the comments.\nSystem Info\nlangchain==0.2.15\nlangchain-anthropic==0.1.23\nlangchain-community==0.2.13\nlangchain-core==0.2.35\nlangchain-openai==0.1.23\nlangchain-text-splitters==0.2.2\nMacOS\nPython 3.12.4", "created_at": "2024-08-31", "closed_at": "2024-09-01", "labels": [], "State": "closed", "Author": "McCReuben"}
{"issue_number": 1561, "issue_title": "DOC: NameError due to undefined supervisor_chain in agent_supervisor.ipynb", "issue_body": "Issue with current documentation:\nWhen running the Jupyter notebook file agent_supervisor.ipynb located in examples/multi_agent/, I encountered a NameError. The error occurs in the following cell:\nworkflow.add_node(\"Researcher\", research_node)\nworkflow.add_node(\"Coder\", code_node)\nworkflow.add_node(\"supervisor\", supervisor_chain)\nNameError                                 Traceback (most recent call last)\nCell In[8], line 29\n     27 workflow.add_node(\"Researcher\", research_node)\n     28 workflow.add_node(\"Coder\", code_node)\n---> 29 workflow.add_node(\"supervisor\", supervisor_chain)\n\nNameError: name 'supervisor_chain' is not defined\n\nIdea or request for content:\nNo response", "created_at": "2024-08-31", "closed_at": "2024-08-31", "labels": [], "State": "closed", "Author": "ggeutzzang"}
{"issue_number": 1542, "issue_title": "Unable to trace with Langsmith when using async code", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom typing import Literal\n\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.tools import tool\nfrom langgraph.graph import StateGraph, MessagesState\nfrom langgraph.prebuilt import ToolNode\n\n@tool\nasync def search(query: str):\n    \"\"\"Call to surf the web.\"\"\"\n    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\n        return \"It's 60 degrees and foggy.\"\n    return \"It's 90 degrees and sunny.\"\n\ntools = [search]\n\ntool_node = ToolNode(tools)\n\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(tools)\n\nasync def should_continue(state: MessagesState) -> Literal[\"tools\", \"__end__\"]:\n    messages = state['messages']\n    last_message = messages[-1]\n    if last_message.tool_calls:\n        return \"tools\"\n    return \"__end__\"\n\n\nasync def call_model(state: MessagesState):\n    messages = state['messages']\n\n    # Invoking `model` will automatically infer the correct tracing context\n    response = await model.ainvoke(messages)\n    return {\"messages\": [response]}\n\n\nworkflow = StateGraph(MessagesState)\n\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"tools\", tool_node)\n\nworkflow.add_edge(\"__start__\", \"agent\")\nworkflow.add_conditional_edges(\n    \"agent\",\n    should_continue,\n)\nworkflow.add_edge(\"tools\", 'agent')\n\napp = workflow.compile()\n\nasync def main():\n    final_state = await app.ainvoke(\n        {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]},\n        config={\"configurable\": {\"thread_id\": 22242}}\n    )\n    print(final_state[\"messages\"][-1].content)\n\nimport asyncio\nasyncio.run(main())\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nI am trying langgraph with langsmith for tracing,\nusing this code in the langsmith-langgraph guide: https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langgraph#2-log-a-trace-1\nAs the above code is synchronous, all works out well, but when I shift to async, my traces arent grouped, the chat model, tool, langraph are all separate\n\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.5.0: Wed May  1 20:13:18 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T6030\nPython Version:  3.10.13 (main, Dec 17 2023, 21:51:58) [Clang 15.0.0 (clang-1500.1.0.2.5)]\n\nPackage Information\n\nlangchain_core: 0.2.26\nlangchain: 0.2.11\nlangchain_community: 0.2.10\nlangsmith: 0.1.104\nlangchain_anthropic: 0.1.23\nlangchain_openai: 0.1.20\nlangchain_text_splitters: 0.2.2\nlangchainhub: 0.1.21\nlanggraph: 0.1.17\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlangserve\n", "created_at": "2024-08-30", "closed_at": "2024-08-30", "labels": [], "State": "closed", "Author": "sirjan-ws-ext"}
{"issue_number": 1529, "issue_title": "Error In Postgres db connection code ", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nimport psycopg\nfrom psycopg.rows import dict_row\nfrom psycopg_pool import ConnectionPool\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\n\nDATABASE_URL = \"postgresql://user:password@localhost:5432/mydatabase\"\n\nconnection_kwargs = {\n    \"autocommit\": True,\n    \"prepare_threshold\": 0,\n}\npool = ConnectionPool(\n    conninfo=DATABASE_URL, \n    kwargs= connection_kwargs,\n    max_size=20)\n\ndef get_db():\n    with pool.connection() as conn:\n        yield conn\n\n\n\n\n\n@cv_app.post(\"/message\")\ndef get_response(request: ChatModel, conn: psycopg.Connection = Depends(get_db)):\n\n    thread_id = request.thread\n    config={\"configurable\": {\"thread_id\": thread_id}}\n\n    logger.info(f\"Poool {pool}\")\n    checkpointer = PostgresSaver(conn)\n    output = checkpointer.get(config=config)\nError Message and Stack Trace (if applicable)\nFile \"C:\\Users\\MOL\\miniconda3\\envs\\bot_v2\\Lib\\site-packages\\langgraph\\checkpoint\\base\\__init__.py\", line 236, in get\n    if value := self.get_tuple(config):\n                ^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\MOL\\miniconda3\\envs\\bot_v2\\Lib\\site-packages\\langgraph\\checkpoint\\postgres\\__init__.py\", line 214, in get_tuple\n    \"checkpoint_id\": value[\"checkpoint_id\"],\nDescription\nI am trying to get the data from the postgres based on the config but there seems to be error in the postgres part of code in langgraph\nSystem Info\nThere are the versions i am using\npsycopg==3.2.1\npsycopg-binary==3.2.1\npsycopg-pool==3.2.2\nlangchain-postgres==0.0.9\nlanggraph==0.2.14\nlanggraph-checkpoint==1.0.6\nlanggraph-checkpoint-postgres==1.0.3\nI am using the lastest version of postgres as docker in my system", "created_at": "2024-08-29", "closed_at": "2024-08-29", "labels": [], "State": "closed", "Author": "vigneshmj1997"}
{"issue_number": 1513, "issue_title": "No {replan} variable in planner_prompt in the LLM Compiler example and the wfh/llm-compiler LangShimth's hub prompt template?", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nprompt = ChatPromptTemplate.from_messages([\n  (\"system\", \"Given a user query, create a plan to solve it with the utmost parallelizability. Each plan should comprise an action from the following {num_tools} types:\n{tool_descriptions}\n{num_tools}. join(): Collects and combines results from prior actions.\n\n - An LLM agent is called upon invoking join() to either finalize the user query or wait until the plans are executed.\n - join should always be the last action in the plan, and will be called in two scenarios:\n   (a) if the answer can be determined by gathering the outputs from tasks to generate the final response.\n   (b) if the answer cannot be determined in the planning phase before you execute the plans. Guidelines:\n - Each action described above contains input/output types and description.\n    - You must strictly adhere to the input and output types for each action.\n    - The action descriptions contain the guidelines. You MUST strictly follow those guidelines when you use the actions.\n - Each action in the plan should strictly be one of the above types. Follow the Python conventions for each action.\n - Each action MUST have a unique ID, which is strictly increasing.\n - Inputs for actions can either be constants or outputs from preceding actions. In the latter case, use the format $id to denote the ID of the previous action whose output will be the input.\n - Always call join as the last action in the plan. Say '<END_OF_PLAN>' after you call join\n - Ensure the plan maximizes parallelizability.\n - Only use the provided action types. If a query cannot be addressed using these, invoke the join action for the next steps.\n - Never introduce new actions other than the ones provided.\"),\n  (\"placeholder\", \"{messages}\"),\n  (\"system\", \"Remember, ONLY respond with the task list in the correct format! E.g.:\nidx. tool(arg_name=args)\"),\n])\n\ndef create_planner(\n    llm: BaseChatModel, tools: Sequence[BaseTool], base_prompt: ChatPromptTemplate\n):\n    tool_descriptions = \"\\n\".join(\n        f\"{i+1}. {tool.description}\\n\"\n        for i, tool in enumerate(\n            tools\n        )  # +1 to offset the 0 starting index, we want it count normally from 1.\n    )\n    planner_prompt = base_prompt.partial(\n        replan=\"\",\n        num_tools=len(tools)\n        + 1,  # Add one because we're adding the join() tool at the end.\n        tool_descriptions=tool_descriptions,\n    )\n    replanner_prompt = base_prompt.partial(\n        replan=' - You are given \"Previous Plan\" which is the plan that the previous agent created along with the execution results '\n        \"(given as Observation) of each plan and a general thought (given as Thought) about the executed results.\"\n        'You MUST use these information to create the next plan under \"Current Plan\".\\n'\n        ' - When starting the Current Plan, you should start with \"Thought\" that outlines the strategy for the next plan.\\n'\n        \" - In the Current Plan, you should NEVER repeat the actions that are already executed in the Previous Plan.\\n\"\n        \" - You must continue the task index from the end of the previous one. Do not repeat task indices.\",\n        num_tools=len(tools) + 1,\n        tool_descriptions=tool_descriptions,\n    )\n\n    def should_replan(state: list):\n        # Context is passed as a system message\n        return isinstance(state[-1], SystemMessage)\n\n    def wrap_messages(state: list):\n        return {\"messages\": state}\n\n    def wrap_and_get_last_index(state: list):\n        next_task = 0\n        for message in state[::-1]:\n            if isinstance(message, FunctionMessage):\n                next_task = message.additional_kwargs[\"idx\"] + 1\n                break\n        state[-1].content = state[-1].content + f\" - Begin counting at : {next_task}\"\n        return {\"messages\": state}\n\n    return (\n        RunnableBranch(\n            (should_replan, wrap_and_get_last_index | replanner_prompt),\n            wrap_messages | planner_prompt,\n        )\n        | llm\n        | LLMCompilerPlanParser(tools=tools)\n    )\nError Message and Stack Trace (if applicable)\nError: Input variables `replan` are not used in any of the prompt messages.\nDescription\nI'm trying to use the llm compiler example code, but I get the error of 'replan' variable not in the planner_prompt. Checked the wfh/llm-compiler LangShimth's hub template, and I don't see the variable either. __\nSystem Info\nwindows", "created_at": "2024-08-28", "closed_at": "2024-08-29", "labels": [], "State": "closed", "Author": "maciejNisztuk"}
{"issue_number": 1511, "issue_title": "InvalidUpdateError raised in graph.astream() when given a list of HumanMessage", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nFor easy reproduction, I added the exact same code of the tutorial to a Google Collab, where I encounter the same error:\nhttps://colab.research.google.com/drive/1fxIsAfZkLjWumc_HJfymTgiUYZkUjO3q?usp=sharing\nBut this is a minimal example:\nfrom langchain_core.messages import HumanMessage\nfrom langgraph.graph import StateGraph\nfrom typing_extensions import TypedDict\nfrom typing import Annotated, List\nfrom langgraph.graph.message import add_messages\nimport asyncio\n\n\nclass State(TypedDict):\n    messages: Annotated[List[HumanMessage], add_messages]\n\n\nasync def dummy_node(state: State):\n    return state\n\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"dummy\", dummy_node)\nbuilder.add_edge(\"dummy\", \"dummy\")\ngraph = builder.compile()\n\n\nasync def main():\n    async for event in graph.astream([HumanMessage(content=\"This is a test message\")]):\n        print(event)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nError Message and Stack Trace (if applicable)\n----> 1 async for event in graph.astream(\n      2     [\n      3         HumanMessage(\n      4             content=\"Generate an essay on the topicality of The Little Prince and its message in modern life\"\n      5         )\n\n8 frames\n/usr/local/lib/python3.10/dist-packages/langgraph/graph/state.py in _get_state_key(input, config, key)\n    541                 return value if value is not None else SKIP_WRITE\n    542             else:\n--> 543                 raise InvalidUpdateError(f\"Expected dict, got {input}\")\n    544 \n    545         # state updaters\n\nInvalidUpdateError: Expected dict, got [HumanMessage(content='Generate an essay on the topicality of The Little Prince and its message in modern life')]\nDescription\nI'm trying to reproduce the Basic Reflection notebook. However, in all environments I tested (Python 3.10 - 3.12, notebook and package environment) I get a InvalidUpdateError: Expected dict, got [HumanMessage(content=\"...\")].\nWhat exactly does the astream() method expect? The type signature Union[dict[str, Any], Any] and the description \"The input to the graph.\" is not very specific imO\nSystem Info\nlangchain==0.2.14\nlangchain-core==0.2.35\nlangchain-openai==0.1.22\nlangchain-text-splitters==0.2.2\nmac m3\nPython 3.12.4", "created_at": "2024-08-28", "closed_at": "2024-08-28", "labels": [], "State": "closed", "Author": "niklasmartin"}
{"issue_number": 1504, "issue_title": "ToolNode output is broken for non-english", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nlanggraph.prebuilt.tool_node.py\n\ndef str_output(output: Any) -> str:\n    if isinstance(output, str):\n        return output\n    else:\n        try:\n            return json.dumps(output)\n        except Exception:\n            return str(output)\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nI found the tool node output is encoded.\nAnd when I check _run_one and _arun_one function, they use str_output function.\nAnd I checked the arg, the input is alreadt encoded before return str or json.dumps.\nI don't know about that well...\nthis is not completely suitable code but I can solve my problem temporarily\ndef str_output(output: Any) -> str:\n    return json.dumps(json.loads(output), ensure_ascii=False)\nAnd I think the origin problem presents here: langchain_core.tools.base\ndef _stringify(content: Any) -> str:\n    try:\n        return json.dumps(content)\n    except Exception:\n        return str(content)\nthey just got a dumps without ensure_ascii=False.\nThank you for reading and I am forwarding to fixing this quickly\nSystem Info\nJust simple error", "created_at": "2024-08-28", "closed_at": "2024-08-29", "labels": [], "State": "closed", "Author": "sangmandu"}
{"issue_number": 1489, "issue_title": "Langgraph requires REDIS_URI which isn't used in my project and the project was running fine previously.", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nlanggraph up\n\nError Message and Stack Trace (if applicable)\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/starlette/config.py\", line 115, in get\nlanggraph-api-1       |     raise KeyError(f\"Config '{key}' is missing, and has no default.\")\nlanggraph-api-1       | KeyError: \"Config 'REDIS_URI' is missing, and has no default.\"\n\nDescription\nThe code was working fine. Last Friday but not working on Monday without any change.\nSystem Info\nlanggraph up\nStarting LangGraph API server...\nFor local dev, requires env var LANGSMITH_API_KEY with access to LangGraph Cloud closed beta.\nFor production use, requires a license key in env var LANGGRAPH_CLOUD_LICENSE_KEY.\n\\ Building...#0 building with \"desktop-linux\" instance using docker driver\n\n#1 [langgraph-api internal] load build definition from Dockerfile\n#1 transferring dockerfile: 910B done\n#1 DONE 0.0s\n\n#2 [langgraph-api internal] load metadata for docker.io/langchain/langgraph-api:3.11\n#2 DONE 0.0s\n\n#3 [langgraph-api internal] load .dockerignore\n#3 transferring context: 2B done\n#3 DONE 0.0s\n\n#4 [langgraph-api 1/6] FROM docker.io/langchain/langgraph-api:3.11\n#4 DONE 0.0s\n\n#5 [langgraph-api internal] load build context\n#5 transferring context: 2.96kB done\n#5 DONE 0.0s\n\n#6 [langgraph-api 3/6] RUN PYTHONDONTWRITEBYTECODE=1 pip install --no-cache-dir -c /api/constraints.txt -r /deps/__outer_multi_agents/multi_agents/requirements.txt\n#6 CACHED\n\n#7 [langgraph-api 4/6] ADD ./multi_agents /deps/__outer_multi_agents/multi_agents\n#7 CACHED\n\n#8 [langgraph-api 5/6] RUN set -ex &&     for line in '[project]'                 'name = \"multi_agents\"'                 'version = \"0.1\"'                 '[tool.setuptools.package-data]'                 '\"*\" = [\"**/*\"]'; do         echo \"\" >> /deps/__outer_multi_agents/pyproject.toml;     done\n#8 CACHED\n\n#9 [langgraph-api 2/6] ADD multi_agents/requirements.txt /deps/__outer_multi_agents/multi_agents/requirements.txt\n#9 CACHED\n\n#10 [langgraph-api 6/6] RUN PYTHONDONTWRITEBYTECODE=1 pip install --no-cache-dir -c /api/constraints.txt -e /deps/*\n#10 CACHED\n\n#11 [langgraph-api] exporting to image\n#11 exporting layers done\n#11 writing image sha256:4baf2352d999dd5b079dbe8bbf3c39bf054613fc26a9268b20dbd4cc7812d549 done\n#11 naming to docker.io/library/aiua-poc-system-langgraph-api done\n#11 DONE 0.0s\nAttaching to langgraph-api-1, langgraph-postgres-1\nlanggraph-postgres-1  | \nlanggraph-postgres-1  | PostgreSQL Database directory appears to contain a database; Skipping initialization\nlanggraph-postgres-1  | \nlanggraph-api-1 exited with code 1\ntime=\"2024-08-27T15:04:57+10:00\" level=warning msg=\"The \\\"line\\\" variable is not set. Defaulting to a blank string.\"\n Container aiua-poc-system-langgraph-postgres-1  Created\n Container aiua-poc-system-langgraph-api-1  Recreate\n Container aiua-poc-system-langgraph-api-1  Recreated\nlanggraph-postgres-1  | 2024-08-27 05:04:57.930 UTC [1] LOG:  starting PostgreSQL 16.4 (Debian 16.4-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit\nlanggraph-postgres-1  | 2024-08-27 05:04:57.930 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\nlanggraph-postgres-1  | 2024-08-27 05:04:57.930 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\nlanggraph-postgres-1  | 2024-08-27 05:04:57.931 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\nlanggraph-postgres-1  | 2024-08-27 05:04:57.934 UTC [29] LOG:  database system was shut down at 2024-08-27 04:49:44 UTC\nlanggraph-postgres-1  | 2024-08-27 05:04:57.939 UTC [1] LOG:  database system is ready to accept connections\nlanggraph-api-1       | Traceback (most recent call last):\nlanggraph-api-1       |   File \"/usr/local/bin/uvicorn\", line 8, in <module>\nlanggraph-api-1       |     sys.exit(main())\nlanggraph-api-1       |              ^^^^^^\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/click/core.py\", line 1157, in __call__\nlanggraph-api-1       |     return self.main(*args, **kwargs)\nlanggraph-api-1       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/click/core.py\", line 1078, in main\nlanggraph-api-1       |     rv = self.invoke(ctx)\nlanggraph-api-1       |          ^^^^^^^^^^^^^^^^\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/click/core.py\", line 1434, in invoke\nlanggraph-api-1       |     return ctx.invoke(self.callback, **ctx.params)\nlanggraph-api-1       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/click/core.py\", line 783, in invoke\nlanggraph-api-1       |     return __callback(*args, **kwargs)\nlanggraph-api-1       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/main.py\", line 410, in main\nlanggraph-api-1       |     run(\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/main.py\", line 577, in run\nlanggraph-api-1       |     server.run()\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 65, in run\nlanggraph-api-1       |     return asyncio.run(self.serve(sockets=sockets))\nlanggraph-api-1       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 190, in run\nlanggraph-api-1       |     return runner.run(main)\nlanggraph-api-1       |            ^^^^^^^^^^^^^^^^\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 118, in run\nlanggraph-api-1       |     return self._loop.run_until_complete(task)\nlanggraph-api-1       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlanggraph-api-1       |   File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 69, in serve\nlanggraph-api-1       |     await self._serve(sockets)\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 76, in _serve\nlanggraph-api-1       |     config.load()\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/config.py\", line 434, in load\nlanggraph-api-1       |     self.loaded_app = import_from_string(self.app)\nlanggraph-api-1       |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 19, in import_from_string\nlanggraph-api-1       |     module = importlib.import_module(module_str)\nlanggraph-api-1       |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\nlanggraph-api-1       |     return _bootstrap._gcd_import(name[level:], package, level)\nlanggraph-api-1       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlanggraph-api-1       |   File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\nlanggraph-api-1       |   File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\nlanggraph-api-1       |   File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\nlanggraph-api-1       |   File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\nlanggraph-api-1       |   File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\nlanggraph-api-1       |   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\nlanggraph-api-1       |   File \"/api/langgraph_api/server.py\", line 6, in <module>\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/langgraph_license/middleware.py\", line 9, in <module>\nlanggraph-api-1       |     from langgraph_license.validation import get_license_status\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/langgraph_license/validation.py\", line 9, in <module>\nlanggraph-api-1       |     from langgraph_api import config\nlanggraph-api-1       |   File \"/api/langgraph_api/config.py\", line 10, in <module>\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/starlette/config.py\", line 98, in __call__\nlanggraph-api-1       |     return self.get(key, cast, default)\nlanggraph-api-1       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/starlette/config.py\", line 115, in get\nlanggraph-api-1       |     raise KeyError(f\"Config '{key}' is missing, and has no default.\")\nlanggraph-api-1       | KeyError: \"Config 'REDIS_URI' is missing, and has no default.\"\nAborting on container exit...\n Container aiua-poc-system-langgraph-api-1  Stopping\n Container aiua-poc-system-langgraph-api-1  Stopped\n", "created_at": "2024-08-27", "closed_at": "2024-08-27", "labels": [], "State": "closed", "Author": "kbatsuren"}
{"issue_number": 1473, "issue_title": "[SOLVED] LangGraph plots incorrectly the workflow", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nimport os\nfrom dotenv import load_dotenv\nimport sys\n\nload_dotenv()\nWORKDIR=os.getenv(\"WORKDIR\")\nos.chdir(WORKDIR)\nsys.path.append(WORKDIR)\n\nfrom langchain_google_genai.chat_models import ChatGoogleGenerativeAI\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langgraph.graph import StateGraph\nfrom src.utils import State, GraphInput, GraphOutput, GraphConfig, check_chapter\nfrom src.nodes import *\nfrom src.routers import *\nfrom langgraph.graph import END\nfrom src.utils import State\n\ndef should_go_to_brainstorming_writer(state: State):\n    if state.get('instructor_documents', '') == '':\n        return \"human_feedback\"\n    else:\n        return \"brainstorming_writer\"\n    \ndef should_continue_with_critique(state: State):\n    if state.get('is_plan_approved', None) is None: \n        return \"brainstorming_critique\"\n    elif state['is_plan_approved'] == True:\n        return \"writer\"\n    else:\n        return \"brainstorming_critique\"\n    \ndef has_writer_ended_book(state: State):\n    if state['current_chapter'] == len(state['chapters_summaries']):\n        return END\n    else:\n        return \"writer\"\n\nworkflow = StateGraph(State, \n                      input = GraphInput,\n                      config_schema = GraphConfig)\n\nworkflow.add_node(\"instructor\", get_clear_instructions)\nworkflow.set_entry_point(\"instructor\")\nworkflow.add_node(\"human_feedback\", read_human_feedback)\nworkflow.add_node(\"brainstorming_writer\", making_writer_brainstorming)\nworkflow.add_node(\"brainstorming_critique\", brainstorming_critique)\nworkflow.add_node(\"writer\", generate_content)\nworkflow.add_conditional_edges(\n    \"instructor\",\n    should_go_to_brainstorming_writer\n)\nworkflow.add_edge(\"human_feedback\",\"instructor\")\nworkflow.add_conditional_edges(\n    \"brainstorming_writer\",\n    should_continue_with_critique\n)\n\nworkflow.add_edge(\"brainstorming_critique\",\"brainstorming_writer\")\nworkflow.add_conditional_edges(\n    \"writer\",\n    has_writer_ended_book\n)\n\napp = workflow.compile(\n    interrupt_before=['human_feedback']\n    )\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nLangGraph plots my workflow in the following way, which is not correct:\n\nThe workflow should look like:\n\nIt created relationships where there are no ones.\nSystem Info\nlangchain                    0.2.14\nlangchain-community          0.2.12\nlangchain-core               0.2.34\nlangchain-google-genai       1.0.10\nlangchain-groq               0.1.9\nlangchain-openai             0.1.22\nlangchain-text-splitters     0.2.2\nlanggraph                    0.2.14\nlanggraph-checkpoint         1.0.6\nlangsmith                    0.1.104", "created_at": "2024-08-26", "closed_at": "2024-08-26", "labels": [], "State": "closed", "Author": "Nachoeigu"}
{"issue_number": 1469, "issue_title": "DOC: Missing import in `langgraph/examples/docs/quickstart.ipynb`", "issue_body": "Issue with current documentation:\nI'm new here, so I started with: https://github.com/danwild/langgraph/blob/main/examples/docs/quickstart.ipynb\nThe 3rd python cell fails to import START from langgraph.graph before it's used, which breaks notebook execution.\nNameError                                 Traceback (most recent call last)\nCell In[6], [line 14](vscode-notebook-cell:?execution_count=6&line=14)\n     [10](vscode-notebook-cell:?execution_count=6&line=10) graph.add_node(\"oracle\", model)\n     [11](vscode-notebook-cell:?execution_count=6&line=11) graph.add_edge(\"oracle\", END)\n---> [14](vscode-notebook-cell:?execution_count=6&line=14) graph.add_edge(START, \"oracle\")\n     [16](vscode-notebook-cell:?execution_count=6&line=16) runnable = graph.compile()\n\nNameError: name 'START' is not defined\n\n\nIdea or request for content:\nNo response", "created_at": "2024-08-26", "closed_at": "2024-08-26", "labels": [], "State": "closed", "Author": "danwild"}
{"issue_number": 1468, "issue_title": "`langgraph up` error with `docker.io/langchain/langgraph-api:3.11` image ", "issue_body": "Example Code\nlanggraph up\n\nError Message and Stack Trace (if applicable)\nStarting LangGraph API server...\nFor local dev, requires env var LANGSMITH_API_KEY with access to LangGraph Cloud closed beta.\nFor production use, requires a license key in env var LANGGRAPH_CLOUD_LICENSE_KEY.\n| Pulling...+ docker pull langchain/langgraph-api:3.11\n\\ Pulling...3.11: Pulling from langchain/langgraph-api\nDigest: sha256:f38476de9d38e566a93346da0cc4f7b83329590721d94876ba361b27a1aa23ee\nStatus: Image is up to date for langchain/langgraph-api:3.11\ndocker.io/langchain/langgraph-api:3.11\n...\nlanggraph-api-1       | 2024-08-26T01:34:36.616232Z [info     ] Waiting for application startup. [uvicorn.error] api_revision=20fd12a api_variant=licensed filename=on.py func_name=startup lineno=48\nlanggraph-api-1       | 2024-08-26T01:34:36.616422Z [warning  ] No license key found, running in test mode with LangSmith API key. For production use, set LANGGRAPH_CLOUD_LICENSE_KEY in environment. [langgraph_license.validation] api_revision=20fd12a api_variant=licensed filename=validation.py func_name=get_license_status lineno=80\nlanggraph-api-1       | 2024-08-26T01:34:36.916378Z [info     ] HTTP Request: GET https://api.smith.langchain.com/auth?langgraph-api=true \"HTTP/1.1 200 OK\" [httpx] api_revision=20fd12a api_variant=licensed filename=_client.py func_name=_send_single_request lineno=1773\nlanggraph-api-1       | 2024-08-26T01:34:36.963153Z [error    ] Traceback (most recent call last):\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 730, in lifespan\nlanggraph-api-1       |     async with self.lifespan_context(app) as maybe_state:\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/contextlib.py\", line 210, in __aenter__\nlanggraph-api-1       |     return await anext(self.gen)\nlanggraph-api-1       |            ^^^^^^^^^^^^^^^^^^^^^\nlanggraph-api-1       |   File \"/api/langgraph_api/lifespan.py\", line 21, in lifespan\nlanggraph-api-1       |   File \"/api/langgraph_api/shared/graph.py\", line 147, in collect_graphs_from_env\nlanggraph-api-1       |   File \"/api/langgraph_api/shared/graph.py\", line 113, in is_js_spec\nlanggraph-api-1       |   File \"<frozen posixpath>\", line 118, in splitext\nlanggraph-api-1       | TypeError: expected str, bytes or os.PathLike object, not NoneType\nlanggraph-api-1       |  [uvicorn.error] api_revision=20fd12a api_variant=licensed filename=on.py func_name=send lineno=121\nlanggraph-api-1       | 2024-08-26T01:34:36.963336Z [error    ] Application startup failed. Exiting. [uvicorn.error] api_revision=20fd12a api_variant=licensed filename=on.py func_name=startup lineno=59\nlanggraph-api-1 exited with code 3\nCommand exited with non-zero status 3.\nDescription\nI was using langgraph on my custom application perfectly until yesterday, but I just got that error today.\nI didn't change any langgraph.json and requirements, but I am encountering an internal error with langgraph.\nWhat I found is that the docker.io/langchain/langgraph-api:3.11 image keeps updating today.\nToday, I saw two new updates for that image, as shown in the screenshot below.\nSo my questions:\n\nIs the docker.io/langchain/langgraph-api:3.11 unstable version yet?\nCan I specify the version of langgraph-api docker image by using langgraph.json or other configurations?\n\n\nSystem Info\nlangchain==0.2.14\nlangchain-core==0.2.34\nlangchain-openai==0.1.22\nlangchain-text-splitters==0.2.2\n\nThis was my working requirement until yesterday.\nI also tested langgraph up by upgrading all langchain and langgraph packages but got the same error.", "created_at": "2024-08-26", "closed_at": "2024-08-26", "labels": [], "State": "closed", "Author": "seyeong-han"}
{"issue_number": 1466, "issue_title": "DOC: AWS Serverless examples", "issue_body": "Issue with current documentation:\nHi, would it be possible to add an example showing best practices for using LangGraph with AWS Lambda?\nIn particular I'd like to know what the recommended way is to communicate between nodes that may each be implemented as lambda functions. Is RemoteRunnable relevant here?\nIdea or request for content:\nLangGraph example that deploys two lambda agents as nodes and communicates between them?", "created_at": "2024-08-25", "closed_at": "2024-08-25", "labels": [], "State": "closed", "Author": "austinmw"}
{"issue_number": 1464, "issue_title": "Before `interrupt_after`, is it necessary to first decide which node (execute the `path` function)?", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom collections.abc import Callable\nfrom typing import Annotated, Literal, TypedDict\n\nfrom langchain_core.messages import AnyMessage\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import END, StateGraph, add_messages\n\n\nclass State(TypedDict, total=False):\n    messages: Annotated[list[AnyMessage], add_messages]\n    reply: str\n\n\ndef ai_message(msg: str) -> Callable[..., State]:\n    def handler(_: State) -> State:\n        return {\"messages\": [(\"ai\", msg)]}\n\n    return handler\n\n\ndef route(state: State) -> Literal[\"A\", \"B\"]:\n    print(\"This is the ROUTE function\")\n    # This will be printed twice\n    # The first one is before `interrupt_after` and the second one is in `update`\n    return \"A\" if state[\"reply\"] == \"A\" else \"B\"\n\n\ngraph_builder = StateGraph(State)\ngraph_builder.set_entry_point(\"enter\")\n# It must be initialized first; otherwise, an KeyError will occur when the route function tries to get 'reply'.\ngraph_builder.add_node(\"enter\", lambda _: {\"reply\": \"\"})\ngraph_builder.add_node(\"send_question\", ai_message(\"Do you want to go to A or B?\"))\ngraph_builder.add_node(\"A\", ai_message(\"You are in A\"))\ngraph_builder.add_node(\"B\", ai_message(\"You are in B\"))\n\ngraph_builder.add_edge(\"enter\", \"send_question\")\ngraph_builder.add_conditional_edges(\"send_question\", route)\ngraph_builder.add_edge(\"A\", END)\ngraph_builder.add_edge(\"B\", END)\nmemory = MemorySaver()\napp = graph_builder.compile(checkpointer=memory, interrupt_after=[\"send_question\"])\n\n\ndef print_divider(text: str) -> None:\n    print(\"=\" * 20 + text + \"=\" * 20)\n\n\nthread_config = {\"configurable\": {\"thread_id\": \"1\"}}\nprint(app.invoke({\"messages\": []}, thread_config))\n\nprint_divider(\"interrupt_after\")\nprint(\"next_node is \", app.get_state(thread_config).next)  # it will print ('B', )\n\nprint_divider(\"update\")\napp.update_state(thread_config, {\"reply\": \"A\"})\nprint(app.invoke(None, thread_config))\nError Message and Stack Trace (if applicable)\nThis is the ROUTE function\n{'messages': [AIMessage(content='Do you want to go to A or B?', id='20531714-03d8-4e81-a74b-badea641d187')], 'reply': ''}\n====================interrupt_after====================\nnext_node is  ('B',)\n====================update====================\nThis is the ROUTE function\n{'messages': [AIMessage(content='Do you want to go to A or B?', id='20531714-03d8-4e81-a74b-badea641d187'), AIMessage(content='You are in A', id='f761d343-f602-4236-b1ea-128b4d16022a')], 'reply': 'A'}\nDescription\nBefore each interrupt_after, it always needs to decide the next node to go to, so it must first execute the path function of add_conditional_edges.\nWhen the graph is interrupted and execution resumes, the path function is executed again, determining a new path.\nAt this point, the path function executed during the interruption seems redundant, as the displayed next node might be incorrect.\nHowever, I remember that in earlier versions, resuming the graph execution didn't re-execute this function. (But I think the old way was less intuitive, and the new approach is better.)\nIs it possible to design it so that when encountering add_conditional_edges with interrupt_after, the path function isn't executed first, and the next_node points to multiple nodes?\nIn the example above, if it always has to execute first, I would have to initialize reply beforehand; otherwise, it would encounter a KeyError.\nHowever, this pre-execution of the function doesn't make any sense since it seems to execute first and obtain a result that might not be the final choice.\nAnd it will re-select and possibly change the final path when resuming execution.\nSystem Info\nlanggraph==0.2.14\nlanggraph-checkpoint==1.0.5\nplatform==windows\npython-version==3.12.5", "created_at": "2024-08-25", "closed_at": "2024-08-26", "labels": [], "State": "closed", "Author": "gbaian10"}
{"issue_number": 1462, "issue_title": "Constraints on the input for `add_edge` when it is a list", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nQ1: When the start_key in add_edge is a list, the end_key can't be __end__.\nfrom langgraph.graph import END, MessageGraph, StateGraph\n\n\ndef foo(state: MessageGraph) -> None:\n    return\n\n\ngraph_builder = StateGraph(MessageGraph)\ngraph_builder.set_entry_point(\"enter\")\nfor node in [\"enter\", \"A\", \"B\"]:\n    graph_builder.add_node(node, foo)\ngraph_builder.add_conditional_edges(\"enter\", lambda _: True, {True: \"A\", False: \"B\"})\n\ngraph_builder.add_edge([\"A\", \"B\"], END)\n# graph_builder.add_edge(\"A\", END)\n# graph_builder.add_edge(\"B\", END)\n\napp = graph_builder.compile()\nQ2: When the input is a list, you must add the node before adding an edge.\nHowever, if the input is a str, you don't necessarily need to add the node first.\nfrom langgraph.graph import END, MessageGraph, StateGraph\n\n\ndef foo(state: MessageGraph) -> None:\n    return\n\n\ngraph_builder = StateGraph(MessageGraph)\ngraph_builder.set_entry_point(\"enter\")\n\ngraph_builder.add_edge([\"A\", \"B\"], END)\n# graph_builder.add_edge(\"A\", END)\n# graph_builder.add_edge(\"B\", END)\n\nfor node in [\"enter\", \"A\", \"B\"]:\n    graph_builder.add_node(node, foo)\ngraph_builder.add_conditional_edges(\"enter\", lambda _: True, {True: \"A\", False: \"B\"})\n\napp = graph_builder.compile()\nError Message and Stack Trace (if applicable)\nQ1\nTraceback (most recent call last):\n  File \".\\issue.py\", line 14, in <module>\n    graph_builder.add_edge([\"A\", \"B\"], END)\n  File \".\\langgraph\\graph\\state.py\", line 378, in add_edge\n    raise ValueError(f\"Need to add_node `{end_key}` first\")\nValueError: Need to add_node `__end__` first\nQ2\nTraceback (most recent call last):\n  File \".\\issue.py\", line 11, in <module>\n    graph_builder.add_edge([\"A\", \"B\"], END)\n  File \".\\langgraph\\graph\\state.py\", line 374, in add_edge\n    raise ValueError(f\"Need to add_node `{start}` first\")\nValueError: Need to add_node `A` first\nDescription\nBasically, Q1 is a subissue of Q2.\n\nIs it necessary to check if the node has already been added in the high-level module of the code?\nI removed these parts and it still runs, and it can solve the problem mentioned above.\nAt least the result of draw_mermaid_png is the same. I haven't conducted further tests.\n\n\nHowever, I found that when the input is a list, it uses waiting_edges, but when the input is a str, it doesn't use waiting_edges.\nAfter further observation, I noticed that during subsequent compilation, nodes are always added first.\nAt this point, it makes me reconsider whether the previous check for node existence in add_edge is necessary.\n\nUpon further observation, I found that the underlying behavior differs when the input is a string or a list, such as with different channel_name values.\nSince I currently don't understand the underlying workings of langgraph (e.g., how channels operate), I'm unsure if directly modifying the preliminary checks in the higher-level module's add_edge might cause additional issues.\n\nA simple question to summarize the above explanation: Will the outputs be exactly the same for the following two inputs?\ngraph_builder.add_edge([\"A\", \"B\"], END)\ngraph_builder.add_edge(\"A\", END)\ngraph_builder.add_edge(\"B\", END)\nSystem Info\nlanggraph==0.2.14\nplatform==windows\npython-version==3.12.5", "created_at": "2024-08-25", "closed_at": "2024-08-27", "labels": [], "State": "closed", "Author": "gbaian10"}
{"issue_number": 1459, "issue_title": "I cannot get LLMCompiler work ", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langchain_openai import ChatOpenAI\n\n    llm = ChatOpenAI(model=llm_model_name) \n    calculate = get_math_tool(llm)\n\n\n\n    search = TavilySearchResults(\n        max_results=1,\n        description='tavily_search_results_json(query=\"the search query\") - a search engine.',\n    )\n\n    tools = [search, calculate]\n\n\n\n\n    # Part 2: Planner\n    # region Planner\n    from typing import Sequence\n\n    from langchain import hub\n    from langchain_core.language_models import BaseChatModel\n    from langchain_core.messages import (\n        BaseMessage,\n        FunctionMessage,\n        HumanMessage,\n        SystemMessage,\n    )\n    from langchain_core.prompts import ChatPromptTemplate\n    from langchain_core.runnables import RunnableBranch\n    from langchain_core.tools import BaseTool\n    from langchain_openai import ChatOpenAI\n    from utils.llm_compiler_output_parser import LLMCompilerPlanParser, Task\n\n    prompt = hub.pull(\"wfh/llm-compiler\")\n\n\n\n    def create_planner(\n    llm: BaseChatModel, tools: Sequence[BaseTool], base_prompt: ChatPromptTemplate\n):\n        tool_descriptions = \"\\n\".join(\n            f\"{i+1}. {tool.description}\\n\"\n            for i, tool in enumerate(\n                tools\n            )  # +1 to offset the 0 starting index, we want it count normally from 1.\n        )\n        planner_prompt = base_prompt.partial(\n            replan=\"\",\n            num_tools=len(tools)\n            + 1,  # Add one because we're adding the join() tool at the end.\n            tool_descriptions=tool_descriptions,\n        )\n        replanner_prompt = base_prompt.partial(\n            replan=' - You are given \"Previous Plan\" which is the plan that the previous agent created along with the execution results '\n            \"(given as Observation) of each plan and a general thought (given as Thought) about the executed results.\"\n            'You MUST use these information to create the next plan under \"Current Plan\".\\n'\n            ' - When starting the Current Plan, you should start with \"Thought\" that outlines the strategy for the next plan.\\n'\n            \" - In the Current Plan, you should NEVER repeat the actions that are already executed in the Previous Plan.\\n\"\n            \" - You must continue the task index from the end of the previous one. Do not repeat task indices.\", # type: ignore\n            num_tools=len(tools) + 1,\n            tool_descriptions=tool_descriptions,\n        )\n\n        def should_replan(state: list):\n            # Context is passed as a system message\n            return isinstance(state[-1], SystemMessage)\n\n        def wrap_messages(state: list):\n            return {\"messages\": state}\n\n        def wrap_and_get_last_index(state: list):\n            next_task = 0\n            for message in state[::-1]:\n                if isinstance(message, FunctionMessage):\n                    next_task = message.additional_kwargs[\"idx\"] + 1\n                    break\n            state[-1].content = state[-1].content + f\" - Begin counting at : {next_task}\"\n            return {\"messages\": state}\n\n        return (\n            RunnableBranch(\n                (should_replan, wrap_and_get_last_index | replanner_prompt),\n                wrap_messages | planner_prompt,\n            )\n            | llm\n            | LLMCompilerPlanParser(tools=tools)\n        )\n\n\n\n    # This is the primary \"agent\" in our application\n    planner = create_planner(llm, tools, prompt)\n\n    # endregion\n\n    # 3. Task Fetching Unit\n    # region Task Fetching Unit\n    import re\n    import time\n    from concurrent.futures import ThreadPoolExecutor, wait\n    from typing import Any, Dict, Iterable, List, Union\n\n    from langchain_core.runnables import (\n        chain as as_runnable,\n    )\n    from typing_extensions import TypedDict\n\n\n    def _get_observations(messages: List[BaseMessage]) -> Dict[int, Any]:\n        # Get all previous tool responses\n        results = {}\n        for message in messages[::-1]:\n            if isinstance(message, FunctionMessage):\n                results[int(message.additional_kwargs[\"idx\"])] = message.content\n        return results\n\n\n    class SchedulerInput(TypedDict):\n        messages: List[BaseMessage]\n        tasks: Iterable[Task]\n\n\n    def _execute_task(task, observations, config):\n        tool_to_use = task[\"tool\"]\n        if isinstance(tool_to_use, str):\n            return tool_to_use\n        args = task[\"args\"]\n        try:\n            if isinstance(args, str):\n                resolved_args = _resolve_arg(args, observations)\n            elif isinstance(args, dict):\n                resolved_args = {\n                    key: _resolve_arg(val, observations) for key, val in args.items()\n                }\n            else:\n                # This will likely fail\n                resolved_args = args\n        except Exception as e:\n            return (\n                f\"ERROR(Failed to call {tool_to_use.name} with args {args}.)\"\n                f\" Args could not be resolved. Error: {repr(e)}\"\n            )\n        try:\n            return tool_to_use.invoke(resolved_args, config)\n        except Exception as e:\n            return (\n                f\"ERROR(Failed to call {tool_to_use.name} with args {args}.\"\n                + f\" Args resolved to {resolved_args}. Error: {repr(e)})\"\n            )\n\n\n    def _resolve_arg(arg: Union[str, Any], observations: Dict[int, Any]):\n        # $1 or ${1} -> 1\n        ID_PATTERN = r\"\\$\\{?(\\d+)\\}?\"\n\n        def replace_match(match):\n            # If the string is ${123}, match.group(0) is ${123}, and match.group(1) is 123.\n\n            # Return the match group, in this case the index, from the string. This is the index\n            # number we get back.\n            idx = int(match.group(1))\n            return str(observations.get(idx, match.group(0)))\n\n        # For dependencies on other tasks\n        if isinstance(arg, str):\n            return re.sub(ID_PATTERN, replace_match, arg)\n        elif isinstance(arg, list):\n            return [_resolve_arg(a, observations) for a in arg]\n        else:\n            return str(arg)\n\n\n    @as_runnable\n    def schedule_task(task_inputs, config):\n        task: Task = task_inputs[\"task\"]\n        observations: Dict[int, Any] = task_inputs[\"observations\"]\n        try:\n            observation = _execute_task(task, observations, config)\n        except Exception:\n            import traceback\n\n            observation = traceback.format_exception()  # repr(e) +\n        observations[task[\"idx\"]] = observation\n\n\n    def schedule_pending_task(\n        task: Task, observations: Dict[int, Any], retry_after: float = 0.2\n    ):\n        while True:\n            deps = task[\"dependencies\"]\n            if deps and (any([dep not in observations for dep in deps])):\n                # Dependencies not yet satisfied\n                time.sleep(retry_after)\n                continue\n            schedule_task.invoke({\"task\": task, \"observations\": observations})\n            break\n\n\n    @as_runnable\n    def schedule_tasks(scheduler_input: SchedulerInput) -> List[FunctionMessage]:\n        \"\"\"Group the tasks into a DAG schedule.\"\"\"\n        # For streaming, we are making a few simplifying assumption:\n        # 1. The LLM does not create cyclic dependencies\n        # 2. That the LLM will not generate tasks with future deps\n        # If this ceases to be a good assumption, you can either\n        # adjust to do a proper topological sort (not-stream)\n        # or use a more complicated data structure\n        tasks = scheduler_input[\"tasks\"]\n        args_for_tasks = {}\n        messages = scheduler_input[\"messages\"]\n        # If we are re-planning, we may have calls that depend on previous\n        # plans. Start with those.\n        observations = _get_observations(messages)\n        task_names = {}\n        originals = set(observations)\n        # ^^ We assume each task inserts a different key above to\n        # avoid race conditions...\n        futures = []\n        retry_after = 0.25  # Retry every quarter second\n        with ThreadPoolExecutor() as executor:\n            for task in tasks:\n                deps = task[\"dependencies\"]\n                task_names[task[\"idx\"]] = (\n                    task[\"tool\"] if isinstance(task[\"tool\"], str) else task[\"tool\"].name\n                )\n                args_for_tasks[task[\"idx\"]] = task[\"args\"]\n                if (\n                    # Depends on other tasks\n                    deps\n                    and (any([dep not in observations for dep in deps]))\n                ):\n                    futures.append(\n                        executor.submit(\n                            schedule_pending_task, task, observations, retry_after\n                        )\n                    )\n                else:\n                    # No deps or all deps satisfied\n                    # can schedule now\n                    schedule_task.invoke(dict(task=task, observations=observations))\n                    # futures.append(executor.submit(schedule_task.invoke dict(task=task, observations=observations)))\n\n            # All tasks have been submitted or enqueued\n            # Wait for them to complete\n            wait(futures)\n        # Convert observations to new tool messages to add to the state\n        new_observations = {\n            k: (task_names[k], args_for_tasks[k], observations[k])\n            for k in sorted(observations.keys() - originals)\n        }\n        tool_messages = [\n            FunctionMessage(\n                name=name, content=str(obs), additional_kwargs={\"idx\": k, \"args\": task_args}\n            )\n            for k, (name, task_args, obs) in new_observations.items()\n        ]\n        return tool_messages\n    \n\n    import itertools\n\n\n    @as_runnable\n    def plan_and_schedule(state):\n        messages = state[\"messages\"]\n        tasks = planner.stream(messages)\n        # Begin executing the planner immediately\n        try:\n            tasks = itertools.chain([next(tasks)], tasks)\n        except StopIteration:\n            # Handle the case where tasks is empty.\n            tasks = iter([])\n        scheduled_tasks = schedule_tasks.invoke(\n            {\n                \"messages\": messages,\n                \"tasks\": tasks,\n            }\n        )\n        return {\"messages\": [scheduled_tasks]}\n    \n    # endregion\n    \n    # 4. \"Joiner\"\n    # region Joiner\n    from langchain.chains.openai_functions import create_structured_output_runnable\n    from   langchain_core.messages import AIMessage\n    from langchain_core.pydantic_v1 import BaseModel, Field\n\n\n    class FinalResponse(BaseModel):\n        \"\"\"The final response/answer.\"\"\"\n\n        response: str\n\n\n    class Replan(BaseModel):\n        feedback: str = Field(\n            description=\"Analysis of the previous attempts and recommendations on what needs to be fixed.\"\n        )\n\n\n    class JoinOutputs(BaseModel):\n        \"\"\"Decide whether to replan or whether you can return the final response.\"\"\"\n\n        thought: str = Field(\n            description=\"The chain of thought reasoning for the selected action\"\n        )\n        action: Union[FinalResponse, Replan]\n\n\n    joiner_prompt = hub.pull(\"wfh/llm-compiler-joiner\").partial(\n        examples=\"\"\n    )  # You can optionally add examples\n    \n\n    runnable = create_structured_output_runnable(JoinOutputs, llm, joiner_prompt)\n\n    \n    # We will select only the most recent messages in the state, and format the output to be more useful for the planner, should the agent need to loop.\n    def _parse_joiner_output(decision: JoinOutputs) -> List[BaseMessage]:\n        response = [AIMessage(content=f\"Thought: {decision.thought}\")]\n        if isinstance(decision.action, Replan):\n            return response + [\n                SystemMessage(\n                    content=f\"Context from last attempt: {decision.action.feedback}\"\n                )\n            ]\n        else:\n            return {\"messages\": response + [AIMessage(content=decision.action.response)]}\n\n\n    def select_recent_messages(state) -> dict:\n        messages = state[\"messages\"]\n        selected = []\n        for msg in messages[::-1]:\n            selected.append(msg)\n            if isinstance(msg, HumanMessage):\n                break\n        return {\"messages\": selected[::-1]}\n\n\n    joiner = select_recent_messages | runnable | _parse_joiner_output\n\n    # endregion\n\n    # 5. Compose using LangGraph\n    # region LangGraph\n    from langgraph.graph import END, StateGraph, START\n    from langgraph.graph.message import add_messages\n    from typing import Annotated\n\n\n    class State(TypedDict):\n        messages: Annotated[list, add_messages]\n\n    from langgraph.graph import MessageGraph, END\n    graph_builder = StateGraph(State)\n    # graph_builder = MessageGraph()\n\n    # 1.  Define vertices\n    # We defined plan_and_schedule above already\n    # Assign each node to a state variable to update\n    graph_builder.add_node(\"plan_and_schedule\", plan_and_schedule)\n    graph_builder.add_node(\"join\", joiner)\n\n\n    ## Define edges\n    graph_builder.add_edge(\"plan_and_schedule\", \"join\")\n\n    ### This condition determines looping logic\n\n\n    # def should_continue(state):\n    #     messages = state[\"messages\"]\n    #     if isinstance(messages[-1], AIMessage):\n    #         return END\n    #     return \"plan_and_schedule\"\n\n    from langgraph.graph import END, StateGraph, MessagesState\n    from typing import Annotated, Literal, TypedDict\n    \n        # Define the function that determines whether to continue or not\n    # def should_continue(state: MessagesState) -> Literal[\"plan_and_schedule\", END]:\n    #     messages = state['messages']\n    #     last_message = messages[-1]\n    #     if isinstance(messages[-1], AIMessage):\n    #         return END\n    #     return \"plan_and_schedule\"\n\n    def should_continue(state: List[BaseMessage]):\n        if isinstance(state[-1], AIMessage):\n            return END\n        return \"plan_and_schedule\"\n\n\n\n    graph_builder.add_conditional_edges(\n        \"join\",\n        # Next, we pass in the function that will determine which node is called next.\n        should_continue)\n    graph_builder.add_edge(START, \"plan_and_schedule\")\n    \n    chain = graph_builder.compile()\n    #endregion\n\n\n\n    steps = chain.stream(\n        {\"messages\": HumanMessage(content=\"What's the oldest parrot alive, and how much longer is that than the average?\")},\n        {\n            \"recursion_limit\": 100,\n        }\n    )\n    for step in steps:\n        print(step)\n        print(\"---\")\n    # final_state = chain.invoke(\n    # {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]},\n    # config={\"configurable\": {\"thread_id\": 42}}\n    # )\n    # print(final_state[\"messages\"][-1].content)\n    # print(steps[-1])\n\n    response = dict()\n    response['answer'] = ''\n    response['context'] = ''\n\n    return(response)\n    \n    # response['answer'] = final_state[\"messages\"][-1].content\n    # response['context'] = [x.content for x in final_state[\"messages\"]\n\n    #     Conclusion\n    # The planner output parsing format is fragile if your function requires more than 1 or 2 arguments. We could make it more robust by using streaming tool calling.\n    # Variable substitution is fragile in the example above. It could be made more robust by using a fine-tuned model and a more robust syntax (using e.g., Lark or a tool calling schema)\n    # The state can grow quite long if you require multiple re-planning runs. To handle, you could add a message compressor once you go above a certain token limit.\nError Message and Stack Trace (if applicable)\n\"content\": \"1. tavily_search_results_json(query=\\\"oldest parrot alive\\\")  \\n2. tavily_search_results_json(query=\\\"average lifespan of parrots\\\")  \\n3. join()  \\n<END_OF_PLAN>\",\n            \"response_metadata\": {\n              \"finish_reason\": \"stop\",\n              \"model_name\": \"gpt-4o-mini-2024-07-18\",\n              \"system_fingerprint\": \"fp_48196bc67a\"\n            },\n            \"type\": \"AIMessageChunk\",\n            \"id\": \"run-438d4b1f-9c34-40ee-adcc-bd84f9d7224e\",\n            \"tool_calls\": [],\n            \"invalid_tool_calls\": []\n          }\n        }\n      }\n    ]\n  ],\n  \"llm_output\": null,\n  \"run\": null\n}\n[chain/end] [chain:LangGraph > chain:plan_and_schedule > chain:plan_and_schedule > chain:RunnableSequence > parser:LLMCompilerPlanParser] [5.75s] Exiting Parser run with output:\n{\n  \"idx\": 3,\n  \"tool\": \"join\",\n  \"args\": [],\n  \"dependencies\": [\n    1,\n    2\n  ],\n  \"thought\": null\n}\n[chain/end] [chain:LangGraph > chain:plan_and_schedule > chain:plan_and_schedule > chain:RunnableSequence] [6.31s] Exiting \nChain run with output:\n{\n  \"idx\": 3,\n  \"tool\": \"join\",\n  \"args\": [],\n  \"dependencies\": [\n    1,\n    2\n  ],\n  \"thought\": null\n}\n[chain/end] [chain:LangGraph > chain:plan_and_schedule > chain:plan_and_schedule > chain:schedule_tasks] [5.63s] Exiting Chain run with output:\n[outputs]\n[chain/end] [chain:LangGraph > chain:plan_and_schedule > chain:plan_and_schedule] [6.33s] Exiting Chain run with output:   \n[outputs]\n[chain/start] [chain:LangGraph > chain:plan_and_schedule > chain:ChannelWrite<plan_and_schedule,messages>] Entering Chain run with input:\n[inputs]\n[chain/end] [chain:LangGraph > chain:plan_and_schedule > chain:ChannelWrite<plan_and_schedule,messages>] [3ms] Exiting Chain run with output:\n[outputs]\n[chain/end] [chain:LangGraph > chain:plan_and_schedule] [6.34s] Exiting Chain run with output:\n[outputs]\n{'plan_and_schedule': {'messages': [[FunctionMessage(content='[{\\'url\\': \\'https://en.wikipedia.org/wiki/Cookie_(cockatoo)\\', \\'content\\': \\'He was one of the longest-lived birds on record[4] and was recognised by the Guinness World Records as the oldest living parrot in the world.[5]\\\\nThe next-oldest pink cockatoo to be found in a zoological setting was a 31-year-old female bird located at Paradise Wildlife Sanctuary, England.[3] Information published by the World Parrot Trust states longevity for Cookie\\\\\\'s species in captivity is on average 40\u201360 years.[6]\\\\nLife[edit]\\\\nCookie was Brookfield Zoo\\\\\\'s oldest resident and the last surviving member of the animal collection from the time of the zoo\\\\\\'s opening in 1934, having arrived from Taronga Zoo of Sydney, New South Wales, Australia, in the same year and judged to be one year old at the time.[7]\\\\nIn the 1950s an attempt was made to introduce Cookie to a female pink cockatoo, but Cookie rejected her as \"she was \nnot nice to him\".[8]\\\\n In 2007, Cookie was diagnosed with, and placed on medication and nutritional supplements for, osteoarthritis and osteoporosis\\\\xa0\u2013 medical conditions which occur commonly in aging animals and humans alike,[7] although it \nis believed that the latter may also have been brought on as a result of being fed a seed-only diet for the first 40 years \nof his life, in the years before the dietary requirements of his species were fully understood.[9]\\\\nCookie was \"retired\" from exhibition at the zoo in 2009 (following a few months of weekend-only appearances) in order to preserve his health, after it was noticed by staff that his appetite, demeanor and stress levels improved markedly when not on public display. age.[11] A memorial at the zoo was unveiled in September 2017.[12]\\\\nIn 2020, Cookie became the subject of a poetry collection \nby Barbara Gregorich entitled Cookie the Cockatoo: Everything Changes.[13]\\\\nSee also[edit]\\\\nReferences[edit]\\\\nExternal links[edit] He was believed to be the oldest member of his species alive in captivity, at the age of 82 in June 2015,[1][2] \nhaving significantly exceeded the average lifespan for his kind.[3] He was moved to a permanent residence in the keepers\\\\\\' office of the zoo\\\\\\'s Perching Bird House, although he made occasional appearances for special events, such as his birthday celebration, which was held each June.[3]\\'}]', additional_kwargs={'idx': 1, 'args': {'query': 'oldest parrot alive'}}, name='tavily_search_results_json'), FunctionMessage(content='[{\\'url\\': \\'https://www.thesprucepets.com/how-long-do-parrots-and-other-pet-birds-live-1238433\\', \\'content\\': \"It\\'s possible that a pet bird can outlive its owners\\\\nThe Spruce / Adrienne Legault\\\\nParrots and other birds can live up to 10 to 50 years or more depending on the type and the conditions they live in. They vary in size from small birds that can fit in the palm of your hand to large birds the size of a cat and their lifespans are just as variable.\\\\n Also, for birds who live longer some owners have to make a plan of where the bird is going in the circumstance the bird outlives the owner.\\\\n In reality, there is a wide range in the age that pet birds might reach and certainly, some will live longer (or shorter amounts of time) than the ages listed.\\\\n Potential owners need to be aware of the longevity of their bird so they can be prepared to provide proper care for them for as long as they live.\\\\n\"}]', additional_kwargs={'idx': 2, 'args': {'query': 'average lifespan of parrots'}}, name='tavily_search_results_json'), FunctionMessage(content='join', additional_kwargs={'idx': 3, 'args': ()}, name='join')]]}}\n---\n[chain/error] [chain:LangGraph] [6.39s] Chain run errored with error:\n\"NotImplementedError(\\\"Unsupported message type: <class 'list'>\\\")Traceback (most recent call last):\\n\\n\\n  File \\\"C:\\\\Users\\\\erdem\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\langgraph\\\\pregel\\\\__init__.py\\\", line 910, in stream\\n    while loop.tick(\\n          ^^^^^^^^^^\\n\\n\\n  File \\\"C:\\\\Users\\\\erdem\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\langgraph\\\\pregel\\\\loop.py\\\", line 178, in tick\\n    apply_writes(\\n\\n\\n  File \\\"C:\\\\Users\\\\erdem\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\langgraph\\\\pregel\\\\algo.py\\\", line 195, in apply_writes\\n    updated = channels[chan].update(vals)\\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"C:\\\\Users\\\\erdem\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\langgraph\\\\channels\\\\binop.py\\\", line 104, in update\\n    self.value = self.operator(self.value, value)\\n\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"C:\\\\Users\\\\erdem\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\langgraph\\\\graph\\\\message.py\\\", line 70, in add_messages\\n    right = [message_chunk_to_message(m) for m in convert_to_messages(right)]\\n                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"C:\\\\Users\\\\erdem\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\langchain_core\\\\messages\\\\utils.py\\\", line 304, in convert_to_messages\\n    return [_convert_to_message(m) for m in messages]\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"C:\\\\Users\\\\erdem\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\langchain_core\\\\messages\\\\utils.py\\\", line 304, in <listcomp>\\n    return [_convert_to_message(m) for m in messages]\\n            ^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"C:\\\\Users\\\\erdem\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\langchain_core\\\\messages\\\\utils.py\\\", line 283, in _convert_to_message\\n    raise NotImplementedError(f\\\"Unsupported message type: {type(message)}\\\")\\n\\n\\nNotImplementedError: Unsupported message type: <class 'list'>\"\nE\n======================================================================\nERROR: test_query1 (__main__.TestLangchainAgent.test_query1)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"C:\\Users\\erdem\\Documents\\code\\LLMFinance\\tests\\test1.py\", line 134, in test_query1\n    response = get_llm_response(\n               ^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erdem\\Documents\\code\\LLMFinance\\utils\\entry.py\", line 49, in get_llm_response\n    return get_llm_response_langchain_llm_compiler(query, top_k, token_chunk_size, custom_prompt_template)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erdem\\Documents\\code\\LLMFinance\\utils\\langchain_agent_setups.py\", line 638, in get_llm_response_langchain_llm_compiler\n    for step in steps:\n  File \"C:\\Users\\erdem\\AppData\\Roaming\\Python\\Python311\\site-packages\\langgraph\\pregel\\__init__.py\", line 910, in stream   \n    while loop.tick(\n          ^^^^^^^^^^\n  File \"C:\\Users\\erdem\\AppData\\Roaming\\Python\\Python311\\site-packages\\langgraph\\pregel\\loop.py\", line 178, in tick\n    apply_writes(\n  File \"C:\\Users\\erdem\\AppData\\Roaming\\Python\\Python311\\site-packages\\langgraph\\pregel\\algo.py\", line 195, in apply_writes \n    updated = channels[chan].update(vals)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erdem\\AppData\\Roaming\\Python\\Python311\\site-packages\\langgraph\\channels\\binop.py\", line 104, in update    \n    self.value = self.operator(self.value, value)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erdem\\AppData\\Roaming\\Python\\Python311\\site-packages\\langgraph\\graph\\message.py\", line 70, in add_messages    right = [message_chunk_to_message(m) for m in convert_to_messages(right)]\n                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erdem\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\messages\\utils.py\", line 304, in convert_to_messages\n    return [_convert_to_message(m) for m in messages]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erdem\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\messages\\utils.py\", line 304, in <listcomp>\n    return [_convert_to_message(m) for m in messages]\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\erdem\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\messages\\utils.py\", line 283, in _convert_to_message\n    raise NotImplementedError(f\"Unsupported message type: {type(message)}\")\nNotImplementedError: Unsupported message type: <class 'list'>\nDescription\nI am trying to get LLM compiler work\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.19045\nPython Version:  3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.2.32\nlangchain: 0.2.14\nlangchain_community: 0.0.38\nlangsmith: 0.1.93\nlangchain_chroma: 0.1.3\nlangchain_cli: 0.0.30\nlangchain_experimental: 0.0.64\nlangchain_openai: 0.1.20\nlangchain_text_splitters: 0.2.2\nlangchainhub: 0.1.21\nlanggraph: 0.2.3\nlangserve: 0.2.2\n\nOther Dependencies\n\naiohttp: 3.9.3\naiosqlite: Installed. No version info available.\naleph-alpha-client: Installed. No version info available.\nanthropic: Installed. No version info available.\narxiv: Installed. No version info available.\nassemblyai: Installed. No version info available.\nasync-timeout: 4.0.3\natlassian-python-api: Installed. No version info available.\nazure-ai-documentintelligence: Installed. No version info available.\nazure-identity: Installed. No version info available.\nazure-search-documents: Installed. No version info available.\nbeautifulsoup4: 4.12.3\nbibtexparser: Installed. No version info available.\ncassio: Installed. No version info available.\nchardet: 4.0.0\nchromadb: 0.5.3\ncloudpickle: 2.2.1\ncohere: Installed. No version info available.\ndatabricks-vectorsearch: Installed. No version info available.\ndataclasses-json: 0.6.7\ndatasets: 2.21.0\ndgml-utils: Installed. No version info available.\nelasticsearch: Installed. No version info available.\nesprima: Installed. No version info available.\nfaiss-cpu: Installed. No version info available.\nfastapi: 0.112.1\nfeedparser: Installed. No version info available.\nfireworks-ai: Installed. No version info available.\nfriendli-client: Installed. No version info available.\ngeopandas: Installed. No version info available.\ngitpython: 3.1.43\ngoogle-cloud-documentai: Installed. No version info available.\ngql: Installed. No version info available.\ngradientai: Installed. No version info available.\nhdbcli: Installed. No version info available.\nhologres-vector: Installed. No version info available.\nhtml2text: Installed. No version info available.\nhttpx: 0.27.0\nhttpx-sse: Installed. No version info available.\njavelin-sdk: Installed. No version info available.\njinja2: 3.1.3\njq: Installed. No version info available.\njsonpatch: 1.33\njsonschema: 4.19.2\nlanggraph-checkpoint: 1.0.3\nlangserve[all]: Installed. No version info available.\nlibcst: 1.4.0\nlxml: 4.9.3\nmarkdownify: Installed. No version info available.\nmotor: Installed. No version info available.\nmsal: Installed. No version info available.\nmwparserfromhell: Installed. No version info available.\nmwxml: Installed. No version info available.\nnewspaper3k: Installed. No version info available.\nnumexpr: 2.8.7\nnumpy: 1.26.4\nnvidia-riva-client: Installed. No version info available.\noci: Installed. No version info available.\nopenai: 1.36.1\nopenapi-pydantic: Installed. No version info available.\noracle-ads: Installed. No version info available.\noracledb: Installed. No version info available.\norjson: 3.10.6\npackaging: 24.1\npandas: 2.1.4\npdfminer-six: Installed. No version info available.\npgvector: 0.2.5\npraw: Installed. No version info available.\npremai: Installed. No version info available.\npsychicapi: Installed. No version info available.\npy-trello: Installed. No version info available.\npydantic: 1.10.12\npyjwt: 2.4.0\npymupdf: Installed. No version info available.\npypdf: 4.3.0\npypdfium2: Installed. No version info available.\npyproject-toml: 0.0.10\npyspark: Installed. No version info available.\nPyYAML: 6.0.1\nrank-bm25: Installed. No version info available.\nrapidfuzz: Installed. No version info available.\nrapidocr-onnxruntime: Installed. No version info available.\nrdflib: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrspace_client: Installed. No version info available.\nscikit-learn: 1.2.2\nSQLAlchemy: 2.0.25\nsqlite-vss: Installed. No version info available.\nsse-starlette: 1.8.2\nstreamlit: 1.30.0\nsympy: 1.12\ntelethon: Installed. No version info available.\ntenacity: 8.4.2\ntidb-vector: Installed. No version info available.\ntiktoken: 0.7.0\ntimescale-vector: Installed. No version info available.\ntomlkit: 0.12.5\ntqdm: 4.66.4\ntree-sitter: Installed. No version info available.\ntree-sitter-languages: Installed. No version info available.\ntyper: 0.9.4\ntyper[all]: Installed. No version info available.\ntypes-requests: Installed. No version info available.\ntyping-extensions: 4.9.0\nupstash-redis: Installed. No version info available.\nuvicorn: 0.23.2\nvdms: Installed. No version info available.\nxata: Installed. No version info available.\nxmltodict: Installed. No version info available.\n", "created_at": "2024-08-24", "closed_at": "2024-09-04", "labels": [], "State": "closed", "Author": "erdult"}
{"issue_number": 2209, "issue_title": "Issue: Multiple builds in the single deployment not supported", "issue_body": "Issue you'd like to raise.\nI was hosting my langgraph on langsmith but after some builds it was not updating the code that I wrote in the new build and was still trying to execute the same code from the last build which didn't even existed.\nSuggestion:\nNo response", "created_at": "2024-10-24", "closed_at": "2024-10-29", "labels": [], "State": "closed", "Author": "Pratham271"}
{"issue_number": 2169, "issue_title": "Inconsistency .invoke and .stream for graph's last callback and output_keys affecting streamed values", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom typing import TypedDict\n\nfrom langchain_core.callbacks.base import BaseCallbackHandler\nfrom langchain_core.runnables.config import RunnableConfig\nfrom langgraph.graph import END, StateGraph\nfrom langgraph.graph.graph import START\n\n\nclass CustomCallbackHandler(BaseCallbackHandler):\n    def on_chain_end(self, _outputs, **kwargs):\n        print(\"on_chain_end\", _outputs)\n\n\nclass OutputType(TypedDict):\n    a: int\n\n\nclass State(TypedDict):\n    a: int\n    b: int\n\n\ngraph = StateGraph(State, output=OutputType)\ngraph.add_node(\"node_a\", lambda state: {\"a\": state[\"a\"] + 1})\ngraph.add_node(\"node_b\", lambda state: {\"b\": state[\"b\"] + 1})\n\ngraph.add_edge(START, \"node_a\")\ngraph.add_edge(\"node_a\", \"node_b\")\ngraph.add_edge(\"node_b\", END)\n\ninput = {\"a\": 0, \"b\": 0}\n\nconfig = RunnableConfig(callbacks=[CustomCallbackHandler()])\n\ncompiled_graph = graph.compile()\n\ninvoke_result = compiled_graph.invoke(input, config)\nprint(\"invoke result: \", invoke_result)\n\nprint(\"**************\")\nprint(\"graph without output keys defined\")\nprint(\"**************\")\ngraph_without_output_keys_results = []\nfor stream_output in compiled_graph.stream(input, config):\n    graph_without_output_keys_results.append(stream_output)\n\nprint(\"*** stream outputs ***\")\nfor stream_output in graph_without_output_keys_results:\n    print(stream_output)\n\nprint(\"**************\")\nprint(\"graph with output keys (a) defined\")\nprint(\"**************\")\ngraph_with_output_keys_results = []\nfor stream_output in compiled_graph.stream(input, config, output_keys=[\"a\"]):\n    graph_with_output_keys_results.append(stream_output)\n\nprint(\"*** stream outputs ***\")\nfor stream_output in graph_with_output_keys_results:\n    print(stream_output)\nError Message and Stack Trace (if applicable)\n[Annotated result]\n\non_chain_end {'a': 0, 'b': 0}\non_chain_end {'a': 1}\non_chain_end {'a': 1}\non_chain_end {'b': 1}\non_chain_end {'b': 1}\non_chain_end {'a': 1} <--- This is the graph's final callback that I am tracing\ninvoke result:  {'a': 1}\n**************\ngraph without output keys defined\n**************\non_chain_end {'a': 0, 'b': 0}\non_chain_end {'a': 1}\non_chain_end {'a': 1}\non_chain_end {'b': 1}\non_chain_end {'b': 1}\non_chain_end {'a': 1, 'b': 1} <-- This is the graph's final callback result when using stream\n*** stream outputs ***\n{'node_a': {'a': 1}}\n{'node_b': {'b': 1}}\n**************\ngraph with output keys (a) defined\n**************\non_chain_end {'a': 0, 'b': 0}\non_chain_end {'a': 1}\non_chain_end {'a': 1}\non_chain_end {'b': 1}\non_chain_end {'b': 1}\non_chain_end {'a': 1} <-- This is the graph's final callback result when using stream with output_keys\n*** stream outputs ***\n{'node_a': {'a': 1}}\n{'node_b': None} <-- I expect this not to be None, as I want to stream content, that is not necessarily returned from the graph in the end\nDescription\nHere is a minimal repo to show the encountered inconsistency: https://github.com/kaiwend/langgraph-streaming-inconsistency\nI have been working with langgraph for a while now, mostly invoking my graph using .invoke. Now after some time, I wanted to introduce streaming into my app. The overall goal is to get some internals from the graph execution out earlier to the user and not just after the whole invocation is done, e.g. logging and streaming the output to the user. While doing that, I noticed some inconsistencies on how the streaming behaves in combination with callbacks. I am using tracing that uses the callbacks to collect traces. I am working with quite large amounts of data, so limiting what is coming in and out of nodes is crucial for the task at hand. However when using streaming, I noticed the following difference to the non-streamed version:\n\nI have an output TypedDict, which I assign to my graph StateGraph(State, output=OutputType). When using .invoke, the output type is used and I only get what's defined in OutputType. This also leads to the last callback also being triggered with OutputType.\nWhen using streaming on the other hand, the last callback from the graph get my whole state, which is a problem in regards to the sheer size of my state's contents\nI tried adding output_keys= (keys from my OutputType) to my graph.stream call. This lead to the last callback being triggered correctly, with only returning OutputType. However this also had the side effect of the streamed values becoming limited to those output_keys as well.\n\nTo summarize, I think there are 2 potential issues at hand:\n\nWhen streaming the graph, the final callback is called with the whole state instead of being limited to the type that is defined in output=\nWhen setting output_keys= on graph.stream, the streamed values in between are also filtered by those keys and not only the final output keys, which is the opposite of what I would have expected, but might be intended?\n\nSystem Info\nlanggraph==0.2.39\nlanggraph-checkpoint==2.0.1\nlanggraph-sdk==0.1.33\nmac os\nPython 3.11.6", "created_at": "2024-10-24", "closed_at": null, "labels": [], "State": "open", "Author": "kaiwend"}
{"issue_number": 2155, "issue_title": "Serialization error when executing traceback at the end of workflow execution", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom pydantic import BaseModel\nfrom snowflake.snowpark import Session\n\n\nclass BaseModelWithExclusions(BaseModel):\n    class Config:\n        arbitrary_types_allowed = True  # Allow arbitrary types like Session\n\n\nclass GetCategoriesInput(BaseModelWithExclusions):\n    sp_session: Session\n    data: str\n\n\nclass SQLSnowparkQueryInput(BaseModelWithExclusions):\n    sp_session: Session\n    query: str\n\n\n\n# in another file:\nfor s in self._workflow.stream(message=user_prompt):\n        if \"end\" not in s:\n            print(\"####\")\n            print(s)\n            # print(s[\"messages\"])\n            print(\"----\")\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\n  File \"/Users/henrymac/Documents/data_athena_snowflake/src/multi_agents/workflows/chatbot_v0/graph.py\", line 263, in <module>\n    exec_thread_id = chatbot_manager.run()\n                     ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/henrymac/Documents/data_athena_snowflake/src/multi_agents/workflows/chatbot_v0/graph.py\", line 240, in run\n    for s in self._workflow.stream(message=user_prompt):\n  File \"/Users/henrymac/Documents/data_athena_snowflake/.venv3.11/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 1240, in stream\n    with SyncPregelLoop(\n  File \"/Users/henrymac/Documents/data_athena_snowflake/.venv3.11/lib/python3.11/site-packages/langgraph/pregel/loop.py\", line 754, in __exit__\n    return self.stack.__exit__(exc_type, exc_value, traceback)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/henrymac/.pyenv/versions/3.11.2/lib/python3.11/contextlib.py\", line 589, in __exit__\n    raise exc_details[1]\n  File \"/Users/henrymac/.pyenv/versions/3.11.2/lib/python3.11/contextlib.py\", line 574, in __exit__\n    if cb(*exc_details):\n       ^^^^^^^^^^^^^^^^\n  File \"/Users/henrymac/Documents/data_athena_snowflake/.venv3.11/lib/python3.11/site-packages/langgraph/pregel/executor.py\", line 105, in __exit__\n    task.result()\n  File \"/Users/henrymac/.pyenv/versions/3.11.2/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/henrymac/.pyenv/versions/3.11.2/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/henrymac/Documents/data_athena_snowflake/.venv3.11/lib/python3.11/site-packages/langgraph/pregel/loop.py\", line 680, in _checkpointer_put_after_previous\n    prev.result()\n  File \"/Users/henrymac/.pyenv/versions/3.11.2/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/henrymac/.pyenv/versions/3.11.2/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/henrymac/Documents/data_athena_snowflake/.venv3.11/lib/python3.11/site-packages/langgraph/pregel/executor.py\", line 70, in done\n    task.result()\n  File \"/Users/henrymac/.pyenv/versions/3.11.2/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/henrymac/.pyenv/versions/3.11.2/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/henrymac/.pyenv/versions/3.11.2/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/henrymac/Documents/data_athena_snowflake/.venv3.11/lib/python3.11/site-packages/langgraph/pregel/loop.py\", line 682, in _checkpointer_put_after_previous\n    cast(BaseCheckpointSaver, self.checkpointer).put(\n  File \"/Users/henrymac/Documents/data_athena_snowflake/.venv3.11/lib/python3.11/site-packages/langgraph/checkpoint/memory/__init__.py\", line 330, in put\n    self.serde.dumps_typed(c),\n    ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/henrymac/Documents/data_athena_snowflake/.venv3.11/lib/python3.11/site-packages/langgraph/checkpoint/serde/jsonplus.py\", line 192, in dumps_typed\n    return \"msgpack\", _msgpack_enc(obj)\n                      ^^^^^^^^^^^^^^^^^\n  File \"/Users/henrymac/Documents/data_athena_snowflake/.venv3.11/lib/python3.11/site-packages/langgraph/checkpoint/serde/jsonplus.py\", line 476, in _msgpack_enc\n    return enc.pack(data)\n           ^^^^^^^^^^^^^^\n  File \"msgpack/_packer.pyx\", line 279, in msgpack._cmsgpack.Packer.pack\n  File \"msgpack/_packer.pyx\", line 276, in msgpack._cmsgpack.Packer.pack\n  File \"msgpack/_packer.pyx\", line 265, in msgpack._cmsgpack.Packer._pack\n  File \"msgpack/_packer.pyx\", line 213, in msgpack._cmsgpack.Packer._pack_inner\n  File \"msgpack/_packer.pyx\", line 265, in msgpack._cmsgpack.Packer._pack\n  File \"msgpack/_packer.pyx\", line 213, in msgpack._cmsgpack.Packer._pack_inner\n  File \"msgpack/_packer.pyx\", line 265, in msgpack._cmsgpack.Packer._pack\n  File \"msgpack/_packer.pyx\", line 232, in msgpack._cmsgpack.Packer._pack_inner\n  File \"msgpack/_packer.pyx\", line 267, in msgpack._cmsgpack.Packer._pack\n  File \"/Users/henrymac/Documents/data_athena_snowflake/.venv3.11/lib/python3.11/site-packages/langgraph/checkpoint/serde/jsonplus.py\", line 227, in _msgpack_default\n    _msgpack_enc(\n  File \"/Users/henrymac/Documents/data_athena_snowflake/.venv3.11/lib/python3.11/site-packages/langgraph/checkpoint/serde/jsonplus.py\", line 476, in _msgpack_enc\n    return enc.pack(data)\n           ^^^^^^^^^^^^^^\n  File \"msgpack/_packer.pyx\", line 279, in msgpack._cmsgpack.Packer.pack\n  File \"msgpack/_packer.pyx\", line 276, in msgpack._cmsgpack.Packer.pack\n  File \"msgpack/_packer.pyx\", line 265, in msgpack._cmsgpack.Packer._pack\n  File \"msgpack/_packer.pyx\", line 232, in msgpack._cmsgpack.Packer._pack_inner\n  File \"msgpack/_packer.pyx\", line 265, in msgpack._cmsgpack.Packer._pack\n  File \"msgpack/_packer.pyx\", line 213, in msgpack._cmsgpack.Packer._pack_inner\n  File \"msgpack/_packer.pyx\", line 265, in msgpack._cmsgpack.Packer._pack\n  File \"msgpack/_packer.pyx\", line 232, in msgpack._cmsgpack.Packer._pack_inner\n  File \"msgpack/_packer.pyx\", line 265, in msgpack._cmsgpack.Packer._pack\n  File \"msgpack/_packer.pyx\", line 213, in msgpack._cmsgpack.Packer._pack_inner\n  File \"msgpack/_packer.pyx\", line 265, in msgpack._cmsgpack.Packer._pack\n  File \"msgpack/_packer.pyx\", line 213, in msgpack._cmsgpack.Packer._pack_inner\n  File \"msgpack/_packer.pyx\", line 265, in msgpack._cmsgpack.Packer._pack\n  File \"msgpack/_packer.pyx\", line 213, in msgpack._cmsgpack.Packer._pack_inner\n  File \"msgpack/_packer.pyx\", line 267, in msgpack._cmsgpack.Packer._pack\n  File \"/Users/henrymac/Documents/data_athena_snowflake/.venv3.11/lib/python3.11/site-packages/langgraph/checkpoint/serde/jsonplus.py\", line 409, in _msgpack_default\n    raise TypeError(f\"Object of type {obj.__class__.__name__} is not serializable\")\nTypeError: Object of type Session is not serializable\nDescription\nI have defined two custom base models for my tool calls since I need to pass an active snowflake session to my tools.\nIt works as expected, the workflow runs from start to end perfectly, but at the end of the workflow.stream execution, the traceback tries to do a serialization.\nSo, how can I modify this behaviour and use a different serialization to avoid this error?\nfor s in self._workflow.stream(message=user_prompt):\n                if \"end\" not in s:\n                    print(\"####\")\n                    print(s)\n                    # print(s[\"messages\"])\n                    print(\"----\")\n\nSystem Info\nlangchain==0.3.1\nlangchain-anthropic==0.2.1\nlangchain-core==0.3.6\nlangchain-openai==0.2.1\nlangchain-text-splitters==0.3.0\nplatform mac\npython version 3.11", "created_at": "2024-10-22", "closed_at": "2024-10-22", "labels": [], "State": "closed", "Author": "henryf3"}
{"issue_number": 2153, "issue_title": "Must write to at least one of ['messages', 'next'] error in Langgraph/Supervisor Code", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.messages import HumanMessage, BaseMessage\nfrom langchain_openai import ChatOpenAI\n\nfrom typing import Annotated, Literal, Sequence\nfrom typing_extensions import TypedDict\nfrom pydantic import BaseModel\n\nimport functools\nimport operator\n\nfrom langgraph.graph import END, StateGraph, START\nfrom langgraph.prebuilt import create_react_agent\n\nfrom tools import read_pdf, save_json, fetch_document\n\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\nos.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n\nllm = ChatOpenAI(\n    model=\"gpt-4o-mini\",\n    temperature=0.5,\n)\n\nFIRST_DOC_PATH = \"docs\\sps_101\\syllabus\\SPS 101 A-B Syllabus Spring 2022.pdf\"\n\n\ndef agent_node(state, agent, name):\n    result = agent.invoke(state)\n    return {\n        \"messages\": [HumanMessage(content=result[\"messages\"][-1].content, name=name)]\n    }\n\nmembers = [\"Outliner\", \"Question Generator\", \"Answer Generator\", \"Document Saver\"]\n\nsystem_prompt = (\n    \"You are a supervisor tasked with managing a conversation between the\"\n    \" following workers:  {members}. Given the following user request,\"\n    \" respond with the worker to act next. Each worker will perform a\"\n    \" task and respond with their results and status. When finished,\"\n    \" respond with FINISH.\"\n)\n\noptions =  [\"FINISH\"] + members\n\nclass RouteResponse(BaseModel):\n    next_agent: Literal[*options]\n\nprompt = ChatPromptTemplate(\n    [\n        (\"system\", system_prompt),\n        MessagesPlaceholder(variable_name=\"messages\"),\n        (\n            \"system\",\n            \"Given the conversation above, who should act next?\"\n            \" Or should we FINISH? Select one of: {options}\",\n        )\n    ]\n).partial(options=str(options), members=\", \".join(members))\n\n\ndef supervisor_agent(state):\n    supervisor_chain = prompt | llm.with_structured_output(RouteResponse)\n    return supervisor_chain.invoke(state)\n\n\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], operator.add]\n    next: str\n\n\noutliner_agent = create_react_agent(llm, tools=[read_pdf])\noutliner_node = functools.partial(agent_node, agent=outliner_agent, name=\"Outliner\")\n\nquestion_generator_agent = create_react_agent(llm, tools=[fetch_document])\nquestion_generator_node = functools.partial(agent_node, agent=question_generator_agent, name=\"Question Generator\")\n\nanswer_generator_agent = create_react_agent(llm, tools=[])\nanswer_generator_node = functools.partial(agent_node, agent=answer_generator_agent, name=\"Answer Generator\")\n\ndocument_saver_agent = create_react_agent(llm, tools=[save_json])\ndocument_saver_node = functools.partial(agent_node, agent=document_saver_agent, name=\"Document Saver\")\n\n\nworkflow = StateGraph(AgentState)\nworkflow.add_node(\"Outliner\", outliner_agent)\nworkflow.add_node(\"Question Generator\", question_generator_agent)\nworkflow.add_node(\"Answer Generator\", answer_generator_agent)\nworkflow.add_node(\"Document Saver\", document_saver_agent)\nworkflow.add_node(\"Supervisor\", supervisor_agent)\n\nfor member in members:\n    workflow.add_edge(member, \"Supervisor\")\n\nconditional_map = {k: k for k in members}\nconditional_map[\"FINISH\"] = END\nworkflow.add_conditional_edges(\"Supervisor\", lambda x: x[\"next\"], conditional_map)\nworkflow.add_edge(START, \"Supervisor\")\n\ngraph = workflow.compile()\n\nfor s in graph.stream(\n    {\"messages\": [HumanMessage(content=f\"Read the syllabus document with path: {FIRST_DOC_PATH} and create an outline for a question, then generate the question using the weekly lecture slides, then, generate an answer for the question, and finally, save the document which contains question and answer to a JSON file.\")]},\n    {\"recursion_limit\": 100},\n    debug=True\n):\n    if \"__end__\" not in s:\n        print(s)\n        print(\"----\")\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nMy code is 90 percent same as the LangGraph's Supervisor code. I just want to build a simple supervisor based agentic workflow, but I got this error. I couldn't find a solution on other issues. Please help me!\nSystem Info\nannotated-types==0.7.0\nanyio==4.6.0\nasttokens==2.4.1\ncertifi==2024.8.30\ncharset-normalizer==3.4.0\ncolorama==0.4.6\ncomm==0.2.2\ndebugpy==1.8.7\ndecorator==5.1.1\ndistro==1.9.0\nexecuting==2.1.0\nh11==0.14.0\nhttpcore==1.0.6\nhttpx==0.27.2\nidna==3.10\nipykernel==6.29.5\nipython==8.28.0\njedi==0.19.1\njiter==0.6.1\njsonpatch==1.33\njsonpointer==3.0.0\njupyter_client==8.6.3\njupyter_core==5.7.2\nlangchain-core==0.3.12\nlangchain-openai==0.2.3\nlanggraph==0.2.35\nlanggraph-checkpoint==2.0.1\nlangsmith==0.1.132\nmatplotlib-inline==0.1.7\nmsgpack==1.1.0\nnest-asyncio==1.6.0\nopenai==1.52.0\norjson==3.10.7\npackaging==24.1\nparso==0.8.4\nplatformdirs==4.3.6\nprompt_toolkit==3.0.48\npsutil==6.1.0\npure_eval==0.2.3\npydantic==2.9.2\npydantic_core==2.23.4\nPygments==2.18.0\npypdf==5.0.1\npython-dateutil==2.9.0.post0\npython-dotenv==1.0.1\npywin32==308\nPyYAML==6.0.2\npyzmq==26.2.0\nregex==2024.9.11\nrequests==2.32.3\nrequests-toolbelt==1.0.0\nsix==1.16.0\nsniffio==1.3.1\nstack-data==0.6.3\ntenacity==8.5.0\ntiktoken==0.8.0\ntornado==6.4.1\ntqdm==4.66.5\ntraitlets==5.14.3\ntyping_extensions==4.12.2\nurllib3==2.2.3\nwcwidth==0.2.13", "created_at": "2024-10-21", "closed_at": "2024-10-23", "labels": [], "State": "closed", "Author": "fatih-sarioglu"}
{"issue_number": 2150, "issue_title": "Why can't I decide which node to navigate to at runtime based on the value of state?", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\n#This is my route funciton:\ndef route_to_agent(self,state):\n        if not state[\"next_agent\"]: \n            return self.entry_agent_node\n        last_message = state[\"messages\"][-1]\n        if isinstance(last_message,AIMessage):\n            return END\n        return state[\"next_agent\"]\n\n#This how I define conditional edges  \nself.graph.add_conditional_edges(\n            START,\n            self.route_to_agent,\n            {agent.name: agent.name for agent in self.agent_factory.get_all_agents().values()}\n        )\nfor agent in self.agent_factory.get_all_agents().values():\n    self.graph.add_conditional_edges(\n        agent.name,\n        self.route_to_agent,\n        {next_agent: next_agent for next_agent in agent.next_agents}\n    )\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nhow can I define rightly?\nSystem Info\nraceback (most recent call last):\nFile \"/Users/zuoyuxuan/Documents/Code/QINIU/agent_flow/swarm_workflow.py\", line 47, in \ngraph = graph.compile(checkpointer=memory)\nFile \"/Users/zuoyuxuan/anaconda3/envs/agent_flow/lib/python3.10/site-packages/langgraph/graph/state.py\", line 430, in compile\nself.validate(\nFile \"/Users/zuoyuxuan/anaconda3/envs/agent_flow/lib/python3.10/site-packages/langgraph/graph/graph.py\", line 396, in validate\nraise ValueError(f\"Found edge ending at unknown node {target}\")\nValueError: Found edge ending at unknown node None", "created_at": "2024-10-21", "closed_at": "2024-10-21", "labels": [], "State": "closed", "Author": "teatimekon"}
{"issue_number": 2146, "issue_title": "Thread level persistence not working in LangGraph StateGraph with multiple nodes and edges", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nclass WFM:\n    def __init__(self,thread_id: str):\n        self.sql_agent = SQLAgent()\n        \n        self.memory = MemorySaver()\n        # self.memory = ConversationBufferMemory(memory_key=\"history\")\n        # Initialize a default or dynamic thread_id\n        self.thread_id = thread_id  # Generates a unique ID\n        self.langfuse_callback = CallbackHandler(\n            trace_name=\"Agent\",\n            session_id=str(uuid.uuid4()),\n            user_id=\"Test User\"\n        )\n\n   def create_workflow(self) -> StateGraph:\n        \n        workflow = StateGraph(State)\n\n        \n        # # Add nodes to the graph\n        \n        workflow.add_node(\"classify_input\",self.sql_agent.classify_input)\n        \n        workflow.add_node(\"parse_question\", self.sql_agent.parse_question)\n        \n        workflow.add_node(\"generate_sql\", self.sql_agent.generate_sql)\n        workflow.add_node(\"validate_sql\", self.sql_agent.validate_sql)\n        workflow.add_node(\"execute_sql\", self.sql_agent.execute_sql)\n        \n\n\n        # Define edges\n        \n        workflow.add_edge(\"classify_input\", \"parse_question\")\n\n        \n        workflow.add_edge(\"parse_question\", \"generate_sql\")\n        \n        workflow.add_edge(\"generate_sql\", \"validate_sql\")\n        workflow.add_edge(\"validate_sql\", \"execute_sql\")\n        workflow.add_edge(\"execute_sql\", END)\n        workflow.set_entry_point(\"classify_input\")\n\n        return workflow\n    def event_stream(self, question: str) -> dict:\n        \n        app = self.create_workflow().compile(checkpointer=self.memory)\n        print(\"THREAD ID in EVENT STREAM END = {}\".format(self.thread_id))\n        \n        result = app.invoke({\"question\": question},\n                            config={\"callbacks\": [self.langfuse_callback],\n                                    \"configurable\": {\"thread_id\": self.thread_id}}, debug=True)\n        \n        print(\"ANSWER IN EVENT STREAM END = {}\".format(result))\n        return result\n\nclass SQLAgent:\n    def __init__(self):\n        \n        self.llm_manager = LLMManager()\n\n    def parse_question(self, state: dict) -> dict:\n        \n        classification = state.get(\"classification\")\n        question = state[\"question\"]\n        schema = state[\"table_schema\"]\n\n        # Check for Classification\n        \n        if classification == \"database_query\":\n\n            prompt = ChatPromptTemplate.from_messages(\n                [\n                    (\n                        \"system\",\n                        \"\"\"You are a data analyst that can help summarize SQL tables and parse user questions about a database. \n                        \n                        \"\"\",\n                    ),\n                \n                    (\n                        \"human\",\n                        \"===Database schema:\\n{schema}\\n\\n===User question:\\n{question}\\n\\nIdentify relevant tables and columns:\",\n                    ),\n                ]\n            )\n\n            output_parser = JsonOutputParser()\n\n\n            response = self.llm_manager.invoke(prompt, schema=schema, question=question)\n\n            parsed_response = output_parser.parse(response)\n\n            \n            state[\"parsed_question\"] = parsed_response\n            return {\"parsed_question\": parsed_response}\n\n        elif classification == \"small_talk\":\n\n            prompt = ChatPromptTemplate.from_messages(\n                [\n                    (\"system\", \"You are a friendly assistant that engages the user in small talk.\"),\n                    (\"human\", \"{question}\")\n                ]\n            )\n\n            response = self.llm_manager.invoke(prompt, question=question)\n\n            \n            state[\"answer\"] = response\n            state[\"skip_sql_steps\"] = True\n            \n            return state\n        else:\n            state[\"answer\"] = \"I'm sorry, I didn't understand that. Could you please rephrase that?\"\n            state[\"skip_sql_steps\"] = True\n            \n            return state\nError Message and Stack Trace (if applicable)\nWhen I execute the above, memory doesn't seems to work even though I am initiating my LangGraph using MemorySaver() as depicted in https://langchain-ai.github.io/langgraph/how-tos/persistence/.\n\nFor example if I say \"I am Bob!\" and then ask \"What is my name?\", it doesn't remember.\nDescription\nWhen I execute the above, memory doesn't seems to work even though I am initiating my LangGraph using MemorySaver() as depicted in https://langchain-ai.github.io/langgraph/how-tos/persistence/.\nFor example if I say \"I am Bob!\" and then ask \"What is my name?\", it doesn't remember.\nSystem Info\nLangchain 0.3.2\nLangGraph 0.2.34", "created_at": "2024-10-20", "closed_at": "2024-10-21", "labels": [], "State": "closed", "Author": "A-Sur590"}
{"issue_number": 2142, "issue_title": "Subgraph state is not inserted to persistance db.", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nSubgraph class:\n\n\nimport functools\nfrom typing import Annotated, Sequence, TypedDict, Literal\nfrom common.schema import  ResearchSchema\nfrom langchain_core.messages import (\n    BaseMessage,\n    HumanMessage,\n    AIMessage\n)\nfrom common.constants import RESEARCH_COLLECTION\nfrom common.firestore_db import update_research_chat, get_doc\nfrom langgraph.checkpoint.postgres import PostgresSaver\nfrom psycopg_pool import ConnectionPool\nimport os\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\nfrom langgraph.graph import END, StateGraph, START, add_messages, MessagesState\nfrom langgraph.prebuilt import ToolNode\nfrom common.utils import convert_message_to_dict\nfrom tools import ToolSaveHandler\nfrom uuid import uuid4\nimport ast\nfrom common.firestore_db import update_multiagent_research_chat\n\n\nclass NeoState(MessagesState):\n    sender: str\n    limit:int\n\nclass Agent:\n    def __init__(self, llm, tools, inputs, history, feedback_limit, user_query ,system_message: str,simple = False, agent_type = True):\n        self.llm = llm\n        self.tools = tools\n        self.system_message = system_message\n        self.simple = simple\n        self.inputs=inputs\n        self.user_query = user_query\n        self.history=history\n        self.feedback_limit = feedback_limit\n        self.agent=self.create_reviewer_agent()\n        if agent_type:\n            self.agent = self.create_generator_agent()\n\n\n\n    def create_reviewer_agent(self):\n        \"\"\"Create an agent.\"\"\"\n        prompt = ChatPromptTemplate.from_messages(\n            [\n                (\n                    \"system\",\n                    \"\"\"\n1. Follow Instructions:\n- Adhere to any specific instructions provided by the user, below are user instructions that are essential for producing the correct feedback or critique response.:\n```{system_message}``` \n- To ensure the output aligns with the user's requirements. These guidelines are essential for producing the correct feedback or critique response.\n\nYour primary goal is to ensure that the final answer is accurate, well-supported, and complete. Collaborate efficiently with the other assistants, provide valuable critiques, and guide the team toward the best possible outcome.\n                    \"\"\",\n                ),\n                \n                MessagesPlaceholder(variable_name=\"messages\"),\n            ]\n        )\n        prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in self.tools]))\n        return prompt | self.llm.bind_tools(self.tools)\n        \n\n    def create_generator_agent(self):\n        \"\"\"Create an agent.\"\"\"\n        if self.simple:\n            prompt = ChatPromptTemplate.from_messages(\n                [\n                    (\n                        \"system\",\n                        self.system_message\n                    ),\n                    MessagesPlaceholder(variable_name=\"history\"),\n                    MessagesPlaceholder(variable_name=\"messages\"),\n                ]\n            )\n            prompt = prompt.partial(history=self.history)\n            prompt = prompt.partial(system_message=self.system_message)\n            prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in self.tools]))\n            return prompt | self.llm.bind_tools(self.tools)\n        else:\n\n            prompt = ChatPromptTemplate.from_messages(\n                [\n                    (\n                        \"system\",\n                        \"\"\"\n\n\n**1. Follow Instructions:**\n- Adhere to any specific instructions provided by the user, below are user instructions that are essential for producing the correct response.:\n```{system_message}``` \n- To ensure the output aligns with the user's requirements. These guidelines are essential for producing the correct response.\n\nYour sole purpose is to generate effective, accurate, and well-constructed responses based on the user's query and always do the work according to the instructions, whether using tools or relying on knowledge. Stay within your role, continue generating content, and contribute toward the final solution.\n                        \"\"\",\n                    ),\n                    MessagesPlaceholder(variable_name=\"history\"),\n                    MessagesPlaceholder(variable_name=\"messages\"),\n                ]\n            )\n            prompt = prompt.partial(history=self.history)\n            prompt = prompt.partial(system_message=self.system_message)\n            prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in self.tools]))\n            return prompt | self.llm.bind_tools(self.tools)\n\n    def agent_node(self, state:NeoState, name):\n        result = self.agent.invoke(state)\n        response = [result]\n        cnt=self.feedback_limit\n        if 'limit' in state:\n            cnt = state['limit']\n        if result.tool_calls:\n            pass\n        else:\n            result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n            post_fill = HumanMessage(**result.dict(exclude={\"type\", \"name\"}),name=name, limit=cnt)\n            post_fill.id = str(uuid4())\n            post_fill.content = f\"Reviewer has to review the responses generated by Generator agent and provide feedback.\"\n            response = [result, post_fill]\n            if name=='reviewer':\n                post_fill.content = f\"\"\"Generator has to generate and compile the user query by incorporating feedback.\nGiven any feedback you need to always generate the user query response.\noriginal user query:\\n {self.user_query}\"\"\"\n                cnt=cnt-1\n                return {\n                \"messages\": response,\n                \"sender\": name,\n                \"limit\": cnt\n                }\n\n        return {\n        \"messages\": response,\n        \"sender\": name,\n        \"limit\": cnt,\n        }\n\nclass NeoleadsWorkflow:\n    def __init__(self, inputs, generator_system_message,reviewer_system_message ,user_prompt, tools, llm, feedback_limit, chatHistory, config, chat, research,callbacks=[], approval_tool_names=[],simple =False):\n        self.inputs=inputs\n        self.generator_system_message = self.convert_prompt(generator_system_message,inputs,False)\n        self.reviewer_system_message = self.convert_prompt(reviewer_system_message,inputs,False)\n        self.simple = simple\n        self.user_query = self.convert_prompt(user_prompt,inputs,False)\n        self.user_prompt = self.convert_prompt(user_prompt,inputs=inputs)\n        self.DB_URI = os.getenv('DB_URI')\n        self.connection_kwargs = {\n            \"autocommit\": True,\n            \"prepare_threshold\": 0,\n        }\n        self.chat = chat\n        self.research = research\n        self.approval_tool_names=approval_tool_names\n        self.tools=tools\n        self.config=config\n        self.callbacks=callbacks\n        self.history=chatHistory\n        # self.research=research\n        self.snapshot = None\n        self.feedback_limit=feedback_limit\n        self.llm=llm\n        self.intermediate_steps=[]\n        self.final_response=\"\"\n        self.workflow = self.create_workflow()\n        # print(self.workflow.get_graph().draw_mermaid())\n        # print(self.workflow.get_graph().print_ascii())\n        \n\n\n    def convert_prompt(self,prompt,inputs, tick=True):\n        if tick:\n            prompt=\"Generator will only generate or answer user query:\\n\" + prompt +\"\\n\\n Reviewer will only review and give feedback:\\n\"\n        input_variables=[key for key,value in inputs.items()]\n        template=PromptTemplate(template=prompt,input_variables=input_variables)\n        return template.invoke(inputs).text\n    \n    def human_review_node(self,state:NeoState):\n        pass\n    \n    def route_after_human(self,state:NeoState):\n        if isinstance(state[\"messages\"][-1], AIMessage):\n            return \"call_tool\"\n        else:\n            return \"continue\"\n\n    def create_workflow(self):\n\n        generator_agent = Agent(self.llm, self.tools,self.inputs,self.history, self.feedback_limit,self.user_query,self.generator_system_message,self.simple)\n        reviewer_agent = Agent(self.llm, self.tools,self.inputs,self.history,self.feedback_limit,self.user_query,self.reviewer_system_message,agent_type=False)\n        tool_node = ToolNode(self.tools)\n\n        workflow = StateGraph(NeoState)\n\n        workflow.add_node(\"generator\", functools.partial(generator_agent.agent_node, name=\"generator\"))\n        workflow.add_node(\"reviewer\", functools.partial(reviewer_agent.agent_node, name=\"reviewer\"))\n        workflow.add_node(\"human_review_node\", self.human_review_node)\n        workflow.add_node(\"call_tool\", tool_node)\n        workflow.add_conditional_edges(\"human_review_node\", self.route_after_human)\n        workflow.add_conditional_edges(\n            \"generator\",\n            self.router,\n            {\"continue\": \"reviewer\", \"call_tool\": \"call_tool\", \"human_review_node\":\"human_review_node\",\"__end__\": END},\n        )\n        workflow.add_conditional_edges(\n            \"reviewer\",\n            self.router,\n            {\"continue\": \"generator\", \"call_tool\": \"call_tool\",\"human_review_node\":\"human_review_node\", \"__end__\": END},\n        )\n\n        workflow.add_conditional_edges(\n            \"call_tool\",\n            lambda x: x[\"sender\"],\n            {\n                \"generator\": \"generator\",\n                \"reviewer\": \"reviewer\",\n            },\n        )\n        workflow.add_edge(START, \"generator\")\n\n        return workflow.compile(interrupt_before=[\"human_review_node\"])\n    \n    def before_tool(self,tool_calls):\n        tool_name = tool_calls[0]['name']\n        input_str = tool_calls[0][\"args\"]\n        artifect = {\n            \"tool_input\": str(input_str),\n            \"tool_name\": tool_name,\n            \"file_name\": \"\",\n            \"response_url\": \"\",\n            \"approval\":True\n        }\n        if not self.research.chatHistory[-1].metadata:\n            self.research.chatHistory[-1].metadata = {}\n        if 'tools' in self.research.chatHistory[-1].metadata:\n            if len(self.research.chatHistory[-1].metadata['tools'])>0 and self.research.chatHistory[-1].metadata['tools'][-1]['approval']:\n                self.research.chatHistory[-1].metadata['tools'][-1]['approval'] = False\n            else:\n                self.research.chatHistory[-1].metadata['tools'].append(artifect)\n        else:\n            self.research.chatHistory[-1].metadata['tools'] = [artifect]\n        if self.chat:\n            update_multiagent_research_chat(self.chat,self.research)\n\n    def router(self, state:NeoState) -> Literal[\"call_tool\", \"__end__\", \"continue\"]:\n        messages = state[\"messages\"]\n        last_message = messages[-1]\n        if last_message.tool_calls:\n            if last_message.tool_calls[0]['name'] in self.approval_tool_names:\n                self.before_tool(last_message.tool_calls)\n                return \"human_review_node\"\n            else:\n                return \"call_tool\"\n        # if \"FINAL ANSWER\" in last_message.content:\n        #     return \"__end__\"\n        if last_message.name=='generator' and last_message.limit==0:\n            return \"__end__\"\n        return \"continue\"\n    ```\n\nMulti agent Class:\n\n```python\nimport functools\nimport operator\nfrom typing import Sequence, TypedDict, Literal, List, Dict, Any\nfrom langchain_core.messages import BaseMessage, HumanMessage,AIMessage\nfrom langgraph.graph import END, StateGraph, START,add_messages,MessagesState\nfrom langgraph.checkpoint.postgres import PostgresSaver\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom typing import Annotated\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom common.schema import MultiAgentChatHistorySchema,MultiAgentResearchSchema\nfrom pydantic import BaseModel\nimport os\nfrom multiagent_api.src.langgraph_wrapper import NeoleadsWorkflow\nfrom psycopg_pool import ConnectionPool\nfrom langchain_core.messages import RemoveMessage\nfrom uuid import uuid4\n\nclass AgentState(MessagesState):\n    next: str\n    steps: str\nclass RouteResponse(BaseModel):\n    next: str\n    steps: str\n\nclass AgentInfo(BaseModel):\n    name: str\n    system_prompt: str\nclass DynamicMultiAgentWorkflow:\n    def __init__(self, llm, inputs, config, chathistory, callbacks=[],system_message=\"\"):\n        self.inputs=inputs\n        self.llm = llm\n        self.callbacks = callbacks\n        self.DB_URI = os.getenv('DB_URI')\n        self.connection_kwargs = {\n            \"autocommit\": True,\n            \"prepare_threshold\": 0,\n        }\n        self.system_message = system_message\n        self.config = config\n        self.chathistory = chathistory\n        self.final_response = \"No response yet\"\n        self.members = []\n        self.final_output_prompt = ChatPromptTemplate.from_messages([\n            (\"system\", \"\"\" \nYour task is to generate a response or answer based on the user query, context of the conversation, agent responses, and any user feedback provided. Follow these instructions to ensure the response meets the user's expectations:\n\nInstructions:\n 1. Analyze the User Query:\n\n   - Start by understanding the user's query clearly. Identify the key points and goals the user wants to achieve.\n\n 2. Review the Context:\n\n   - Consider the conversation history to understand the progress made so far.\n   - Incorporate all relevant details from previous agent responses that contribute to solving the user's query.\n\n 3. Integrate Agent Responses:\n\n   - Use information and data from agent responses that have been provided so far in the conversation.\n   - If multiple agent responses exist, synthesize the information to ensure a coherent and comprehensive answer.\n\n 4. Consider User Feedback:\n\n   - If the user has provided any feedback during the process, adjust the answer accordingly.\n   - The feedback should guide any changes in the response structure or content to better meet the user's expectations.\n\n 5. Generate the Response:\n\n   - Formulate a complete answer based on the information gathered from the user query, context, agent responses, and feedback.\n   - Ensure that the answer directly addresses the user's needs, providing a clear and actionable resolution\n            \nOutput Format:\n - Clear and eloborate answer that meets the user's expectations.\n\nNote:\n - Always prioritize user feedback to tailor the final response to their expectations.\n - Ensure clarity and conciseness in the final response.\n             \"\"\"),\n            MessagesPlaceholder(variable_name=\"messages\"),\n        ])\n        self.agents: List[AgentInfo] = []\n        self.workflow = StateGraph(AgentState)\n        self.supervisor_prompt = ChatPromptTemplate.from_messages([\n            (\"system\", \"\"\"\nAs a supervisor, your role is to manage a conversation between multiple AI workers, each with unique capabilities, to resolve the user's query efficiently. Based on the user's query, conversation history, and the available workers, your task is to determine the next step by selecting the most appropriate worker. If the query has been fully answered, select FINISH. Follow the guidelines below to manage the workflow effectively, while also ensuring that user feedback is taken seriously and incorporated into the decision-making process.\n\n\n\n***Supervisor Instructions:***\n\n 1. Select the Next Worker:\n\n    - Choose a worker from the provided {options} list based on their capabilities and how they can contribute to answering the user's query.\n    - Incorporate any user feedback that may influence which worker should be selected or how the task should proceed.\n    - if user feedback is to continue than you have to select the next worker.\n    - If no worker is suitable or the query has already been fully answered, select FINISH.\n\n\n 2. Complete the Task:\n\n    - If the query has been fully answered or no further worker action is needed, select FINISH to end the task.\n    - Return FINISH immediately when no more actions are required.\n    - Ensure the final output reflects any user feedback that has been incorporated during the process.\n\n***Output Format:***\n 1. Worker Name or FINISH:\n\n    - Select and provide the name of the chosen worker from the options list, or select FINISH if the task is complete.\n\n 2. User Query Answer (from Context):\n\n    - Present the response to the user's query based on the current context. This answer should be actual response made by the workers for the user's query.\n    - Ensure the answer addresses the user's query and incorporates with any feedback provided by the user.\n\n 3. Steps:\n\n    - Provide a detailed list of steps, outlining what each worker should do next based on their capabilities and the user query:\n      - Step 1: Assign the first task to a suitable worker, specifying the exact action required based on their capabilities.\n      - Step 2: Once the first worker completes their task, assign the next step to another worker, ensuring that the step matches the worker's specific capabilities.\n      - Continue assigning tasks until the query is fully resolved, assigning only tasks that each worker is capable of performing.\n      - If any user feedback is provided during the process, adjust the steps accordingly to reflect that feedback.\n    - Once the query is answered, return FINISH immediately.\n             \n**Follow Additional Instructions From User:**\n - Adhere to any specific instructions provided by the user, below are user instructions that are essential for producing the correct response.:\n   ```{system_message}``` \n - To ensure the output aligns with the user's requirements. These guidelines are essential for producing the correct response.\n             \n\n\n             \n\n             \"\"\"),\n            MessagesPlaceholder(variable_name=\"history\"),\n            MessagesPlaceholder(variable_name=\"messages\"),\n        ])\n        \n        self.workflow.add_node(\"supervisor\", self.supervisor_agent)\n        self.workflow.add_node(\"human_feedback\", self.human_feedback_node)\n        self.workflow.add_node(\"FINISH\", self.finish_node)\n\n    def add_agent(self, name: str, system_prompt:str, reviewer_prompt:str ,feedback_limit, config, user_prompt:str, tools, llm, chat, research ,approval_tool_names=[]):\n        agent_info = AgentInfo(name=name, system_prompt=system_prompt)\n        agent = NeoleadsWorkflow(self.inputs, system_prompt, reviewer_prompt,user_prompt, tools, llm, feedback_limit, self.chathistory, config, chat, research,self.callbacks, approval_tool_names)\n        # agent = create_react_agent(self.llm, tools=self.tools)\n        node = functools.partial(self.agent_node, agent=agent.create_workflow(), name=name)\n        self.workflow.add_node(name, node)\n        self.members.append(name)\n        self.agents.append(agent_info)\n\n    def compile(self):\n        options = [\"FINISH\"] + self.members\n        agent_info = \"\\n\\n\".join([f\"*** {agent.name}:***\\n```{agent.system_prompt}```\" for agent in self.agents])\n        self.supervisor_prompt = self.supervisor_prompt.partial(\n            options=str(options),\n            agent_info=agent_info,\n            history = self.chathistory,\n            system_message = self.system_message\n        )\n\n        conditional_map = {k: k for k in self.members}\n        conditional_map[\"FINISH\"] = \"FINISH\"\n        self.workflow.add_conditional_edges(\n            \"supervisor\",\n            lambda x: x[\"next\"],\n            conditional_map\n        )\n        self.workflow.add_edge(\"FINISH\",END)\n        for i in range(len(self.members)):\n            self.workflow.add_edge(self.members[i], \"human_feedback\")\n        self.workflow.add_edge(\"human_feedback\", \"supervisor\")\n        self.workflow.add_edge(START, \"supervisor\")\n        \n        \n\n    \n    def human_feedback_node(self, state: Dict[str, Any]) -> Dict[str, Any]:\n        return state\n    \n    def feedback_run(self, message: str):\n        self.compile()\n        with ConnectionPool(\n            conninfo=self.DB_URI,\n            kwargs=self.connection_kwargs,\n        ) as pool:\n            checkpointer = PostgresSaver(pool)\n            # self.config[\"configurable\"] = {\"thread_id\": \"1\"}\n            graph = self.workflow.compile(checkpointer = checkpointer, interrupt_before=[\"human_feedback\"])\n            messages = graph.get_state(self.config).values[\"messages\"]\n\n            graph.update_state(self.config, {\"messages\": [RemoveMessage(id=m.id) for m in messages]})\n            messages = graph.get_state(self.config).values[\"messages\"]\n\n            message = [HumanMessage(content=\"My Feedback:\"+message)]\n            graph.update_state(self.config, {\"messages\": message}, as_node=\"human_feedback\")\n            graph.get_state(self.config)\n            self.config['recursion_limit'] = 150\n            self.config['callbacks'] = self.callbacks\n            for s in graph.stream(None, config=self.config,subgraphs=True):\n                if \"__end__\" not in s:\n                    print(s)\n                    if \"FINISH\" in s:\n                        self.final_response = s[\"FINISH\"]['messages'][-1].content\n                    print(\"----\")\n    \n    def run(self, input_message: str):\n        self.compile()\n        with ConnectionPool(\n            conninfo=self.DB_URI,\n            kwargs=self.connection_kwargs,\n        ) as pool:\n            checkpointer = PostgresSaver(pool)\n            # memory = MemorySaver()\n            # checkpointer.setup()\n            print(self.config)\n            graph = self.workflow.compile(checkpointer = checkpointer, interrupt_before=[\"human_feedback\"])\n            print(graph.get_graph(xray=True).draw_mermaid())\n            self.config['recursion_limit'] = 150\n            self.config['callbacks'] = self.callbacks\n            # self.config[\"configurable\"] = {\"thread_id\": \"1\"}\n            for s in graph.stream({\"messages\": [HumanMessage(content=input_message)],\"user_feedback\": \"No Feedback till now, so continue your work\",}, config=self.config,stream_mode=\"values\",subgraphs=True):\n                print(s)\n                # if \"__end__\" not in s:\n                #     print(s)\n                #     if \"FINISH\" in s:\n                #         self.final_response = s[\"FINISH\"]['messages'][-1].content\n                #     print(\"----\")\n            state = graph.get_state(self.config, subgraphs=True)\n            print(\"------\")\n            print(state.tasks[0])\n            print(\"------\")\n            \n            # print(\"********\")\n            # print(graph)\n\n    @staticmethod\n    def agent_node(state: Dict[str, Any], agent: Any, name: str) -> Dict[str, List[HumanMessage]]:\n        state[\"messages\"].append(HumanMessage(content=state[\"steps\"]))\n        result = agent.invoke(state)\n        return {\"messages\": [AIMessage(content=result[\"messages\"][-1].content, name=name, id=uuid4()), \n                             HumanMessage(content=f\"\"\"From the Above assistant response if the task that was requested is complete respond with FINISH\n                                           if you are a supervisor agent, if you are any other agent always answer what you are designed to do.\"\"\",id=uuid4())]}\n\n    def supervisor_agent(self, state: Dict[str, Any]) -> Dict[str, str]:\n        supervisor_chain = (\n            self.supervisor_prompt\n            | self.llm.with_structured_output(RouteResponse)\n        )\n        return supervisor_chain.invoke(state)\n    \n    def finish_node(self, state: Dict[str, Any]):\n        chain = (\n            self.final_output_prompt\n            | self.llm\n        )\n        result = chain.invoke(state)\n        return {\"messages\":[AIMessage(**result.dict(exclude={\"type\", \"name\"}))]}\n    \n\n\n\n### Error Message and Stack Trace (if applicable)\n\n```shell\nwhen streaming subgraph latest response has the state values:\n\n(('GenerateKeywordsIdeas:63abfcb8-9b4b-ec50-2659-cbfa9def5157',), {'messages': [HumanMessage(content='find seed keywords for neoleads.com', additional_kwargs={}, response_metadata={}, id='c3c68ab2-87d7-41a0-b055-fe7355e83712'), HumanMessage(content='Step 1: Use the scraping tool to extract data from neoleads.com.\\nStep 2: Identify seed keywords from the scraped data, focusing on meta tags, content, and key phrases used on the website.\\nStep 3: Use Google Search, Bing Search, and YouTube Search to find related keywords and searches based on the extracted seed keywords.\\nStep 4: Compile a comprehensive list of seed keywords and related searches, ensuring they are relevant to neoleads.com and its services.', additional_kwargs={}, response_metadata={}, id='f7db98e8-d26d-4a72-861c-c96c88148a69'), AIMessage(content=[{'type': 'text', 'text': \"Certainly! I'll follow the steps you've outlined to find seed keywords for neoleads.com. Let's begin by scraping the website content and then use that information to find related keywords.\\n\\nStep 1: Let's use the scraping tool to extract data from neoleads.com.\", 'index': 0}, {'type': 'tool_use', 'id': 'toolu_bdrk_01Vn43sFEdBcL3K6xUzE1hU1', 'name': 'apify_website_content_crawler', 'input': {}, 'index': 1, 'partial_json': '{\"query\": \"https://neoleads.com\"}'}], additional_kwargs={}, response_metadata={'stop_reason': 'tool_use', 'stop_sequence': None}, id='run-ffa3b9bc-4810-4560-8afc-dd9b892c6c23-0', tool_calls=[{'name': 'apify_website_content_crawler', 'args': {'query': 'https://neoleads.com'}, 'id': 'toolu_bdrk_01Vn43sFEdBcL3K6xUzE1hU1', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1721, 'output_tokens': 113, 'total_tokens': 1834})], 'sender': 'generator', 'limit': 0})\n\nbut once the i print tasks for subgraph, state value is none:\nPregelTask(id='63abfcb8-9b4b-ec50-2659-cbfa9def5157', name='GenerateKeywordsIdeas', path=('__pregel_pull', 'GenerateKeywordsIdeas'), error=None, interrupts=(), state=None, result=None)\n\nDescription\nThe state value should be inserted in persistance db.\nSystem Info\nrequests\nlangchain\nlangchain-openai\nlangchain-aws\nlanggraph\nlangchain-community\nfirebase_admin\nsupabase\nrollbar\ntavily-python\ngoogle-search-results\npandas\napify-client\nmailchimp-marketing\nhubspot-api-client\nwikipedia\nlangchain-google-community\nlangchain-anthropic\npsycopg\npsycopg-binary\npsycopg-pool\nlanggraph-checkpoint-postgres", "created_at": "2024-10-19", "closed_at": "2025-01-29", "labels": ["question"], "State": "closed", "Author": "jhachirag7"}
{"issue_number": 2137, "issue_title": "DOC: Bug in Memory example", "issue_body": "Issue with current documentation:\nBug\nIf I run the example in this Memory migration page, then I get then this error:\nTraceback (most recent call last):\n  File \"/Users/usr/file.py\", line 33, in <module>\n    app = workflow.compile(\n          ^^^^^^^^^^^^^^^^^\n  File \"/Users/usr/.venv/lib/python3.11/site-packages/langgraph/graph/state.py\", line 211, in compile\n    self.validate(\n  File \"/Users/usr/.venv/lib/python3.11/site-packages/langgraph/graph/graph.py\", line 273, in validate\n    raise ValueError(f\"Node '{node}' is a dead-end\")\nValueError: Node 'model' is a dead-end\nFix\nA way to fix it can be:\nworkflow.add_edge(START, \"model\")\nworkflow.add_node(\"model\", call_model)\nworkflow.add_edge(\"model\", END)  # <-- Add  this\nIdea or request for content:\nNo response", "created_at": "2024-10-18", "closed_at": "2024-10-21", "labels": [], "State": "closed", "Author": "ltoniazzi"}
{"issue_number": 2135, "issue_title": "Unable to save playwright Page object in the Sqlite database of langraph", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nclass AgentState(TypedDict):\n    task: str\n    browser_page: Optional[Page]\n\n\ndef run(playwright: Playwright) -> None:\n        browser =  playwright.chromium.launch(headless=False)\n        context =  browser.new_context()\n        page =  context.new_page()\n        with SqliteSaver.from_conn_string(\":memory:\") as memory:\n            graph = builder.compile(checkpointer=memory)\n            thread = {\"configurable\": {\"thread_id\": \"1\"}}\n            try:\n                for s in graph.stream({\n                    'task': \"\",\n                    \"max_revisions\": 0,\n                    \"revision_number\": 1,\n                    \"browser_page\":page\n                }, thread):\n                    print(s)\n            except Exception as e:\n                print(f\"Error in astream: {e}\")\n                raise e\nError Message and Stack Trace (if applicable)\nError in astream: Object of type Page is not serializable\nTraceback (most recent call last):\nFile \"/Users/air/Github/0dfx/plan_agent/new.py\", line 201, in <module>\nrun(playwright)\nFile \"/Users/air/Github/0dfx/plan_agent/new.py\", line 196, in run\nraise e\nFile \"/Users/air/Github/0dfx/plan_agent/new.py\", line 186, in run\nfor s in graph.stream({\n^^^^^^^^^^^^^^\nFile \"/opt/anaconda3/envs/lang/lib/python3.12/site-packages/langgraph/pregel/init.py\", line 1240, in stream\nwith SyncPregelLoop(\n^^^^^^^^^^^^^^^\nFile \"/opt/anaconda3/envs/lang/lib/python3.12/site-packages/langgraph/pregel/loop.py\", line 754, in exit\nreturn self.stack.exit(exc_type, exc_value, traceback)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/opt/anaconda3/envs/lang/lib/python3.12/contextlib.py\", line 610, in exit\nraise exc_details[1]\nFile \"/opt/anaconda3/envs/lang/lib/python3.12/contextlib.py\", line 595, in exit\nif cb(*exc_details):\n^^^^^^^^^^^^^^^^\nFile \"/opt/anaconda3/envs/lang/lib/python3.12/site-packages/langgraph/pregel/executor.py\", line 105, in exit\ntask.result()\nFile \"/opt/anaconda3/envs/lang/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\nreturn self.__get_result()\n^^^^^^^^^^^^^^^^^^^\nFile \"/opt/anaconda3/envs/lang/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\nraise self._exception\nFile \"/opt/anaconda3/envs/lang/lib/python3.12/site-packages/langgraph/pregel/loop.py\", line 680, in _checkpointer_put_after_previous\nprev.result()\nFile \"/opt/anaconda3/envs/lang/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\nreturn self.__get_result()\n^^^^^^^^^^^^^^^^^^^\nFile \"/opt/anaconda3/envs/lang/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\nraise self._exception\nFile \"/opt/anaconda3/envs/lang/lib/python3.12/site-packages/langgraph/pregel/executor.py\", line 70, in done\ntask.result()\nFile \"/opt/anaconda3/envs/lang/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\nreturn self.__get_result()\n^^^^^^^^^^^^^^^^^^^\nFile \"/opt/anaconda3/envs/lang/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\nraise self._exception\nFile \"/opt/anaconda3/envs/lang/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\nresult = self.fn(*self.args, **self.kwargs)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/opt/anaconda3/envs/lang/lib/python3.12/site-packages/langgraph/pregel/loop.py\", line 682, in checkpointer_put_after_previous\ncast(BaseCheckpointSaver, self.checkpointer).put(\nFile \"/opt/anaconda3/envs/lang/lib/python3.12/site-packages/langgraph/checkpoint/sqlite/init.py\", line 399, in put\ntype, serialized_checkpoint = self.serde.dumps_typed(checkpoint)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/opt/anaconda3/envs/lang/lib/python3.12/site-packages/langgraph/checkpoint/serde/jsonplus.py\", line 192, in dumps_typed\nreturn \"msgpack\", _msgpack_enc(obj)\n^^^^^^^^^^^^^^^^^\nFile \"/opt/anaconda3/envs/lang/lib/python3.12/site-packages/langgraph/checkpoint/serde/jsonplus.py\", line 487, in _msgpack_enc\nreturn enc.pack(data)\n^^^^^^^^^^^^^^\nFile \"msgpack/_packer.pyx\", line 279, in msgpack._cmsgpack.Packer.pack\nFile \"msgpack/_packer.pyx\", line 276, in msgpack._cmsgpack.Packer.pack\nFile \"msgpack/_packer.pyx\", line 265, in msgpack._cmsgpack.Packer._pack\nFile \"msgpack/_packer.pyx\", line 213, in msgpack._cmsgpack.Packer._pack_inner\nFile \"msgpack/_packer.pyx\", line 265, in msgpack._cmsgpack.Packer._pack\nFile \"msgpack/_packer.pyx\", line 213, in msgpack._cmsgpack.Packer._pack_inner\nFile \"msgpack/_packer.pyx\", line 265, in msgpack._cmsgpack.Packer._pack\nFile \"msgpack/_packer.pyx\", line 213, in msgpack._cmsgpack.Packer._pack_inner\nFile \"msgpack/_packer.pyx\", line 267, in msgpack._cmsgpack.Packer._pack\nFile \"/opt/anaconda3/envs/lang/lib/python3.12/site-packages/langgraph/checkpoint/serde/jsonplus.py\", line 420, in _msgpack_default\nraise TypeError(f\"Object of type {obj.class.name} is not serializable\")\nTypeError: Object of type Page is not serializable\nDescription\n\nI am trying to open a browser page object and then use langraph inside it.\nSince langraph saves the agent state including Page objects in my case to the database.\nI get the Error as langraph is unable to serialize the page object for storing in the database.\n\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.5.0: Wed May  1 20:16:51 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T8103\nPython Version:  3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:07:06) [Clang 17.0.6 ]\n\nPackage Information\n\nlangchain_core: 0.3.6\nlangchain: 0.3.1\nlangsmith: 0.1.129\nlangchain_google_genai: 2.0.1\nlangchain_openai: 0.2.1\nlangchain_text_splitters: 0.3.0\nlanggraph: 0.2.28\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.10.8\nasync-timeout: Installed. No version info available.\ngoogle-generativeai: 0.8.3\nhttpx: 0.27.2\njsonpatch: 1.33\nlanggraph-checkpoint: 1.0.12\nnumpy: 1.26.4\nopenai: 1.50.2\norjson: 3.10.7\npackaging: 24.1\npillow: 10.4.0\npydantic: 2.9.2\nPyYAML: 6.0.2\nrequests: 2.32.3\nSQLAlchemy: 2.0.35\ntenacity: 8.5.0\ntiktoken: 0.7.0\ntyping-extensions: 4.12.2\n", "created_at": "2024-10-18", "closed_at": "2024-10-23", "labels": [], "State": "closed", "Author": "Zubair2019"}
{"issue_number": 2119, "issue_title": "RuntimeError: dictionary changed size during iteration", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom app.graph.graph import build_graph\n\ngraph = build_graph(...)\nevents = graph.stream({...}, stream_mode='values')\n\nevents_filtered = []\nfor index, event in enumerate(events):\n    if event.get(''):        \n        events_filtered.append(event)\nError Message and Stack Trace (if applicable)\nRuntimeError: dictionary changed size during iteration\n  File \"celery/app/trace.py\", line 736, in __protected_call__\n    return self.run(*args, **kwargs)\n  File \"app/main.py\", line 114, in process_message_task\n    return asyncio.run(process_message(my_dict))\n  File \"asyncio/runners.py\", line 190, in run\n    return runner.run(main)\n  File \"asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n  File \"asyncio/base_events.py\", line 654, in run_until_complete\n    return future.result()\n  File \"app/main.py\", line 270, in process_message\n    for index, event in enumerate(events):\n  File \"/opt/render/project/src/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 920, in stream\n    with SyncPregelLoop(\n  File \"/opt/render/project/src/.venv/lib/python3.11/site-packages/langgraph/pregel/loop.py\", line 524, in __exit__\n    return self.stack.__exit__(exc_type, exc_value, traceback)\n  File \"contextlib.py\", line 601, in __exit__\n    raise exc_details[1]\n  File \"contextlib.py\", line 586, in __exit__\n    if cb(*exc_details):\n  File \"/opt/render/project/src/.venv/lib/python3.11/site-packages/langgraph/pregel/executor.py\", line 84, in __exit__\n    if tasks := {t for t in self.tasks if not t.done()}:\n  File \"/opt/render/project/src/.venv/lib/python3.11/site-packages/langgraph/pregel/executor.py\", line 84, in <setcomp>\n    if tasks := {t for t in self.tasks if not t.done()}:\nDescription\nTLDR; I'm iterating over an Iterator[dict] (returned by graph.stream()), and I get the error mentioned above.\nIts very odd, because on my side the only thing I'm doing with the namespace events: Iterator[dict] is looping over it, and with event: dict is only calling dict.get, so only reading, not mutating.\nYet I get this error of modifying while iterating, I would imagine the error comes from langgraph. I suppose it has something to do with async? perhaps its modified by another \"thread\" concurrently?\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #40~22.04.3-Ubuntu SMP PREEMPT_DYNAMIC Tue Jul 30 17:30:19 UTC 2\nPython Version:  3.11.4 (main, Jun 18 2023, 17:04:26) [GCC 11.3.0]\n\nPackage Information\n\nlangchain_core: 0.2.27\nlangchain: 0.2.12\nlangchain_community: 0.2.11\nlangsmith: 0.1.96\nlangchain_anthropic: 0.1.22\nlangchain_groq: 0.1.9\nlangchain_openai: 0.1.20\nlangchain_text_splitters: 0.2.2\nlanggraph: 0.2.14\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlangserve\n", "created_at": "2024-10-15", "closed_at": "2024-10-15", "labels": [], "State": "closed", "Author": "shner-elmo"}
{"issue_number": 2107, "issue_title": "RemainingSteps value type should be `int` and not `bool`", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nNA\nError Message and Stack Trace (if applicable)\nType \"int\" is not assignable to return type \"bool\"\nDescription\nCurrent langgraph code reference:\nhttps://github.com/langchain-ai/langgraph/blob/main/libs/langgraph/langgraph/managed/is_last_step.py#L16\nclass RemainingStepsManager(ManagedValue[bool]):\n    def __call__(self) -> bool:\n        return self.loop.stop - self.loop.step\n\n\nRemainingSteps = Annotated[bool, RemainingStepsManager]\nInstead of bool it should have been int.\nSystem Info\npip freeze | grep \"langgraph\"\nlanggraph==0.2.37\nlanggraph-checkpoint==2.0.1\nlanggraph-checkpoint-postgres==2.0.1\nlanggraph-cli==0.1.52\nlanggraph-sdk==0.1.33", "created_at": "2024-10-15", "closed_at": "2024-10-15", "labels": [], "State": "closed", "Author": "abprime"}
{"issue_number": 2097, "issue_title": "DOC: Agent Executor Docs deleted after 0.2.19?", "issue_body": "Issue with current documentation:\nJust wondering why the agent executor examples were deleted post 0.2.19?\nie https://github.com/langchain-ai/langgraph/blob/0.2.19/examples/chat_agent_executor_with_function_calling/dynamically-returning-directly.ipynb\nThese go with the online introductions: https://www.youtube.com/watch?v=rabXcLaAlqE&list=PLfaIDFEXuae16n2TWUkKq5PgJ0w6Pkwtg&index=5\nIs there a switch in API?\nIf there is a newer class / process maybe it's worth updating the descriptions in the youtube\nIdea or request for content:\nNo response", "created_at": "2024-10-14", "closed_at": "2024-10-14", "labels": [], "State": "closed", "Author": "Data-drone"}
{"issue_number": 2084, "issue_title": "Using python -O option causes graph to return None", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langchain_core.runnables import RunnableConfig\nfrom typing_extensions import Annotated, TypedDict\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph\n\n\ndef reducer(a: list, b: int | None) -> list:\n    if b is not None:\n        return a + [b]\n    return a\n\n\nclass State(TypedDict):\n    x: Annotated[list, reducer]\n\n\nclass ConfigSchema(TypedDict):\n    r: float\n\n\ngraph = StateGraph(State, config_schema=ConfigSchema)\n\n\ndef node(state: State, config: RunnableConfig) -> dict:\n    r = config[\"configurable\"].get(\"r\", 1.0)\n    x = state[\"x\"][-1]\n    next_value = x * r * (1 - x)\n    return {\"x\": next_value}\n\n\ngraph.add_node(\"A\", node)\ngraph.set_entry_point(\"A\")\ngraph.set_finish_point(\"A\")\ncompiled = graph.compile()\n\nstep1 = compiled.invoke({\"x\": 0.5}, {\"configurable\": {\"r\": 3.0}})\nprint(step1)\nError Message and Stack Trace (if applicable)\n$ python test.py\n{'x': [0.5, 0.75]}\n$ python -O test.py\nNone\nDescription\nEvery graph run (invoke, stream) result to a None output when using the -O optimization flag. This issue started occurring after the major version upgrade from 0.1.X to 0.2.X.\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP Wed Oct  2 16:38:00 UTC 2024\nPython Version:  3.12.2 (main, Mar 12 2024, 11:02:14) [GCC 12.2.0]\n\nPackage Information\n\nlangchain_core: 0.3.10\nlangsmith: 0.1.134\nlanggraph: 0.2.35\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nhttpx: 0.27.2\njsonpatch: 1.33\nlanggraph-checkpoint: 2.0.1\norjson: 3.10.7\npackaging: 24.1\npydantic: 2.9.2\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\ntenacity: 8.5.0\ntyping-extensions: 4.12.2\n", "created_at": "2024-10-11", "closed_at": "2024-10-11", "labels": [], "State": "closed", "Author": "maxence-oden"}
{"issue_number": 2081, "issue_title": "Is Last step logic doesn't work per execution", "issue_body": "The step count keeps increasing every invoke. How can we reset or make the lastStepManager start from 1 again for every invoke. The recursion_limit is per invoke not for across the invoke calls.\n\nlanggraph/how-tos/return-when-recursion-limit-hits/\nBuild language agents as graphs\nhttps://langchain-ai.github.io/langgraph/how-tos/return-when-recursion-limit-hits/?h=recursion\nOriginally posted by @giscus in #2025", "created_at": "2024-10-11", "closed_at": "2024-10-15", "labels": [], "State": "closed", "Author": "abprime"}
{"issue_number": 2080, "issue_title": "Copy of CompiledStateGraph request mandatory to explicit \"update={}\"", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom typing import TypedDict\n\nclass AgentState(TypedDict):\n    hello_msg: str\n\ndef node__entry_point(agent_state: AgentState):\n    agent_state[\"hello_msg\"] = \"Hello, World!\"\n    return agent_state\n\ndef node__END(agent_state: AgentState):\n    return agent_state\n\nfrom langgraph.graph import StateGraph, END\n\n\nworkflow = StateGraph(AgentState)\n\nworkflow.add_node(\"entry_point\", node__entry_point)\nworkflow.add_node(\"END\", node__END)\n\nworkflow.add_edge(\"entry_point\", \"END\")\nworkflow.add_edge(\"END\", END)\n\nworkflow.set_entry_point(\"entry_point\")\n\napp = workflow.compile()\n\n# This does not work\ncopy_app = app.copy()\n# And this works\ncopy_app = app.copy(update={})\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\n  File \"c:\\Users\\sdarco\\OneDrive - CAMOZZI GROUP SPA\\Desktop\\test.py\", line 29, in <module>\n    copy_app = app.copy()\n               ^^^^^^^^^^\nTypeError: Pregel.copy() missing 1 required positional argument: 'update'\nDescription\nYesterday 10/10, while updating langgraph to latest version I had this exception on my code. Older versions works with app.copy(), actually works only with app.copy(update={}).\nAlso no documentation is available on this parameter (see on pregel/init.py, line 255)\nSystem Info\nlanggraph==0.2.35\nplatform Windows 10 Enterprise LTSC\npython 3.11.10", "created_at": "2024-10-11", "closed_at": "2024-10-14", "labels": [], "State": "closed", "Author": "SimoneDArco"}
{"issue_number": 2075, "issue_title": "DOC: Chatbots > Customer Support throwing the error: \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'\"", "issue_body": "Issue with current documentation:\nhttps://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/#utility\nRunning the notebook throws the following error:\n---------------------------------------------------------------------------\nBadRequestError                           Traceback (most recent call last)\n...\n    [762](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:762)                 \"stop\": stop,\n    [763](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:763)                 \"store\": store,\n    [764](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:764)                 \"stream\": stream,\n    [765](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:765)                 \"stream_options\": stream_options,\n    [766](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:766)                 \"temperature\": temperature,\n    [767](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:767)                 \"tool_choice\": tool_choice,\n    [768](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:768)                 \"tools\": tools,\n    [769](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:769)                 \"top_logprobs\": top_logprobs,\n    [770](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:770)                 \"top_p\": top_p,\n    [771](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:771)                 \"user\": user,\n    [772](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:772)             },\n    [773](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:773)             completion_create_params.CompletionCreateParams,\n    [774](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:774)         ),\n    [775](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:775)         options=make_request_options(\n    [776](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:776)             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n    [777](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:777)         ),\n    [778](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:778)         cast_to=ChatCompletion,\n    [779](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:779)         stream=stream or False,\n    [780](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:780)         stream_cls=Stream[ChatCompletionChunk],\n    [781](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:781)     )\n\nFile ~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1277, in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)\n   [1263](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1263) def post(\n   [1264](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1264)     self,\n   [1265](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1265)     path: str,\n   (...)\n   [1272](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1272)     stream_cls: type[_StreamT] | None = None,\n   [1273](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1273) ) -> ResponseT | _StreamT:\n   [1274](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1274)     opts = FinalRequestOptions.construct(\n   [1275](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1275)         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   [1276](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1276)     )\n-> [1277](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1277)     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n\nFile ~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:954, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    [951](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:951) else:\n    [952](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:952)     retries_taken = 0\n--> [954](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:954) return self._request(\n    [955](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:955)     cast_to=cast_to,\n    [956](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:956)     options=options,\n    [957](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:957)     stream=stream,\n    [958](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:958)     stream_cls=stream_cls,\n    [959](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:959)     retries_taken=retries_taken,\n    [960](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:960) )\n\nFile ~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1058, in SyncAPIClient._request(self, cast_to, options, retries_taken, stream, stream_cls)\n   [1055](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1055)         err.response.read()\n   [1057](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1057)     log.debug(\"Re-raising status error\")\n-> [1058](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1058)     raise self._make_status_error_from_response(err.response) from None\n   [1060](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1060) return self._process_response(\n   [1061](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1061)     cast_to=cast_to,\n   [1062](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1062)     options=options,\n   (...)\n   [1066](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1066)     retries_taken=retries_taken,\n   [1067](https://file+.vscode-resource.vscode-cdn.net/Users/justinwinter/Projects/experiments/~/Library/Caches/pypoetry/virtualenvs/-AxVTkH6j-py3.12/lib/python3.12/site-packages/openai/_base_client.py:1067) )\n\nBadRequestError: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_bsyGIHSEUgnD3EgLzihthfQT\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}\n\n\nOutput before error\n================================ Human Message =================================\n\nHi there, what time is my flight?\n================================== Ai Message ==================================\n\nYour flight LX0112 is scheduled to depart from Charles de Gaulle Airport (CDG) at 12:01 PM local time on October 10, 2024. It is set to arrive at EuroAirport Basel Mulhouse Freiburg (BSL) at 1:31 PM local time on the same day.\n================================ Human Message =================================\n\nAm i allowed to update my flight to something sooner? I want to leave later today.\n================================== Ai Message ==================================\nTool Calls:\n  lookup_policy (call_RYFiRp5nNOpAXbzVNmGgrR6Z)\n Call ID: call_RYFiRp5nNOpAXbzVNmGgrR6Z\n  Args:\n    query: flight change policy\n================================= Tool Message =================================\nName: lookup_policy\n\n\n## Booking and Cancellation\n\n1. How can I change my booking?\n\t* The ticket number must start with 724 (SWISS ticket no./plate).\n\t* The ticket was not paid for by barter or voucher (there are exceptions to voucher payments; if the ticket was paid for in full by voucher, then it may be possible to rebook online under certain circumstances. If it is not possible to rebook online because of the payment method, then you will be informed accordingly during the rebooking process).\n\t* There must be an active flight booking for your ticket. It is not possible to rebook open tickets or tickets without the corresponding flight segments online at the moment.\n\t* It is currently only possible to rebook outbound (one-way) tickets or return tickets with single flight routes (point-to-point).\n2. Which tickets/bookings cannot be rebooked online currently?\n\t* Bookings containing flight segments with other airlines\n\t* Bookings containing reservations, where a ticket has not yet been issued\n\t* Bookings with several valid tickets for the same person and route\n\t* Tickets with a status other than O (open) (A)\n\t* Bookings with segments with a status other than OK (e.g. containing flight segments with the status Waitlist) (HK|RR)\n\t* Tickets that do not display the tariff calculation (IT tickets)\n\t* Bookings that contain special services (e.g. transportation of animals/transportation of medica ... (truncated)\n================================== Ai Message ==================================\n\nYou are allowed to change your flight to a later time today, as long as you meet the following conditions:\n\n1. Your ticket number starts with 724 and was not paid for by barter or voucher.\n2. You have an active flight booking, and the flight is not part of a group booking.\n3. Your flight booking does not include special services like transportation of animals or medical equipment.\n4. You are changing a single flight route, not a multi-leg journey.\n\nSince your flight is scheduled to depart soon, I will assist you in making the change. Let me transfer your request to the appropriate assistant to handle this update for you.\nTool Calls:\n  ToFlightBookingAssistant (call_K5zmNjUW0oczo8Cta31vohfX)\n Call ID: call_K5zmNjUW0oczo8Cta31vohfX\n  Args:\n    request: The user wants to reschedule their flight LX0112 from CDG to BSL for a later time today.\nCurrently in:  update_flight\n================================= Tool Message =================================\n\nThe assistant is now the Flight Updates & Booking Assistant. Reflect on the above conversation between the host assistant and the user. The user's intent is unsatisfied. Use the provided tools to assist the user. Remember, you are Flight Updates & Booking Assistant, and the booking, update, other other action is not complete until after you have successfully invoked the appropriate tool. If the user changes their mind or needs help for other tasks, call the CompleteOrEscalate function to let the primary host assistant take control. Do not mention who you are - just act as the proxy for the assistant.\nCurrently in:  update_flight\n================================== Ai Message ==================================\nTool Calls:\n  search_flights (call_c3neUfUSOb2VwXMqmrokSeu9)\n Call ID: call_c3neUfUSOb2VwXMqmrokSeu9\n  Args:\n    departure_airport: CDG\n    arrival_airport: BSL\n    start_time: 2024-10-10T12:30:00\n    end_time: 2024-10-10T23:59:00\nCurrently in:  update_flight\n================================= Tool Message =================================\nName: search_flights\n\n[]\nCurrently in:  update_flight\n================================== Ai Message ==================================\nTool Calls:\n  search_flights (call_y5QDJ828M0IeEDaNd1WjjTM9)\n Call ID: call_y5QDJ828M0IeEDaNd1WjjTM9\n  Args:\n    departure_airport: CDG\n    arrival_airport: BSL\n    start_time: 2024-10-10T14:00:00\n    end_time: 2024-10-11T01:00:00\nCurrently in:  update_flight\n================================= Tool Message =================================\nName: search_flights\n\n[]\nCurrently in:  update_flight\n================================== Ai Message ==================================\nTool Calls:\n  CompleteOrEscalate (call_pVsOZFjY9AMcAqOOiQkI19Lb)\n Call ID: call_pVsOZFjY9AMcAqOOiQkI19Lb\n  Args:\n    reason: Unable to find available flights for the user to reschedule to a later time today.\n================================= Tool Message =================================\n\nResuming dialog with the host assistant. Please reflect on the past conversation and assist the user as needed.\n================================== Ai Message ==================================\n\nI couldn't find any available flights from Charles de Gaulle Airport (CDG) to EuroAirport Basel Mulhouse Freiburg (BSL) later today. If you have any flexibility with your travel plans, I recommend checking back later or considering alternate travel arrangements. If there's anything else I can assist you with, please let me know!\n================================ Human Message =================================\n\nUpdate my flight to sometime next week then\n================================== Ai Message ==================================\nTool Calls:\n  ToFlightBookingAssistant (call_7vEHTv2npOPxWtByz6D12WeP)\n Call ID: call_7vEHTv2npOPxWtByz6D12WeP\n  Args:\n    request: The user wants to reschedule their flight LX0112 from CDG to BSL to a date next week.\nCurrently in:  update_flight\n================================= Tool Message =================================\n\nThe assistant is now the Flight Updates & Booking Assistant. Reflect on the above conversation between the host assistant and the user. The user's intent is unsatisfied. Use the provided tools to assist the user. Remember, you are Flight Updates & Booking Assistant, and the booking, update, other other action is not complete until after you have successfully invoked the appropriate tool. If the user changes their mind or needs help for other tasks, call the CompleteOrEscalate function to let the primary host assistant take control. Do not mention who you are - just act as the proxy for the assistant.\nCurrently in:  update_flight\n================================== Ai Message ==================================\nTool Calls:\n  search_flights (call_5WCBkWDvUSIiPrJkPEWKDM3M)\n Call ID: call_5WCBkWDvUSIiPrJkPEWKDM3M\n  Args:\n    departure_airport: CDG\n    arrival_airport: BSL\n    start_time: 2024-10-17T00:00:00\n    end_time: 2024-10-17T23:59:00\nCurrently in:  update_flight\n================================= Tool Message =================================\nName: search_flights\n\n[{\"flight_id\": 19251, \"flight_no\": \"LX0112\", \"scheduled_departure\": \"2024-10-17 12:01:18.665554-04:00\", \"scheduled_arrival\": \"2024-10-17 13:31:18.665554-04:00\", \"departure_airport\": \"CDG\", \"arrival_airport\": \"BSL\", \"status\": \"Scheduled\", \"aircraft_code\": \"SU9\", \"actual_departure\": null, \"actual_arrival\": null}]\nCurrently in:  update_flight\n================================== Ai Message ==================================\nTool Calls:\n  update_ticket_to_new_flight (call_pVsOZFjY9AMcAqOOiQkI19Lb)\n Call ID: call_pVsOZFjY9AMcAqOOiQkI19Lb\n  Args:\n    ticket_no: 7240005432906569\n    new_flight_id: 19251\nCurrently in:  update_flight\n================================ Human Message =================================\n\nThe next available option is great\nCurrently in:  update_flight\nCurrently in:  update_flight\n================================== Ai Message ==================================\n\nLet's proceed with updating your flight to October 17, 2024. I'll handle the update for you.\nTool Calls:\n  update_ticket_to_new_flight (call_KxH0vjgwfyW6ShhxtSPjdUdX)\n Call ID: call_KxH0vjgwfyW6ShhxtSPjdUdX\n  Args:\n    ticket_no: 7240005432906569\n    new_flight_id: 19251\nCurrently in:  update_flight\n================================ Human Message =================================\n\nwhat about lodging and transportation?\nCurrently in:  update_flight\nCurrently in:  update_flight\n================================== Ai Message ==================================\nTool Calls:\n  CompleteOrEscalate (call_puWvVoar1bOkql3LXM9f3pkB)\n Call ID: call_puWvVoar1bOkql3LXM9f3pkB\n  Args:\n    reason: The user needs assistance with lodging and transportation, which is outside my capabilities.\n================================= Tool Message =================================\n\nResuming dialog with the host assistant. Please reflect on the past conversation and assist the user as needed.\n================================== Ai Message ==================================\n\nI can help you with lodging and transportation arrangements. Could you please provide the location, dates, and any specific preferences you have for booking a hotel and renting a car?\n================================ Human Message =================================\n\nYeah i think i'd like an affordable hotel for my week-long stay (7 days). And I'll want to rent a car.\n================================== Ai Message ==================================\nTool Calls:\n  ToHotelBookingAssistant (call_6sqBva3nmPmTdm3qP98iQ3yU)\n Call ID: call_6sqBva3nmPmTdm3qP98iQ3yU\n  Args:\n    location: Basel\n    checkin_date: 2024-10-17\n    checkout_date: 2024-10-24\n    request: Looking for an affordable hotel for a 7-day stay.\n  ToBookCarRental (call_bsyGIHSEUgnD3EgLzihthfQT)\n Call ID: call_bsyGIHSEUgnD3EgLzihthfQT\n  Args:\n    location: Basel\n    start_date: 2024-10-17\n    end_date: 2024-10-24\n    request: Looking to rent a car for a week-long stay.\nCurrently in:  book_hotel\n================================= Tool Message =================================\n\nThe assistant is now the Hotel Booking Assistant. Reflect on the above conversation between the host assistant and the user. The user's intent is unsatisfied. Use the provided tools to assist the user. Remember, you are Hotel Booking Assistant, and the booking, update, other other action is not complete until after you have successfully invoked the appropriate tool. If the user changes their mind or needs help for other tasks, call the CompleteOrEscalate function to let the primary host assistant take control. Do not mention who you are - just act as the proxy for the assistant.\n\nMy python env:\nlangchain                 0.2.16          Building applications with LLMs through composability\nlangchain-anthropic       0.1.23          An integration package connecting AnthropicMessages and LangChain\nlangchain-community       0.2.17          Community contributed LangChain integrations.\nlangchain-core            0.2.41          Building applications with LLMs through composability\nlangchain-mistralai       0.1.13          An integration package connecting Mistral and LangChain\nlangchain-openai          0.1.25          An integration package connecting OpenAI and LangChain\nlangchain-text-splitters  0.2.4           LangChain text splitting utilities\nlanggraph                 0.0.69          langgraph\nlangserve                 0.2.3           \nlangsmith                 0.1.132         Client library to connect to the LangSmith LLM Tracing and Evaluation Platform.\npydantic                  1.10.18         Data validation and settings management using python type hints\n\n\nIdea or request for content:\nNo response", "created_at": "2024-10-10", "closed_at": "2024-10-15", "labels": [], "State": "closed", "Author": "justinlevi"}
{"issue_number": 2073, "issue_title": "python REPL tool not working the for code execution", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nThis is the requirement.\nHow to run/execute Python codes using the pythonRepl tool?\nThe tool should be one node in the graph.\nThe just above node creates the Python code and pass to the node that contains the pythonRepl tool\ncode I tried\nfrom langchain_experimental.utilities import PythonREPL\nfrom langchain_core.tools import tool\nrepl = PythonREPL()\n\n@tool\ndef python_repl(code):\n    \"\"\"\n    Use this to execute python code without any kind of error.\n\n    \"\"\"\n    try:\n        print(\"Result of code execution here\")\n        result = repl.run(code)\n        print(\"Result of code execution\",result)\n    except BaseException as e:\n        return f\"Failed to execute. Error :  {repr(e)}\"\n\n    return f\"{result}\"\n\n\ndef codeExecution(cls, state: State) -> State:\n        codeToExecute = state['codeToExecute']\n        print(\"codeToExecute\")\n        print(codeToExecute)\n        llm_with_tools = llm.bind_tools([python_repl])\n        messages = [\n            SystemMessage(\n                content=\"\"\" You have got the task to execute code. Use the python_repl tool to execute it. I will a message and your task is to detect if it was successfully run or produced an error.\n                    If the code produced an error just return \"True\". If it was successfully executed, return \"False\" \"\"\"\n            ),\n            HumanMessage(content=codeToExecute),\n        ]\n        ai_msg = llm_with_tools.invoke(messages)\n        messages.append(ai_msg)\n        for tool_call in ai_msg.tool_calls:\n            selected_tool = {\"python_repl\": python_repl}[tool_call[\"name\"].lower()]\n            tool_output = selected_tool.invoke(tool_call[\"args\"])\n            state[\"error_message\"] = tool_output\n            messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))\n\n        result = llm_with_tools.invoke(messages)\n        state[\"error\"] = result.content\n        return state\n\n\nError Message and Stack Trace (if applicable)\nFile \"<path>.local/lib/python3.9/site-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 88, in exec_func_with_error_handling\n    result = func()\n  File \"<path>.local/lib/python3.9/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 579, in code_to_exec\n    exec(code, module.__dict__)\n  File \"<path>file\", line 85, in <module>\n    executeGraph(taskSpecificGraph,query,st.session_state.datasetColumnNames,st.session_state.datasetPath)\n  File \"<path>file.py\", line 368, in executeGraph\n    graphExecutionResult = taskSpecificGraph.invoke({\"query\": query,\"datasetColumnNames\":datasetColumnNames,\"datasetPath\":datasetPath,\"iterations\": 1})\n  File \"<path>.local/lib/python3.9/site-packages/langgraph/pregel/__init__.py\", line 1263, in invoke\n    for chunk in self.stream(\n  File \"<path>.local/lib/python3.9/site-packages/langgraph/pregel/__init__.py\", line 948, in stream\n    _panic_or_proceed(done, inflight, loop.step)\n  File \"<path>.local/lib/python3.9/site-packages/langgraph/pregel/__init__.py\", line 1349, in _panic_or_proceed\n    raise exc\n  File \"<path>.local/lib/python3.9/site-packages/langgraph/pregel/executor.py\", line 60, in done\n    task.result()\n  File \"/usr/lib/python3.9/concurrent/futures/_base.py\", line 439, in result\n    return self.__get_result()\n  File \"/usr/lib/python3.9/concurrent/futures/_base.py\", line 391, in __get_result\n    raise self._exception\n  File \"/usr/lib/python3.9/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"<path>.local/lib/python3.9/site-packages/langgraph/pregel/retry.py\", line 25, in run_with_retry\n    task.proc.invoke(task.input, task.config)\n  File \"<path>.local/lib/python3.9/site-packages/langchain_core/runnables/base.py\", line 2878, in invoke\n    input = context.run(step.invoke, input, config)\n  File \"<path>.local/lib/python3.9/site-packages/langgraph/utils.py\", line 102, in invoke\n    ret = context.run(self.func, input, **kwargs)\n  File \"<path>.local/lib/python3.9/site-packages/langgraph/graph/graph.py\", line 82, in _route\n    return self._finish(writer, input, result)\n  File \"<path>.local/lib/python3.9/site-packages/langgraph/graph/graph.py\", line 109, in _finish\n    destinations = [r if isinstance(r, Send) else self.ends[r] for r in result]\n  File \"<path>.local/lib/python3.9/site-packages/langgraph/graph/graph.py\", line 109, in <listcomp>\n    destinations = [r if isinstance(r, Send) else self.ends[r] for r in result]\nKeyError: None\n\n\n### Description\n\nHow to run/execute Python codes using the pythonRepl tool?\nThe tool should be one node in the graph.\nThe just above node creates the Python code and pass to the node that contains the pythonRepl tool\nIf any error occurs, we should get the error message also from the tool\n\n### System Info\n\nubuntu OS\nlangchain==0.2.16\nlangchain-community==0.2.16\nlangchain-core==0.2.39\nlangchain-experimental==0.0.64\nlanggraph==0.1.14\nlanggraph-checkpoint==1.0.11\n", "created_at": "2024-10-10", "closed_at": "2024-10-11", "labels": [], "State": "closed", "Author": "pradeepdev-1995"}
{"issue_number": 2063, "issue_title": "Langgraph visualization not working in streamlit", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nThe below code working fine in google colab and I am getting the graph visualization\nfrom IPython.display import display, Image\nfrom langchain_core.runnables.graph import MermaidDrawMethod\nworkflow = StateGraph(State)\nworkflow.add_node(\"categorize\", categorize)\nworkflow.add_node(\"analyze_sentiment\", analyze_sentiment)\n....other nodes....\nworkflow.add_edge(\"categorize\", \"analyze_sentiment\")\n.....other edeges........\n# Set entry point\nworkflow.set_entry_point(\"categorize\")\n# Compile the graph\napp = workflow.compile()\ndisplay(\n    Image(\n        app.get_graph().draw_mermaid_png(\n            draw_method=MermaidDrawMethod.API,\n        )\n    )\n)\n\n\nBut the same visualization not working in streamlit UI\nfrom IPython.display import display, Image\nfrom langchain_core.runnables.graph import MermaidDrawMethod\ndisplay(\n    Image(\n        app.get_graph().draw_mermaid_png(\n            draw_method=MermaidDrawMethod.API,\n        )\n    )\n)\n\nSo how to visualize a langgraph in streamlit UI?\n\n\n### Error Message and Stack Trace (if applicable)\n\n_No response_\n\n### Description\n\nThe below code working fine in google colab and I am getting the graph visualization\n\nfrom IPython.display import display, Image\nfrom langchain_core.runnables.graph import MermaidDrawMethod\nworkflow = StateGraph(State)\nworkflow.add_node(\"categorize\", categorize)\nworkflow.add_node(\"analyze_sentiment\", analyze_sentiment)\nworkflow.add_edge(\"categorize\", \"analyze_sentiment\")\nSet entry point\nworkflow.set_entry_point(\"categorize\")\nCompile the graph\napp = workflow.compile()\ndisplay(\nImage(\napp.get_graph().draw_mermaid_png(\ndraw_method=MermaidDrawMethod.API,\n)\n)\n)\nBut the same visualization not working in streamlit UI\n\nfrom IPython.display import display, Image\nfrom langchain_core.runnables.graph import MermaidDrawMethod\ndisplay(\nImage(\napp.get_graph().draw_mermaid_png(\ndraw_method=MermaidDrawMethod.API,\n)\n)\n)\nSo how to visualize a langgraph in streamlit UI?\n\n### System Info\n\nubuntu OS\nlangchain==0.2.16\nlangchain-community==0.2.16\nlangchain-core==0.2.39\nlangchain-experimental==0.0.64\nlanggraph==0.1.14\nlanggraph-checkpoint==1.0.11\n", "created_at": "2024-10-09", "closed_at": "2024-10-09", "labels": [], "State": "closed", "Author": "pradeepdev-1995"}
{"issue_number": 2062, "issue_title": "Undefined table 'checkpoints' when using Postgres", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langgraph.checkpoint.postgres import PostgresSaver\nfrom psycopg import Connection\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5432/llama_db?sslmode=disable\"\nconnection_kwargs = {\n    \"autocommit\": True,\n    \"prepare_threshold\": 0,\n}\n\nwith Connection.connect(DB_URI, **connection_kwargs) as conn:\n    checkpointer = PostgresSaver(conn)\n    # NOTE: you need to call .setup() the first time you're using your checkpointer\n    checkpointer.setup()\n    # Finally, we compile it!\n    app = workflow.compile(checkpointer=checkpointer)\n\n    config = {\"configurable\": {\"thread_id\": \"1\"}}\n\n    while True:\n        user_input=input(\"User: \")\n        if user_input.lower() in [\"quit\",\"q\"]:\n            print(\"Good Bye\")\n            break\n\n        for event in app.stream({'messages': user_input}, config, stream_mode='updates'):\n            for value in event.values():\n                pass\n\n            print_update(event)\n            \n        print(\"Assistant:\",value[\"messages\"][0].content)\nError Message and Stack Trace (if applicable)\npsycopg.errors.UndefinedTable: relation \"checkpoints\" does not exist\nLINE 34: from checkpoints WHERE thread_id = $1 AND checkpoint_ns = $2...\nDescription\nI was trying this example to create a custom chatbot with memory that is managed according to my logic instead of 6 messages. When i try to use Postgres  instead of MemorySaver() I get the above error message that 'checkpoints' table doesn't exist.\nSystem Info\nUbuntu 20.04\nPython 3.11.10\nlanggraph==0.2.34\nlanggraph-checkpoint==2.0.1\nlanggraph-checkpoint-postgres==2.0.1", "created_at": "2024-10-09", "closed_at": "2024-10-21", "labels": [], "State": "closed", "Author": "vignesh-spericorn"}
{"issue_number": 2061, "issue_title": "Calling `langgraph up` i get the error `langgraph-api-1 | FileNotFoundError: [Errno 2] No such file or directory: '/deps/langgraph-example-pyproject/my_agent\\\\agent.py`", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nlanggraph up\nError Message and Stack Trace (if applicable)\nlanggraph-api-1       | 2024-10-09T06:31:32.686169Z [error    ] Traceback (most recent call last):\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 693, in lifespan\nlanggraph-api-1       |     async with self.lifespan_context(app) as maybe_state:\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/contextlib.py\", line 210, in __aenter__\nlanggraph-api-1       |     return await anext(self.gen)\nlanggraph-api-1       |            ^^^^^^^^^^^^^^^^^^^^^\nlanggraph-api-1       |   File \"/api/langgraph_api/lifespan.py\", line 23, in lifespan\nlanggraph-api-1       |   File \"/api/langgraph_api/shared/graph.py\", line 205, in collect_graphs_from_env\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 590, in run_in_executor\nlanggraph-api-1       |     return await asyncio.get_running_loop().run_in_executor(\nlanggraph-api-1       |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\nlanggraph-api-1       |     result = self.fn(*self.args, **self.kwargs)\nlanggraph-api-1       |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 581, in wrapper\nlanggraph-api-1       |     return func(*args, **kwargs)\nlanggraph-api-1       |            ^^^^^^^^^^^^^^^^^^^^^\nlanggraph-api-1       |   File \"/api/langgraph_api/shared/graph.py\", line 233, in _graph_from_spec\nlanggraph-api-1       |   File \"<frozen importlib._bootstrap_external>\", line 936, in exec_module\nlanggraph-api-1       |   File \"<frozen importlib._bootstrap_external>\", line 1073, in get_code\nlanggraph-api-1       |   File \"<frozen importlib._bootstrap_external>\", line 1130, in get_data\nlanggraph-api-1       | FileNotFoundError: [Errno 2] No such file or directory: '/deps/langgraph-example-pyproject/my_agent\\\\agent.py'\nlanggraph-api-1       |  [uvicorn.error] api_revision=e09c235 api_variant=licensed\nlanggraph-api-1       | 2024-10-09T06:31:32.686561Z [error    ] Application startup failed. Exiting. [uvicorn.error] api_revision=e09c235 api_variant=licensed\nlanggraph-api-1 exited with code 3\nDescription\nI want to test the langgraph locally fist before deploy and when i run langgraph up i get the following error langgraph-api-1 | FileNotFoundError: [Errno 2] No such file or directory: '/deps/langgraph-example-pyproject/my_agent\\\\agent.py. While i encountered this issue i cloned the example project from this and attepmpted to serve it up but i got the same issue. Might it be because i am using windows? But it shouldnt matter since it runs in a docker container.\nSystem Info\nlanggraph.json\n`\n{\n\"dependencies\": [\".\"],\n\"graphs\": {\n\"agent\": \"./my_agent/agent.py:graph\"\n},\n\"env\": \".env\"\n}\n`\npyproject.toml\n`\n[tool.poetry]\nname = \"my_agent\"\nversion = \"0.1.0\"\ndescription = \"Example LangGraph project for deployment to LangGraph Cloud\"\nauthors = [\n\"langchain-ai\"\n]\npackages = [\n{ include = \"my_agent\" },\n]\n[tool.poetry.dependencies]\npython = \">=3.9.0,<3.13\"\nlanggraph = \"^0.2.0\"\nlangchain_anthropic = \"^0.1.0\"\nlangchain_core = \"^0.2.0\"\nlangchain_openai = \"^0.1.0\"\ntavily-python = \"^0.3.0\"\nlangchain_community = \"^0.2.0\"\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n`\nOS Name\tMicrosoft Windows 10 Enterprise\nVersion\t10.0.19045 Build 19045\n\u276f langchain --version\nlangchain-cli 0.0.31", "created_at": "2024-10-09", "closed_at": "2025-03-05", "labels": [], "State": "closed", "Author": "Andrei-Tocut"}
{"issue_number": 2055, "issue_title": "Larger outputs cause streaming errors: peer closed connection without sending complete message body ", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nRun notebook testing.ipynb from the repo below. \n\nTo reproduce this on the demo langgraph, I 10x'd search volume in a single search iteration, and then this cell kicks off 10 threads, half of which end up breaking with the issue I'm describing, pretty much every time. \n\n\nfrom langgraph_sdk import get_client\nimport asyncio\n\n# Initialize the client\nclient = get_client(url=\"http://localhost:8123\")  # Update this URL as needed\n\nasync def run_search_agent_for_company(company):\n    try:\n        # Get default assistant\n        assistants = await client.assistants.search()\n        assistant = [a for a in assistants if not a[\"config\"]][0]\n        assistant_id = assistant[\"assistant_id\"]\n        print(f\"Using assistant with ID: {assistant_id} for {company}\")\n\n        # Create a thread\n        thread = await client.threads.create()\n        thread_id = thread[\"thread_id\"]\n        print(f\"Created thread with ID: {thread_id} for {company}\")\n\n        input_data = {\n            \"topic\": topic.format(company=company),\n            \"extraction_schema\": schema,\n            \"configurable\": {\n                \"model_name\": \"anthropic/claude-3-5-sonnet-20240620\",\n                \"max_loops\": 50,\n                \"max_info_tool_calls\": 10,\n                \"max_search_results\": 200\n            },\n            \"messages\": [\n                {\n                    \"role\": \"user\", \n                    \"content\": f\"Make a plan to complete the request for {company}\"\n                }\n            ],\n            'current_plan': f\"Initial plan: Analyze the request for {company}\",\n            'iteration_number': 0,\n        }\n\n        # Execute a run on the thread\n        async for chunk in client.runs.stream(\n            thread_id,\n            assistant_id,\n            input=input_data,\n            stream_mode=\"updates\",\n            config={\n                \"recursion_limit\": 50\n            }\n        ):\n            if chunk.data and chunk.event != \"metadata\":\n                print(f\"{company}: {chunk.data}\")\n                if 'error' in chunk.data:\n                    print(f\"Error encountered for {company}: {chunk.data['error']}\")\n                    print(f\"Error message: {chunk.data['message']}\")\n                    break\n\n        # Get final state\n        final_state = await client.threads.get_state(thread_id)\n        print(f\"Final results for {company}:\", final_state)\n        return final_state\n\n    except Exception as e:\n        print(f\"An error occurred for {company}: {str(e)}\")\n\nasync def run_all_companies():\n    companies = [\n        \"Mars\", \"Hershey's\", \"Nestl\u00e9\", \"Ferrero\", \"Mondelez International\",\n        \"Lindt & Spr\u00fcngli\", \"Perfetti Van Melle\", \"Haribo\", \"Meiji\", \"Tootsie Roll Industries\",\n        \"Ghirardelli Chocolate Company\", \"Godiva Chocolatier\", \"Cadbury\", \"Jelly Belly Candy Company\",\n        \"Russell Stover Chocolates\", \"Storck\", \"Pladis\", \"Arcor\", \"Lotte Confectionery\", \"Ezaki Glico\",\n        \"Fazer\", \"Cloetta\", \"Ritter Sport\", \"Tony's Chocolonely\", \"Guylian\", \"Cemoi\", \"Leaf Brands\",\n        \"Ferrara Candy Company\", \"Just Born\", \"Bahlsen\"\n    ]\n    \n    tasks = []\n    results = {}\n    for company in companies:\n        task = asyncio.create_task(run_search_agent_for_company(company))\n        tasks.append((company, task))\n    \n    for company, task in tasks:\n        result = await task\n        results[company] = result\n\n    return results\n\n# Run the async function for all companies and get the results\nsearch_results = await run_all_companies()\n\n\n### Error Message and Stack Trace (if applicable)\n\n```shell\nClient side error: \n\n\npeer closed connection without sending complete message body (incomplete chunked read)\n\nLangsmith error:\nCancelledError()Traceback (most recent call last):\n\n\n  File \"/usr/local/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 1502, in astream\n    async for _ in runner.atick(\n\n\n  File \"/usr/local/lib/python3.11/site-packages/langgraph/pregel/runner.py\", line 130, in atick\n    await arun_with_retry(t, retry_policy, stream=self.use_astream)\n\n\n  File \"/usr/local/lib/python3.11/site-packages/langgraph/pregel/retry.py\", line 102, in arun_with_retry\n    await task.proc.ainvoke(task.input, config)\n\n\n  File \"/usr/local/lib/python3.11/site-packages/langgraph/utils/runnable.py\", line 452, in ainvoke\n    input = await asyncio.create_task(coro, context=context)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n  File \"/usr/local/lib/python3.11/site-packages/langgraph/prebuilt/tool_node.py\", line 148, in ainvoke\n    return await super().ainvoke(input, config, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n  File \"/usr/local/lib/python3.11/site-packages/langgraph/utils/runnable.py\", line 235, in ainvoke\n    ret = await asyncio.create_task(coro, context=context)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n  File \"/usr/local/lib/python3.11/site-packages/langgraph/prebuilt/tool_node.py\", line 162, in _afunc\n    outputs = await asyncio.gather(\n              ^^^^^^^^^^^^^^^^^^^^^\n\n\n  File \"/usr/local/lib/python3.11/site-packages/langgraph/prebuilt/tool_node.py\", line 192, in _arun_one\n    tool_message: ToolMessage = await self.tools_by_name[call[\"name\"]].ainvoke(\n                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n  File \"/usr/local/lib/python3.11/site-packages/langchain_core/tools/structured.py\", line 58, in ainvoke\n    return await super().ainvoke(input, config, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n  File \"/usr/local/lib/python3.11/site-packages/langchain_core/tools/base.py\", line 490, in ainvoke\n    return await self.arun(tool_input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n  File \"/usr/local/lib/python3.11/site-packages/langchain_core/tools/base.py\", line 767, in arun\n    response = await asyncio.create_task(coro, context=context)  # type: ignore\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n  File \"/usr/local/lib/python3.11/site-packages/langchain_core/tools/structured.py\", line 96, in _arun\n    return await self.coroutine(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n  File \"/deps/data-enrichment/src/enrichment_agent/tools.py\", line 55, in extensive_search\n    result = await search(query, config=config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n  File \"/deps/data-enrichment/src/enrichment_agent/tools.py\", line 33, in search\n    result = await wrapped.ainvoke({\"query\": query})\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n  File \"/usr/local/lib/python3.11/site-packages/langchain_core/tools/base.py\", line 490, in ainvoke\n    return await self.arun(tool_input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n  File \"/usr/local/lib/python3.11/site-packages/langchain_core/tools/base.py\", line 767, in arun\n    response = await asyncio.create_task(coro, context=context)  # type: ignore\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n  File \"/usr/local/lib/python3.11/site-packages/langchain_community/tools/tavily_search/tool.py\", line 178, in _arun\n    raw_results = await self.api_wrapper.raw_results_async(\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n\n  File \"/usr/local/lib/python3.11/site-packages/langchain_community/utilities/tavily_search.py\", line 149, in raw_results_async\n    results_json_str = await fetch()\n                       ^^^^^^^^^^^^^\n\n\n  File \"/usr/local/lib/python3.11/site-packages/langchain_community/utilities/tavily_search.py\", line 142, in fetch\n    async with session.post(f\"{TAVILY_API_URL}/search\", json=params) as res:\n\n\n  File \"/usr/local/lib/python3.11/site-packages/aiohttp/client.py\", line 1357, in __aenter__\n    self._resp: _RetType = await self._coro\n                           ^^^^^^^^^^^^^^^^\n\n\n  File \"/usr/local/lib/python3.11/site-packages/aiohttp/client.py\", line 688, in _request\n    await resp.start(conn)\n\n\n  File \"/usr/local/lib/python3.11/site-packages/aiohttp/client_reqrep.py\", line 1058, in start\n    message, payload = await protocol.read()  # type: ignore[union-attr]\n                       ^^^^^^^^^^^^^^^^^^^^^\n\n\n  File \"/usr/local/lib/python3.11/site-packages/aiohttp/streams.py\", line 643, in read\n    await self._waiter\n\n\nasyncio.exceptions.CancelledError\n\n\n\n### Description\n\nWhen I kick off several langgraph threads at once, each of which process larger volumes, I get \"peer closed connection ..\" errors for some of the threads pretty much every time. \n\nI was able to reproduce this with a sample langgraph by adding some more search calls on the search step and kicking off several threads at once (which I do with my langgraph as well). \n\n\nHere's the repo: https://github.com/antoremin/data-enrichment\n\nto reproduce, langgraph up the graph and run testing.ipynb locally \n\nhttps://github.com/antoremin/data-enrichment\n\n### System Info\n\nSystem Information\n------------------\n> OS:  Darwin\n> OS Version:  Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:46 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6031\n> Python Version:  3.9.11 (main, Dec 13 2023, 15:51:08) \n[Clang 14.0.3 (clang-1403.0.22.14.1)]\n\nLanggraph's pyproject.toml: \n\n[project]\nname = \"enrichment-agent\"\nversion = \"0.0.1\"\ndescription = \"An agent that populates and enriches custom schemas\"\nauthors = [\n    { name = \"William Fu-Hinthorn\", email = \"13333726+hinthornw@users.noreply.github.com\" },\n]\nreadme = \"README.md\"\nlicense = { text = \"MIT\" }\nrequires-python = \">=3.9\"\ndependencies = [\n    \"langgraph>=0.2.19\",\n    \"langchain-openai>=0.1.22\",\n    \"langchain-anthropic>=0.1.23\",\n    \"langchain>=0.2.14\",\n    \"langchain-fireworks>=0.1.7\",\n    \"python-dotenv>=1.0.1\",\n    \"langchain-community>=0.2.13\",\n    \"transformers\",\n]\n\n[project.optional-dependencies]\ndev = [\"mypy>=1.11.1\", \"ruff>=0.6.1\", \"pytest-asyncio\"]\n\n[build-system]\nrequires = [\"setuptools>=73.0.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.setuptools]\npackages = [\"enrichment_agent\"]\n[tool.setuptools.package-dir]\n\"enrichment_agent\" = \"src/enrichment_agent\"\n\"langgraph.templates.enrichment_agent\" = \"src/enrichment_agent\"\n\n\n[tool.setuptools.package-data]\n\"*\" = [\"py.typed\"]\n\n[tool.ruff]\nlint.select = [\n    \"E\",    # pycodestyle\n    \"F\",    # pyflakes\n    \"I\",    # isort\n    \"D\",    # pydocstyle\n    \"D401\", # First line should be in imperative mood\n    \"T201\",\n    \"UP\",\n]\ninclude = [\"*.py\", \"*.pyi\", \"*.ipynb\"]\nlint.ignore = [\"UP006\", \"UP007\", \"UP035\", \"D417\", \"E501\"]\n[tool.ruff.lint.per-file-ignores]\n\"tests/*\" = [\"D\", \"UP\"]\n\"ntbk/*\" = [\"D\", \"UP\", \"T201\"]\n[tool.ruff.lint.pydocstyle]\nconvention = \"google\"\n", "created_at": "2024-10-08", "closed_at": "2024-10-23", "labels": [], "State": "closed", "Author": "antoremin"}
{"issue_number": 2042, "issue_title": "Issue while using langgraph.checkpoint.sqlite import SqliteSaver", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nlanggraph                   0.2.19\nlanggraph-checkpoint        1.0.9\nlanggraph-checkpoint-sqlite 1.0.3\n\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nmemory = SqliteSaver.from_conn_string(\":memory:\")\nError Message and Stack Trace (if applicable)\nAttributeError: '_GeneratorContextManager' object has no attribute 'get_next_version'\nDescription\nwhile using MemorySaver the code is working fine. Getting this error while trying to update this to SQLlite.\nfrom langgraph.checkpoint.memory import MemorySaver\nmemory = MemorySaver()\nchanged to\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nmemory = SqliteSaver.from_conn_string(\":memory:\")\nSystem Info\nlanggraph==0.2.19\nlanggraph-checkpoint==1.0.9\nlanggraph-checkpoint-sqlite==1.0.3", "created_at": "2024-10-08", "closed_at": "2024-10-08", "labels": [], "State": "closed", "Author": "jinooos"}
{"issue_number": 2033, "issue_title": "`CheckpointNS` `ConfigurableFieldSpec` model is not in sync with the Base SQL schema definitions", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom langchain_openai import ChatOpenAI\nfrom psycopg_pool import ConnectionPool, AsyncConnectionPool\nfrom langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\nfrom db.utils import get_db_url\nfrom langgraph.graph.message import add_messages\nfrom langgraph.graph import StateGraph, START, END\n\napool = AsyncConnectionPool(\n    conninfo=get_db_url(),\n    max_size=20,\n    open=False,\n    kwargs={'autocommit': True, 'prepare_threshold': 0}\n)\n\n\nasync def setup_async_checkpointer():\n    await apool.open()\n    apostgres_checkpointer = AsyncPostgresSaver(apool)\n    await apostgres_checkpointer.setup()\n    return apostgres_checkpointer\n\n_apostgres_checkpointer = None\n\n\nasync def apostgres_checkpointer():\n    global _apostgres_checkpointer\n    if not _apostgres_checkpointer:\n        _apostgres_checkpointer = await setup_async_checkpointer()\n    return _apostgres_checkpointer\n\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\n\nasync def graph():\n    graph_builder = StateGraph(State)\n    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n\n    def chatbot(state: State):\n        return {\"messages\": [llm.invoke(state[\"messages\"])]}\n\n    graph_builder.add_node(\"chatbot\", chatbot)\n    graph_builder.add_edge(START, \"chatbot\")\n    graph_builder.add_edge(\"chatbot\", END)\n    memory = await apostgres_checkpointer()\n    graph = graph_builder.compile(checkpointer=memory)\n    return graph\n\n\nasync def main():\n    g = await graph()\n    config = {\"configurable\": {\"thread_id\": \"1\"}}\n    config_model = g.config_schema(include=(\"configurable\",))\n    new_config = config_model(**config).model_dump()\n    print(new_config)\n    async for event in g.astream_log({\"messages\": [(\"user\", 'hi')]}, config=new_config):\n        continue\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\nError Message and Stack Trace (if applicable)\n$ python -m ex\nerror ignored terminating <psycopg.AsyncPipeline [IDLE, pipeline=ON] (host=localhost database=rdagents) at 0x11168fcd0>: pipeline aborted\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/Users/riyavsinha/projects/rd-agents/ex.py\", line 66, in <module>\n    asyncio.run(main())\n  File \"/Users/riyavsinha/opt/miniconda3/envs/rdagents/lib/python3.11/asyncio/runners.py\", line 190, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/Users/riyavsinha/opt/miniconda3/envs/rdagents/lib/python3.11/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/riyavsinha/opt/miniconda3/envs/rdagents/lib/python3.11/asyncio/base_events.py\", line 650, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/Users/riyavsinha/projects/rd-agents/ex.py\", line 59, in main\n    async for event in g.astream_log({\"messages\": [(\"user\", 'hi')]}, config=new_config):\n  File \"/Users/riyavsinha/opt/miniconda3/envs/rdagents/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1110, in astream_log\n    async for item in _astream_log_implementation(  # type: ignore\n  File \"/Users/riyavsinha/opt/miniconda3/envs/rdagents/lib/python3.11/site-packages/langchain_core/tracers/log_stream.py\", line 673, in _astream_log_implementation\n    await task\n  File \"/Users/riyavsinha/opt/miniconda3/envs/rdagents/lib/python3.11/site-packages/langchain_core/tracers/log_stream.py\", line 627, in consume_astream\n    async for chunk in runnable.astream(input, config, **kwargs):\n  File \"/Users/riyavsinha/opt/miniconda3/envs/rdagents/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 1464, in astream\n    async with AsyncPregelLoop(\n  File \"/Users/riyavsinha/opt/miniconda3/envs/rdagents/lib/python3.11/site-packages/langgraph/pregel/loop.py\", line 898, in __aexit__\n    return await asyncio.shield(\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/riyavsinha/opt/miniconda3/envs/rdagents/lib/python3.11/contextlib.py\", line 730, in __aexit__\n    raise exc_details[1]\n  File \"/Users/riyavsinha/opt/miniconda3/envs/rdagents/lib/python3.11/contextlib.py\", line 713, in __aexit__\n    cb_suppress = await cb(*exc_details)\n                  ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/riyavsinha/opt/miniconda3/envs/rdagents/lib/python3.11/site-packages/langgraph/pregel/executor.py\", line 179, in __aexit__\n    raise exc\n  File \"/Users/riyavsinha/opt/miniconda3/envs/rdagents/lib/python3.11/site-packages/langgraph/pregel/loop.py\", line 821, in _checkpointer_put_after_previous\n    await prev\n  File \"/Users/riyavsinha/opt/miniconda3/envs/rdagents/lib/python3.11/site-packages/langgraph/pregel/loop.py\", line 823, in _checkpointer_put_after_previous\n    await cast(BaseCheckpointSaver, self.checkpointer).aput(\n  File \"/Users/riyavsinha/opt/miniconda3/envs/rdagents/lib/python3.11/site-packages/langgraph/checkpoint/postgres/aio.py\", line 268, in aput\n    async with self._cursor(pipeline=True) as cur:\n  File \"/Users/riyavsinha/opt/miniconda3/envs/rdagents/lib/python3.11/contextlib.py\", line 211, in __aexit__\n    await anext(self.gen)\n  File \"/Users/riyavsinha/opt/miniconda3/envs/rdagents/lib/python3.11/contextlib.py\", line 222, in __aexit__\n    await self.gen.athrow(typ, value, traceback)\n  File \"/Users/riyavsinha/opt/miniconda3/envs/rdagents/lib/python3.11/contextlib.py\", line 222, in __aexit__\n    await self.gen.athrow(typ, value, traceback)\n  File \"/Users/riyavsinha/opt/miniconda3/envs/rdagents/lib/python3.11/site-packages/psycopg_pool/pool_async.py\", line 195, in connection\n    yield conn\n  File \"/Users/riyavsinha/opt/miniconda3/envs/rdagents/lib/python3.11/site-packages/langgraph/checkpoint/postgres/aio.py\", line 34, in _get_connection\n    yield conn\n  File \"/Users/riyavsinha/opt/miniconda3/envs/rdagents/lib/python3.11/site-packages/langgraph/checkpoint/postgres/aio.py\", line 341, in _cursor\n    async with self.lock, conn.pipeline(), conn.cursor(\n  File \"/Users/riyavsinha/opt/miniconda3/envs/rdagents/lib/python3.11/contextlib.py\", line 211, in __aexit__\n    await anext(self.gen)\n  File \"/Users/riyavsinha/opt/miniconda3/envs/rdagents/lib/python3.11/site-packages/psycopg/connection_async.py\", line 333, in pipeline\n    async with pipeline:\n  File \"/Users/riyavsinha/opt/miniconda3/envs/rdagents/lib/python3.11/site-packages/psycopg/_pipeline.py\", line 293, in __aexit__\n    raise exc2.with_traceback(None)\npsycopg.errors.NotNullViolation: null value in column \"checkpoint_ns\" of relation \"checkpoint_blobs\" violates not-null constraint\nDETAIL:  Failing row contains (1, null, __start__, 00000000000000000000000000000001.0.28923935122858313, msgpack, \\x81a86d657373616765739192a475736572a26869).\nDescription\nWhen using the config schema from the Langgraph Runnable to validate configs, the ConfigurableFieldSpec for CheckpointNS has default None. Then, when the Postgres checkpointer tries to put the checkpoint into the DB, this fails because None is converted to NULL, and the field is NOT NULL.\nInstead, as you can see here, the SQL definition for this column has a default of an empty string, which is what the default should be for the CheckpointNS ConfigurableFied Spec.\nI have already provided the fix in #2019 .\nSystem Info\n$ python -m langchain_core.sys_info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 22.5.0: Thu Jun  8 22:22:20 PDT 2023; root:xnu-8796.121.3~7/RELEASE_ARM64_T6000\nPython Version:  3.11.0 (main, Mar  1 2023, 12:33:14) [Clang 14.0.6 ]\n\nPackage Information\n\nlangchain_core: 0.3.7\nlangchain: 0.3.1\nlangchain_community: 0.3.1\nlangsmith: 0.1.129\nlangchain_cli: 0.0.31\nlangchain_experimental: 0.3.2\nlangchain_huggingface: 0.1.0\nlangchain_openai: 0.2.1\nlangchain_postgres: 0.0.12\nlangchain_text_splitters: 0.3.0\nlanggraph: 0.2.34\nlangserve: 0.3.0\n\nOther Dependencies\n\naiohttp: 3.9.5\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.6\nfastapi: 0.110.3\ngitpython: 3.1.43\ngritql: 0.1.5\nhttpx: 0.27.0\nhuggingface-hub: 0.23.4\njsonpatch: 1.33\nlanggraph-checkpoint: 2.0.0\nlangserve[all]: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.51.0\norjson: 3.10.3\npackaging: 23.2\npgvector: 0.2.5\npsycopg: 3.1.19\npsycopg-pool: 3.2.2\npydantic: 2.9.2\npydantic-settings: 2.5.2\nPyYAML: 6.0.1\nrequests: 2.32.3\nsentence-transformers: 3.0.1\nSQLAlchemy: 2.0.35\nsqlalchemy: 2.0.35\nsse-starlette: 1.8.2\ntenacity: 8.3.0\ntiktoken: 0.7.0\ntokenizers: 0.19.1\ntomlkit: 0.12.5\ntransformers: 4.42.3\ntyper[all]: Installed. No version info available.\ntyping-extensions: 4.12.1\nuvicorn: 0.23.2\n", "created_at": "2024-10-07", "closed_at": "2024-10-07", "labels": [], "State": "closed", "Author": "riyavsinha"}
{"issue_number": 2030, "issue_title": "persistence_postgres error ", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nhttps://langchain-ai.github.io/langgraph/how-tos/persistence_postgres/ \n\nMy code is all the same as above and the difference is that I used tool_calling_agent(langgraph) not create_react_agent,\nError Message and Stack Trace (if applicable)\n---> 26 for event in graph.stream(\n     27     *request,stream_mode=\"values\",\n     28 ):\n     29     event[\"messages\"][-1].pretty_print()\n     31 pool.close()\n\nFile /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1248, in Pregel.stream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\n   1244 if \"custom\" in stream_modes:\n   1245     config[CONF][CONFIG_KEY_STREAM_WRITER] = lambda c: stream.put(\n   1246         ((), \"custom\", c)\n   1247     )\n-> 1248 with SyncPregelLoop(\n   1249     input,\n   1250     stream=StreamProtocol(stream.put, stream_modes),\n   1251     config=config,\n   1252     store=store,\n   1253     checkpointer=checkpointer,\n   1254     nodes=self.nodes,\n   1255     specs=self.channels,\n   1256     output_keys=output_keys,\n   1257     stream_keys=self.stream_channels_asis,\n   1258     debug=debug,\n   1259 ) as loop:\n   1260     # create runner\n   1261     runner = PregelRunner(\n   1262         submit=loop.submit,\n   1263         put_writes=loop.put_writes,\n   1264     )\n   1265     # enable subgraph streaming\n\nFile /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langgraph/pregel/loop.py:727, in SyncPregelLoop.__enter__(self)\n    725         raise CheckpointNotLatest\n    726 elif self.checkpointer:\n--> 727     saved = self.checkpointer.get_tuple(self.checkpoint_config)\n    728 else:\n    729     saved = None\n\nFile /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/langgraph/checkpoint/postgres/__init__.py:228, in PostgresSaver.get_tuple(self, config)\n    225     where = \"WHERE thread_id = %s AND checkpoint_ns = %s ORDER BY checkpoint_id DESC LIMIT 1\"\n    227 with self._cursor() as cur:\n--> 228     cur.execute(\n    229         self.SELECT_SQL + where,\n    230         args,\n    231         binary=True,\n    232     )\n    234     for value in cur:\n    235         return CheckpointTuple(\n    236             {\n    237                 \"configurable\": {\n   (...)\n    260             self._load_writes(value[\"pending_writes\"]),\n    261         )\n\nFile /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/psycopg/cursor.py:97, in Cursor.execute(self, query, params, prepare, binary)\n     93         self._conn.wait(\n     94             self._execute_gen(query, params, prepare=prepare, binary=binary)\n     95         )\n     96 except e._NO_TRACEBACK as ex:\n---> 97     raise ex.with_traceback(None)\n     98 return self\n\nDuplicatePreparedStatement: prepared statement \"_pg3_0\" already exists\nDescription\nhttps://langchain-ai.github.io/langgraph/how-tos/persistence_postgres/\nkwargs={\n\"autocommit\": True,\n\"prepare_threshold\": 0,\n}\nIn , when threshold is a large number, the typical llm agent outputs well, but errors occur after using the tool.\nIf threshold 0, llm agent output will not come out and an error will occur.\nSystem Info\nlanggraph==0.2.34\nlanggraph-checkpoint==2.0.0\nlanggraph-checkpoint-postgres==2.0.0\nI am using Postgres in supabase project", "created_at": "2024-10-07", "closed_at": "2024-10-31", "labels": [], "State": "closed", "Author": "naturesh"}
{"issue_number": 2024, "issue_title": "Websocket implementation", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\ndef stream(\n        self, path: str, method: str, *, json: Optional[dict] = None\n    ) -> Iterator[StreamPart]:\n        \"\"\"Stream the results of a request using SSE.\"\"\"\n        headers, content = encode_json(json)\n        with httpx_sse.connect_sse(\n            self.client, method, path, headers=headers, content=content\n        ) as sse:\n            try:\n                sse.response.raise_for_status()\n            except httpx.HTTPStatusError as e:\n                body = sse.response.read().decode()\n                if sys.version_info >= (3, 11):\n                    e.add_note(body)\n                else:\n                    logger.error(f\"Error from langgraph-api: {body}\", exc_info=e)\n                raise e\n            for event in sse.iter_sse():\n                yield StreamPart(\n                    event.event, orjson.loads(event.data) if event.data else None\n                )\nError Message and Stack Trace (if applicable)\nI want support websocket as well\nDescription\nI want support websocket as well\nSystem Info\nscikit-image==0.22.0\nscikit-learn==1.3.2\nscipy==1.10.1\nseaborn==0.13.0\nSend2Trash==1.8.2\nsentry-sdk==1.36.0\nsetproctitle==1.3.3\nshap==0.46.0\nshellingham==1.5.4\nsix @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work\nslicer==0.0.8\nsmmap==5.0.1\nsniffio==1.3.0\nsoupsieve==2.5\nSQLAlchemy==2.0.23\nsqlparse==0.4.4\nstack-data==0.6.3\nstarlette==0.37.2\nstatsmodels==0.14.0\nsympy @ file:///work/ci_py311_2/sympy_1679339311852/work\ntables==3.9.1\ntabulate==0.9.0\ntenacity==8.2.3\ntensorboard==2.17.0\ntensorboard-data-server==0.7.1\ntensorflow-estimator==2.15.0\ntensorflow-io-gcs-filesystem==0.34.0\ntermcolor==2.3.0\nterminado==0.18.1\nthreadpoolctl==3.2.0\ntifffile==2023.9.26\ntime-machine==2.14.1\ntimm==0.9.7\ntinycss2==1.2.1\ntokenizers==0.19.1\ntomlkit==0.12.3\ntoolz==0.12.1\ntorch==2.3.0\ntorch-summary==1.4.5\ntorch_cluster==1.6.3+pt23cu121\ntorch_geometric==2.4.0\ntorch_scatter==2.1.2+pt23cu121\ntorch_sparse==0.6.18+pt23cu121\ntorch_spline_conv==1.2.2+pt23cu121\ntorchaudio==2.3.0\ntorchinfo==1.8.0\ntorchmetrics==1.3.0.post0\ntorchsummary==1.5.1\ntorchvision==0.18.0\ntornado @ file:///croot/tornado_1696936946304/work\ntqdm==4.66.1\ntraitlets==5.9.0\ntransformers==4.44.2\ntriton==2.3.0\ntyped-argument-parser==1.7.2\ntyper==0.12.3\ntypes-python-dateutil==2.9.0.20240316\ntyping-inspect==0.9.0\ntyping_extensions==4.8.0\ntzdata==2023.3\nujson==5.10.0\nuri-template==1.3.0\nurllib3==2.2.2\nuvicorn==0.25.0\nuvloop==0.19.0\nwandb==0.17.6\nwatchfiles==0.22.0\nwcwidth @ file:///home/conda/feedstock_root/build_artifacts/wcwidth_1704731205417/work\nwebcolors==1.13\nwebencodings==0.5.1\nwebsocket-client==1.7.0\nwebsockets==12.0\nWerkzeug==3.0.0\nwrapt==1.14.1\nxgboost==2.0.3\nxxhash==3.4.1\nyarl==1.9.4", "created_at": "2024-10-07", "closed_at": "2024-10-07", "labels": [], "State": "closed", "Author": "moghadas76"}
{"issue_number": 2016, "issue_title": "New deployments fail with \"couldn't import transformers package\" issue on the first run. ", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\n## trimming messages with a tokenizer: \n\n    trimmed_messages = trim_messages(\n        messages_with_prompt,\n        max_tokens=max_tokens,\n        strategy=\"last\",\n        token_counter=token_counter,\n        allow_partial=True,\n    )\nError Message and Stack Trace (if applicable)\nImportError('Could not import transformers python package. This is needed in order to calculate get_token_ids. Please install it with `pip install transformers`.')Traceback (most recent call last):\n\n\n  File \"/usr/local/lib/python3.11/site-packages/langchain_core/language_models/base.py\", line 61, in get_tokenizer\n    from transformers import GPT2TokenizerFast  # type: ignore[import]\nDescription\nI'm running a langgraph with trim_messages() that uses a tokenizer. I added transformers package to my pyproject file.\nEvery time I deploy a new version, the very first run fails with an error described above. When I kick off a new run right after, everything works OK. I'm assuming it has something to do with the way transformers package gets installed - maybe it takes more time or something.\nSystem Info\nlangchain==0.3.0\nlangchain-anthropic==0.2.0\nlangchain-cli==0.0.21\nlangchain-community==0.2.0rc1\nlangchain-core==0.3.2\nlangchain-experimental==0.0.8\nlangchain-fireworks==0.1.3\nlangchain-google-genai==1.0.4\nlangchain-groq==0.1.3\nlangchain-mistralai==0.1.7\nlangchain-openai==0.1.7\nlangchain-text-splitters==0.3.0\nlangchainhub==0.1.14\nmac\nPython 3.9.11\nSame behavior on Langgraph Cloud", "created_at": "2024-10-06", "closed_at": "2024-10-08", "labels": [], "State": "closed", "Author": "antoremin"}
{"issue_number": 2011, "issue_title": "\"--watch\" flag doesn't work on langgraph up", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nlanggraph up --watch\nError Message and Stack Trace (if applicable)\nlanggraph up --watch\nStarting LangGraph API server...\nFor local dev, requires env var LANGSMITH_API_KEY with access to LangGraph Cloud closed beta.\nFor production use, requires a license key in env var LANGGRAPH_CLOUD_LICENSE_KEY.\n\\ Building...unknown flag: --watch\nDescription\nlanggraph-cli documentation has --watch flag however when I tried to run, it doesn't work.\nCould you help to resolve?\nI am using latest cli as well.\nhttps://langchain-ai.github.io/langgraph/cloud/reference/cli/#up\nSystem Info\nwindows\npython 3.12.5", "created_at": "2024-10-04", "closed_at": "2024-12-20", "labels": [], "State": "closed", "Author": "junan-trustarc"}
{"issue_number": 2010, "issue_title": "\"langgraph test\" doesn't work with latest langgraph", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nuvicorn==0.31.0\npython-dotenv==1.0.1\nlangchain==0.3.2\nlangchain-core==0.3.9\nlangchain_openai==0.2.1\nlangchain-voyageai==0.1.2\nlangchain_cohere==0.3.0\nlangchain_experimental==0.3.2\nlanggraph==0.2.34\nlangsmith==0.1.131\nlangchain_pinecone==0.2.0\nlanggraph-checkpoint-postgres==2.0.0\nwikipedia==1.4.0\npinecone-client[grpc]==5.0.1\npinecone-text==0.9.0\npsycopg[binary,pool]==3.2.3\npsycopg-pool==3.2.3\nrequests==2.32.3\npytest==8.3.3\nsetuptools==75.1.0\nError Message and Stack Trace (if applicable)\nouter_arc-ai-langgraph/src/requirements.txt\" did not complete successfully: exit code: 1\n------\n > [3/7] RUN PYTHONDONTWRITEBYTECODE=1 pip install --no-cache-dir -c /api/constraints.txt -r /deps/__outer_arc-ai-langgraph/src/requirements.txt:\n2.030\n2.030 The conflict is caused by:\n2.030     The user requested langchain-core==0.3.9\n2.030     The user requested (constraint) langchain-core<0.3.0,>=0.2.27\n2.030\n2.030 To fix this you could try to:\n2.030 1. loosen the range of package versions you've specified\n2.030 2. remove package versions to allow pip to attempt to solve the dependency conflict\n2.030\n2.147 ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\n------\nDockerfile:6\n--------------------\n   4 |\n   5 |     ADD requirements.txt /deps/__outer_arc-ai-langgraph/src/requirements.txt\n   6 | >>> RUN PYTHONDONTWRITEBYTECODE=1 pip install --no-cache-dir -c /api/constraints.txt -r /deps/__outer_arc-ai-langgraph/src/requirements.txt\n   7 |\n   8 |     ADD . /deps/__outer_arc-ai-langgraph/src\n--------------------\nERROR: failed to solve: process \"/bin/sh -c PYTHONDONTWRITEBYTECODE=1 pip install --no-cache-dir -c /api/constraints.txt -r /deps/__outer_arc-ai-langgraph/src/requirements.txt\" did not complete successfully: exit code: 1\nDescription\n\"langgraph up\" is working however \"langgraph test\" complains with dependencies, could you update constraints.txt to make it working? thanks.\nSystem Info\nwindows\npython 3.12.5", "created_at": "2024-10-04", "closed_at": "2024-12-18", "labels": [], "State": "closed", "Author": "junan-trustarc"}
{"issue_number": 2004, "issue_title": "Storm Example Falling with MultipleSubgraphsError", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nasync def conduct_interviews(state: ResearchState):\n    topic = state[\"topic\"]\n    initial_states = [\n        {\n            \"editor\": editor,\n            \"messages\": [\n                AIMessage(\n                    content=f\"So you said you were writing an article on {topic}?\",\n                    name=\"Subject_Matter_Expert\",\n                )\n            ],\n        }\n        for editor in state[\"editors\"]\n    ]\n    # We call in to the sub-graph here to parallelize the interviews\n    interview_results = await interview_graph.abatch(initial_states)\n\n    return {\n        **state,\n        \"interview_results\": interview_results,\n    }\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nMultipleSubgraphsError                    Traceback (most recent call last)\nCell In[39], line 2\n      1 config = {\"configurable\": {\"thread_id\": \"my-thread\"}}\n----> 2 async for step in storm.astream(\n      3     {\n      4         \"topic\": \"Groq, NVIDIA, Llamma.cpp and the future of LLM Inference\",\n      5     },\n      6     config,\n      7 ):\n      8     name = next(iter(step))\n      9     print(name)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/project-mercury-Rog6NQKe-py3.11/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1502, in Pregel.astream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\n   1491 # Similarly to Bulk Synchronous Parallel / Pregel model\n   1492 # computation proceeds in steps, while there are channel updates\n   1493 # channel updates from step N are only visible in step N+1\n   1494 # channels are guaranteed to be immutable for the duration of the step,\n   1495 # with channel updates applied only at the transition between steps\n   1496 while loop.tick(\n   1497     input_keys=self.input_channels,\n   1498     interrupt_before=interrupt_before_,\n   1499     interrupt_after=interrupt_after_,\n   1500     manager=run_manager,\n   1501 ):\n-> 1502     async for _ in runner.atick(\n   1503         loop.tasks.values(),\n   1504         timeout=self.step_timeout,\n   1505         retry_policy=self.retry_policy,\n   1506         get_waiter=get_waiter,\n   1507     ):\n   1508         # emit output\n   1509         for o in output():\n   1510             yield o\n\nFile ~/Library/Caches/pypoetry/virtualenvs/project-mercury-Rog6NQKe-py3.11/lib/python3.11/site-packages/langgraph/pregel/runner.py:130, in PregelRunner.atick(self, tasks, reraise, timeout, retry_policy, get_waiter)\n    128 t = tasks[0]\n    129 try:\n--> 130     await arun_with_retry(t, retry_policy, stream=self.use_astream)\n    131     self.commit(t, None)\n    132 except Exception as exc:\n\nFile ~/Library/Caches/pypoetry/virtualenvs/project-mercury-Rog6NQKe-py3.11/lib/python3.11/site-packages/langgraph/pregel/retry.py:102, in arun_with_retry(task, retry_policy, stream)\n    100         pass\n    101 else:\n--> 102     await task.proc.ainvoke(task.input, config)\n    103 # if successful, end\n    104 break\n\nFile ~/Library/Caches/pypoetry/virtualenvs/project-mercury-Rog6NQKe-py3.11/lib/python3.11/site-packages/langgraph/utils/runnable.py:452, in RunnableSeq.ainvoke(self, input, config, **kwargs)\n    450     coro = step.ainvoke(input, config)\n    451 if ASYNCIO_ACCEPTS_CONTEXT:\n--> 452     input = await asyncio.create_task(coro, context=context)\n    453 else:\n    454     input = await asyncio.create_task(coro)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/project-mercury-Rog6NQKe-py3.11/lib/python3.11/site-packages/langgraph/utils/runnable.py:235, in RunnableCallable.ainvoke(self, input, config, **kwargs)\n    233 if ASYNCIO_ACCEPTS_CONTEXT:\n    234     coro = cast(Coroutine[None, None, Any], self.afunc(input, **kwargs))\n--> 235     ret = await asyncio.create_task(coro, context=context)\n    236 else:\n    237     ret = await self.afunc(input, **kwargs)\n\nCell In[36], line 33\n     20 initial_states = [\n     21     {\n     22         \"editor\": editor,\n   (...)\n     30     for editor in state[\"editors\"]\n     31 ]\n     32 # We call in to the sub-graph here to parallelize the interviews\n---> 33 interview_results = await interview_graph.abatch(initial_states)\n     35 return {\n     36     **state,\n     37     \"interview_results\": interview_results,\n     38 }\n\nFile ~/Library/Caches/pypoetry/virtualenvs/project-mercury-Rog6NQKe-py3.11/lib/python3.11/site-packages/langchain_core/runnables/base.py:905, in Runnable.abatch(self, inputs, config, return_exceptions, **kwargs)\n    902         return await self.ainvoke(input, config, **kwargs)\n    904 coros = map(ainvoke, inputs, configs)\n--> 905 return await gather_with_concurrency(configs[0].get(\"max_concurrency\"), *coros)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/project-mercury-Rog6NQKe-py3.11/lib/python3.11/site-packages/langchain_core/runnables/utils.py:68, in gather_with_concurrency(n, *coros)\n     58 \"\"\"Gather coroutines with a limit on the number of concurrent coroutines.\n     59 \n     60 Args:\n   (...)\n     65     The results of the coroutines.\n     66 \"\"\"\n     67 if n is None:\n---> 68     return await asyncio.gather(*coros)\n     70 semaphore = asyncio.Semaphore(n)\n     72 return await asyncio.gather(*(gated_coro(semaphore, c) for c in coros))\n\nFile ~/Library/Caches/pypoetry/virtualenvs/project-mercury-Rog6NQKe-py3.11/lib/python3.11/site-packages/langchain_core/runnables/base.py:902, in Runnable.abatch.<locals>.ainvoke(input, config)\n    900         return e\n    901 else:\n--> 902     return await self.ainvoke(input, config, **kwargs)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/project-mercury-Rog6NQKe-py3.11/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1613, in Pregel.ainvoke(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\n   1611 else:\n   1612     chunks = []\n-> 1613 async for chunk in self.astream(\n   1614     input,\n   1615     config,\n   1616     stream_mode=stream_mode,\n   1617     output_keys=output_keys,\n   1618     interrupt_before=interrupt_before,\n   1619     interrupt_after=interrupt_after,\n   1620     debug=debug,\n   1621     **kwargs,\n   1622 ):\n   1623     if stream_mode == \"values\":\n   1624         latest = chunk\n\nFile ~/Library/Caches/pypoetry/virtualenvs/project-mercury-Rog6NQKe-py3.11/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1464, in Pregel.astream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\n   1460 if \"custom\" in stream_modes:\n   1461     config[CONF][CONFIG_KEY_STREAM_WRITER] = lambda c: stream.put_nowait(\n   1462         ((), \"custom\", c)\n   1463     )\n-> 1464 async with AsyncPregelLoop(\n   1465     input,\n   1466     stream=StreamProtocol(stream.put_nowait, stream_modes),\n   1467     config=config,\n   1468     store=store,\n   1469     checkpointer=checkpointer,\n   1470     nodes=self.nodes,\n   1471     specs=self.channels,\n   1472     output_keys=output_keys,\n   1473     stream_keys=self.stream_channels_asis,\n   1474 ) as loop:\n   1475     # create runner\n   1476     runner = PregelRunner(\n   1477         submit=loop.submit,\n   1478         put_writes=loop.put_writes,\n   1479         use_astream=do_stream is not None,\n   1480     )\n   1481     # enable subgraph streaming\n\nFile ~/Library/Caches/pypoetry/virtualenvs/project-mercury-Rog6NQKe-py3.11/lib/python3.11/site-packages/langgraph/pregel/loop.py:789, in AsyncPregelLoop.__init__(self, input, stream, config, store, checkpointer, nodes, specs, output_keys, stream_keys, check_subgraphs, debug)\n    774 def __init__(\n    775     self,\n    776     input: Optional[Any],\n   (...)\n    787     debug: bool = False,\n    788 ) -> None:\n--> 789     super().__init__(\n    790         input,\n    791         stream=stream,\n    792         config=config,\n    793         checkpointer=checkpointer,\n    794         store=store,\n    795         nodes=nodes,\n    796         specs=specs,\n    797         output_keys=output_keys,\n    798         stream_keys=stream_keys,\n    799         check_subgraphs=check_subgraphs,\n    800         debug=debug,\n    801     )\n    802     self.stack = AsyncExitStack()\n    803     if checkpointer:\n\nFile ~/Library/Caches/pypoetry/virtualenvs/project-mercury-Rog6NQKe-py3.11/lib/python3.11/site-packages/langgraph/pregel/loop.py:230, in PregelLoop.__init__(self, input, stream, config, store, checkpointer, nodes, specs, output_keys, stream_keys, check_subgraphs, debug)\n    228 if check_subgraphs and self.is_nested and self.checkpointer is not None:\n    229     if self.config[CONF][CONFIG_KEY_CHECKPOINT_NS] in _SEEN_CHECKPOINT_NS:\n--> 230         raise MultipleSubgraphsError\n    231     else:\n    232         _SEEN_CHECKPOINT_NS.add(self.config[CONF][CONFIG_KEY_CHECKPOINT_NS])\n\nMultipleSubgraphsError:\nDescription\nWhen running the storm demo on that latest versions of langraph the demo is failing now when during the \"interviews\"\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.5.0: Wed May  1 20:14:38 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T6020\nPython Version:  3.11.9 (v3.11.9:de54cf5be3, Apr  2 2024, 07:12:50) [Clang 13.0.0 (clang-1300.0.29.30)]\n\nPackage Information\n\nlangchain_core: 0.3.8\nlangchain: 0.3.2\nlangchain_community: 0.3.1\nlangsmith: 0.1.131\nlangchain_chroma: 0.1.2\nlangchain_fireworks: 0.2.0\nlangchain_ibm: 0.2.1\nlangchain_ollama: 0.2.0\nlangchain_openai: 0.2.1\nlangchain_text_splitters: 0.3.0\nlanggraph: 0.2.34\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.10.8\nasync-timeout: Installed. No version info available.\nchromadb: 0.5.11\ndataclasses-json: 0.6.7\nfastapi: 0.110.3\nfireworks-ai: 0.15.4\nhttpx: 0.27.2\nibm-watsonx-ai: 1.1.11\njsonpatch: 1.33\nlanggraph-checkpoint: 2.0.0\nnumpy: 1.26.4\nollama: 0.3.3\nopenai: 1.51.0\norjson: 3.10.7\npackaging: 24.1\npydantic: 2.9.2\npydantic-settings: 2.5.2\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.35\ntenacity: 8.5.0\ntiktoken: 0.7.0\ntyping-extensions: 4.12.2\n", "created_at": "2024-10-04", "closed_at": "2024-10-11", "labels": [], "State": "closed", "Author": "deanchanter"}
{"issue_number": 2003, "issue_title": "DOC: ValidationError in LangGraph Customer Support Bot Example", "issue_body": "Issue with current documentation:\nThis Example: https://github.com/langchain-ai/langgraph/blob/main/docs/docs/tutorials/customer-support/customer-support.ipynb\nDoes not work, starting from Part 2 Example Conversation\n\nError:\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\nCell In[18], [line 25](vscode-notebook-cell:?execution_count=18&line=25)\n     [21](vscode-notebook-cell:?execution_count=18&line=21) for question in tutorial_questions:\n     [22](vscode-notebook-cell:?execution_count=18&line=22)     events = part_2_graph.stream(\n     [23](vscode-notebook-cell:?execution_count=18&line=23)         {\"messages\": (\"user\", question)}, config, stream_mode=\"values\"\n     [24](vscode-notebook-cell:?execution_count=18&line=24)     )\n---> [25](vscode-notebook-cell:?execution_count=18&line=25)     for event in events:\n     [26](vscode-notebook-cell:?execution_count=18&line=26)         _print_event(event, _printed)\n     [27](vscode-notebook-cell:?execution_count=18&line=27)     snapshot = part_2_graph.get_state(config)\n\nFile ~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:876, in Pregel.stream(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\n    [869](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:869) done, inflight = concurrent.futures.wait(\n    [870](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:870)     futures,\n    [871](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:871)     return_when=concurrent.futures.FIRST_EXCEPTION,\n    [872](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:872)     timeout=self.step_timeout,\n    [873](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:873) )\n    [875](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:875) # panic on failure or timeout\n--> [876](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:876) _panic_or_proceed(done, inflight, step)\n    [878](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:878) # combine pending writes from all tasks\n    [879](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:879) pending_writes = deque[tuple[str, Any]]()\n\nFile ~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1422, in _panic_or_proceed(done, inflight, step)\n   [1420](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1420)             inflight.pop().cancel()\n   [1421](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1421)         # raise the exception\n-> [1422](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1422)         raise exc\n   [1424](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1424) if inflight:\n   [1425](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1425)     # if we got here means we timed out\n   [1426](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1426)     while inflight:\n   [1427](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1427)         # cancel all pending tasks\n\nFile /usr/lib/python3.10/concurrent/futures/thread.py:58, in _WorkItem.run(self)\n     [55](https://file+.vscode-resource.vscode-cdn.net/usr/lib/python3.10/concurrent/futures/thread.py:55)     return\n     [57](https://file+.vscode-resource.vscode-cdn.net/usr/lib/python3.10/concurrent/futures/thread.py:57) try:\n---> [58](https://file+.vscode-resource.vscode-cdn.net/usr/lib/python3.10/concurrent/futures/thread.py:58)     result = self.fn(*self.args, **self.kwargs)\n     [59](https://file+.vscode-resource.vscode-cdn.net/usr/lib/python3.10/concurrent/futures/thread.py:59) except BaseException as exc:\n     [60](https://file+.vscode-resource.vscode-cdn.net/usr/lib/python3.10/concurrent/futures/thread.py:60)     self.future.set_exception(exc)\n\nFile ~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/retry.py:66, in run_with_retry(task, retry_policy)\n     [64](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/retry.py:64) task.writes.clear()\n     [65](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/retry.py:65) # run the task\n---> [66](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/retry.py:66) task.proc.invoke(task.input, task.config)\n     [67](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/retry.py:67) # if successful, end\n     [68](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/pregel/retry.py:68) break\n\nFile ~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:2368, in RunnableSequence.invoke(self, input, config)\n   [2366](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:2366) try:\n   [2367](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:2367)     for i, step in enumerate(self.steps):\n-> [2368](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:2368)         input = step.invoke(\n   [2369](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:2369)             input,\n   [2370](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:2370)             # mark each step as a child run\n   [2371](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:2371)             patch_config(\n   [2372](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:2372)                 config, callbacks=run_manager.get_child(f\"seq:step:{i+1}\")\n   [2373](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:2373)             ),\n   [2374](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:2374)         )\n   [2375](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:2375) # finish the root run\n   [2376](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:2376) except BaseException as e:\n\nFile ~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/utils.py:89, in RunnableCallable.invoke(self, input, config)\n     [83](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/utils.py:83)     context.run(var_child_runnable_config.set, config)\n     [84](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/utils.py:84)     kwargs = (\n     [85](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/utils.py:85)         {**self.kwargs, \"config\": config}\n     [86](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/utils.py:86)         if accepts_config(self.func)\n     [87](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/utils.py:87)         else self.kwargs\n     [88](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/utils.py:88)     )\n---> [89](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/utils.py:89)     ret = context.run(self.func, input, **kwargs)\n     [90](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/utils.py:90) if isinstance(ret, Runnable) and self.recurse:\n     [91](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langgraph/utils.py:91)     return ret.invoke(input, config)\n\nCell In[16], [line 9](vscode-notebook-cell:?execution_count=16&line=9)\n      [8](vscode-notebook-cell:?execution_count=16&line=8) def user_info(state: State):\n----> [9](vscode-notebook-cell:?execution_count=16&line=9)     return {\"user_info\": fetch_user_flight_information.invoke({})}\n\nFile ~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:260, in BaseTool.invoke(self, input, config, **kwargs)\n    [253](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:253) def invoke(\n    [254](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:254)     self,\n    [255](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:255)     input: Union[str, Dict],\n    [256](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:256)     config: Optional[RunnableConfig] = None,\n    [257](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:257)     **kwargs: Any,\n    [258](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:258) ) -> Any:\n    [259](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:259)     config = ensure_config(config)\n--> [260](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:260)     return self.run(\n    [261](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:261)         input,\n    [262](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:262)         callbacks=config.get(\"callbacks\"),\n    [263](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:263)         tags=config.get(\"tags\"),\n    [264](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:264)         metadata=config.get(\"metadata\"),\n    [265](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:265)         run_name=config.get(\"run_name\"),\n    [266](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:266)         run_id=config.pop(\"run_id\", None),\n    [267](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:267)         config=config,\n    [268](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:268)         **kwargs,\n    [269](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:269)     )\n\nFile ~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:417, in BaseTool.run(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, **kwargs)\n    [415](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:415) except ValidationError as e:\n    [416](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:416)     if not self.handle_validation_error:\n--> [417](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:417)         raise e\n    [418](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:418)     elif isinstance(self.handle_validation_error, bool):\n    [419](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:419)         observation = \"Tool input validation error\"\n\nFile ~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:406, in BaseTool.run(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, **kwargs)\n    [404](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:404) context = copy_context()\n    [405](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:405) context.run(var_child_runnable_config.set, child_config)\n--> [406](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:406) parsed_input = self._parse_input(tool_input)\n    [407](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:407) tool_args, tool_kwargs = self._to_args_and_kwargs(parsed_input)\n    [408](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:408) observation = (\n    [409](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:409)     context.run(\n    [410](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:410)         self._run, *tool_args, run_manager=run_manager, **tool_kwargs\n   (...)\n    [413](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:413)     else context.run(self._run, *tool_args, **tool_kwargs)\n    [414](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:414) )\n\nFile ~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:304, in BaseTool._parse_input(self, tool_input)\n    [302](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:302) else:\n    [303](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:303)     if input_args is not None:\n--> [304](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:304)         result = input_args.parse_obj(tool_input)\n    [305](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:305)         return {\n    [306](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:306)             k: getattr(result, k)\n    [307](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:307)             for k, v in result.dict().items()\n    [308](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:308)             if k in tool_input\n    [309](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:309)         }\n    [310](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/langchain_core/tools.py:310) return tool_input\n\nFile ~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/pydantic/v1/main.py:526, in BaseModel.parse_obj(cls, obj)\n    [524](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/pydantic/v1/main.py:524)         exc = TypeError(f'{cls.__name__} expected dict not {obj.__class__.__name__}')\n    [525](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/pydantic/v1/main.py:525)         raise ValidationError([ErrorWrapper(exc, loc=ROOT_KEY)], cls) from e\n--> [526](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/pydantic/v1/main.py:526) return cls(**obj)\n\nFile ~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/pydantic/v1/main.py:341, in BaseModel.__init__(__pydantic_self__, **data)\n    [339](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/pydantic/v1/main.py:339) values, fields_set, validation_error = validate_model(__pydantic_self__.__class__, data)\n    [340](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/pydantic/v1/main.py:340) if validation_error:\n--> [341](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/pydantic/v1/main.py:341)     raise validation_error\n    [342](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/pydantic/v1/main.py:342) try:\n    [343](https://file+.vscode-resource.vscode-cdn.net/home/rohan/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/~/Desktop/work/playground/calm-langgraph-customer-service-comparison/langgraph_implementation/venv/lib/python3.10/site-packages/pydantic/v1/main.py:343)     object_setattr(__pydantic_self__, '__dict__', values)\n\nValidationError: 1 validation error for fetch_user_flight_informationSchema\nconfig\n  field required (type=value_error.missing)\n\nIdea or request for content:\nThe Official Examples Should be working out of the box, it helps with quick testing and evaluation", "created_at": "2024-10-04", "closed_at": "2024-10-07", "labels": [], "State": "closed", "Author": "rohanbalkondekar"}
{"issue_number": 1988, "issue_title": "Making JSONPlusSerializer Backward compataible", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nSelf-explainatory from the description and doesn't require code.\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nLanggraph agents cannot be deployed in production unless the JSONPlusSerializer is made backward compatible. Currently, during the checkpointing process in the Checkpointer class, we serialize both the checkpoint and metadata using JSONPlusSerializer, which includes the module path in its output. However, if the user changes the module path as part of a migration, this introduces a backward compatibility issue.\nMost applications run across multiple instances, and code deployments typically occur incrementally, instance by instance. As a result, while the module path is updated on one instance, the new path is stored during serialization. If a request with the same thread_id is processed by an instance where the code change hasn't been applied yet, the system will attempt to load the checkpoint using the new module path which instance is not aware of. This mismatch triggers an error during the get_tuple process, stating that the module path does not exist. And in desrialisation this exception is caught and returns None which kind of restarts the execution.\nTo ensure smooth transitions and prevent failures during such deployments, it's critical to make the JSONPlusSerializer handle these scenarios in a backward-compatible manner.\nThis issue is discussed in #1965 as well but answer is not satisfactory and was closed. So raising again.\nSystem Info\nlangchain==0.2.16\nlangchain-community>=0.2.15\nlangchain-core>=0.2.15\nlangchain-google-community\nlangchain-openai\nlangchain-pinecone\nlanggraph==0.2.21", "created_at": "2024-10-03", "closed_at": "2024-10-07", "labels": ["enhancement"], "State": "closed", "Author": "Sanath91009"}
{"issue_number": 1978, "issue_title": "No useful traceback when using pydantic schema and failing validation", "issue_body": "Privileged issue\n\n I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.\n\nIssue Content\nThe code below fails on entry to ok_node, but the stack trace has no information about which node failed.\nGiven that the same schema is often shared between all nodes in the graph (at least by default) -- it makes it impossible to determine where the run time error is occurring.\nfrom langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\n\nfrom pydantic import BaseModel\n\n# The overall state of the graph (this is the public state shared across nodes)\nclass OverallState(BaseModel):\n    a: str\n\ndef bad_node(state: OverallState):\n    return {\n        \"a\": 123 # Invalid\n    }\n\ndef ok_node(state: OverallState):\n    return {\n        \"a\": \"goodbye\"\n    }\n\n\n# Build the state graph\nbuilder = StateGraph(OverallState)\nbuilder.add_node(bad_node)\nbuilder.add_node(ok_node)\nbuilder.add_edge(START, \"bad_node\")\nbuilder.add_edge(\"bad_node\", \"ok_node\")\nbuilder.add_edge(\"ok_node\", END)\ngraph = builder.compile()\n\n# Test the graph with a valid input\ngraph.invoke({ \"a\": \"hello\"})\nPart of error trace\n354 return tasks\nFile ~/.pyenv/versions/3.11.4/envs/core/lib/python3.11/site-packages/langgraph/pregel/algo.py:495, in prepare_single_task(task_path, task_id_checksum, checkpoint, processes, channels, managed, config, step, for_execution, store, checkpointer, manager)\n485 if triggers := sorted(\n486     chan\n487     for chan in proc.triggers\n(...)\n492     > seen.get(chan, null_version)\n493 ):\n494     try:\n--> 495         val = next(\n496             _proc_input(\n497                 step, proc, managed, channels, for_execution=for_execution\n498             )\n499         )\n500     except StopIteration:\n501         return\nFile ~/.pyenv/versions/3.11.4/envs/core/lib/python3.11/site-packages/langgraph/pregel/algo.py:627, in _proc_input(step, proc, managed, channels, for_execution)\n625 # If the process has a mapper, apply it to the value\n626 if for_execution and proc.mapper is not None:\n--> 627     val = proc.mapper(val)\n629 yield val\nFile ~/.pyenv/versions/3.11.4/envs/core/lib/python3.11/site-packages/langgraph/graph/state.py:704, in _coerce_state(schema, input)\n703 def _coerce_state(schema: Type[Any], input: dict[str, Any]) -> dict[str, Any]:\n--> 704     return schema(**input)\nFile ~/.pyenv/versions/3.11.4/envs/core/lib/python3.11/site-packages/pydantic/main.py:212, in BaseModel.init(self, **data)\n210 # __tracebackhide__ tells pytest and some other tools to omit this function from tracebacks\n211 tracebackhide = True\n--> 212 validated_self = self.pydantic_validator.validate_python(data, self_instance=self)\n213 if self is not validated_self:\n214     warnings.warn(\n215         'A custom validator is returning a value other than self.\\n'\n216         \"Returning anything other than self from a top level model validator isn't supported when validating via __init__.\\n\"\n217         'See the model_validator docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.',\n218         category=None,\n219     )\nValidationError: 1 validation error for OverallState\na\nInput should be a valid string [type=string_type, input_value=123, input_type=int]\nFor further information visit https://errors.pydantic.dev/2.9/v/string_type", "created_at": "2024-10-02", "closed_at": "2024-11-27", "labels": [], "State": "closed", "Author": "eyurtsev"}
{"issue_number": 1977, "issue_title": "Surprising results when using pydantic for state schema", "issue_body": "Privileged issue\n\n I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.\n\nIssue Content\nRun time validation with pydantic schema does not work with output schema.\nfrom langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\n\nfrom pydantic import BaseModel\n\n# The overall state of the graph (this is the public state shared across nodes)\nclass OverallState(BaseModel):\n    a: float\n\ndef node_1(state: OverallState) -> OverallState:\n    return {\n        \"a\": \"foo\"\n    }\n\n# Build the state graph\nbuilder = StateGraph(OverallState, output=OverallState)\nbuilder.add_node(node_1)  # node_1 is the first node\nbuilder.add_edge(START, \"node_1\")  # Start the graph with node_1\nbuilder.add_edge(\"node_1\", END)  # End the graph after node_1\ngraph = builder.compile()\n\ngraph.invoke(\n    {\n        \"a\": 2.3\n    }\n)\nResult\n{'a': 'foo'}\nThis result does not match the output schema!\nExpected\nExpected a validation error", "created_at": "2024-10-02", "closed_at": null, "labels": ["maintainer"], "State": "open", "Author": "eyurtsev"}
{"issue_number": 1966, "issue_title": "DOC: Use a context object in state gives 404", "issue_body": "Issue with current documentation:\nThe section, which describes how to use the objects inside state context, disappeared. The GSearch link leads to 404 Use a context object in state\nIdea or request for content:\nWe should bring it back, as it was before.", "created_at": "2024-10-02", "closed_at": "2024-10-02", "labels": [], "State": "closed", "Author": "Stihotvor"}
{"issue_number": 1965, "issue_title": "Making JSONPlusSerializer Backward compataible", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nSelf-explainatory from the description and doesn't require code.\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nLanggraph agents cannot be deployed in production unless the JSONPlusSerializer is made backward compatible. Currently, during the checkpointing process in the Checkpointer class, we serialize both the checkpoint and metadata using JSONPlusSerializer, which includes the module path in its output. However, if the user changes the module path as part of a migration, this introduces a backward compatibility issue.\nMost applications run across multiple instances, and code deployments typically occur incrementally, instance by instance. As a result, while the module path is updated on one instance, the new path is stored during serialization. If a request with the same thread_id is processed by an instance where the code change hasn't been applied yet, the system will attempt to load the checkpoint using the new module path which instance is not aware of. This mismatch triggers an error during the get_tuple process, stating that the module path does not exist.\nTo ensure smooth transitions and prevent failures during such deployments, it's critical to make the JSONPlusSerializer handle these scenarios in a backward-compatible manner.\nSystem Info\nlangchain==0.2.16\nlangchain-community>=0.2.15\nlangchain-core>=0.2.15\nlangchain-google-community\nlangchain-openai\nlangchain-pinecone\nlanggraph==0.2.21", "created_at": "2024-10-02", "closed_at": "2024-10-02", "labels": [], "State": "closed", "Author": "Sanath91009"}
{"issue_number": 1964, "issue_title": "Sink node issue, if multiple subgraphs are used in parallel", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START, END\nfrom typing import Annotated\nimport operator\nfrom functools import partial\nfrom IPython.display import Image, display\n\nclass ChildState(TypedDict):\n    question: str\n    results: Annotated[list[str], operator.add]\n\nclass ParentState(TypedDict):\n    question: str\n    results: Annotated[list[str], operator.add]\n    \n\n    \ndef child_node(state: dict, child_result_txt:str) -> dict:\n    results = [child_result_txt]\n    return {\"results\": results}\n\ndef create_child(child_result_txt:str, child_state_cls:TypedDict)->StateGraph:\n    child = StateGraph(child_state_cls)\n    child.add_node(\"child\", partial(child_node, child_result_txt=child_result_txt))\n    child.add_edge(START, \"child\")\n    child.add_edge(\"child\", END)\n    return child.compile()\n\ndef parent_question_generation_node(state:dict)->dict:\n    return {\"question\": \"what's up?\"}\n\ndef parent_sink_node(state:dict)->dict:\n    return {\"results\": state[\"results\"]}\n\nparent = StateGraph(ParentState)\nparent.add_node(\"question_generation\", parent_question_generation_node)\nparent.add_node(\"child1\", create_child(\"child 1 result\", ChildState))\nparent.add_node(\"child2\", create_child(\"child 2 result\", ChildState))\nparent.add_node(\"sink\", parent_sink_node)\n\nparent.add_edge(START, \"question_generation\")\nparent.add_edge(\"question_generation\", \"child1\")\nparent.add_edge(\"question_generation\", \"child2\")\nparent.add_edge([\"child1\",\"child2\"], \"sink\")\nparent.add_edge(\"sink\", END)\n\nparent_graph = parent.compile()\ndisplay(Image(parent_graph.get_graph(xray=1).draw_mermaid_png()))\nstate = ParentState(question=\"hi\", results=[])\nparent_graph.invoke(state)\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nInvalidUpdateError                        Traceback (most recent call last)\nCell In[2], line 50\n     48 display(Image(parent_graph.get_graph(xray=1).draw_mermaid_png()))\n     49 state = ParentState(question=\"hi\", results=[])\n---> 50 parent_graph.invoke(state)\n\nFile ~/anaconda3/envs/rag-usecase/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1468, in Pregel.invoke(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\n   1466 else:\n   1467     chunks = []\n-> 1468 for chunk in self.stream(\n   1469     input,\n   1470     config,\n   1471     stream_mode=stream_mode,\n   1472     output_keys=output_keys,\n   1473     interrupt_before=interrupt_before,\n   1474     interrupt_after=interrupt_after,\n   1475     debug=debug,\n   1476     **kwargs,\n   1477 ):\n   1478     if stream_mode == \"values\":\n   1479         latest = chunk\n\nFile ~/anaconda3/envs/rag-usecase/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1216, in Pregel.stream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\n   1210     loop.config[\"configurable\"][CONFIG_KEY_STREAM] = loop.stream\n   1211 # Similarly to Bulk Synchronous Parallel / Pregel model\n   1212 # computation proceeds in steps, while there are channel updates\n   1213 # channel updates from step N are only visible in step N+1\n   1214 # channels are guaranteed to be immutable for the duration of the step,\n   1215 # with channel updates applied only at the transition between steps\n-> 1216 while loop.tick(\n   1217     input_keys=self.input_channels,\n   1218     interrupt_before=interrupt_before,\n   1219     interrupt_after=interrupt_after,\n   1220     manager=run_manager,\n   1221 ):\n   1222     for _ in runner.tick(\n   1223         loop.tasks.values(),\n   1224         timeout=self.step_timeout,\n   1225         retry_policy=self.retry_policy,\n   1226     ):\n   1227         # emit output\n   1228         for o in output():\n\nFile ~/anaconda3/envs/rag-usecase/lib/python3.11/site-packages/langgraph/pregel/loop.py:285, in PregelLoop.tick(self, input_keys, interrupt_after, interrupt_before, manager)\n    277     print_step_writes(\n    278         self.step,\n    279         writes,\n   (...)\n    282         else self.stream_keys,\n    283     )\n    284 # all tasks have finished\n--> 285 mv_writes = apply_writes(\n    286     self.checkpoint,\n    287     self.channels,\n    288     self.tasks.values(),\n    289     self.checkpointer_get_next_version,\n    290 )\n    291 # apply writes to managed values\n    292 for key, values in mv_writes.items():\n\nFile ~/anaconda3/envs/rag-usecase/lib/python3.11/site-packages/langgraph/pregel/algo.py:223, in apply_writes(checkpoint, channels, tasks, get_next_version)\n    221 for chan, vals in pending_writes_by_channel.items():\n    222     if chan in channels:\n--> 223         if channels[chan].update(vals) and get_next_version is not None:\n    224             checkpoint[\"channel_versions\"][chan] = get_next_version(\n    225                 max_version, channels[chan]\n    226             )\n    227         updated_channels.add(chan)\n\nFile ~/anaconda3/envs/rag-usecase/lib/python3.11/site-packages/langgraph/channels/last_value.py:38, in LastValue.update(self, values)\n     36     return False\n     37 if len(values) != 1:\n---> 38     raise InvalidUpdateError(\n     39         f\"At key '{self.key}': Can receive only one value per step. Use an Annotated key to handle multiple values.\"\n     40     )\n     42 self.value = values[-1]\n     43 return True\n\nInvalidUpdateError: At key 'question': Can receive only one value per step. Use an Annotated key to handle multiple values.\nDescription\nI am trying to have multiple subgraphs in parallel, and use a sink node. that leads to the mentioned issue.\nIf i replace in my code sample the subgraphs by usual nodes, everything works as expected.\nAs workaround, i wrote my own annotation operator as follows:\n`def own_reducer(left:str, right:str)->str:\nreturn left\nclass ChildState(TypedDict):\nquestion: Annotated[str, own_reducer]\nresults: Annotated[list[str], operator.add]\nclass ParentState(TypedDict):\nquestion: Annotated[str, own_reducer]\nresults: Annotated[list[str], operator.add]`\nwith those states/annotations, everything works again. But as I understand, this workaround should not be necessary\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #45~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Sep 11 15:25:05 UTC 2\nPython Version:  3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]\n\nPackage Information\n\nlangchain_core: 0.2.40\nlangchain: 0.2.9\nlangchain_community: 0.2.4\nlangsmith: 0.1.120\nlangchain_openai: 0.1.17\nlangchain_qdrant: 0.1.2\nlangchain_text_splitters: 0.2.2\nlanggraph: 0.2.21\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.9.5\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.7\nfastembed: Installed. No version info available.\nhttpx: 0.27.0\njsonpatch: 1.33\nlanggraph-checkpoint: 1.0.9\nnumpy: 1.26.4\nopenai: 1.42.0\norjson: 3.10.6\npackaging: 23.2\npydantic: 2.8.2\nPyYAML: 6.0.2\nqdrant-client: 1.10.1\nrequests: 2.32.3\nSQLAlchemy: 2.0.31\ntenacity: 8.5.0\ntiktoken: 0.7.0\ntyping-extensions: 4.12.2\n", "created_at": "2024-10-02", "closed_at": "2024-10-14", "labels": [], "State": "closed", "Author": "a-klos"}
{"issue_number": 1963, "issue_title": "Checkpointer crashes on `jsonplus` with StructuredTools", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\ntools = [\n    StructuredTool.from_function(\n         func=self.customer.find_customer,\n         name='find_customer',\n         description='Description',\n    ),\n    ... other tools like this one ^\n]\n\nmemory = SqliteSaver(conn=sqlite3_conn)\n\ncreate_react_agent(\n   model=openai,\n   tools=tools,\n   state_modifier=prompt,\n   checkpointer=memory,\n)\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\nFile \"/home/michael/PycharmProjects/oro-ai/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 1560, in invoke\n    for chunk in self.stream(\n  File \"/home/michael/PycharmProjects/oro-ai/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 1248, in stream\n    with SyncPregelLoop(\n  File \"/home/michael/PycharmProjects/oro-ai/.venv/lib/python3.11/site-packages/langgraph/pregel/loop.py\", line 770, in __exit__\n    return self.stack.__exit__(exc_type, exc_value, traceback)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/michael/.pyenv/versions/3.11.9/lib/python3.11/contextlib.py\", line 601, in __exit__\n    raise exc_details[1]\n  File \"/home/michael/.pyenv/versions/3.11.9/lib/python3.11/contextlib.py\", line 586, in __exit__\n    if cb(*exc_details):\n       ^^^^^^^^^^^^^^^^\n  File \"/home/michael/PycharmProjects/oro-ai/.venv/lib/python3.11/site-packages/langgraph/pregel/executor.py\", line 105, in __exit__\n    task.result()\n  File \"/home/michael/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/michael/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/home/michael/PycharmProjects/oro-ai/.venv/lib/python3.11/site-packages/langgraph/pregel/executor.py\", line 70, in done\n    task.result()\n  File \"/home/michael/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/michael/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/home/michael/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/michael/PycharmProjects/oro-ai/.venv/lib/python3.11/site-packages/langgraph/pregel/loop.py\", line 696, in _checkpointer_put_after_previous\n    prev.result()\n  File \"/home/michael/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/michael/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/home/michael/PycharmProjects/oro-ai/.venv/lib/python3.11/site-packages/langgraph/pregel/executor.py\", line 70, in done\n    task.result()\n  File \"/home/michael/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/michael/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/home/michael/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/michael/PycharmProjects/oro-ai/.venv/lib/python3.11/site-packages/langgraph/pregel/loop.py\", line 696, in _checkpointer_put_after_previous\n    prev.result()\n  File \"/home/michael/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/michael/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/home/michael/PycharmProjects/oro-ai/.venv/lib/python3.11/site-packages/langgraph/pregel/executor.py\", line 70, in done\n    task.result()\n  File \"/home/michael/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/michael/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/home/michael/.pyenv/versions/3.11.9/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/michael/PycharmProjects/oro-ai/.venv/lib/python3.11/site-packages/langgraph/pregel/loop.py\", line 698, in _checkpointer_put_after_previous\n    cast(BaseCheckpointSaver, self.checkpointer).put(\n  File \"/home/michael/PycharmProjects/oro-ai/.venv/lib/python3.11/site-packages/langgraph/checkpoint/sqlite/__init__.py\", line 399, in put\n    type_, serialized_checkpoint = self.serde.dumps_typed(checkpoint)\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/michael/PycharmProjects/oro-ai/.venv/lib/python3.11/site-packages/langgraph/checkpoint/serde/jsonplus.py\", line 193, in dumps_typed\n    return \"msgpack\", _msgpack_enc(obj)\n                      ^^^^^^^^^^^^^^^^^\n  File \"/home/michael/PycharmProjects/oro-ai/.venv/lib/python3.11/site-packages/langgraph/checkpoint/serde/jsonplus.py\", line 500, in _msgpack_enc\n    return enc.pack(data)\n           ^^^^^^^^^^^^^^\n  File \"msgpack/_packer.pyx\", line 279, in msgpack._cmsgpack.Packer.pack\n  File \"msgpack/_packer.pyx\", line 276, in msgpack._cmsgpack.Packer.pack\n  File \"msgpack/_packer.pyx\", line 265, in msgpack._cmsgpack.Packer._pack\n  File \"msgpack/_packer.pyx\", line 213, in msgpack._cmsgpack.Packer._pack_inner\n  File \"msgpack/_packer.pyx\", line 265, in msgpack._cmsgpack.Packer._pack\n  File \"msgpack/_packer.pyx\", line 213, in msgpack._cmsgpack.Packer._pack_inner\n  File \"msgpack/_packer.pyx\", line 265, in msgpack._cmsgpack.Packer._pack\n  File \"msgpack/_packer.pyx\", line 213, in msgpack._cmsgpack.Packer._pack_inner\n  File \"msgpack/_packer.pyx\", line 265, in msgpack._cmsgpack.Packer._pack\n  File \"msgpack/_packer.pyx\", line 232, in msgpack._cmsgpack.Packer._pack_inner\n  File \"msgpack/_packer.pyx\", line 265, in msgpack._cmsgpack.Packer._pack\n  File \"msgpack/_packer.pyx\", line 213, in msgpack._cmsgpack.Packer._pack_inner\n  File \"msgpack/_packer.pyx\", line 267, in msgpack._cmsgpack.Packer._pack\n  File \"/home/michael/PycharmProjects/oro-ai/.venv/lib/python3.11/site-packages/langgraph/checkpoint/serde/jsonplus.py\", line 232, in _msgpack_default\n    obj.model_dump(),\n    ^^^^^^^^^^^^^^^^\nTypeError: BaseModel.model_dump() missing 1 required positional argument: 'self'\nDescription\nSqliteSaver fails with create_react_agent() construct and StructuredTools\nActually, the same happens with the basic MemorySaver()\nVersions:\nlangchain = \"^0.3\"\nlangchain-cli = \"^0.0.31\"\nlangchain-openai = \"^0.2\"\nlanggraph = \"^0.2\"\nlanggraph-checkpoint-sqlite = \"^2.0.0\"\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #26-Ubuntu SMP PREEMPT_DYNAMIC Tue Jun 18 12:35:22 UTC 2024\nPython Version:  3.11.9 (main, Aug 12 2024, 07:24:05) [GCC 11.4.0]\n\nPackage Information\n\nlangchain_core: 0.3.7\nlangchain: 0.3.1\nlangchain_community: 0.3.1\nlangsmith: 0.1.129\nlangchain_cli: 0.0.31\nlangchain_openai: 0.2.1\nlangchain_text_splitters: 0.3.0\nlanggraph: 0.2.32\nlangserve: 0.3.0\n\nOther Dependencies\n\naiohttp: 3.10.8\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.7\nfastapi: 0.115.0\ngitpython: 3.1.43\ngritql: 0.1.5\nhttpx: 0.27.2\njsonpatch: 1.33\nlanggraph-checkpoint: 2.0.0\nlangserve[all]: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.51.0\norjson: 3.10.7\npackaging: 24.1\npydantic: 2.9.2\npydantic-settings: 2.5.2\nPyYAML: 6.0.2\nrequests: 2.32.3\nSQLAlchemy: 2.0.35\nsse-starlette: 1.8.2\ntenacity: 8.4.2\ntiktoken: 0.7.0\ntomlkit: 0.12.5\ntyper[all]: Installed. No version info available.\ntyping-extensions: 4.12.2\nuvicorn: 0.23.2\n", "created_at": "2024-10-02", "closed_at": "2024-10-02", "labels": [], "State": "closed", "Author": "michael-rubel"}
{"issue_number": 1950, "issue_title": "Passing private state with Class Node", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom typing import Annotated\nfrom langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\n\n\n# The overall state of the graph\nclass OverallState(TypedDict):\n    question: str\n    answer: str\n\n\n# This is what the node that generates the query will return\nclass QueryOutputState(TypedDict):\n    query: str\n\n\n# This is what the node that retrieves the documents will return\nclass DocumentOutputState(TypedDict):\n    docs: list[str]\n\n\n# This is what the node that generates the final answer will take in\nclass GenerateInputState(OverallState, DocumentOutputState):\n    pass\n\n\n# Node to generate query\ndef generate_query(state: OverallState) -> QueryOutputState:\n    # Replace this with real logic\n    return {\"query\": state[\"question\"][:2]}\n\n\nclass RetrieveDocumentsNode:\n    def __init__(self):\n        self.docs = []\n\n    def __call__(self, state: QueryOutputState) -> DocumentOutputState:\n        self.docs = [state[\"query\"]] * 2\n        return {\"docs\": self.docs}\n\n\n# Node to generate answer\ndef generate(state: GenerateInputState) -> OverallState:\n    return {\"answer\": \"\\n\\n\".join(state[\"docs\"] + [state[\"question\"]])}\n\n\ngraph = StateGraph(OverallState)\ngraph.add_node(generate_query)\ngraph.add_node(\"retrieve_documents\", RetrieveDocumentsNode())\ngraph.add_node(generate)\ngraph.add_edge(START, \"generate_query\")\ngraph.add_edge(\"generate_query\", \"retrieve_documents\")\ngraph.add_edge(\"retrieve_documents\", \"generate\")\ngraph.add_edge(\"generate\", END)\ngraph = graph.compile()\n\ngraph.invoke({\"question\": \"foo\"})\nError Message and Stack Trace (if applicable)\n{\n\t\"name\": \"InvalidUpdateError\",\n\t\"message\": \"Must write to at least one of ['question', 'answer', 'docs']\",\n\t\"stack\": \"---------------------------------------------------------------------------\nInvalidUpdateError                        Traceback (most recent call last)\nCell In[14], line 57\n     54 graph.add_edge(\\\"generate\\\", END)\n     55 graph = graph.compile()\n---> 57 graph.invoke({\\\"question\\\": \\\"foo\\\"})\n\nFile ~/Library/Caches/pypoetry/virtualenvs/aidenmind-syQbv40a-py3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1551, in Pregel.invoke(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\n   1549 else:\n   1550     chunks = []\n-> 1551 for chunk in self.stream(\n   1552     input,\n   1553     config,\n   1554     stream_mode=stream_mode,\n   1555     output_keys=output_keys,\n   1556     interrupt_before=interrupt_before,\n   1557     interrupt_after=interrupt_after,\n   1558     debug=debug,\n   1559     **kwargs,\n   1560 ):\n   1561     if stream_mode == \\\"values\\\":\n   1562         latest = chunk\n\nFile ~/Library/Caches/pypoetry/virtualenvs/aidenmind-syQbv40a-py3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1290, in Pregel.stream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\n   1279     # Similarly to Bulk Synchronous Parallel / Pregel model\n   1280     # computation proceeds in steps, while there are channel updates\n   1281     # channel updates from step N are only visible in step N+1\n   1282     # channels are guaranteed to be immutable for the duration of the step,\n   1283     # with channel updates applied only at the transition between steps\n   1284     while loop.tick(\n   1285         input_keys=self.input_channels,\n   1286         interrupt_before=interrupt_before_,\n   1287         interrupt_after=interrupt_after_,\n   1288         manager=run_manager,\n   1289     ):\n-> 1290         for _ in runner.tick(\n   1291             loop.tasks.values(),\n   1292             timeout=self.step_timeout,\n   1293             retry_policy=self.retry_policy,\n   1294             get_waiter=get_waiter,\n   1295         ):\n   1296             # emit output\n   1297             yield from output()\n   1298 # emit output\n\nFile ~/Library/Caches/pypoetry/virtualenvs/aidenmind-syQbv40a-py3.12/lib/python3.12/site-packages/langgraph/pregel/runner.py:56, in PregelRunner.tick(self, tasks, reraise, timeout, retry_policy, get_waiter)\n     54 t = tasks[0]\n     55 try:\n---> 56     run_with_retry(t, retry_policy)\n     57     self.commit(t, None)\n     58 except Exception as exc:\n\nFile ~/Library/Caches/pypoetry/virtualenvs/aidenmind-syQbv40a-py3.12/lib/python3.12/site-packages/langgraph/pregel/retry.py:29, in run_with_retry(task, retry_policy)\n     27 task.writes.clear()\n     28 # run the task\n---> 29 task.proc.invoke(task.input, config)\n     30 # if successful, end\n     31 break\n\nFile ~/Library/Caches/pypoetry/virtualenvs/aidenmind-syQbv40a-py3.12/lib/python3.12/site-packages/langgraph/utils/runnable.py:387, in RunnableSeq.invoke(self, input, config, **kwargs)\n    385             input = context.run(step.invoke, input, config, **kwargs)\n    386         else:\n--> 387             input = context.run(step.invoke, input, config)\n    388 # finish the root run\n    389 except BaseException as e:\n\nFile ~/Library/Caches/pypoetry/virtualenvs/aidenmind-syQbv40a-py3.12/lib/python3.12/site-packages/langgraph/utils/runnable.py:159, in RunnableCallable.invoke(self, input, config, **kwargs)\n    157     context = copy_context()\n    158     context.run(_set_config_context, child_config)\n--> 159     ret = context.run(self.func, input, **kwargs)\n    160 except BaseException as e:\n    161     run_manager.on_chain_error(e)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/aidenmind-syQbv40a-py3.12/lib/python3.12/site-packages/langgraph/pregel/write.py:85, in ChannelWrite._write(self, input, config)\n     78 def _write(self, input: Any, config: RunnableConfig) -> None:\n     79     writes = [\n     80         ChannelWriteEntry(write.channel, input, write.skip_none, write.mapper)\n     81         if isinstance(write, ChannelWriteEntry) and write.value is PASSTHROUGH\n     82         else write\n     83         for write in self.writes\n     84     ]\n---> 85     self.do_write(\n     86         config,\n     87         writes,\n     88         self.require_at_least_one_of if input is not None else None,\n     89     )\n     90     return input\n\nFile ~/Library/Caches/pypoetry/virtualenvs/aidenmind-syQbv40a-py3.12/lib/python3.12/site-packages/langgraph/pregel/write.py:138, in ChannelWrite.do_write(config, writes, require_at_least_one_of)\n    136 if require_at_least_one_of is not None:\n    137     if not {chan for chan, _ in filtered} & set(require_at_least_one_of):\n--> 138         raise InvalidUpdateError(\n    139             f\\\"Must write to at least one of {require_at_least_one_of}\\\"\n    140         )\n    141 write: TYPE_SEND = config[CONF][CONFIG_KEY_SEND]\n    142 write(sends + filtered)\n\nInvalidUpdateError: Must write to at least one of ['question', 'answer', 'docs']\"\n}\nDescription\nI copied the example from https://langchain-ai.github.io/langgraph/how-tos/pass_private_state/#define-and-use-the-graph\nBut instead of function Node I used a class Node to demonstrate it doesn't work as expected. (Must write to at least in global defined State)\n-> We should be able to achieve similar results with Class Node than with Function Node\nSystem Info\nlanggraph = \"0.2.28\"\nlangchain-core = \"0.3.5\"\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6030\nPython Version:  3.12.4 (main, Jun  6 2024, 18:26:44) [Clang 15.0.0 (clang-1500.3.9.4)]\n\nPackage Information\n\nlangchain_core: 0.3.6\nlangchain: 0.3.1\nlangsmith: 0.1.129\nlangchain_anthropic: 0.2.1\nlangchain_openai: 0.2.1\nlangchain_text_splitters: 0.3.0\nlanggraph: 0.2.28\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.10.8\nanthropic: 0.34.2\nasync-timeout: Installed. No version info available.\ndefusedxml: 0.7.1\nhttpx: 0.27.2\njsonpatch: 1.33\nlanggraph-checkpoint: 1.0.12\nnumpy: 1.26.4\nopenai: 1.50.2\norjson: 3.10.7\npackaging: 24.1\npydantic: 2.9.2\nPyYAML: 6.0.2\nrequests: 2.32.3\nSQLAlchemy: 2.0.35\ntenacity: 8.5.0\ntiktoken: 0.7.0\ntyping-extensions: 4.12.2\n", "created_at": "2024-10-01", "closed_at": "2024-10-04", "labels": [], "State": "closed", "Author": "bdjafer"}
{"issue_number": 1937, "issue_title": "`ImportError: cannot import name 'V' from 'langgraph.store.base' (/usr/local/lib/python3.9/site-packages/langgraph/store/base/__init__.py)` in langgraph 0.2.16", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langgraph.prebuilt import create_react_agent\nError Message and Stack Trace (if applicable)\n% docker run -it python:3.9 bash\nroot@c4eb11becd2c:/# pip install langgraph==0.2.16\nCollecting langgraph==0.2.16\n  Downloading langgraph-0.2.16-py3-none-any.whl (91 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 91.1/91.1 kB 641.3 kB/s eta 0:00:00\nCollecting langchain-core<0.3,>=0.2.27\n  Downloading langchain_core-0.2.41-py3-none-any.whl (397 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 397.0/397.0 kB 673.2 kB/s eta 0:00:00\nCollecting langgraph-checkpoint<2.0.0,>=1.0.2\n  Downloading langgraph_checkpoint-1.0.14-py3-none-any.whl (22 kB)\nCollecting packaging<25,>=23.2\n  Downloading packaging-24.1-py3-none-any.whl (53 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 54.0/54.0 kB 803.9 kB/s eta 0:00:00\nCollecting pydantic<3,>=1\n  Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 434.9/434.9 kB 751.0 kB/s eta 0:00:00\nCollecting langsmith<0.2.0,>=0.1.112\n  Downloading langsmith-0.1.129-py3-none-any.whl (292 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 292.2/292.2 kB 718.9 kB/s eta 0:00:00\nCollecting tenacity!=8.4.0,<9.0.0,>=8.1.0\n  Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\nCollecting typing-extensions>=4.7\n  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nCollecting jsonpatch<2.0,>=1.33\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nCollecting PyYAML>=5.3\n  Downloading PyYAML-6.0.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (720 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 720.9/720.9 kB 752.9 kB/s eta 0:00:00\nCollecting msgpack<2.0.0,>=1.1.0\n  Downloading msgpack-1.1.0-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (371 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 371.3/371.3 kB 752.5 kB/s eta 0:00:00\nCollecting jsonpointer>=1.9\n  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\nCollecting httpx<1,>=0.23.0\n  Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 76.4/76.4 kB 806.8 kB/s eta 0:00:00\nCollecting orjson<4.0.0,>=3.9.14\n  Downloading orjson-3.10.7-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (147 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 147.9/147.9 kB 814.9 kB/s eta 0:00:00\nCollecting requests<3,>=2\n  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 64.9/64.9 kB 779.7 kB/s eta 0:00:00\nCollecting pydantic-core==2.23.4\n  Downloading pydantic_core-2.23.4-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.8 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.8/1.8 MB 752.9 kB/s eta 0:00:00\nCollecting annotated-types>=0.6.0\n  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\nCollecting certifi\n  Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 167.3/167.3 kB 775.5 kB/s eta 0:00:00\nCollecting httpcore==1.*\n  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 77.9/77.9 kB 807.9 kB/s eta 0:00:00\nCollecting sniffio\n  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\nCollecting anyio\n  Downloading anyio-4.6.0-py3-none-any.whl (89 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 89.6/89.6 kB 733.7 kB/s eta 0:00:00\nCollecting idna\n  Downloading idna-3.10-py3-none-any.whl (70 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 70.4/70.4 kB 819.4 kB/s eta 0:00:00\nCollecting h11<0.15,>=0.13\n  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 58.3/58.3 kB 760.5 kB/s eta 0:00:00\nCollecting charset-normalizer<4,>=2\n  Downloading charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (138 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 138.3/138.3 kB 731.5 kB/s eta 0:00:00\nCollecting urllib3<3,>=1.21.1\n  Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 126.3/126.3 kB 697.2 kB/s eta 0:00:00\nCollecting exceptiongroup>=1.0.2\n  Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\nInstalling collected packages: urllib3, typing-extensions, tenacity, sniffio, PyYAML, packaging, orjson, msgpack, jsonpointer, idna, h11, exceptiongroup, charset-normalizer, certifi, annotated-types, requests, pydantic-core, jsonpatch, httpcore, anyio, pydantic, httpx, langsmith, langchain-core, langgraph-checkpoint, langgraph\nSuccessfully installed PyYAML-6.0.2 annotated-types-0.7.0 anyio-4.6.0 certifi-2024.8.30 charset-normalizer-3.3.2 exceptiongroup-1.2.2 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 idna-3.10 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.2.41 langgraph-0.2.16 langgraph-checkpoint-1.0.14 langsmith-0.1.129 msgpack-1.1.0 orjson-3.10.7 packaging-24.1 pydantic-2.9.2 pydantic-core-2.23.4 requests-2.32.3 sniffio-1.3.1 tenacity-8.5.0 typing-extensions-4.12.2 urllib3-2.2.3\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n[notice] A new release of pip is available: 23.0.1 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n\nroot@c4eb11becd2c:/# python\nPython 3.9.18 (main, Jan 17 2024, 05:48:03) \n[GCC 12.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> from langgraph.prebuilt import create_react_agent\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/local/lib/python3.9/site-packages/langgraph/prebuilt/__init__.py\", line 3, in <module>\n    from langgraph.prebuilt.chat_agent_executor import create_react_agent\n  File \"/usr/local/lib/python3.9/site-packages/langgraph/prebuilt/chat_agent_executor.py\", line 23, in <module>\n    from langgraph.graph import END, StateGraph\n  File \"/usr/local/lib/python3.9/site-packages/langgraph/graph/__init__.py\", line 1, in <module>\n    from langgraph.graph.graph import END, START, Graph\n  File \"/usr/local/lib/python3.9/site-packages/langgraph/graph/graph.py\", line 37, in <module>\n    from langgraph.pregel import Channel, Pregel\n  File \"/usr/local/lib/python3.9/site-packages/langgraph/pregel/__init__.py\", line 91, in <module>\n    from langgraph.pregel.loop import AsyncPregelLoop, SyncPregelLoop\n  File \"/usr/local/lib/python3.9/site-packages/langgraph/pregel/loop.py\", line 86, in <module>\n    from langgraph.store.batch import AsyncBatchedStore\n  File \"/usr/local/lib/python3.9/site-packages/langgraph/store/batch.py\", line 4, in <module>\n    from langgraph.store.base import BaseStore, V\nImportError: cannot import name 'V' from 'langgraph.store.base' (/usr/local/lib/python3.9/site-packages/langgraph/store/base/__init__.py)\nDescription\nfrom langgraph.prebuilt import create_react_agent fails with ImportError: cannot import name 'V' from 'langgraph.store.base'.\nSystem Info\n# pip freeze | grep lang\nlangchain-core==0.2.41\nlanggraph==0.2.16\nlanggraph-checkpoint==1.0.14\nlangsmith==0.1.129\n", "created_at": "2024-10-01", "closed_at": "2024-10-01", "labels": [], "State": "closed", "Author": "harupy"}
{"issue_number": 1933, "issue_title": "can't deploy langgraph 0.2.31 to langgrah cloud", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nlangchain==0.3.1\nlangchain-core==0.3.7\nlangchain_openai==0.2.1\nlangchain-voyageai==0.1.2\nlangchain_cohere==0.3.0\nlangchain_experimental==0.3.2\nlanggraph==0.2.31\nlangsmith==0.1.129\nlangchain_pinecone==0.2.0\nlanggraph-checkpoint-postgres==1.0.11\nError Message and Stack Trace (if applicable)\n01/10/2024, 01:21:36\nStep #11:  > [3/7] RUN PYTHONDONTWRITEBYTECODE=1 pip install --no-cache-dir -c /api/constraints.txt -r /deps/__outer_arc-ai-api/src/requirements.txt:\n01/10/2024, 01:21:36\nStep #11: 3.757     The user requested (constraint) langgraph<0.2.29,>=0.2.18\n01/10/2024, 01:21:36\nDescription\nI got this error from langgraph cloud while deploying, where this \"constraints.txt\" comes from ?\nHow to resolve it?\nSystem Info\nwindow\npython 3.12.5", "created_at": "2024-10-01", "closed_at": "2024-10-02", "labels": [], "State": "closed", "Author": "junan-trustarc"}
{"issue_number": 1916, "issue_title": "Validation Errors When Passing Graph State To Tools", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nimport json\nfrom typing import Annotated, Sequence, TypedDict\n\nfrom langchain_core.tools import tool\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain_core.messages import ToolMessage, SystemMessage, BaseMessage\n\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.prebuilt import InjectedState\nfrom langgraph.graph.message import add_messages\n\n\nclass AgentState(TypedDict):\n    \"\"\"The state of the agent.\"\"\"\n    messages: Annotated[Sequence[BaseMessage], add_messages]\n\n\n# Define a dummy get_weather tool with no state argument\n@tool(parse_docstring=True)\ndef get_weather(location: str):\n    \"\"\"\n    Call to get the weather from a specific location.\n\n    Args:\n        location: The location to get the weather from.\n    \"\"\"\n    # This is a placeholder for the actual implementation\n    # Don't let the LLM know this though \ud83d\ude0a\n    if any([city in location.lower() for city in ['sf', 'san francisco']]):\n        return \"It's sunny in San Francisco, but you better look out if you're a Gemini \ud83d\ude08.\"\n    else:\n        return f\"I am not sure what the weather is in {location}\"\n\n\n# ONLY add the (injected) state argument to this tool to show its broken\n@tool(parse_docstring=True)\ndef get_weather_with_state(\n    location: str,\n    state: Annotated[dict, InjectedState]\n):\n    \"\"\"\n    Call to get the BROKEN weather from a specific location. Use this if a user asks for BROKEN weather only, otherwise use get_weather.\n\n    Args:\n        location: The location to get the weather from.\n    \"\"\"\n    # This is a placeholder for the actual implementation\n    # Don't let the LLM know this though \ud83d\ude0a\n    print('get_weather state: ', state)\n\n    if any([city in location.lower() for city in ['sf', 'san francisco']]):\n        return \"It's sunny in San Francisco, but you better look out if you're a Gemini \ud83d\ude08.\"\n    else:\n        return f\"I am not sure what the weather is in {location}\"\n\n\n# Define our tools\ntools = [get_weather, get_weather_with_state]\ntools_by_name = {tool.name: tool for tool in tools}\n\n# Define our model\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\nmodel = model.bind_tools(tools)\n\n\n# Define our tool node\ndef tool_node(state: AgentState):\n    outputs = []\n    for tool_call in state['messages'][-1].tool_calls:\n        tool_result = tools_by_name[tool_call[\"name\"]].invoke(\n            tool_call[\"args\"]\n        )\n        outputs.append(\n            ToolMessage(\n                content=json.dumps(tool_result),\n                name=tool_call[\"name\"],\n                tool_call_id=tool_call[\"id\"],\n            )\n        )\n    return {\"messages\": outputs}\n\n\n# Define the node that calls the model\ndef call_model(\n    state: AgentState,\n    config: RunnableConfig,\n):\n    # this is similar to customizing the create_react_agent with state_modifier, but is a lot more flexible\n    system_prompt = SystemMessage(\n        \"You are a helpful AI assistant, please respond to the users query to the best of your ability!\")\n\n    response = model.invoke([system_prompt] + state['messages'], config)\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": [response]}\n\n\n# Define the conditional edge that determines whether to continue or not\ndef should_continue(state: AgentState):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    # If there is no function call, then we finish\n    if not last_message.tool_calls:\n        return \"end\"\n    # Otherwise if there is, we continue\n    else:\n        return \"continue\"\n\n\n# Define a new graph\nworkflow = StateGraph(AgentState)\n\n# Define the two nodes we will cycle between\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"tools\", tool_node)\n\n# Set the entrypoint as `agent`\n# This means that this node is the first one called\nworkflow.set_entry_point(\"agent\")\n\n# We now add a conditional edge\nworkflow.add_conditional_edges(\n    # First, we define the start node. We use `agent`.\n    # This means these are the edges taken after the `agent` node is called.\n    \"agent\",\n    # Next, we pass in the function that will determine which node is called next.\n    should_continue,\n    # Finally we pass in a mapping.\n    # The keys are strings, and the values are other nodes.\n    # END is a special node marking that the graph should finish.\n    # What will happen is we will call `should_continue`, and then the output of that\n    # will be matched against the keys in this mapping.\n    # Based on which one it matches, that node will then be called.\n    {\n        # If `tools`, then we call the tool node.\n        \"continue\": \"tools\",\n        # Otherwise we finish.\n        \"end\": END,\n    },\n)\n\n# We now add a normal edge from `tools` to `agent`.\n# This means that after `tools` is called, `agent` node is called next.\nworkflow.add_edge(\"tools\", \"agent\")\n\n# Now we can compile and visualize our graph\ngraph = workflow.compile()\n\n\n# Helper function for formatting the stream nicely\ndef print_stream(stream):\n    for s in stream:\n        message = s[\"messages\"][-1]\n        if isinstance(message, tuple):\n            print(message)\n        else:\n            message.pretty_print()\n\n\n# invoke the stateless (working) tool first to show it works\ninputs = {\"messages\": [(\"user\", \"what is the weather in sf\")]}\nprint_stream(graph.stream(inputs, stream_mode=\"values\"))\n\n# invoke the stateful (non-working) tool next to show its broken (pydantic validation errors)\ninputs = {\"messages\": [(\"user\", \"what is the BROKEN weather in sf\")]}\nprint_stream(graph.stream(inputs, stream_mode=\"values\"))\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\nCell In[1], line 166\n    164 # invoke the stateful (non-working) tool next to show its broken (pydantic validation errors)\n    165 inputs = {\"messages\": [(\"user\", \"what is the BROKEN weather in sf\")]}\n--> 166 print_stream(graph.stream(inputs, stream_mode=\"values\"))\n\nCell In[1], line 152\n    151 def print_stream(stream):\n--> 152     for s in stream:\n    153         message = s[\"messages\"][-1]\n    154         if isinstance(message, tuple):\n\nFile ~/Library/Caches/pypoetry/virtualenvs/ask-harmony-FAOMPRZw-py3.11/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1298, in Pregel.stream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\n   1287     # Similarly to Bulk Synchronous Parallel / Pregel model\n   1288     # computation proceeds in steps, while there are channel updates\n   1289     # channel updates from step N are only visible in step N+1\n   1290     # channels are guaranteed to be immutable for the duration of the step,\n   1291     # with channel updates applied only at the transition between steps\n   1292     while loop.tick(\n   1293         input_keys=self.input_channels,\n   1294         interrupt_before=interrupt_before_,\n   1295         interrupt_after=interrupt_after_,\n   1296         manager=run_manager,\n   1297     ):\n-> 1298         for _ in runner.tick(\n   1299             loop.tasks.values(),\n   1300             timeout=self.step_timeout,\n   1301             retry_policy=self.retry_policy,\n   1302             get_waiter=get_waiter,\n   1303         ):\n   1304             # emit output\n   1305             yield from output()\n   1306 # emit output\n\nFile ~/Library/Caches/pypoetry/virtualenvs/ask-harmony-FAOMPRZw-py3.11/lib/python3.11/site-packages/langgraph/pregel/runner.py:56, in PregelRunner.tick(self, tasks, reraise, timeout, retry_policy, get_waiter)\n     54 t = tasks[0]\n     55 try:\n---> 56     run_with_retry(t, retry_policy)\n     57     self.commit(t, None)\n     58 except Exception as exc:\n\nFile ~/Library/Caches/pypoetry/virtualenvs/ask-harmony-FAOMPRZw-py3.11/lib/python3.11/site-packages/langgraph/pregel/retry.py:29, in run_with_retry(task, retry_policy)\n     27 task.writes.clear()\n     28 # run the task\n---> 29 task.proc.invoke(task.input, config)\n     30 # if successful, end\n     31 break\n\nFile ~/Library/Caches/pypoetry/virtualenvs/ask-harmony-FAOMPRZw-py3.11/lib/python3.11/site-packages/langgraph/utils/runnable.py:405, in RunnableSeq.invoke(self, input, config, **kwargs)\n    403 context.run(_set_config_context, config)\n    404 if i == 0:\n--> 405     input = context.run(step.invoke, input, config, **kwargs)\n    406 else:\n    407     input = context.run(step.invoke, input, config)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/ask-harmony-FAOMPRZw-py3.11/lib/python3.11/site-packages/langgraph/utils/runnable.py:181, in RunnableCallable.invoke(self, input, config, **kwargs)\n    179 else:\n    180     context.run(_set_config_context, config)\n--> 181     ret = context.run(self.func, input, **kwargs)\n    182 if isinstance(ret, Runnable) and self.recurse:\n    183     return ret.invoke(input, config)\n\nCell In[1], line 71\n     69 outputs = []\n     70 for tool_call in state['messages'][-1].tool_calls:\n---> 71     tool_result = tools_by_name[tool_call[\"name\"]].invoke(\n     72         tool_call[\"args\"]\n     73     )\n     74     outputs.append(\n     75         ToolMessage(\n     76             content=json.dumps(tool_result),\n   (...)\n     79         )\n     80     )\n     81 return {\"messages\": outputs}\n\nFile ~/Library/Caches/pypoetry/virtualenvs/ask-harmony-FAOMPRZw-py3.11/lib/python3.11/site-packages/langchain_core/tools/base.py:485, in BaseTool.invoke(self, input, config, **kwargs)\n    478 def invoke(\n    479     self,\n    480     input: Union[str, dict, ToolCall],\n    481     config: Optional[RunnableConfig] = None,\n    482     **kwargs: Any,\n    483 ) -> Any:\n    484     tool_input, kwargs = _prep_run_args(input, config, **kwargs)\n--> 485     return self.run(tool_input, **kwargs)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/ask-harmony-FAOMPRZw-py3.11/lib/python3.11/site-packages/langchain_core/tools/base.py:688, in BaseTool.run(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\n    686 if error_to_raise:\n    687     run_manager.on_tool_error(error_to_raise)\n--> 688     raise error_to_raise\n    689 output = _format_output(content, artifact, tool_call_id, self.name, status)\n    690 run_manager.on_tool_end(output, color=color, name=self.name, **kwargs)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/ask-harmony-FAOMPRZw-py3.11/lib/python3.11/site-packages/langchain_core/tools/base.py:651, in BaseTool.run(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\n    649 context = copy_context()\n    650 context.run(_set_config_context, child_config)\n--> 651 tool_args, tool_kwargs = self._to_args_and_kwargs(tool_input)\n    652 if signature(self._run).parameters.get(\"run_manager\"):\n    653     tool_kwargs[\"run_manager\"] = run_manager\n\nFile ~/Library/Caches/pypoetry/virtualenvs/ask-harmony-FAOMPRZw-py3.11/lib/python3.11/site-packages/langchain_core/tools/base.py:574, in BaseTool._to_args_and_kwargs(self, tool_input)\n    573 def _to_args_and_kwargs(self, tool_input: Union[str, dict]) -> tuple[tuple, dict]:\n--> 574     tool_input = self._parse_input(tool_input)\n    575     # For backwards compatibility, if run_input is a string,\n    576     # pass as a positional argument.\n    577     if isinstance(tool_input, str):\n\nFile ~/Library/Caches/pypoetry/virtualenvs/ask-harmony-FAOMPRZw-py3.11/lib/python3.11/site-packages/langchain_core/tools/base.py:516, in BaseTool._parse_input(self, tool_input)\n    514 if input_args is not None:\n    515     if issubclass(input_args, BaseModel):\n--> 516         result = input_args.model_validate(tool_input)\n    517         result_dict = result.model_dump()\n    518     elif issubclass(input_args, BaseModelV1):\n\nFile ~/Library/Caches/pypoetry/virtualenvs/ask-harmony-FAOMPRZw-py3.11/lib/python3.11/site-packages/pydantic/main.py:596, in BaseModel.model_validate(cls, obj, strict, from_attributes, context)\n    594 # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    595 __tracebackhide__ = True\n--> 596 return cls.__pydantic_validator__.validate_python(\n    597     obj, strict=strict, from_attributes=from_attributes, context=context\n    598 )\n\nValidationError: 1 validation error for get_weather_with_state\nstate\n  Field required [type=missing, input_value={'location': 'San Francisco'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\nDescription\nWhen using state: Annotated[dict, InjectedState] in a tool argument and omitting it from the docstring, the state still throws a validation exception.\nI copied the example code directly from the create react agent from scratch example, and added a 2nd tool which injects the state. There are no other differences in the tools, and you can verify the schema's with get_input_schema and tool_call_schema properly omit the state key as expected.\n\nHowever, when the model calls the tool, it throws a pydantic validation exception saying the state key is missing.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.5.0: Wed May  1 20:14:38 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T6020\nPython Version:  3.11.7 (main, May  6 2024, 13:40:41) [Clang 15.0.0 (clang-1500.3.9.4)]\n\nPackage Information\n\nlangchain_core: 0.3.6\nlangsmith: 0.1.129\nlangchain_openai: 0.2.1\nlanggraph: 0.2.29\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nhttpx: 0.27.2\njsonpatch: 1.33\nlanggraph-checkpoint: 1.0.13\nopenai: 1.50.2\norjson: 3.10.7\npackaging: 24.1\npydantic: 2.9.2\nPyYAML: 6.0.2\nrequests: 2.32.3\ntenacity: 8.5.0\ntiktoken: 0.7.0\ntyping-extensions: 4.12.2\n", "created_at": "2024-09-30", "closed_at": "2024-09-30", "labels": [], "State": "closed", "Author": "fletchertyler914"}
{"issue_number": 1914, "issue_title": "Missing langgraph.store.base", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nN/A\nError Message and Stack Trace (if applicable)\n\"/opt/homebrew/lib/python3.10/site-packages/langgraph/checkpoint/serde/jsonplus.py\", line 29, in <module>\n    from langgraph.store.base import Item\nModuleNotFoundError: No module named 'langgraph.store.base'\nDescription\nI encountered a ModuleNotFoundError when running a script that imports langgraph.store.base. It seems that the module is missing from the installed package, even though langgraph is installed.\nSystem Info\nlanggraph==0.2.29\nlanggraph-checkpoint==1.0.13", "created_at": "2024-09-30", "closed_at": "2024-09-30", "labels": [], "State": "closed", "Author": "smannathan"}
{"issue_number": 1909, "issue_title": "PrivateState not working if the node is activated by a Send object", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom typing import List, TypedDict, Annotated\nfrom pydantic import BaseModel\nfrom langgraph.types import Send\nfrom langgraph.graph import StateGraph, START, END\nimport operator\n\n\ndef router(state) -> List[Send]:\n    return [\n        Send(\n            \"node_activate_by_Send\",\n            {\"var\": var},\n        )\n        for var in state[\"list_of_vars\"]\n    ]\n\nclass PrivateState(BaseModel):\n    private_var: Annotated[List[str], operator.add]\n\ndef node_activate_by_Send(state) -> PrivateState:\n    return {\"private_var\": [state[\"var\"]]}\n\nclass State(TypedDict):\n    list_of_vars: List[str]\n\ng = StateGraph(State)\n\ng.add_conditional_edges(START, router)\n\ng.add_node(node_activate_by_Send)\ng.add_edge(\"node_activate_by_Send\", END)\n\napp = g.compile()\n\napp.invoke({\"list_of_vars\": [\"a\", \"b\", \"c\"]})\nError Message and Stack Trace (if applicable)\nInvalidUpdateError: Must write to at least one of ['list_of_vars']\nDescription\nPrivateState is not working when the node is activated by a Send object.\nSystem Info\nlangchain==0.3.1\nlangchain-anthropic==0.2.1\nlangchain-core==0.3.6\nlangchain-openai==0.2.1\nlangchain-text-splitters==0.3.0\nlanggraph==0.2.28", "created_at": "2024-09-30", "closed_at": "2024-09-30", "labels": [], "State": "closed", "Author": "minki-j"}
{"issue_number": 1878, "issue_title": "AsyncPostgresSaver - unsupported Unicode escape sequence", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\nfrom langgraph.prebuilt import create_react_agent\n\nfrom typing import Literal\n\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\n\n@tool\ndef get_weather(city: Literal[\"nyc\", \"sf\"]):\n    \"\"\"Use this to get weather information.\"\"\"\n    if city == \"nyc\":\n        return \"It might be cloudy in nyc\"\n    elif city == \"sf\":\n        return \"It's always sunny in sf\"\n    else:\n        raise AssertionError(\"Unknown city\")\n\ntools = [get_weather]\nmodel = ChatOpenAI(model_name=\"gpt-4o-mini\")\n\nasync with AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer:\n    graph = create_react_agent(model, tools=tools, checkpointer=checkpointer)\n    config = {\"configurable\": {\"thread_id\": \"6\"}}\n    res = await graph.ainvoke(\n        {\"messages\": [(\"human\", \"what's the \\x00 weather in nyc\")]}, config\n    )\n    checkpoint_tuples = [c async for c in checkpointer.alist(config)]\nError Message and Stack Trace (if applicable)\nUntranslatableCharacter                   Traceback (most recent call last)\nCell In[15], line 4\n      2 graph = create_react_agent(model, tools=tools, checkpointer=checkpointer)\n      3 config = {\"configurable\": {\"thread_id\": \"6\"}}\n----> 4 res = await graph.ainvoke(\n      5     {\"messages\": [(\"human\", \"what's the \\x00 weather in nyc\")]}, config\n      6 )\n      7 checkpoint_tuples = [c async for c in checkpointer.alist(config)]\n\nFile /venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1592, in Pregel.ainvoke(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\n   1590 else:\n   1591     chunks = []\n-> 1592 async for chunk in self.astream(\n   1593     input,\n   1594     config,\n   1595     stream_mode=stream_mode,\n   1596     output_keys=output_keys,\n   1597     interrupt_before=interrupt_before,\n   1598     interrupt_after=interrupt_after,\n   1599     debug=debug,\n   1600     **kwargs,\n   1601 ):\n   1602     if stream_mode == \"values\":\n   1603         latest = chunk\n\nFile /venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1443, in Pregel.astream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\n   1439 if \"custom\" in stream_modes:\n   1440     config[CONF][CONFIG_KEY_STREAM_WRITER] = lambda c: stream.put_nowait(\n   1441         ((), \"custom\", c)\n   1442     )\n-> 1443 async with AsyncPregelLoop(\n   1444     input,\n   1445     stream=StreamProtocol(stream.put_nowait, stream_modes),\n   1446     config=config,\n   1447     store=self.store,\n   1448     checkpointer=checkpointer,\n   1449     nodes=self.nodes,\n   1450     specs=self.channels,\n   1451     output_keys=output_keys,\n   1452     stream_keys=self.stream_channels_asis,\n   1453 ) as loop:\n   1454     # create runner\n   1455     runner = PregelRunner(\n   1456         submit=loop.submit,\n   1457         put_writes=loop.put_writes,\n   1458         use_astream=do_stream is not None,\n   1459     )\n   1460     # enable subgraph streaming\n\nFile /venv/lib/python3.12/site-packages/langgraph/pregel/loop.py:883, in AsyncPregelLoop.__aexit__(self, exc_type, exc_value, traceback)\n    876 async def __aexit__(\n    877     self,\n    878     exc_type: Optional[Type[BaseException]],\n   (...)\n    881 ) -> Optional[bool]:\n    882     # unwind stack\n--> 883     return await asyncio.shield(\n    884         self.stack.__aexit__(exc_type, exc_value, traceback)\n    885     )\n\nFile /usr/local/lib/python3.12/contextlib.py:754, in AsyncExitStack.__aexit__(self, *exc_details)\n    750 try:\n    751     # bare \"raise exc_details[1]\" replaces our carefully\n    752     # set-up context\n    753     fixed_ctx = exc_details[1].__context__\n--> 754     raise exc_details[1]\n    755 except BaseException:\n    756     exc_details[1].__context__ = fixed_ctx\n\nFile /usr/local/lib/python3.12/contextlib.py:737, in AsyncExitStack.__aexit__(self, *exc_details)\n    735     cb_suppress = cb(*exc_details)\n    736 else:\n--> 737     cb_suppress = await cb(*exc_details)\n    739 if cb_suppress:\n    740     suppressed_exc = True\n\nFile /venv/lib/python3.12/site-packages/langgraph/pregel/executor.py:179, in AsyncBackgroundExecutor.__aexit__(self, exc_type, exc_value, traceback)\n    177 try:\n    178     if exc := task.exception():\n--> 179         raise exc\n    180 except asyncio.CancelledError:\n    181     pass\n\nFile /venv/lib/python3.12/site-packages/langgraph/pregel/loop.py:806, in AsyncPregelLoop._checkpointer_put_after_previous(self, prev, config, checkpoint, metadata, new_versions)\n    804 try:\n    805     if prev is not None:\n--> 806         await prev\n    807 finally:\n    808     await cast(BaseCheckpointSaver, self.checkpointer).aput(\n    809         config, checkpoint, metadata, new_versions\n    810     )\n\nFile /venv/lib/python3.12/site-packages/langgraph/pregel/loop.py:806, in AsyncPregelLoop._checkpointer_put_after_previous(self, prev, config, checkpoint, metadata, new_versions)\n    804 try:\n    805     if prev is not None:\n--> 806         await prev\n    807 finally:\n    808     await cast(BaseCheckpointSaver, self.checkpointer).aput(\n    809         config, checkpoint, metadata, new_versions\n    810     )\n\n    [... skipping similar frames: AsyncPregelLoop._checkpointer_put_after_previous at line 806 (1 times)]\n\nFile /venv/lib/python3.12/site-packages/langgraph/pregel/loop.py:806, in AsyncPregelLoop._checkpointer_put_after_previous(self, prev, config, checkpoint, metadata, new_versions)\n    804 try:\n    805     if prev is not None:\n--> 806         await prev\n    807 finally:\n    808     await cast(BaseCheckpointSaver, self.checkpointer).aput(\n    809         config, checkpoint, metadata, new_versions\n    810     )\n\nFile /venv/lib/python3.12/site-packages/langgraph/pregel/loop.py:808, in AsyncPregelLoop._checkpointer_put_after_previous(self, prev, config, checkpoint, metadata, new_versions)\n    806         await prev\n    807 finally:\n--> 808     await cast(BaseCheckpointSaver, self.checkpointer).aput(\n    809         config, checkpoint, metadata, new_versions\n    810     )\n\nFile /venv/lib/python3.12/site-packages/langgraph/checkpoint/postgres/aio.py:267, in AsyncPostgresSaver.aput(self, config, checkpoint, metadata, new_versions)\n    258 copy = checkpoint.copy()\n    259 next_config = {\n    260     \"configurable\": {\n    261         \"thread_id\": thread_id,\n   (...)\n    264     }\n    265 }\n--> 267 async with self._cursor(pipeline=True) as cur:\n    268     await cur.executemany(\n    269         self.UPSERT_CHECKPOINT_BLOBS_SQL,\n    270         await asyncio.to_thread(\n   (...)\n    276         ),\n    277     )\n    278     await cur.execute(\n    279         self.UPSERT_CHECKPOINTS_SQL,\n    280         (\n   (...)\n    287         ),\n    288     )\n\nFile /usr/local/lib/python3.12/contextlib.py:217, in _AsyncGeneratorContextManager.__aexit__(self, typ, value, traceback)\n    215 if typ is None:\n    216     try:\n--> 217         await anext(self.gen)\n    218     except StopAsyncIteration:\n    219         return False\n\nFile /venv/lib/python3.12/site-packages/langgraph/checkpoint/postgres/aio.py:340, in AsyncPostgresSaver._cursor(self, pipeline)\n    336             await self.pipe.sync()\n    337 elif pipeline:\n    338     # a connection not in pipeline mode can only be used by one\n    339     # thread/coroutine at a time, so we acquire a lock\n--> 340     async with self.lock, conn.pipeline(), conn.cursor(\n    341         binary=True, row_factory=dict_row\n    342     ) as cur:\n    343         yield cur\n    344 else:\n\nFile /usr/local/lib/python3.12/contextlib.py:217, in _AsyncGeneratorContextManager.__aexit__(self, typ, value, traceback)\n    215 if typ is None:\n    216     try:\n--> 217         await anext(self.gen)\n    218     except StopAsyncIteration:\n    219         return False\n\nFile /venv/lib/python3.12/site-packages/psycopg/connection_async.py:398, in AsyncConnection.pipeline(self)\n    395         pipeline = self._pipeline = AsyncPipeline(self)\n    397 try:\n--> 398     async with pipeline:\n    399         yield pipeline\n    400 finally:\n\nFile /venv/lib/python3.12/site-packages/psycopg/_pipeline.py:266, in AsyncPipeline.__aexit__(self, exc_type, exc_val, exc_tb)\n    264         logger.warning(\"error ignored terminating %r: %s\", self, exc2)\n    265     else:\n--> 266         raise exc2.with_traceback(None)\n    267 finally:\n    268     self._exit(exc_val)\nDescription\nUntranslatableCharacter Exception When Processing Input with Null Character (\\x00) or ('x0000)\nDescription\nI encountered an UntranslatableCharacter exception when attempting to process input containing a null character (\\x00) using the langgraph library. The exception occurs when invoking the graph with a message that includes the null character.\nTo Reproduce\n\n\nSet up the environment:\n\nPython version: 3.12\nEnsure langgraph and its dependencies are installed (including psycopg for PostgreSQL interaction).\n\n\n\nUse the following code snippet:\ngraph = create_react_agent(model, tools=tools, checkpointer=checkpointer)\nconfig = {\"configurable\": {\"thread_id\": \"6\"}}\nres = await graph.ainvoke(\n    {\"messages\": [(\"human\", \"what's the \\x00 weather in nyc\")]}, config\n)\ncheckpoint_tuples = [c async for c in checkpointer.alist(config)]\n\n\nRun the code and observe the exception.\n\n\nExpected Behavior\nThe system should handle inputs containing null characters gracefully, either by processing the input correctly or by providing a clear and informative error message indicating that null characters are not supported.\nActual Behavior\nAn UntranslatableCharacter exception is raised, and the process fails with the following stack trace:\nSystem Info\nlanggraph==0.2.26\nlanggraph-checkpoint==1.0.11\nlanggraph-checkpoint-postgres==1.0.8\nlangchain==0.3.0\nlangchain-community==0.3.0\nlangchain-core==0.3.5\nlangchain-elasticsearch==0.3.0\nlangchain-google-community==2.0.0\nlangchain-openai==0.2.0\nlangchain-text-splitters==0.3.0\nlangchain-unstructured==0.1.4\nlangchainhub==0.1.21", "created_at": "2024-09-27", "closed_at": "2024-09-27", "labels": [], "State": "closed", "Author": "istvancsabakis"}
{"issue_number": 1866, "issue_title": "Intermittent issue when using SqliteSaver- TypeError: Object of type SecretStr is not serializable", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\nExample Code\nHave not been able to identify a minimal example to recreate this. It seems random.\nimport sqlite3\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nfrom langgraph.graph import StateGraph\n\nclass State(TypedDict):\n    pth:Path\n    item:str\n    other:Optional[dict]\n\nconn = sqlite3.connect(f\"checkpoints.sqlite\", check_same_thread=False)\nmemory = SqliteSaver(conn)\n\ngraph = flow.compile(checkpointer=memory)\n\ngraph.invoke(...\n\nError Message and Stack Trace (if applicable)\nFile ...\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1551, in Pregel.invoke(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\n   1549 else:\n   1550     chunks = []\n-> 1551 for chunk in self.stream(\n   1552     input,\n   1553     config,\n   1554     stream_mode=stream_mode,\n   1555     output_keys=output_keys,\n   1556     interrupt_before=interrupt_before,\n   1557     interrupt_after=interrupt_after,\n   1558     debug=debug,\n   1559     **kwargs,\n   1560 ):\n   1561     if stream_mode == \"values\":\n   1562         latest = chunk\n\nFile ...\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1240, in Pregel.stream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\n   1236 if \"custom\" in stream_modes:\n   1237     config[CONF][CONFIG_KEY_STREAM_WRITER] = lambda c: stream.put(\n   1238         ((), \"custom\", c)\n   1239     )\n-> 1240 with SyncPregelLoop(\n   1241     input,\n   1242     stream=StreamProtocol(stream.put, stream_modes),\n   1243     config=config,\n   1244     store=self.store,\n   1245     checkpointer=checkpointer,\n   1246     nodes=self.nodes,\n   1247     specs=self.channels,\n   1248     output_keys=output_keys,\n   1249     stream_keys=self.stream_channels_asis,\n   1250     debug=debug,\n   1251 ) as loop:\n   1252     # create runner\n   1253     runner = PregelRunner(\n   1254         submit=loop.submit,\n   1255         put_writes=loop.put_writes,\n   1256     )\n   1257     # enable subgraph streaming\n\nFile ...\\Lib\\site-packages\\langgraph\\pregel\\loop.py:754, in SyncPregelLoop.__exit__(self, exc_type, exc_value, traceback)\n    747 def __exit__(\n    748     self,\n    749     exc_type: Optional[Type[BaseException]],\n   (...)\n    752 ) -> Optional[bool]:\n    753     # unwind stack\n--> 754     return self.stack.__exit__(exc_type, exc_value, traceback)\n\nFile ...\\Lib\\contextlib.py:610, in ExitStack.__exit__(self, *exc_details)\n    606 try:\n    607     # bare \"raise exc_details[1]\" replaces our carefully\n    608     # set-up context\n    609     fixed_ctx = exc_details[1].__context__\n--> 610     raise exc_details[1]\n    611 except BaseException:\n    612     exc_details[1].__context__ = fixed_ctx\n\nFile ...\\Lib\\contextlib.py:595, in ExitStack.__exit__(self, *exc_details)\n    593 assert is_sync\n    594 try:\n--> 595     if cb(*exc_details):\n    596         suppressed_exc = True\n    597         pending_raise = False\n\nFile ...\\Lib\\site-packages\\langgraph\\pregel\\executor.py:105, in BackgroundExecutor.__exit__(self, exc_type, exc_value, traceback)\n    103     continue\n    104 try:\n--> 105     task.result()\n    106 except concurrent.futures.CancelledError:\n    107     pass\n\nFile ...\\Lib\\concurrent\\futures\\_base.py:449, in Future.result(self, timeout)\n    447     raise CancelledError()\n    448 elif self._state == FINISHED:\n--> 449     return self.__get_result()\n    451 self._condition.wait(timeout)\n    453 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\nFile ...\\Lib\\concurrent\\futures\\_base.py:401, in Future.__get_result(self)\n    399 if self._exception:\n    400     try:\n--> 401         raise self._exception\n    402     finally:\n    403         # Break a reference cycle with the exception in self._exception\n    404         self = None\n\nFile ...\\Lib\\site-packages\\langgraph\\pregel\\loop.py:680, in SyncPregelLoop._checkpointer_put_after_previous(self, prev, config, checkpoint, metadata, new_versions)\n    678 try:\n    679     if prev is not None:\n--> 680         prev.result()\n    681 finally:\n    682     cast(BaseCheckpointSaver, self.checkpointer).put(\n    683         config, checkpoint, metadata, new_versions\n    684     )\n\nFile ...\\Lib\\concurrent\\futures\\_base.py:449, in Future.result(self, timeout)\n    447     raise CancelledError()\n    448 elif self._state == FINISHED:\n--> 449     return self.__get_result()\n    451 self._condition.wait(timeout)\n    453 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\nFile ...\\Lib\\concurrent\\futures\\_base.py:401, in Future.__get_result(self)\n    399 if self._exception:\n    400     try:\n--> 401         raise self._exception\n    402     finally:\n    403         # Break a reference cycle with the exception in self._exception\n    404         self = None\n\nFile ...\\Lib\\site-packages\\langgraph\\pregel\\executor.py:70, in BackgroundExecutor.done(self, task)\n     68 def done(self, task: concurrent.futures.Future) -> None:\n     69     try:\n---> 70         task.result()\n     71     except GraphInterrupt:\n     72         # This exception is an interruption signal, not an error\n     73         # so we don't want to re-raise it on exit\n     74         self.tasks.pop(task)\n\nFile ...\\Lib\\concurrent\\futures\\_base.py:449, in Future.result(self, timeout)\n    447     raise CancelledError()\n    448 elif self._state == FINISHED:\n--> 449     return self.__get_result()\n    451 self._condition.wait(timeout)\n    453 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\nFile ...\\Lib\\concurrent\\futures\\_base.py:401, in Future.__get_result(self)\n    399 if self._exception:\n    400     try:\n--> 401         raise self._exception\n    402     finally:\n    403         # Break a reference cycle with the exception in self._exception\n    404         self = None\n\nFile ...\\Lib\\concurrent\\futures\\thread.py:58, in _WorkItem.run(self)\n     55     return\n     57 try:\n---> 58     result = self.fn(*self.args, **self.kwargs)\n     59 except BaseException as exc:\n     60     self.future.set_exception(exc)\n\nFile ...\\Lib\\site-packages\\langgraph\\pregel\\loop.py:682, in SyncPregelLoop._checkpointer_put_after_previous(self, prev, config, checkpoint, metadata, new_versions)\n    680         prev.result()\n    681 finally:\n--> 682     cast(BaseCheckpointSaver, self.checkpointer).put(\n    683         config, checkpoint, metadata, new_versions\n    684     )\n\nFile ...\\Lib\\site-packages\\langgraph\\checkpoint\\sqlite\\__init__.py:399, in SqliteSaver.put(self, config, checkpoint, metadata, new_versions)\n    397 thread_id = config[\"configurable\"][\"thread_id\"]\n    398 checkpoint_ns = config[\"configurable\"][\"checkpoint_ns\"]\n--> 399 type_, serialized_checkpoint = self.serde.dumps_typed(checkpoint)\n    400 serialized_metadata = self.jsonplus_serde.dumps(metadata)\n    401 with self.cursor() as cur:\n\nFile ...\\Lib\\site-packages\\langgraph\\checkpoint\\serde\\jsonplus.py:192, in JsonPlusSerializer.dumps_typed(self, obj)\n    190 else:\n    191     try:\n--> 192         return \"msgpack\", _msgpack_enc(obj)\n    193     except UnicodeEncodeError:\n    194         return \"json\", self.dumps(obj)\n\nFile ...\\Lib\\site-packages\\langgraph\\checkpoint\\serde\\jsonplus.py:476, in _msgpack_enc(data)\n    474     enc = msgpack.Packer(default=_msgpack_default)\n    475 try:\n--> 476     return enc.pack(data)\n    477 finally:\n    478     ENC_POOL.append(enc)\n\nFile msgpack/_packer.pyx:279, in msgpack._cmsgpack.Packer.pack()\n\nFile msgpack/_packer.pyx:276, in msgpack._cmsgpack.Packer.pack()\n\nFile msgpack/_packer.pyx:265, in msgpack._cmsgpack.Packer._pack()\n\nFile msgpack/_packer.pyx:213, in msgpack._cmsgpack.Packer._pack_inner()\n\nFile msgpack/_packer.pyx:265, in msgpack._cmsgpack.Packer._pack()\n\nFile msgpack/_packer.pyx:213, in msgpack._cmsgpack.Packer._pack_inner()\n\nFile msgpack/_packer.pyx:265, in msgpack._cmsgpack.Packer._pack()\n\nFile msgpack/_packer.pyx:213, in msgpack._cmsgpack.Packer._pack_inner()\n\nFile msgpack/_packer.pyx:267, in msgpack._cmsgpack.Packer._pack()\n\nFile ...\\Lib\\site-packages\\langgraph\\checkpoint\\serde\\jsonplus.py:227, in _msgpack_default(obj)\n    223 def _msgpack_default(obj: Any) -> Union[str, msgpack.ExtType]:\n    224     if hasattr(obj, \"model_dump\") and callable(obj.model_dump):  # pydantic v2\n    225         return msgpack.ExtType(\n    226             EXT_PYDANTIC_V2,\n--> 227             _msgpack_enc(\n    228                 (\n    229                     obj.__class__.__module__,\n    230                     obj.__class__.__name__,\n    231                     obj.model_dump(),\n    232                     \"model_validate_json\",\n    233                 ),\n    234             ),\n    235         )\n    236     elif hasattr(obj, \"dict\") and callable(obj.dict):  # pydantic v1\n    237         return msgpack.ExtType(\n    238             EXT_PYDANTIC_V1,\n    239             _msgpack_enc(\n   (...)\n    245             ),\n    246         )\n\nFile ...\\Lib\\site-packages\\langgraph\\checkpoint\\serde\\jsonplus.py:476, in _msgpack_enc(data)\n    474     enc = msgpack.Packer(default=_msgpack_default)\n    475 try:\n--> 476     return enc.pack(data)\n    477 finally:\n    478     ENC_POOL.append(enc)\n\nFile msgpack/_packer.pyx:279, in msgpack._cmsgpack.Packer.pack()\n\nFile msgpack/_packer.pyx:276, in msgpack._cmsgpack.Packer.pack()\n\nFile msgpack/_packer.pyx:265, in msgpack._cmsgpack.Packer._pack()\n\nFile msgpack/_packer.pyx:232, in msgpack._cmsgpack.Packer._pack_inner()\n\nFile msgpack/_packer.pyx:265, in msgpack._cmsgpack.Packer._pack()\n\nFile msgpack/_packer.pyx:213, in msgpack._cmsgpack.Packer._pack_inner()\n\nFile msgpack/_packer.pyx:267, in msgpack._cmsgpack.Packer._pack()\n\nFile ...\\Lib\\site-packages\\langgraph\\checkpoint\\serde\\jsonplus.py:409, in _msgpack_default(obj)\n    407     return repr(obj)\n    408 else:\n--> 409     raise TypeError(f\"Object of type {obj.__class__.__name__} is not serializable\")\n\nTypeError: Object of type SecretStr is not serializable\n\nDescription\nGetting this strange intermittent error which kills a running flow at random places in the middle. All of my state keys are typed And none have the SecretStr type.\nthere should be a setting to at least wrap checkpointing in a try/except  block so it doesn't  break the graph mid-execution.\nThis issue seems to have been fixed,  so I don't think it's a  problem with pydantic: pydantic/pydantic#462\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.19045\nPython Version:  3.12.6 | packaged by conda-forge | (main, Sep 22 2024, 14:01:26) [MSC v.1941 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.3.6\nlangchain: 0.3.1\nlangchain_community: 0.3.1\nlangsmith: 0.1.128\nlangchain_ollama: 0.2.0\nlangchain_openai: 0.2.0\nlangchain_text_splitters: 0.3.0\nlanggraph: 0.2.28\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.10.6\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.7\nhttpx: 0.27.2\njsonpatch: 1.33\nlanggraph-checkpoint: 1.0.11\nnumpy: 1.26.4\nollama: 0.3.3\nopenai: 1.48.0\norjson: 3.10.7\npackaging: 24.1\npydantic: 2.8.2\npydantic-settings: 2.5.2\nPyYAML: 6.0.2\nrequests: 2.32.3\nSQLAlchemy: 2.0.35\ntenacity: 8.5.0\ntiktoken: 0.7.0\ntyping-extensions: 4.12.2\n", "created_at": "2024-09-26", "closed_at": "2024-09-27", "labels": [], "State": "closed", "Author": "openSourcerer9000"}
{"issue_number": 1856, "issue_title": "`typing.TypedDict` is not supported in with pydantic in Python < 3.12", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\n# Define agent following the tutorial\nfrom typing import Literal\nfrom langchain_core.runnables import Runnable\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\n\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n\n@tool\ndef get_weather(city: Literal[\"nyc\", \"sf\"]):\n    \"\"\"Use this to get weather information.\"\"\"\n    if city == \"nyc\":\n        return \"It might be cloudy in nyc\"\n    elif city == \"sf\":\n        return \"It's always sunny in sf\"\n    else:\n        raise AssertionError(\"Unknown city\")\n\ntools = [get_weather]\nagent = create_react_agent(model, tools=tools)\n\n\n# Function for traversing the nodes in graph\ndef traverse_node(node):\n    yield node\n\n    if isinstance(node, Runnable):\n        nodes = node.get_graph().nodes.values()\n        for node in nodes:\n            yield from traverse_node(node.data)\n\nlist(traverse_node(agent))\nError Message and Stack Trace (if applicable)\ntests/langchain/test_traversal.py:31: in traverse_node\n    yield from traverse_node(node.data)\n        Runnable   = <class 'langchain_core.runnables.base.Runnable'>\n        node       = Node(id='agent', name='agent', data=RunnableLambda(call_model), metadata=None)\n        nodes      = dict_values([Node(id='__start__', name='__start__', data=<class 'langchain_core.utils.pydantic.LangGraphInput'>, metad...one), Node(id='__end__', name='__end__', data=<class 'langchain_core.utils.pydantic.LangGraphOutput'>, metadata=None)])\n        traverse_node = <function test_hoge.<locals>.traverse_node at 0x7f8c09212af0>\ntests/langchain/test_traversal.py:27: in traverse_node\n    nodes = node.get_graph().nodes.values()\n        Runnable   = <class 'langchain_core.runnables.base.Runnable'>\n        node       = RunnableLambda(call_model)\n        traverse_node = <function test_hoge.<locals>.traverse_node at 0x7f8c09212af0>\n.venv/dev/lib/python3.9/site-packages/langchain_core/runnables/base.py:4495: in get_graph\n    input_node = graph.add_node(self.get_input_schema(config))\n        __class__  = <class 'langchain_core.runnables.base.RunnableLambda'>\n        config     = None\n        deps       = [RunnableLambda(...)\n| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object a...enum': ['nyc', 'sf'], 'type': 'string'}}, 'required': ['city'], 'type': 'object'}}}]}, config={}, config_factories=[])]\n        graph      = Graph(nodes={}, edges=[])\n        self       = RunnableLambda(call_model)\n.venv/dev/lib/python3.9/site-packages/langchain_core/runnables/base.py:4403: in get_input_schema\n    return super().get_input_schema(config)\n        __class__  = <class 'langchain_core.runnables.base.RunnableLambda'>\n        config     = None\n        func       = <function create_react_agent.<locals>.call_model at 0x7f8c0927fe50>\n        self       = RunnableLambda(call_model)\n.venv/dev/lib/python3.9/site-packages/langchain_core/runnables/base.py:351: in get_input_schema\n    return create_model_v2(\n        config     = None\n        root_type  = <class 'langgraph.prebuilt.chat_agent_executor.AgentState'>\n        self       = RunnableLambda(call_model)\n.venv/dev/lib/python3.9/site-packages/langchain_core/utils/pydantic.py:594: in create_model_v2\n    named_root_model = _create_root_model(\n        field_definitions = {}\n        kwargs     = {'type_': <class 'langgraph.prebuilt.chat_agent_executor.AgentState'>}\n        model_name = 'call_model_input'\n        module_name = 'langchain_core.runnables.base'\n        root       = <class 'langgraph.prebuilt.chat_agent_executor.AgentState'>\n.venv/dev/lib/python3.9/site-packages/langchain_core/utils/pydantic.py:449: in _create_root_model\n    custom_root_type = type(name, (RootModel,), base_class_attributes)\n        base_class_attributes = {'__annotations__': {'root': <class 'langgraph.prebuilt.chat_agent_executor.AgentState'>}, '__class_vars__': set(), '__module__': 'langchain_core.runnables.base', '__private_attributes__': {}, ...}\n        default_   = <object object at 0x7f8b58e8d9a0>\n        model_json_schema = <function _create_root_model.<locals>.model_json_schema at 0x7f8c09170ee0>\n        module_name = 'langchain_core.runnables.base'\n        name       = 'call_model_input'\n        schema     = <function _create_root_model.<locals>.schema at 0x7f8c09170e50>\n        type_      = <class 'langgraph.prebuilt.chat_agent_executor.AgentState'>\n.venv/dev/lib/python3.9/site-packages/pydantic/_internal/_model_construction.py:224: in __new__\n    complete_model_class(\n        BaseModel  = <class 'pydantic.main.BaseModel'>\n        __class__  = <class 'pydantic._internal._model_construction.ModelMetaclass'>\n        __pydantic_generic_metadata__ = None\n        __pydantic_reset_parent_namespace__ = True\n        _create_model_module = None\n        base_field_names = {'root'}\n        base_private_attributes = {}\n        bases      = (<class 'pydantic.root_model.RootModel'>,)\n        class_vars = set()\n        cls        = <class 'langchain_core.runnables.base.call_model_input'>\n        cls_name   = 'call_model_input'\n        config_wrapper = ConfigWrapper(arbitrary_types_allowed=True)\n        kwargs     = {}\n        mcs        = <class 'pydantic._internal._model_construction.ModelMetaclass'>\n        mro        = (<class 'langchain_core.runnables.base.call_model_input'>, <class 'pydantic.root_model.RootModel'>, <class 'pydantic.main.BaseModel'>, <class 'typing.Generic'>, <class 'object'>)\n        namespace  = {'__annotations__': {'root': <class 'langgraph.prebuilt.chat_agent_executor.AgentState'>}, '__class_vars__': set(), '__module__': 'langchain_core.runnables.base', '__private_attributes__': {}, ...}\n        parameters = (~RootModelRootType,)\n        parent_namespace = {'base_class_attributes': {'__annotations__': {'root': <class 'langgraph.prebuilt.chat_agent_executor.AgentState'>}, '... _create_root_model.<locals>.model_json_schema at 0x7f8c09170ee0>, 'module_name': 'langchain_core.runnables.base', ...}\n        parent_parameters = (~RootModelRootType,)\n        private_attributes = {}\n        types_namespace = {'ABC': <class 'abc.ABC'>, 'AddableDict': <class 'langchain_core.runnables.utils.AddableDict'>, 'Any': typing.Any, 'An..._core.runnables.utils.ConfigurableFieldSingleOption, langchain_core.runnables.utils.ConfigurableFieldMultiOption], ...}\n.venv/mlflow-remote/lib/python3.9/site-packages/pydantic/_internal/_model_construction.py:577: in complete_model_class\n    schema = cls.__get_pydantic_core_schema__(cls, handler)\n        cls        = <class 'langchain_core.runnables.base.call_model_input'>\n        cls_name   = 'call_model_input'\n        config_wrapper = ConfigWrapper(arbitrary_types_allowed=True)\n        create_model_module = None\n        gen_schema = <pydantic._internal._generate_schema.GenerateSchema object at 0x7f8c09119950>\n        handler    = <pydantic._internal._schema_generation_shared.CallbackGetCoreSchemaHandler object at 0x7f8c0915ef40>\n        raise_errors = False\n        types_namespace = {'ABC': <class 'abc.ABC'>, 'AddableDict': <class 'langchain_core.runnables.utils.AddableDict'>, 'Any': typing.Any, 'An..._core.runnables.utils.ConfigurableFieldSingleOption, langchain_core.runnables.utils.ConfigurableFieldMultiOption], ...}\n        typevars_map = {}\n.venv/mlflow-remote/lib/python3.9/site-packages/pydantic/main.py:671: in __get_pydantic_core_schema__\n    return handler(source)\n        cls        = <class 'langchain_core.runnables.base.call_model_input'>\n        handler    = <pydantic._internal._schema_generation_shared.CallbackGetCoreSchemaHandler object at 0x7f8c0915ef40>\n        schema     = None\n        source     = <class 'langchain_core.runnables.base.call_model_input'>\n.venv/dev/lib/python3.9/site-packages/pydantic/_internal/_schema_generation_shared.py:83: in __call__\n    schema = self._handler(source_type)\n        self       = <pydantic._internal._schema_generation_shared.CallbackGetCoreSchemaHandler object at 0x7f8c0915ef40>\n        source_type = <class 'langchain_core.runnables.base.call_model_input'>\n.venv/dev/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:655: in generate_schema\n    schema = self._generate_schema_inner(obj)\n        from_dunder_get_core_schema = False\n        obj        = <class 'langchain_core.runnables.base.call_model_input'>\n        schema     = None\n        self       = <pydantic._internal._generate_schema.GenerateSchema object at 0x7f8c09119950>\n.venv/dev/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:924: in _generate_schema_inner\n    return self._model_schema(obj)\n        BaseModel  = <class 'pydantic.main.BaseModel'>\n        obj        = <class 'langchain_core.runnables.base.call_model_input'>\n        self       = <pydantic._internal._generate_schema.GenerateSchema object at 0x7f8c09119950>\n.venv/dev/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:724: in _model_schema\n    root_field = self._common_field_schema('root', fields['root'], decorators)\n        cls        = <class 'langchain_core.runnables.base.call_model_input'>\n        computed_fields = {}\n        config_wrapper = ConfigWrapper(arbitrary_types_allowed=True)\n        core_config = {'title': 'call_model_input'}\n        decorators = DecoratorInfos(validators={}, field_validators={}, root_validators={}, field_serializers={}, model_serializers={}, model_validators={}, computed_fields={})\n        extras_schema = None\n        fields     = {'root': FieldInfo(annotation=AgentState, required=True)}\n        maybe_schema = None\n        metadata   = {'pydantic_js_annotation_functions': [], 'pydantic_js_functions': [functools.partial(<function modify_model_json_schema at 0x7f8b59acf700>, cls=<class 'langchain_core.runnables.base.call_model_input'>, title=None)]}\n        model_ref  = 'langchain_core.runnables.base.call_model_input:94900564014896'\n        model_validators = dict_values([])\n        self       = <pydantic._internal._generate_schema.GenerateSchema object at 0x7f8c09119b80>\n        title      = None\n.venv/dev/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:1308: in _common_field_schema\n    schema = self._apply_annotations(\n        FieldInfo  = <class 'pydantic.fields.FieldInfo'>\n        annotations = []\n        decorators = DecoratorInfos(validators={}, field_validators={}, root_validators={}, field_serializers={}, model_serializers={}, model_validators={}, computed_fields={})\n        field_info = FieldInfo(annotation=AgentState, required=True)\n        name       = 'root'\n        self       = <pydantic._internal._generate_schema.GenerateSchema object at 0x7f8c09119b80>\n        set_discriminator = <function GenerateSchema._common_field_schema.<locals>.set_discriminator at 0x7f8c09170f70>\n        source_type = <class 'langgraph.prebuilt.chat_agent_executor.AgentState'>\n        validators_from_decorators = []\n.venv/dev/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2107: in _apply_annotations\n    schema = get_inner_schema(source_type)\n        annotations = []\n        get_inner_schema = <pydantic._internal._schema_generation_shared.CallbackGetCoreSchemaHandler object at 0x7f8c09280fd0>\n        inner_handler = <function GenerateSchema._apply_annotations.<locals>.inner_handler at 0x7f8c0911a040>\n        pydantic_js_annotation_functions = []\n        res        = None\n        self       = <pydantic._internal._generate_schema.GenerateSchema object at 0x7f8c09119b80>\n        source_type = <class 'langgraph.prebuilt.chat_agent_executor.AgentState'>\n        transform_inner_schema = <function GenerateSchema.<lambda> at 0x7f8b59aded30>\n.venv/dev/lib/python3.9/site-packages/pydantic/_internal/_schema_generation_shared.py:83: in __call__\n    schema = self._handler(source_type)\n        self       = <pydantic._internal._schema_generation_shared.CallbackGetCoreSchemaHandler object at 0x7f8c09280fd0>\n        source_type = <class 'langgraph.prebuilt.chat_agent_executor.AgentState'>\n.venv/dev/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2088: in inner_handler\n    schema = self._generate_schema_inner(obj)\n        from_property = None\n        obj        = <class 'langgraph.prebuilt.chat_agent_executor.AgentState'>\n        self       = <pydantic._internal._generate_schema.GenerateSchema object at 0x7f8c09119b80>\n        source_type = <class 'langgraph.prebuilt.chat_agent_executor.AgentState'>\n        transform_inner_schema = <function GenerateSchema.<lambda> at 0x7f8b59aded30>\n.venv/dev/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:929: in _generate_schema_inner\n    return self.match_type(obj)\n        BaseModel  = <class 'pydantic.main.BaseModel'>\n        obj        = <class 'langgraph.prebuilt.chat_agent_executor.AgentState'>\n        self       = <pydantic._internal._generate_schema.GenerateSchema object at 0x7f8c09119b80>\n.venv/dev/lib/python3.9/site-packages/pydantic/_internal/_generate_schema.py:999: in match_type\n    return self._typed_dict_schema(obj, None)\n        obj        = <class 'langgraph.prebuilt.chat_agent_executor.AgentState'>\n        self       = <pydantic._internal._generate_schema.GenerateSchema object at 0x7f8c09119b80>\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <pydantic._internal._generate_schema.GenerateSchema object at 0x7f8c09119b80>\ntyped_dict_cls = <class 'langgraph.prebuilt.chat_agent_executor.AgentState'>\norigin = None\n\n    def _typed_dict_schema(self, typed_dict_cls: Any, origin: Any) -> core_schema.CoreSchema:\n        \"\"\"Generate schema for a TypedDict.\n    \n        It is not possible to track required/optional keys in TypedDict without __required_keys__\n        since TypedDict.__new__ erases the base classes (it replaces them with just `dict`)\n        and thus we can track usage of total=True/False\n        __required_keys__ was added in Python 3.9\n        (https://github.com/miss-islington/cpython/blob/1e9939657dd1f8eb9f596f77c1084d2d351172fc/Doc/library/typing.rst?plain=1#L1546-L1548)\n        however it is buggy\n        (https://github.com/python/typing_extensions/blob/ac52ac5f2cb0e00e7988bae1e2a1b8257ac88d6d/src/typing_extensions.py#L657-L666).\n    \n        On 3.11 but < 3.12 TypedDict does not preserve inheritance information.\n    \n        Hence to avoid creating validators that do not do what users expect we only\n        support typing.TypedDict on Python >= 3.12 or typing_extension.TypedDict on all versions\n        \"\"\"\n        FieldInfo = import_cached_field_info()\n    \n        with self.model_type_stack.push(typed_dict_cls), self.defs.get_schema_or_ref(typed_dict_cls) as (\n            typed_dict_ref,\n            maybe_schema,\n        ):\n            if maybe_schema is not None:\n                return maybe_schema\n    \n            typevars_map = get_standard_typevars_map(typed_dict_cls)\n            if origin is not None:\n                typed_dict_cls = origin\n    \n            if not _SUPPORTS_TYPEDDICT and type(typed_dict_cls).__module__ == 'typing':\n>               raise PydanticUserError(\n                    'Please use `typing_extensions.TypedDict` instead of `typing.TypedDict` on Python < 3.12.',\n                    code='typed-dict-version',\n                )\nE               pydantic.errors.PydanticUserError: Please use `typing_extensions.TypedDict` instead of `typing.TypedDict` on Python < 3.12.\nE               \nE               For further information visit https://errors.pydantic.dev/2.9/u/typed-dict-version\nDescription\nThe prebuilt ReAct agent code uses typing.TypedDict (code), which is not supported in pydantic v2 with Python < 3.12.\nhttps://docs.pydantic.dev/2.9/errors/usage_errors/#typed-dict-version\nThis causes several errors, for example, calling get_graph() of subnode raises the pydantic user error above. To make it compatible with Python < 3.12, the import should be replaced with typing_extensions.TypedDict as pydantic recommends.\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #134+fips1-Ubuntu SMP Wed Apr 10 14:20:32 UTC 2024\nPython Version:  3.9.6 (default, Jul  9 2024, 14:21:06)\n[GCC 9.4.0]\n\nPackage Information\n\nlangchain_core: 0.3.5\nlangchain: 0.3.0\nlangsmith: 0.1.125\nlangchain_experimental: 0.3.1\nlangchain_openai: 0.2.0\nlangchain_text_splitters: 0.3.0\nlanggraph: 0.2.19\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.9.5\nasync-timeout: 4.0.3\nhttpx: 0.27.0\njsonpatch: 1.33\nlangchain-community: Installed. No version info available.\nlanggraph-checkpoint: 1.0.3\nnumpy: 1.26.4\nopenai: 1.42.0\norjson: 3.10.6\npackaging: 23.2\npydantic: 2.9.2\nPyYAML: 6.0.1\nrequests: 2.32.3\nSQLAlchemy: 2.0.31\ntenacity: 8.5.0\ntiktoken: 0.7.0\ntyping-extensions: 4.12.2\n", "created_at": "2024-09-26", "closed_at": "2024-09-26", "labels": [], "State": "closed", "Author": "B-Step62"}
{"issue_number": 1855, "issue_title": "SQLToolManager.db_query_tool() missing 1 required positional argument: 'query'/'self'", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nhttps://github.com/langchain-ai/langgraph/blob/main/docs/docs/tutorials/sql-agent.ipynb\n\nclass SQLToolManager:\n    def __init__(self):\n        self.interface = Interface()\n        self.db_handler = SQLiteHandler()\n        self.llm = self.interface.get_current_model()\n        self.toolkit = SQLDatabaseToolkit(db=self.db_handler.get_sql_database(), llm=self.llm)\n        self.tools = self.toolkit.get_tools()\n        self.list_tables_tool = next(tool for tool in self.tools if tool.name == \"sql_db_list_tables\")\n        self.get_schema_tool = next(tool for tool in self.tools if tool.name == \"sql_db_schema\")\n\n    def create_tool_node_with_fallback(self, tools: list) -> RunnableWithFallbacks[Any, dict]:\n        \"\"\"\n        Create a ToolNode with a fallback to handle errors and surface them to the agent.\n        \"\"\"\n        return ToolNode(tools).with_fallbacks(\n            [RunnableLambda(self.handle_tool_error)], exception_key=\"error\"\n        )\n\n    def handle_tool_error(self, state) -> dict:\n        error = state.get(\"error\")\n        tool_calls = state[\"messages\"][-1].tool_calls\n        return {\n            \"messages\": [\n                ToolMessage(\n                    content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n                    tool_call_id=tc[\"id\"],\n                )\n                for tc in tool_calls\n            ]\n        }\n\n    @tool\n    def db_query_tool(self, query) -> str:\n        \"\"\"\n        Execute a SQL query against the database and get back the result.\n        If the query is not correct, an error message will be returned.\n        \"\"\"\n        mylogging.info(f\"Executing query: {query}\")\n        result = self.db_handler.db.run_no_throw(query)\n        if not result:\n            return \"Error: Query failed. Please rewrite your query and try again.\"\n        return result\ntest code\nif __name__ == \"__main__\":\n    manager = SQLToolManager()\n    print(manager.list_tables())\n\n    print(manager.get_schema_tool.invoke(\"Artist\"))\n    \n    query = \"SELECT * FROM Artist LIMIT 10;\"\n    #result = manager.db_query_tool.invoke({\"query\": query})\n    result = manager.db_query_tool.invoke(query)\n    print(result)\n    \n    result = manager.run_query(query)\nError Message and Stack Trace (if applicable)\nresult = manager.db_query_tool.invoke(query)  \nFile \"/home/flyang/anaconda3/envs/LLMs/lib/python3.11/site-packages/langchain_core/tools/base.py\", line 397, in invoke\n    return self.run(tool_input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/flyang/anaconda3/envs/LLMs/lib/python3.11/site-packages/langchain_core/tools/base.py\", line 586, in run\n    raise error_to_raise\n  File \"/home/flyang/anaconda3/envs/LLMs/lib/python3.11/site-packages/langchain_core/tools/base.py\", line 555, in run\n    response = context.run(self._run, *tool_args, **tool_kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/flyang/anaconda3/envs/LLMs/lib/python3.11/site-packages/langchain_core/tools/structured.py\", line 69, in _run\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: SQLToolManager.db_query_tool() missing 1 required positional argument: 'query'\n\nresult = manager.db_query_tool.invoke({\"query\": query})\nFile \"/mnt/c/workspace/pr_train/LLMs/src/sqlAgent/sqlTools.py\", line 142, in <module>\n    result = manager.db_query_tool.invoke({\"query\": query})\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/flyang/anaconda3/envs/LLMs/lib/python3.11/site-packages/langchain_core/tools/base.py\", line 397, in invoke\n    return self.run(tool_input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/flyang/anaconda3/envs/LLMs/lib/python3.11/site-packages/langchain_core/tools/base.py\", line 586, in run\n    raise error_to_raise\n  File \"/home/flyang/anaconda3/envs/LLMs/lib/python3.11/site-packages/langchain_core/tools/base.py\", line 555, in run\n    response = context.run(self._run, *tool_args, **tool_kwargs)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/flyang/anaconda3/envs/LLMs/lib/python3.11/site-packages/langchain_core/tools/structured.py\", line 69, in _run\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: SQLToolManager.db_query_tool() missing 1 required positional argument: 'self'\nDescription\nExample Code\nhttps://github.com/langchain-ai/langgraph/blob/main/docs/docs/tutorials/sql-agent.ipynb\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #3672-Microsoft Fri Jan 01 08:00:00 PST 2016\nPython Version:  3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0]\n\nPackage Information\n\nlangchain_core: 0.2.40\nlangchain: 0.2.16\nlangchain_community: 0.2.17\nlangsmith: 0.1.120\nlangchain_cohere: 0.1.9\nlangchain_experimental: 0.0.65\nlangchain_google_community: 1.0.7\nlangchain_huggingface: 0.0.3\nlangchain_milvus: 0.1.4\nlangchain_openai: 0.1.22\nlangchain_text_splitters: 0.2.4\nlanggraph: 0.2.22\nlangserve: 0.2.2\n\nOther Dependencies\n\naiohttp: 3.10.3\nasync-timeout: 4.0.3\nbeautifulsoup4: 4.12.3\ncohere: 5.8.1\ndataclasses-json: 0.6.7\ndb-dtypes: Installed. No version info available.\nfastapi: 0.112.0\ngapic-google-longrunning: Installed. No version info available.\ngoogle-api-core: 2.19.1\ngoogle-api-python-client: 2.141.0\ngoogle-auth-httplib2: 0.2.0\ngoogle-auth-oauthlib: Installed. No version info available.\ngoogle-cloud-aiplatform: 1.63.0\ngoogle-cloud-bigquery: 3.25.0\ngoogle-cloud-bigquery-storage: Installed. No version info available.\ngoogle-cloud-contentwarehouse: Installed. No version info available.\ngoogle-cloud-discoveryengine: Installed. No version info available.\ngoogle-cloud-documentai: Installed. No version info available.\ngoogle-cloud-documentai-toolbox: Installed. No version info available.\ngoogle-cloud-speech: Installed. No version info available.\ngoogle-cloud-storage: 2.18.2\ngoogle-cloud-texttospeech: Installed. No version info available.\ngoogle-cloud-translate: Installed. No version info available.\ngoogle-cloud-vision: 3.7.4\ngooglemaps: Installed. No version info available.\ngrpcio: 1.63.0\nhttpx: 0.27.2\nhuggingface-hub: 0.24.5\njsonpatch: 1.33\nlanggraph-checkpoint: 1.0.9\nnumpy: 1.26.4\nopenai: 1.40.6\norjson: 3.10.7\npackaging: 24.1\npandas: 2.2.2\npyarrow: 17.0.0\npydantic: 2.8.2\npymilvus: 2.4.6\npyproject-toml: 0.0.10\nPyYAML: 6.0.2\nrequests: 2.32.3\nscipy: 1.14.0\nsentence-transformers: 3.0.1\nSQLAlchemy: 2.0.32\nsse-starlette: Installed. No version info available.\ntabulate: 0.9.0\ntenacity: 8.3.0\ntiktoken: 0.7.0\ntokenizers: 0.19.1\ntransformers: 4.44.0\n", "created_at": "2024-09-26", "closed_at": null, "labels": [], "State": "open", "Author": "jason571"}
{"issue_number": 1837, "issue_title": "DOC: <How to implement human in loop for updating information from one state to second state.> ", "issue_body": "Issue with current documentation:\nHi, I was trying to implement human in loop concept but unable to implement. I follow the documentation but not getting desired output. I have attaching code also for review. please guide me how to implement human in loop such a way that graph flow will stop desired node then human will see the output of that node, it will update if required and then send information to next node. Next node will use that information and do remaining code,\nfrom typing import TypedDict\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom IPython.display import Image, display\nclass State(TypedDict):\ninput: str\ndef step_1(state):\nprint(\"---Step 1---\")\nstate['input'] += \" from step 1\"\nreturn state\ndef step_2(state):\nprint(\"---Step 2---\")\nstate['input'] += \" from step 2\"\nreturn state\ndef step_3(state):\nprint(\"---Step 3---\")\nstate['input'] += \" from step 3\"\nreturn state\nbuilder = StateGraph(State)\nbuilder.add_node(\"step_1\", step_1)\nbuilder.add_node(\"step_2\", step_2)\nbuilder.add_node(\"step_3\", step_3)\nbuilder.add_edge(START, \"step_1\")\nbuilder.add_edge(\"step_1\", \"step_2\")\nbuilder.add_edge(\"step_2\", \"step_3\")\nbuilder.add_edge(\"step_3\", END)\nSet up memory\nmemory = MemorySaver()\nAdd\ngraph = builder.compile(checkpointer=memory, interrupt_before=[\"step_3\"])\nView\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\nInput\ninitial_input = {\"input\": \"hello world\"}\nThread\nthread = {\"configurable\": {\"thread_id\": \"1\"}}\nRun the graph until the first interruption\ncurrent_state = initial_input\nfor event in graph.stream(current_state, thread, stream_mode=\"values\"):\nprint(event)\ncurrent_state = event  # Update the current state with the event output\nCapture the output of step_2\nstate_after_step_2 = current_state\ntry:\nuser_edit = input(f\"Edit the state after Step 2: {state_after_step_2['input']}\\nYour edit: \")\nexcept:\nuser_edit = state_after_step_2['input']\nUpdate the state with the user's edit\nstate_after_step_2['input'] = user_edit\n#Continue the graph execution with the updated state\nfor event in graph.stream(state_after_step_2, thread, stream_mode=\"values\"):\nprint(event)\ncurrent_state = event  # Update the current state with the event output\nfor event in graph.stream(None, thread, stream_mode=\"values\"):\nprint(event)\ncurrent_state = event  # Update the current state with the event output\nIdea or request for content:\nPlease provide me doc, solution or blog where i can solve my issues.", "created_at": "2024-09-25", "closed_at": "2024-09-25", "labels": [], "State": "closed", "Author": "atulv24"}
{"issue_number": 1831, "issue_title": "sqlite checkpointer import error", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[46], line 1\n----> 1 from langgraph.checkpoint.sqlite import SqliteSaver\n      2 # SqliteSaver.config_specs\n\nFile ...\\Lib\\site-packages\\langgraph\\checkpoint\\sqlite\\__init__.py:9\n      5 from typing import Any, AsyncIterator, Dict, Iterator, Optional, Sequence, Tuple\n      7 from langchain_core.runnables import RunnableConfig\n----> 9 from langgraph.checkpoint.base import (\n     10     WRITES_IDX_MAP,\n     11     BaseCheckpointSaver,\n     12     ChannelVersions,\n     13     Checkpoint,\n     14     CheckpointMetadata,\n     15     CheckpointTuple,\n     16     SerializerProtocol,\n     17     get_checkpoint_id,\n     18 )\n     19 from langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer\n     20 from langgraph.checkpoint.serde.types import ChannelProtocol\n\nImportError: cannot import name 'WRITES_IDX_MAP' from 'langgraph.checkpoint.base'```\nDescription\nCheckpointing is broken-  something broke with internal import statements.\nThis documentation is also out of date: https://langchain-ai.github.io/langgraph/reference/checkpoints/#sqlitesaver\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.19045\nPython Version:  3.11.8 | packaged by conda-forge | (main, Feb 16 2024, 20:40:50) [MSC v.1937 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.2.34\nlangchain: 0.2.12\nlangchain_community: 0.2.11\nlangsmith: 0.1.127\nlangchain_cli: 0.0.29\nlangchain_ollama: 0.1.1\nlangchain_openai: 0.1.22\nlangchain_text_splitters: 0.2.2\nlanggraph: 0.2.3\nlangserve: 0.2.2\n\nOther Dependencies\n\naiohttp: 3.10.1\nasync-timeout: 4.0.3\ndataclasses-json: 0.6.7\nfastapi: 0.112.0\ngitpython: 3.1.43\nhttpx: 0.27.0\njsonpatch: 1.33\nlanggraph-checkpoint: 1.0.11\nlangserve[all]: Installed. No version info available.\nlibcst: 1.4.0\nnumpy: 1.26.4\nollama: 0.3.1\nopenai: 1.40.1\norjson: 3.10.6\npackaging: 24.1\npydantic: 2.8.2\npyproject-toml: 0.0.10\nPyYAML: 6.0.1\nrequests: 2.31.0\nSQLAlchemy: 2.0.32\nsse-starlette: 1.8.2\ntenacity: 8.5.0\ntiktoken: 0.7.0\ntomlkit: 0.12.5\ntyper[all]: Installed. No version info available.\ntyping-extensions: 4.11.0\nuvicorn: 0.23.2\n", "created_at": "2024-09-24", "closed_at": "2024-10-04", "labels": [], "State": "closed", "Author": "openSourcerer9000"}
{"issue_number": 2504, "issue_title": "When Using Multiple States, Changing the Input State of a Node can Affect the State Fields Received by the Routing Function", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom operator import add\nfrom typing import TypedDict, Annotated\nfrom langgraph.graph import StateGraph, START, END\n\nclass PersonState(TypedDict):\n    name: str\n    money: Annotated[int, add]\n    \nclass PurchasesState(TypedDict):\n    purchases: Annotated[list[str], add]\n    total_cost: Annotated[int, add]\n\nclass OverallState(PersonState, PurchasesState):\n    pass\n\n\n# node_1 operation. \ndef pay_salary(state: PersonState) -> OverallState:\n    print(f\"Node 1: {state}\")\n    return {\"money\": 1000}\n\n# node_2 operation. \ndef subtract_expenses(state: PurchasesState) -> OverallState:\n    print(f\"Node 2: {state}\")\n    return {'money': -100, \"purchases\": [\"new iphone\"], 'total_cost': 100}\n\n# node_3 operation. \ndef add_assistance(state: OverallState) -> OverallState:\n    print(f\"Node 3: {state}\")\n    return {\"money\": 100}\n\n# Routing Function. \ndef check_if_poor(state: OverallState):\n    print(f\"Routing function: {state}\")\n    # if poor, add asssitance\n    if (state['money'] < 100): return \"node_3\"\n    else: return END\n\n\ngraph = StateGraph(OverallState, input = PersonState)\ngraph.add_edge(START, 'node_1')\ngraph.add_node('node_1', pay_salary)\ngraph.add_edge('node_1', 'node_2')\n\ngraph.add_node('node_2', subtract_expenses)\ngraph.add_conditional_edges('node_2', check_if_poor)\n\ngraph.add_node('node_3', add_assistance)\ngraph.add_edge(\"node_3\", END)\n\nworkflow = graph.compile()\noutput_dict = workflow.invoke({\"name\": \"Ahmed\"})\nprint(f\"Final output: {output_dict}\")\nError Message and Stack Trace (if applicable)\n(No exception, but inaccurate console output)\n\nCONSOLE OUTPUT:\nNode 1: {'name': 'Ahmed', 'money': 0}\nNode 2: {'purchases': [], 'total_cost': 0}\nRouting function: {'money': -100, 'purchases': ['new iphone'], 'total_cost': 100}\nNode 3: {'name': 'Ahmed', 'money': 900, 'purchases': ['new iphone'], 'total_cost': 100}\nFinal output: {'name': 'Ahmed', 'money': 1000, 'purchases': ['new iphone'], 'total_cost': 100}\nDescription\n(Derived from Discussion Post #2197 , please see discussion post for detailed description)\nThe Console Output line that is inaccurate:\nRouting function: {'money': -100, 'purchases': ['new iphone'], 'total_cost': 100}\n\nHow does money have a value of -100? In the method pay_salary, the money attribute is set to 1000. Then, in the subtract_expenses method, I subtract 100 from it. So, the money attribute should have a value of 900, not -100.\nThe line that is causing the weird output (after debugging)\ndef subtract_expenses(state: PurchasesState) -> OverallState:\nHow is it causing the issue?\nWhen I change the inputted state from PurchasesState to OverallState, it works as expected, i.e. the output printed in the routing function check_if_poor is:\nRouting function: {'money': 900, 'purchases': ['new iphone'], 'total_cost': 100, 'name': 'Ahmed'}\n\nWhat's my question?\nWhy does changing the type of the inputted state in the function subtract_expenses lead me to receive different values for the money attribute in the routing function check_if_poor?\nThanks in advance!\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.19045\nPython Version:  3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.3.19\nlangchain: 0.3.7\nlangchain_community: 0.3.7\nlangsmith: 0.1.144\nlangchain_openai: 0.2.9\nlangchain_text_splitters: 0.3.2\nlanggraph: 0.2.53\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.7\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.7\nhttpx: 0.27.2\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlanggraph-checkpoint: 2.0.5\nlanggraph-sdk: 0.1.36\nnumpy: 1.26.4\nopenai: 1.55.0\norjson: 3.10.11\npackaging: 24.2\npydantic: 2.10.0\npydantic-settings: 2.6.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.35\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\n", "created_at": "2024-11-21", "closed_at": "2025-03-11", "labels": ["bug"], "State": "closed", "Author": "ahmed33033"}
{"issue_number": 2497, "issue_title": "langgraph-sdk(JS) broken on 0.0.26 in angular", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nhttps://github.com/langchain-ai/langgraph/blob/main/libs/sdk-js/src/client.ts#L56\n\nI can see 0.0.26 has some additional change to use \"process\"\nprocess is not available on spa without SSR. please make optional here.\n\n\n0.0.25 is fine.\nError Message and Stack Trace (if applicable)\ncore.mjs:7195 ERROR ReferenceError: process is not defined\n    at _LangGraphApiService.getClient (lang-graph-api.service.ts:130:12)\n    at _LangGraphApiService.<anonymous> (lang-graph-api.service.ts:62:35)\n    at Generator.next (<anonymous>)\n    at chunk-OWZQ6ICK.js:38:61\n    at __async (chunk-OWZQ6ICK.js:22:10)\nDescription\nthere is break changes in 0.0.26\nSystem Info\nPlatform (window)\nAngular", "created_at": "2024-11-21", "closed_at": "2024-11-21", "labels": [], "State": "closed", "Author": "junan-trustarc"}
{"issue_number": 2479, "issue_title": "LangGraph issue for react agent tool artifact being streamed back in final response", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nimport base64\nimport math\nimport os\nimport pprint\nfrom math import cos, sin\nfrom typing import Tuple\n\n\nimport mlflow\nimport numpy as np\nimport plotly.graph_objects as go\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph.graph import CompiledGraph\nfrom langgraph.prebuilt import create_react_agent\n\n\nos.environ[\"MLFLOW_TRACKING_URI\"] = \"http://localhost:5002\"\n\n\nmlflow.openai.autolog()\n\n\n\n\n@tool\ndef calculate_area(radius: float) -> str:\n   \"\"\"\n   Calculates the area of a circle given its radius.\n\n\n   Args:\n       radius (float): The radius of the circle.\n\n\n   Returns:\n       str: The area of the circle.\n   \"\"\"\n   area = math.pi * (radius**2)\n   return f\"The area of the circle is {area:.2f}\"\n\n\n\n\n@tool(response_format=\"content_and_artifact\")\ndef circle_plot(title: str, radius: float) -> Tuple[str, str]:\n   \"\"\"\n   Plots a circle with a given radius and returns it to the user.\n\n\n   Args:\n       radius (float): The radius of the circle.\n\n\n   Returns:\n       Tuple[str, str]: A tuple containing the name of the plot and the base64-encoded image of the plot.\n   \"\"\"\n   theta = np.linspace(0, 2 * np.pi, 100)\n   x = radius * np.cos(theta)\n   y = radius * np.sin(theta)\n\n\n   fig = go.Figure()\n   fig.add_trace(go.Scatter(x=x, y=y, mode=\"lines\"))\n\n\n   image: bytes = fig.to_image(format=\"png\")\n   image_base64 = f\"data:image/png;base64,{base64.b64encode(image).decode(\"utf-8\")}\"\n   return title, image_base64\n\n\n\n\nif __name__ == \"__main__\":\n   model = ChatOpenAI(\n       model=\"gpt-4\", temperature=0, max_tokens=None, max_retries=5, stop=None\n   )\n   tools = [circle_plot]\n   agent: CompiledGraph = create_react_agent(\n       model=model,\n       tools=[circle_plot, calculate_area],\n   )\n\n\n   query = \"What is the area of a circle with a radius of 5? Circle plot\"\n   input = {\"messages\": [{\"role\": \"human\", \"content\": query}]}\n\n\n   for step in agent.stream(input, stream_mode=\"messages\"):\n       pprint.pprint(step)\nError Message and Stack Trace (if applicable)\n\"The area of the circle with a radius of 5 is 78.54. Here is the plot of the circle:\\n\\n![Circle with radius 5](data:image/png;base64,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\"\nDescription\nWe experienced in our code that the artifact output of the tool is streamed back into the end response. According to the documentation this should not be the case:\nparam artifact: Any = None#\nArtifact of the Tool execution which is not meant to be sent to the model.\nShould only be specified if it is different from the message content, e.g. if only a subset of the full tool output is being passed as message content but the full output is needed in other parts of the code.\nWe created an example python script to demonstrate this. This bug usually happens at random and for you to notice the issue you will have to try and rerun it a couple of times. If it doesnt work try setting it to gpt-4o, then run it and then set it back to gpt-4 and run it again. The issue didnt really occur with gpt-4o and its an edge case but it can cause alot of damage if it occurs. Obviously we can resolve it by simply using a different model and changing the prompt or prompting the artifact not to be streamed back. The core issue is still present that the artifact should never be fed back to the model. The tokens streamed back can occur high costs in a production environment. We also conducted mlflow traces with screenshots attached to demonstrate what is happening. We recommend running the code and one will see in the terminal what is happening as well.\n\n\nSystem Info\nmlflow==2.18.0\nnumpy==1.26.4\nplotly==5.24.1\nlangchain-openai==0.2.1\nlanggraph==0.2.35\nlangchain==0.3.7", "created_at": "2024-11-20", "closed_at": "2024-11-20", "labels": [], "State": "closed", "Author": "tiaan720"}
{"issue_number": 2444, "issue_title": "DOC: `MessageGraph` not appears in `/langgraph/reference/graphs`", "issue_body": "Issue with current documentation:\nI'm learning langgraph's core concepts through Conceptual Documents and Library Reference.\nI noticed in the Glossary Chapter,it metioned MessageGraph,but it not appears in Library Reference,though the lib indeedly export this class:\nfrom langgraph.graph.message import MessageGraph, MessagesState, add_messages\nfrom langgraph.graph.state import GraphCommand, StateGraph\n\n__all__ = [\n    \"END\",\n    \"START\",\n    \"Graph\",\n    \"StateGraph\",\n    \"GraphCommand\",\n    \"MessageGraph\",\n    \"add_messages\",\n    \"MessagesState\",\n]\nIs this omission intentional, or is it an oversight?\nIdea or request for content:\nAdd MessageGraph Class documents in  Library Reference.", "created_at": "2024-11-18", "closed_at": "2024-11-18", "labels": [], "State": "closed", "Author": "WendaoLee"}
{"issue_number": 2424, "issue_title": "There is an issue in the file libs/checkpoint-postgres/langgraph/store/postgres/base.py at line 377.", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\ndef __init__(self, conn):\n        self.store = PostgresStore(conn)\n        self.store.setup()\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\n  File \"D:\\tools-plugin\\tools_plugin\\store\\store.py\", line 63, in get_store_by_type\n    return PGStore(conn)\n           ^^^^^^^^^^^^^\n  File \"D:\\tools-plugin\\tools_plugin\\store\\store.py\", line 28, in __init__\n    self.store.setup()\n  File \"D:\\tools-plugin\\env\\Lib\\site-packages\\langgraph\\store\\postgres\\base.py\", line 377, in setup\n    version = row[\"v\"]\n              ~~~^^^^^\nTypeError: tuple indices must be integers or slices, not str\nDescription\nversion = row[\"v\"] should be  version = row[0]\nSystem Info\n..", "created_at": "2024-11-15", "closed_at": "2024-11-15", "labels": [], "State": "closed", "Author": "1315577677"}
{"issue_number": 2407, "issue_title": "AsyncPostgresSaver uses hardcoded pipeline=True even if no pipeline is available", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nany usage of the AsyncPostgresSaver where pipelines are not supported\nError Message and Stack Trace (if applicable)\n2024-11-13T13:41:45.856830355Z     await self.asgi_app(scope, receive, send)\n2024-11-13T13:41:45.856836855Z   File \"/tmp/8dd03de0847602f/antenv/lib/python3.11/site-packages/opentelemetry/instrumentation/asgi/__init__.py\", line 735, in __call__\n2024-11-13T13:41:45.856842156Z     await self.app(scope, otel_receive, otel_send)\n2024-11-13T13:41:45.856847056Z   File \"/tmp/8dd03de0847602f/antenv/lib/python3.11/site-packages/quart/app.py\", line 1693, in asgi_app\n2024-11-13T13:41:45.856851957Z     await asgi_handler(receive, send)\n2024-11-13T13:41:45.856865358Z   File \"/tmp/8dd03de0847602f/antenv/lib/python3.11/site-packages/quart/asgi.py\", line 52, in __call__\n2024-11-13T13:41:45.856870358Z     raise_task_exceptions(done)\n2024-11-13T13:41:45.856875059Z   File \"/tmp/8dd03de0847602f/antenv/lib/python3.11/site-packages/quart/utils.py\", line 180, in raise_task_exceptions\n2024-11-13T13:41:45.856880059Z     raise task.exception()\n2024-11-13T13:41:45.856884659Z   File \"/tmp/8dd03de0847602f/antenv/lib/python3.11/site-packages/quart/asgi.py\", line 107, in handle_request\n2024-11-13T13:41:45.856889560Z     await asyncio.wait_for(self._send_response(send, response), timeout=timeout)\n2024-11-13T13:41:45.856894360Z   File \"/opt/python/3.11.8/lib/python3.11/asyncio/tasks.py\", line 489, in wait_for\n2024-11-13T13:41:45.856899061Z     return fut.result()\n2024-11-13T13:41:45.856903661Z            ^^^^^^^^^^^^\n2024-11-13T13:41:45.856908361Z   File \"/tmp/8dd03de0847602f/antenv/lib/python3.11/site-packages/quart/asgi.py\", line 134, in _send_response\n2024-11-13T13:41:45.856913262Z     async for data in response_body:\n2024-11-13T13:41:45.856918162Z   File \"/tmp/8dd03de0847602f/decorators.py\", line 73, in wrapper\n2024-11-13T13:41:45.856922863Z     async for chunk in func(*args, **kwargs):\n2024-11-13T13:41:45.856927463Z   File \"/tmp/8dd03de0847602f/approaches/langgraph_approach.py\", line 176, in run\n2024-11-13T13:41:45.856934964Z     async for msg, _ in self.graph.astream(inputs, config, stream_mode=\"messages\"):\n2024-11-13T13:41:45.856940064Z   File \"/tmp/8dd03de0847602f/antenv/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 1506, in astream\n2024-11-13T13:41:45.856944864Z     async with AsyncPregelLoop(\n2024-11-13T13:41:45.856949465Z   File \"/tmp/8dd03de0847602f/antenv/lib/python3.11/site-packages/langgraph/pregel/loop.py\", line 877, in __aexit__\n2024-11-13T13:41:45.856954265Z     return await asyncio.shield(\n2024-11-13T13:41:45.856958866Z            ^^^^^^^^^^^^^^^^^^^^^\n2024-11-13T13:41:45.856963466Z   File \"/opt/python/3.11.8/lib/python3.11/contextlib.py\", line 745, in __aexit__\n2024-11-13T13:41:45.856968166Z     raise exc_details[1]\n2024-11-13T13:41:45.856972667Z   File \"/opt/python/3.11.8/lib/python3.11/contextlib.py\", line 728, in __aexit__\n2024-11-13T13:41:45.856977467Z     cb_suppress = await cb(*exc_details)\n2024-11-13T13:41:45.856981967Z                   ^^^^^^^^^^^^^^^^^^^^^^\n2024-11-13T13:41:45.856991468Z   File \"/tmp/8dd03de0847602f/antenv/lib/python3.11/site-packages/langgraph/pregel/executor.py\", line 191, in __aexit__\n2024-11-13T13:41:45.856996469Z     raise exc\n2024-11-13T13:41:45.857001069Z   File \"/tmp/8dd03de0847602f/antenv/lib/python3.11/site-packages/langgraph/pregel/loop.py\", line 797, in _checkpointer_put_after_previous\n2024-11-13T13:41:45.857005969Z     await prev\n2024-11-13T13:41:45.857010470Z   File \"/tmp/8dd03de0847602f/antenv/lib/python3.11/site-packages/langgraph/pregel/loop.py\", line 799, in _checkpointer_put_after_previous\n2024-11-13T13:41:45.857015370Z     await cast(BaseCheckpointSaver, self.checkpointer).aput(\n2024-11-13T13:41:45.857019971Z   File \"/tmp/8dd03de0847602f/antenv/lib/python3.11/site-packages/langgraph/checkpoint/postgres/aio.py\", line 267, in aput\n2024-11-13T13:41:45.857024671Z     async with self._cursor(pipeline=True) as cur:\n2024-11-13T13:41:45.857029171Z   File \"/opt/python/3.11.8/lib/python3.11/contextlib.py\", line 210, in __aenter__\n2024-11-13T13:41:45.857034172Z     return await anext(self.gen)\n2024-11-13T13:41:45.857038672Z            ^^^^^^^^^^^^^^^^^^^^^\n2024-11-13T13:41:45.857043073Z   File \"/tmp/8dd03de0847602f/antenv/lib/python3.11/site-packages/langgraph/checkpoint/postgres/aio.py\", line 340, in _cursor\n2024-11-13T13:41:45.857047873Z     async with self.lock, conn.pipeline(), conn.cursor(\n2024-11-13T13:41:45.857052373Z   File \"/opt/python/3.11.8/lib/python3.11/contextlib.py\", line 210, in __aenter__\n2024-11-13T13:41:45.857057074Z     return await anext(self.gen)\n2024-11-13T13:41:45.857061474Z            ^^^^^^^^^^^^^^^^^^^^^\n2024-11-13T13:41:45.857065974Z   File \"/tmp/8dd03de0847602f/antenv/lib/python3.11/site-packages/psycopg/connection_async.py\", line 398, in pipeline\n2024-11-13T13:41:45.857070775Z     async with pipeline:\n2024-11-13T13:41:45.857075275Z   File \"/tmp/8dd03de0847602f/antenv/lib/python3.11/site-packages/psycopg/_pipeline.py\", line 249, in __aenter__\n2024-11-13T13:41:45.857080376Z     await self._conn.wait(self._enter_gen())\n2024-11-13T13:41:45.857084976Z   File \"/tmp/8dd03de0847602f/antenv/lib/python3.11/site-packages/psycopg/connection_async.py\", line 414, in wait\n2024-11-13T13:41:45.857089676Z     return await waiting.wait_async(gen, self.pgconn.socket, interval=interval)\n2024-11-13T13:41:45.857094377Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2024-11-13T13:41:45.857098877Z   File \"/tmp/8dd03de0847602f/antenv/lib/python3.11/site-packages/psycopg/waiting.py\", line 132, in wait_async\n2024-11-13T13:41:45.857103578Z     s = next(gen)\n2024-11-13T13:41:45.857107978Z         ^^^^^^^^^\n2024-11-13T13:41:45.857112378Z   File \"/tmp/8dd03de0847602f/antenv/lib/python3.11/site-packages/psycopg/_pipeline.py\", line 69, in _enter_gen\n2024-11-13T13:41:45.857117079Z     capabilities.has_pipeline(check=True)\n2024-11-13T13:41:45.857121579Z   File \"/tmp/8dd03de0847602f/antenv/lib/python3.11/site-packages/psycopg/_capabilities.py\", line 41, in has_pipeline\n2024-11-13T13:41:45.857130180Z     return self._has_feature(\"Connection.pipeline()\", 140000, check=check)\n2024-11-13T13:41:45.857134980Z            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n2024-11-13T13:41:45.857139481Z   File \"/tmp/8dd03de0847602f/antenv/lib/python3.11/site-packages/psycopg/_capabilities.py\", line 92, in _has_feature\n2024-11-13T13:41:45.857144181Z     raise NotSupportedError(msg)\n2024-11-13T13:41:45.857148681Z psycopg.NotSupportedError: the feature 'Connection.pipeline()' is not available: the client libpq version (imported from system libraries) is 13.14; the feature requires libpq version 14.0 or newer\nDescription\nI'm using the AsyncPostgresSaver on a server that has libpq 13.4 available. Only versions of libpq >=14.0 support pipelines.\nUsually this should be respected by the Saver - for example, the from_conn_str() method has a boolean pipeline that can be set. The same applies for the regular constructor, which can be called without a pipeline. I create my AsyncPostgresSaver without a pipeline.\nHowever, there are two calls to _cursor in the code of the AsyncPostgresSaver where the existence of a pipe is not checked, and _cursor is called hardcoded as _cursor(pipeline=True), namely aput_writes() and aput().\nThis will throw an error for all users that use Postgres without pipeline support.\nIt's not fully clear to me why it would ever be necessary to use pipline=True if no pipe exists in the class instance.\nSystem Info\nlanggraph==0.2.45\nlanggraph-checkpoint-postgres==2.0.2", "created_at": "2024-11-13", "closed_at": "2024-11-18", "labels": [], "State": "closed", "Author": "epistoteles"}
{"issue_number": 2403, "issue_title": "Latency for get_state using MemorySaver is very high", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nt0 = time.time()\nsnapshot = graph_manager.graph.get_state(config)\nprint(f\"get_state latency: {(time.time() - t0) * 1000:.2f}ms\")\nmessages = snapshot.values.get(\"messages\")\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nHey! I am building an extremely low-latency application. I noticed that when using MemorySaver, the latency of this simple call is between 20-40ms -- this might sound small but is pretty significant for my use case. Is there any way to get this down?\nSystem Info\nlanggraph 0.2.46\nlanggraph-checkpoint 2.0.3\npython 3.10", "created_at": "2024-11-13", "closed_at": "2024-11-15", "labels": [], "State": "closed", "Author": "mukundt"}
{"issue_number": 2395, "issue_title": "Why the agent can't retrieve for more than 2 chat messages?", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\n@tool\ndef get_user_age(name: str) -> str:\n    \"\"\"Use this tool to find the user's age.\"\"\"\n    # This is a placeholder for the actual implementation\n    if \"bob\" in name.lower():\n        return \"20 years old\"\n    return \"41 years old\"\n\n\ntools = [get_user_age]\nmemory = MemorySaver()\napp = create_react_agent(\n    model=model,\n    tools=tools,\n    checkpointer=memory\n)\nthread_id = uuid.uuid4()\nconfig = {\"configurable\": {\"thread_id\": thread_id}}\n\n\ndef agent_call(content):\n    for event in app.stream({\"messages\": [content]}, config, stream_mode=\"values\"):\n        event[\"messages\"][-1].pretty_print()\n\n\nagent_call(content=\"hi! I'm bob.\")\nagent_call(content=\"My mother is susiy.\")\nagent_call(content=\"do you remember my name? do you remember my mother name?\")\nError Message and Stack Trace (if applicable)\nI find that the MemorySave can only remember 1 message. If I ask 'hi! I'm bob.' then 'do you remember my name? ', the agent can response correctly. However, between these two query add a useless query like '1+1=?', the agent would forget my name. Is there any params solving this problem?\nDescription\nThe third output is \"It seems there was a little confusion. I used the function to find the age of your mother, Susiy, and it appears that she is 41 years old. However, I still haven't learned your name. Could you please tell me your name?\"\nSystem Info\nThe result is wrong.", "created_at": "2024-11-12", "closed_at": "2024-11-15", "labels": [], "State": "closed", "Author": "FireAngelx"}
{"issue_number": 2392, "issue_title": "Subgraph with private state doesn't have a proper state at the last node", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nclass CodeFeedbackAgentPrivateState(BaseModel):\n    interview_question: str = Field(default=\"\")\n    interview_solution: str = Field(default=\"\")\n    code_editor_state: str = Field(default=\"\")\n\n    debugging_result: str = Field(default=\"\")\n    assessment_result: str = Field(default=\"\")\n\nclass DebuggingAgentPrivateState(BaseModel):\n    interview_question: str = Field(default=\"\")\n    interview_solution: str = Field(default=\"\")\n    code_editor_state: str = Field(default=\"\")\n\n    debugging_record: list[str] = Field(default=[])\n\n\ndef initiate_private_state(state):\n    print(\"initiate_private_state: \\n\", state.__annotations__)\n    print(\"type: \", type(state))\n    return {\n        \"interview_question\": state.interview_question,\n        \"interview_solution\": state.interview_solution,\n        \"code_editor_state\": state.code_editor_state,\n    }\n\n\ndef placeholder_node(state: DebuggingAgentPrivateState):\n    return {\n        \"debugging_record\": [\"debugging_record testing\"],\n    }\n\n\ndef placeholder_node2(state: DebuggingAgentPrivateState):\n    return {\n        \"debugging_result\": \"debugging_result testing\",\n    }\n\ndef last_node(state: CodeFeedbackAgentPrivateState):\n\n    print(\"last_node CodeFeedbackAgentPrivateState debugging_result: \\n\", state.debugging_result)\n    return {\n        \"debugging_result\": state.debugging_result, #? BUG: this return is applied to OverallState instead of CodeFeedbackAgentPrivateState\n    }\n\ng = StateGraph(CodeFeedbackAgentPrivateState)\ng.add_edge(START, n(initiate_private_state))\n\ng.add_node(initiate_private_state)\ng.add_edge(n(initiate_private_state), n(placeholder_node))\n\ng.add_node(placeholder_node)\ng.add_edge(n(placeholder_node), n(placeholder_node2))\n\ng.add_node(placeholder_node2)\ng.add_edge(n(placeholder_node2), n(last_node))\n\ng.add_node(last_node)\ng.add_edge(n(last_node), END)\n\ndebugging_agent_graph = g.compile()\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nI have double nested subgraphs.\nLevel 0: main_graph\n\nuses OverAllState as StateGraph schema\nno private states\nincludes a node of code_feedback_agent_graph\n\nLevel 1: code_feedback_agent_graph\n\nuses OverAllState as StateGraph schema\nuses CodeFeedbackAgentPrivateState as a private state\nincludes a node debugging_agent_graph\nthe private state is used when the debugging_agent_graph is triggered\n\nlevel 2: debugging_agent_graph\n\nuses CodeFeedbackAgentPrivateState as StateGraph schema\nuses DebuggingAgentPrivateState as private state\n\nAt the last node of level 2 graph, I'm returning a dictionary that includes state variable of level 1 graph's private state because that is the state that is used when level 2 graph is triggered.\nHowever, I'm getting an error saying the return should include OverAllState variable. Since level 2 graph is using CodeFeedbackAgentPrivateState as its StateGraph schema, the last node's return dict should update that instead of OverAllState. Shouldn't it be?\nSystem Info\nVersion: 0.2.28", "created_at": "2024-11-12", "closed_at": "2024-11-18", "labels": [], "State": "closed", "Author": "minki-j"}
{"issue_number": 2387, "issue_title": "Orphaned Docker Layers in CLI with docker-compose", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\n# langgraph.json\n{\n  \"docker_compose_file\": \"configs/default.docker-compose.yml\",\n  \"graphs\": {\n    \"chat_response\": \"./app/graphs/chat_response.py:graph\",\n  },\n  \"env\": \".env\",\n  \"python_version\": \"3.12\",\n  \"dependencies\": [\".\"]\n}\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nHey, Apologies if you don't consider this a bug, it's in that grey area where devex performance issues cause degredation that could be considered either. But sorry if this is in the wrong place!\nWe're encountering an issue when using langgraph up --watch with the above langgraph.json.\nEssentially the compiled dockerfile lines produced by the CLI in this region create an uncacheable layer in faux_pkgs_str, meaning that the subsequent pip installs can't use the docker build cache. That then means that if you are repetitively saving, you can easily balloon the HD space usage dramatically. I've had quite a few instances where it's exhausted the 50GB volume assigned to my docker VM and the postgres sidecar no longer inserts records. I ended up tearing down regularly and using docker builder prune about once a day on my development machine, meaning we lost traces etc.\nIn our instance, we've resorted to creating a static pyproject.toml file and a static docker-compose.yml using what the CLI creates; however this is a potential maintenance headache long-term.\nIt appears that the package name isn't used for anything outside the install and is the only dynamic content; as such I'd suggest swapping to a static pyproject.toml file that can be added to the container, so that docker can cache it.\nVery happy to open a PR if it would help, but wanted to make the observation/issue report first to gather your thoughts!\nSystem Info\n\u276f python -m langchain_core.sys_info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6030\nPython Version:  3.12.3 (v3.12.3:f6650f9ad7, Apr  9 2024, 08:18:47) [Clang 13.0.0 (clang-1300.0.29.30)]\n\nPackage Information\n\nlangchain_core: 0.3.8\nlangchain: 0.3.1\nlangchain_community: 0.3.1\nlangsmith: 0.1.130\nlangchain_anthropic: 0.2.1\nlangchain_aws: 0.2.1\nlangchain_groq: 0.2.0\nlangchain_openai: 0.2.1\nlangchain_text_splitters: 0.3.0\nlanggraph: 0.2.34\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.10.8\nanthropic: 0.34.2\nasync-timeout: Installed. No version info available.\nboto3: 1.35.32\ndataclasses-json: 0.6.7\ndefusedxml: 0.7.1\ngroq: 0.11.0\nhttpx: 0.27.2\njsonpatch: 1.33\nlanggraph-checkpoint: 2.0.0\nnumpy: 1.26.4\nopenai: 1.51.0\norjson: 3.10.7\npackaging: 24.1\npydantic: 2.9.2\npydantic-settings: 2.5.2\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.35\ntenacity: 8.5.0\ntiktoken: 0.7.0\ntyping-extensions: 4.12.2\n", "created_at": "2024-11-11", "closed_at": "2025-01-16", "labels": ["question"], "State": "closed", "Author": "davies-a"}
{"issue_number": 2386, "issue_title": "Agent Tools: ValueError: `<function_name>` is not strict. Only `strict` function tools can be auto-parsed", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\n############################ Graph flow/src/graph.py\nfrom langgraph.prebuilt import ToolNode, tools_condition\nfrom langgraph.store.memory import InMemoryStore\nfrom langgraph.graph.message import add_messages\nfrom langgraph.graph import StateGraph\n\nfrom langchain_core.messages import SystemMessage, HumanMessage, ToolMessage, AIMessage\nfrom langchain_openai import AzureChatOpenAI\nfrom langchain_core.tools import tool\n\nfrom typing_extensions import Annotated, TypedDict\nimport random\nimport os\n\n############# Graph #############\nclass ConfigSchema(TypedDict):\n    \"\"\"Define the schema for the config object.\"\"\"\n    thread_id: str\n\nclass StateSchema(TypedDict):\n    \"\"\"Define the schema for the state object.\"\"\"\n    prompt: str\n    messages: Annotated[list, add_messages] = []\n\n\n############# Tools #############\n@tool\ndef get_city_population(city:str) -> int:\n    \"\"\"Get the population of a city\"\"\"\n    return random.randint(1000, 1000000)\n\ntools = [get_city_population]\n############# Nodes #############\nclass Agent():\n    name: str = \"Agent\"\n    agent: AzureChatOpenAI = None\n    tools: list = []\n\n    def __init__(self, tools: list = [], name: str = \"Agent\"):\n        self.name = name\n\n        self.agent = AzureChatOpenAI(\n            azure_deployment=os.getenv('MODEL', 'gpt-4o-mini'),\n            azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n            openai_api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n            openai_api_version = os.getenv(\"OPENAI_API_VERSION\"),\n            model_kwargs = {\n                \"response_format\": {\"type\": \"json_object\"}\n            }\n        )\n\n        # Bind tools to the agent if they are provided\n        if len(tools) > 0:\n            self.tools = tools\n\n        # Bind tools to the agent if they are already set\n        if len(self.tools) > 0:\n            self.agent = self.agent.bind_tools(tools)\n            # self.agent = self.agent.bind_tools(tools, strict = True)\n\n        # Disabling this line as it seems not having any effect        \n        # self.agent.bind(response_format={\"type\": \"json_object\"})\n\n    def run(self, state: dict, config: dict) -> dict:\n        existing_messages = state.get('messages', [])\n\n        new_messages = []\n\n        if not existing_messages:\n            new_messages = [\n                SystemMessage(\"Your task is to find the population of a city requested by the user. Your response should be a JSON object with the city name and the population. {'<city>':'<population>'}\"),\n                HumanMessage(state.get('prompt')),\n            ]\n\n        response = self.agent.invoke(existing_messages + new_messages)\n\n        new_messages.append(response)\n\n        return {\n            \"messages\": new_messages\n        }\n\n\ngraph_builder = StateGraph(StateSchema, ConfigSchema)\n\nagent_node = Agent(tools=tools)\ntools_node = ToolNode(tools=tools)\n\n# Add the nodes to the graph\n\ngraph_builder.add_node(agent_node.name, agent_node.run)\ngraph_builder.add_node(tools_node.name, tools_node)\n\ngraph_builder.add_conditional_edges(\n    agent_node.name,\n    tools_condition\n)\ngraph_builder.add_edge(tools_node.name, agent_node.name)\n\n# Configure the graph\ngraph_builder.set_entry_point(agent_node.name)\ngraph_builder.set_finish_point(agent_node.name)\n\nin_memory_store = InMemoryStore()\ngraph = graph_builder.compile(store = in_memory_store)\n\n\n\n\n############################ FastAPI api.py\nfrom fastapi import FastAPI, Body\nfrom pydantic import BaseModel\nfrom flow.src.graph import graph as graph_module\n\napp = FastAPI()\n\nclass GraphInput(BaseModel):\n    prompt: str\n\n@app.post(\"/graph\")\nasync def create_graph(input: GraphInput):\n    return graph_module.invoke({\"prompt\": input.prompt})\nError Message and Stack Trace (if applicable)\nERROR:    Exception in ASGI application\n Traceback (most recent call last):\n   File \"/usr/local/lib/python3.11/site-packages/uvicorn-0.32.0-py3.11.egg/uvicorn/protocols/http/h11_impl.py\", line 406, in run_asgi\n     result = await app(  # type: ignore[func-returns-value]\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.11/site-packages/uvicorn-0.32.0-py3.11.egg/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n     return await self.app(scope, receive, send)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.11/site-packages/fastapi-0.115.4-py3.11.egg/fastapi/applications.py\", line 1054, in __call__\n     await super().__call__(scope, receive, send)\n   File \"/usr/local/lib/python3.11/site-packages/starlette-0.41.2-py3.11.egg/starlette/applications.py\", line 113, in __call__\n     await self.middleware_stack(scope, receive, send)\n   File \"/usr/local/lib/python3.11/site-packages/starlette-0.41.2-py3.11.egg/starlette/middleware/errors.py\", line 187, in __call__\n     raise exc\n   File \"/usr/local/lib/python3.11/site-packages/starlette-0.41.2-py3.11.egg/starlette/middleware/errors.py\", line 165, in __call__\n     await self.app(scope, receive, _send)\n   File \"/usr/local/lib/python3.11/site-packages/starlette-0.41.2-py3.11.egg/starlette/middleware/exceptions.py\", line 62, in __call__\n     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n   File \"/usr/local/lib/python3.11/site-packages/starlette-0.41.2-py3.11.egg/starlette/_exception_handler.py\", line 53, in wrapped_app\n     raise exc\n   File \"/usr/local/lib/python3.11/site-packages/starlette-0.41.2-py3.11.egg/starlette/_exception_handler.py\", line 42, in wrapped_app\n     await app(scope, receive, sender)\n   File \"/usr/local/lib/python3.11/site-packages/starlette-0.41.2-py3.11.egg/starlette/routing.py\", line 715, in __call__\n     await self.middleware_stack(scope, receive, send)\n   File \"/usr/local/lib/python3.11/site-packages/starlette-0.41.2-py3.11.egg/starlette/routing.py\", line 735, in app\n     await route.handle(scope, receive, send)\n   File \"/usr/local/lib/python3.11/site-packages/starlette-0.41.2-py3.11.egg/starlette/routing.py\", line 288, in handle\n     await self.app(scope, receive, send)\n   File \"/usr/local/lib/python3.11/site-packages/starlette-0.41.2-py3.11.egg/starlette/routing.py\", line 76, in app\n     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n   File \"/usr/local/lib/python3.11/site-packages/starlette-0.41.2-py3.11.egg/starlette/_exception_handler.py\", line 53, in wrapped_app\n     raise exc\n   File \"/usr/local/lib/python3.11/site-packages/starlette-0.41.2-py3.11.egg/starlette/_exception_handler.py\", line 42, in wrapped_app\n     await app(scope, receive, sender)\n   File \"/usr/local/lib/python3.11/site-packages/starlette-0.41.2-py3.11.egg/starlette/routing.py\", line 73, in app\n     response = await f(request)\n                ^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.11/site-packages/fastapi-0.115.4-py3.11.egg/fastapi/routing.py\", line 301, in app\n     raw_response = await run_endpoint_function(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.11/site-packages/fastapi-0.115.4-py3.11.egg/fastapi/routing.py\", line 212, in run_endpoint_function\n     return await dependant.call(**values)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/app/api.py\", line 14, in create_graph\n     return graph_module.invoke({\"prompt\": input.prompt})\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.11/site-packages/langgraph-0.2.45-py3.11.egg/langgraph/pregel/__init__.py\", line 1608, in invoke\n     for chunk in self.stream(\n   File \"/usr/local/lib/python3.11/site-packages/langgraph-0.2.45-py3.11.egg/langgraph/pregel/__init__.py\", line 1336, in stream\n     for _ in runner.tick(\n   File \"/usr/local/lib/python3.11/site-packages/langgraph-0.2.45-py3.11.egg/langgraph/pregel/runner.py\", line 58, in tick\n     run_with_retry(t, retry_policy)\n   File \"/usr/local/lib/python3.11/site-packages/langgraph-0.2.45-py3.11.egg/langgraph/pregel/retry.py\", line 29, in run_with_retry\n     task.proc.invoke(task.input, config)\n   File \"/usr/local/lib/python3.11/site-packages/langgraph-0.2.45-py3.11.egg/langgraph/utils/runnable.py\", line 410, in invoke\n     input = context.run(step.invoke, input, config, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.11/site-packages/langgraph-0.2.45-py3.11.egg/langgraph/utils/runnable.py\", line 184, in invoke\n     ret = context.run(self.func, input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/app/flow/src/graph.py\", line 71, in run\n     response = self.agent.invoke(messages)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.11/site-packages/langchain_core-0.3.15-py3.11.egg/langchain_core/runnables/base.py\", line 5354, in invoke\n     return self.bound.invoke(\n            ^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.11/site-packages/langchain_core-0.3.15-py3.11.egg/langchain_core/language_models/chat_models.py\", line 286, in invoke\n     self.generate_prompt(\n   File \"/usr/local/lib/python3.11/site-packages/langchain_core-0.3.15-py3.11.egg/langchain_core/language_models/chat_models.py\", line 786, in generate_prompt\n     return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.11/site-packages/langchain_core-0.3.15-py3.11.egg/langchain_core/language_models/chat_models.py\", line 643, in generate\n     raise e\n   File \"/usr/local/lib/python3.11/site-packages/langchain_core-0.3.15-py3.11.egg/langchain_core/language_models/chat_models.py\", line 633, in generate\n     self._generate_with_cache(\n   File \"/usr/local/lib/python3.11/site-packages/langchain_core-0.3.15-py3.11.egg/langchain_core/language_models/chat_models.py\", line 851, in _generate_with_cache\n     result = self._generate(\n              ^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.11/site-packages/langchain_openai-0.2.6-py3.11.egg/langchain_openai/chat_models/base.py\", line 701, in _generate\n     response = self.root_client.beta.chat.completions.parse(**payload)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n   File \"/usr/local/lib/python3.11/site-packages/openai-1.54.3-py3.11.egg/openai/resources/beta/chat/completions.py\", line 142, in parse\n     _validate_input_tools(tools)\n   File \"/usr/local/lib/python3.11/site-packages/openai-1.54.3-py3.11.egg/openai/lib/_parsing/_completions.py\", line 53, in validate_input_tools\n     raise ValueError(\n ValueError: `get_city_population` is not strict. Only `strict` function tools can be auto-parsed\nDescription\nWe are experiencing an issue with the AzureChatOpenAI binding tools into it. The odd behaviour is that the graph works in LangGraph Studio .. but onces it's triggered via FastAPI .. an error is thrown indicating \"Only strict function tools can be auto-parsed\".\nThe issue seems to be related to the response_format. When declaring the LLM using AzureChatOpenAI, we are passing \"response_format\": {\"type\": \"json_object\"}. We have tried using the option agent.bind(response_format={\"type\": \"json_object\"}) but it doesn't seems to have any effect, if we remove \"JSON\" word from the system prompt ... it won't give any error ... whereas it should respond \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\". So utilizing the \"model_kwargs\" within AzureChatOpenAI did activate the \"response_format\".\nTroubleshooting 1:\n\nSet model_kwargs = {\"response_format\": {\"type\": \"json_object\"}}\nAPI call to the fastapi endpoint: POST /graph {\"prompt\":\"Population of London?\"}\nGet ERROR, Only strict function tools can be auto-parsed\n\nTroubleshooting 2:\n\nSet model_kwargs = {\"response_format\": {\"type\": \"json_object\"}}\nSet agent.bind_tools(tools, strict = True)\nAPI call to the fastapi endpoint: POST /graph {\"prompt\":\"Population of London?\"}\nGet ERROR, langgraph.errors.GraphRecursionError: Recursion limit of 25 reached without hitting a stop condition\n\nTroubleshooting 3:\n\nSetting agent.bind(response_format={\"type\": \"json_object\"})\nAPI call to the fastapi endpoint: POST /graph {\"prompt\":\"Population of London?\"}\nWORKED .. but looks like response_format is not activated.\nRemoved \"JSON\" keyword from the system prompt\nAPI call to the fastapi endpoint: POST /graph {\"prompt\":\"Population of London?\"}\nWORKED, didn't trigger any error .. so json_object feature not \"used\". Which is not a valid option.\n\nTroubleshooting 4:\n\nSet model_kwargs = {\"response_format\": {\"type\": \"json_object\"}}\nRemoved \"JSON\" keyword from the system prompt\nAPI call to the fastapi endpoint: POST /graph {\"prompt\":\"Population of London?\"}\nGet ERROR (which is expected): \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\". Which means ... the \"feature\" about reponse_format is activated.\n\nAnd finally, running the initial version using model_kwargs = {\"response_format\": {\"type\": \"json_object\"}} and just using\nself.agent = agent.bind_tools(tools) WORKS perfect if the graph if triggered using LangGraph Studio.\nWithin the stack error, it shows 'openai/resources/beta/chat/completions.py' ... so not sure if this could be related?\nThanks for your help and support ;-)\nSystem Info\npython -m langchain_core.sys_info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP Mon Aug 12 08:47:01 UTC 2024\nPython Version:  3.11.10 (main, Oct 19 2024, 18:56:55) [GCC 12.2.0]\n\nPackage Information\n\nlangchain_core: 0.3.15\nlangchain: 0.3.7\nlangchain_community: 0.3.5\nlangsmith: 0.1.142\nlangchain_openai: 0.2.6\nlangchain_postgres: 0.0.12\nlangchain_text_splitters: 0.3.2\nlanggraph: 0.2.45\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.0rc1\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.7\nhttpx: 0.27.2\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlanggraph-checkpoint: 2.0.2\nlanggraph-sdk: 0.1.35\nnumpy: 1.26.4\nopenai: 1.54.3\norjson: 3.10.11\npackaging: 24.1\npgvector: 0.2.5\npsycopg: 3.2.3\npsycopg-pool: 3.2.3\npydantic: 2.10.0b1\npydantic-settings: 2.6.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nsqlalchemy: 2.0.35\nSQLAlchemy: 2.0.35\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\n", "created_at": "2024-11-11", "closed_at": "2025-01-14", "labels": [], "State": "closed", "Author": "jaimeescano"}
{"issue_number": 2380, "issue_title": "debug mode True for prebuilt agent doesn't work for ainvoke, astream_events", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nCreate tool executor\n\nCall it with ainvoke, or astream_events\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nDebug mode true doesn't print anything to console on async graph methods.\nI expect debug mode to print the steps to the console.\nSystem Info\nlatest langgraph\nmac\npython 3.12", "created_at": "2024-11-10", "closed_at": "2024-11-14", "labels": [], "State": "closed", "Author": "vikyw89"}
{"issue_number": 2351, "issue_title": "Calling .astream inside of another graph's .astream_events will set stream_mode=\"value\" regardless of config", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\ncannot have code here due to security issue\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nI have a graph A streaming LLM tokens, I tested it and can confirm I get updates as streaming. Then I have another graph B which is invoked by B.astream_events, then invoke A.astream, I can only get values of every GraphState, no matter I set stream_mode to \"value\" or \"update\"\nSystem Info\nlatest langgraph\nmac\npython 3.12", "created_at": "2024-11-06", "closed_at": null, "labels": [], "State": "open", "Author": "lucaslulucaslu"}
{"issue_number": 2341, "issue_title": "OpenAIError in Customer-Support tutotial: The api_key client option must be set either by passing api_key", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nretriever = VectorStoreRetriever.from_docs(docs, openai.Client())\nError Message and Stack Trace (if applicable)\nOpenAIError: The api_key client option must be set either by passing api_key\nDescription\nI'm trying to run the customer-support.ipynb example and I hit an issue under Tools first IN @line 43 with the mentioned error message. I can see the tutorial sets API KEYs for Anthropic and not openai. I assume this is a bug ?\nSystem Info\npython -m langchain_core.sys_info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.22631\nPython Version:  3.13.0 (tags/v3.13.0:60403a5, Oct  7 2024, 09:38:07) [MSC v.1941 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.3.15\nlangchain: 0.3.7\nlangchain_community: 0.3.5\nlangsmith: 0.1.139\nlangchain_anthropic: 0.2.4\nlangchain_openai: 0.2.5\nlangchain_text_splitters: 0.3.2\nlanggraph: 0.2.45\n", "created_at": "2024-11-05", "closed_at": "2024-11-06", "labels": [], "State": "closed", "Author": "giusarno"}
{"issue_number": 2337, "issue_title": "DOC: Multi-agent Systems", "issue_body": "Issue with current documentation:\nI think there is an issue with the \"Supervisor (as tools)\" graphic. It implies that the LLM picks from a number of tools. I don't think that is correct, since only agents pick from tools and everything that is not picked by an agent is a node and therefore requires an edge per node to be called. This is also correctly implemented in the example.\nIdea or request for content:\nSimply replace the \"llm\" with \"agent\".\nYou might as well add some tool-definition to the \"Supervisor with tools\" example for the tools, since it makes it sound like there is no tool definition necessary (which could also lead to no clear distinction between a node and a tool).", "created_at": "2024-11-05", "closed_at": "2024-11-14", "labels": [], "State": "closed", "Author": "kellerkind84"}
{"issue_number": 2336, "issue_title": "langgraph.errors.InvalidUpdateError: Can receive only one value per step. Use an Annotated key to handle multiple values. error when running retry in graph parallel processing", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom typing import TypedDict, Annotated\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    count_a : int\n    count_b : int\n    count_ab : int\n    count_b_true : bool\n    count_ab_check : int\n\n\ndef pass_node(state):\n    print(\"pass_node\")\n    print(state)\n    return state\n\n\ndef pass_node_b(state):\n    print(\"pass_node_b\")\n    count = state.get('count_a', None)\n    count += 1\n    state['count_a'] = count\n    print(state)\n    return {'count_a': count}\n\n\ndef pass_node_c(state):\n    print(\"pass_node_c\")\n    count = state.get('count_b', None)\n    count_ab_check = state.get('count_ab_check', None)\n    if count is None:\n        count = 0\n    if count_ab_check is None:\n        count_ab_check = 0\n    count += 1\n    count_ab_check += 1\n    state['count_b'] = count\n    print(state)\n    return {'count_b': count, \"count_ab_check\": count_ab_check, \"count_b_true\": False}\n\n\ndef pass_node_c_condition(state):\n    print(\"pass_node_c_condition\")\n    if not state['count_b_true']:\n        if state['count_ab_check'] >= 3:\n            print(\"next: but not true\")\n            return \"next\"\n        else:\n            print(\"retry\")\n            return \"retry\"\n    else:\n        print(\"retry\")\n        return \"next\"\n\n\ndef pass_node_d(state):\n    print(\"pass_node_d\")\n    count = state.get('count_ab', None)\n    if count is None:\n        count = 0\n    count += state['count_a'] + state['count_b']\n    state['count_ab'] = count\n    print(state)\n    return state\n\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"a\", pass_node)\nbuilder.add_edge(START, \"a\")\nbuilder.add_node(\"b\", pass_node_b)\nbuilder.add_node(\"c\", pass_node_c)\nbuilder.add_node(\"d\", pass_node)\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"a\", \"c\")\nbuilder.add_edge(\"b\", \"d\")\nbuilder.add_conditional_edges(\"c\", pass_node_c_condition,\n                              {\n                                  \"retry\": \"c\",\n                                  \"next\": \"d\"\n                              })\nbuilder.add_edge(\"d\", END)\ngraph = builder.compile()\n\n\ngraph.invoke({'count_a':1})\nError Message and Stack Trace (if applicable)\npass_node\n{'count_a': 1, 'count_b': None, 'count_ab': None, 'count_b_true': None, 'count_ab_check': None}\npass_node_c\n{'count_a': 1, 'count_b': 1, 'count_ab': None, 'count_b_true': None, 'count_ab_check': None}\npass_node_c_condition\nretry\npass_node_b\n{'count_a': 2, 'count_b': None, 'count_ab': None, 'count_b_true': None, 'count_ab_check': None}\npass_node_c\n{'count_a': 2, 'count_b': 2, 'count_ab': None, 'count_b_true': False, 'count_ab_check': 1}\npass_node_c_condition\nretry\npass_node\n{'count_a': 2, 'count_b': 1, 'count_ab': None, 'count_b_true': False, 'count_ab_check': 1}\n\nlanggraph.errors.InvalidUpdateError: At key 'count_b': Can receive only one value per step. Use an Annotated key to handle multiple values.\nDescription\nHello.\nI'm using a langraph in parallel.\n\nWhen one of the parallel nodes runs incorrectly, I gave it a conditional edge to retry it, and the error \"can receive only one value per step\" occurs.\nThere is no problem if clear the conditional edge running retry.\nWhen I printed it out, in the process of continuing to run retries on parallelized nodes, other nodes seem to be running the next node without waiting for it, but I want to know a solution.\nAttached is the sample and graph structure that simply changed my code.\nI wrote this issue in the discussion, but there is an error code, so I write it in the issue\nThank you.\nSystem Info\nlanggraph==0.2.14\nlanggraph-checkpoint==1.0.6", "created_at": "2024-11-05", "closed_at": "2024-11-18", "labels": [], "State": "closed", "Author": "saeu5407"}
{"issue_number": 2320, "issue_title": "DOC: Bug in Memory Store Example", "issue_body": "Issue with current documentation:\nI tried to reproduce the Memory Store Example but the store parameter seem to not being passed to the node, giving the error:\nTypeError: call_model() missing 1 required positional argument: 'store'\n\n(Note that I might be making a mistake in the code below)\nReproduce:\nfrom langgraph.store.memory import InMemoryStore\nfrom langgraph.graph import StateGraph, START, END, MessagesState\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langchain_core.runnables.config import RunnableConfig\nfrom langchain_core.stores import BaseStore\nimport uuid\n\n# Store\nin_memory_store = InMemoryStore()\nuser_id = \"1\"\nnamespace_for_memory = (user_id, \"memories\")\nmemory_id = str(uuid.uuid4())\nmemory = {\"food_preference\" : \"I like pizza\"}\nin_memory_store.put(namespace_for_memory, memory_id, memory)\n\n# Node\ndef call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n    return {\"messages\": [f\"LLM received {state['messages']=}\"]}\n\n# Graph\nworkflow = StateGraph(MessagesState)\nworkflow.add_node(\"call_model\", call_model)\nworkflow.add_edge(START, \"call_model\")\nworkflow.add_edge(\"call_model\", END)\ngraph = workflow.compile(checkpointer=MemorySaver(), store=in_memory_store)\n\n# Invoke the graph\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": user_id}}\n\n# Invoke\nfor update in graph.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi\"}]}, config, stream_mode=\"updates\"\n):\n    print(update)\nTrace:\nTraceback (most recent call last):\n  File \"/Users/usr/Workspace/demo-langraph/examples/_bug_store.py\", line 31, in <module>\n    for update in graph.stream(\n  File \"/Users/usr/Workspace/demo-langraph/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 1315, in stream\n    for _ in runner.tick(\n  File \"/Users/usr/Workspace/demo-langraph/.venv/lib/python3.11/site-packages/langgraph/pregel/runner.py\", line 56, in tick\n    run_with_retry(t, retry_policy)\n  File \"/Users/usr/Workspace/demo-langraph/.venv/lib/python3.11/site-packages/langgraph/pregel/retry.py\", line 29, in run_with_retry\n    task.proc.invoke(task.input, config)\n  File \"/Users/usr/Workspace/demo-langraph/.venv/lib/python3.11/site-packages/langgraph/utils/runnable.py\", line 410, in invoke\n    input = context.run(step.invoke, input, config, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/usr/Workspace/demo-langraph/.venv/lib/python3.11/site-packages/langgraph/utils/runnable.py\", line 184, in invoke\n    ret = context.run(self.func, input, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: call_model() missing 1 required positional argument: 'store'\n\nEnvironment:\n\nlanggraph=0.2.39\n", "created_at": "2024-11-04", "closed_at": "2024-11-04", "labels": [], "State": "closed", "Author": "ltoniazzi"}
{"issue_number": 2316, "issue_title": "LangGraph Studio (mac) -> AttributeError: module 'mvgbxnshzyfjxntsvqcqbkah' has no attribute 'graph'", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\ndef create_workflow() -> StateGraph:\n    \"\"\"Create the workflow graph with conditional routing\"\"\"\n    \n    # Initialize workflow with State type\n    workflow = StateGraph(State)\n    \n    # Define routing function\n    def router(state: State) -> str:\n        \"\"\"Route based on state's next value\"\"\"\n        return state.get('next', 'triage')\n    \n    # Add nodes\n    workflow.add_node(\"triage\", triage_agent)\n    workflow.add_node(\"data_agent\", data_agent)\n    workflow.add_node(\"access_agent\", access_agent)\n    workflow.add_node(\"human_in_loop\", human_in_loop)\n    \n    # Set entry point\n    workflow.set_entry_point(\"triage\")\n    \n    # Add conditional edges from triage\n    workflow.add_conditional_edges(\n        \"triage\",\n        router,\n        {\n            \"data_agent\": \"data_agent\",\n            \"access_agent\": \"access_agent\",\n            \"human_in_loop\": \"human_in_loop\"\n        }\n    )\n    \n    # Add conditional edges from human_in_loop\n    workflow.add_conditional_edges(\n        \"human_in_loop\",\n        router,\n        {\n            \"data_agent\": \"data_agent\",\n            \"access_agent\": \"access_agent\",\n            \"end\": END\n        }\n    )\n    \n    # Add edges to END from agents\n    workflow.add_conditional_edges(\n        \"data_agent\",\n        lambda x: \"end\",\n        {\"end\": END}\n    )\n    \n    workflow.add_conditional_edges(\n        \"access_agent\",\n        lambda x: \"end\",\n        {\"end\": END}\n    )\n    \n    return workflow.compile()\nError Message and Stack Trace (if applicable)\nFailed to start project lang4\nerror | Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 693, in lifespan\n    async with self.lifespan_context(app) as maybe_state:\n  File \"/usr/local/lib/python3.11/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/api/langgraph_api/lifespan.py\", line 30, in lifespan\n  File \"/api/langgraph_api/shared/graph.py\", line 230, in collect_graphs_from_env\n  File \"/usr/local/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 588, in run_in_executor\n    return await asyncio.get_running_loop().run_in_executor(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 579, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/api/langgraph_api/shared/graph.py\", line 276, in _graph_from_spec\nAttributeError: module 'mvgbxnshzyfjxntsvqcqbkah' has no attribute 'graph'\nDescription\nI am trying to load the folder to Langstudio (mac installation), i am getting the below issue:\nFile \"/usr/local/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 579, in wrapper\nreturn func(*args, **kwargs)\n^^^^^^^^^^^^^^^^^^^^^\nFile \"/api/langgraph_api/shared/graph.py\", line 276, in _graph_from_spec\nAttributeError: module 'mvgbxnshzyfjxntsvqcqbkah' has no attribute 'graph'\nSystem Info\n]", "created_at": "2024-11-04", "closed_at": "2024-11-20", "labels": [], "State": "closed", "Author": "acsankar"}
{"issue_number": 2295, "issue_title": "DOC: Incorrect link to conceptual guide", "issue_body": "Issue with current documentation:\nThis page: https://langchain-ai.github.io/langgraph/tutorials/introduction/#part-1-build-a-basic-chatbot\nHas an incorrect link to the conceptual guide: https://langchain-ai.github.io/langgraph/reference/graphs/?h=add+messages#add_messages\nThe link ends up linking to an API doc entry for the Graph class.\nIdea or request for content:\nIt should go here instead: https://langchain-ai.github.io/langgraph/concepts/", "created_at": "2024-11-01", "closed_at": "2024-11-01", "labels": [], "State": "closed", "Author": "cab938"}
{"issue_number": 2227, "issue_title": "crfeate_react_agent does not support already bound tools with gemini models", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom configs.configs import llm_api_key\nfrom langchain.tools import tool, BaseTool, StructuredTool\nfrom typing import Annotated, List, Dict  \nfrom string import Template \nimport logging\nfrom langchain_core.runnables.config import RunnableConfig\nfrom json import loads, dumps\nfrom typing import Optional, Type\n\nllm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-8b\",google_api_key=llm_api_key.gemini_api_key,temperature=1, tool_call  = True)\n\n@tool\ndef authoring_tool(user_information: RunnableConfig,\n           template: Annotated[str,\"The template created by LLM to be used for personalization\"]) -> str:\n    \"\"\"The tool gets a python string template as an input and the user information . Returns the result of personalizing the template with the user information\"\"\"\n    # t = Template(template) \n    user = user_information.get(\"configurable\", {}).get(\"user\")\n    # final_to_print = t.substitute(loads(user)) \n    final_to_print = template.format(user)\n    logging.info(final_to_print)\n    return final_to_print\n\n\nauthoring_tool_llm = llm.bind_tools(tools=[authoring_tool],tool_choice=\"any\")\nauthor_agent = create_react_agent(authoring_tool_llm, tools=[authoring_tool],state_modifier=author_agent_prompt)\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\n  File \"/Users/vibhatia/anaconda3/lib/python3.11/runpy.py\", line 198, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vibhatia/anaconda3/lib/python3.11/runpy.py\", line 88, in _run_code\n    exec(code, run_globals)\n  File \"/Users/vibhatia/.vscode/extensions/ms-python.debugpy-2024.12.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py\", line 71, in <module>\n    cli.main()\n  File \"/Users/vibhatia/.vscode/extensions/ms-python.debugpy-2024.12.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py\", line 501, in main\n    run()\n  File \"/Users/vibhatia/.vscode/extensions/ms-python.debugpy-2024.12.0-darwin-arm64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py\", line 351, in run_file\n    runpy.run_path(target, run_name=\"__main__\")\n  File \"/Users/vibhatia/.vscode/extensions/ms-python.debugpy-2024.12.0-darwin-arm64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py\", line 310, in run_path\n    return _run_module_code(code, init_globals, run_name, pkg_name=pkg_name, script_name=fname)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vibhatia/.vscode/extensions/ms-python.debugpy-2024.12.0-darwin-arm64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py\", line 127, in _run_module_code\n    _run_code(code, mod_globals, init_globals, mod_name, mod_spec, pkg_name, script_name)\n  File \"/Users/vibhatia/.vscode/extensions/ms-python.debugpy-2024.12.0-darwin-arm64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py\", line 118, in _run_code\n    exec(code, run_globals)\n  File \"/Users/vibhatia/Documents/external/Agents/MarketingAgent/marketingagents/src/usecases/B2BResearchMarketing/main.py\", line 6, in <module>\n    from Agents import  author_node, supervisor_agent, send_email_node,members\n  File \"/Users/vibhatia/Documents/external/Agents/MarketingAgent/marketingagents/src/usecases/B2BResearchMarketing/Agents.py\", line 33, in <module>\n    author_agent = create_react_agent(authoring_tool_llm, tools=[authoring_tool,send_email_tool],state_modifier=author_agent_prompt)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vibhatia/anaconda3/lib/python3.11/site-packages/langgraph/_api/deprecation.py\", line 80, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vibhatia/anaconda3/lib/python3.11/site-packages/langgraph/prebuilt/chat_agent_executor.py\", line 511, in create_react_agent\n    if _should_bind_tools(model, tool_classes):\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vibhatia/anaconda3/lib/python3.11/site-packages/langgraph/prebuilt/chat_agent_executor.py\", line 139, in _should_bind_tools\n    raise ValueError(\nValueError: Number of tools in the model.bind_tools() and tools passed to create_react_agent must match\nDescription\nI am trying to use the gemini models  ChatGoogleGenerativeAI library from langchain_google_genai to create a react agent using the pre build create_react_agent function . But I need to bind the tools before calling the create_react_agent as I nned to pass some specific tool config. when I do this _should_bind_tools gives error raise ValueError(f\"Missing tools '{missing_tools}' in the model.bind_tools()\")\nSystem Info\nlangchain==0.3.4\nlangchain-core==0.3.13\nlangchain-google-genai==2.0.1\nlangchain-openai==0.2.4\nlangchain-text-splitters==0.3.0\nlanggraph==0.2.39\nlanggraph-checkpoint==2.0.1\nlanggraph-sdk==0.1.33", "created_at": "2024-10-30", "closed_at": "2024-10-31", "labels": [], "State": "closed", "Author": "vidit-bhatia"}
{"issue_number": 2222, "issue_title": "No such file error on `langgraph up`  within devcontainer (`.../app_dir/-`)", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nlanggraph.json\n\n{\n  \"dependencies\": [\n    \".\"\n  ],\n  \"graphs\": {\n    \"example_graph\": \"./example_graph.py:example_graph\"\n\n  },\n  \"env\": \"./.env\",\n  \"dockerfile_lines\": [\n  ],\n  \"python_version\": \"3.12\"\n}\nexample_graph.py\nfrom typing import Annotated, Any, NotRequired, Sequence, TypedDict\n\nfrom langchain_core.messages import AIMessage, BaseMessage\nfrom langgraph.graph import StateGraph, add_messages\nfrom langgraph.graph.graph import CompiledGraph\n\n\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], add_messages]\n\ndef node1(state: AgentState) -> AgentState:\n    return {\"messages\": [AIMessage(\"Hello 1\")]}\n\n\nasync def node2(state: AgentState) -> AgentState:\n    return {\"messages\": [AIMessage(\"Hello 2\")]}\n\n\ndef make_graph() -> CompiledGraph:\n    state_graph = StateGraph(state_schema=AgentState)\n    state_graph.add_node(\"node1\", node1)\n    state_graph.add_node(\"node2\", node2)\n\n    state_graph.set_entry_point(\"node1\")\n    state_graph.add_edge(\"node1\", \"node2\")\n    state_graph.set_finish_point(\"node2\")\n\n    return state_graph.compile()\n\nexample_graph = make_graph()\n\ndevcontainer.json\n{\n\t\"name\": \"file_not_found_issue\",\n\t\"image\": \"mcr.microsoft.com/devcontainers/python:1-3.12-bullseye\",\n\t\"features\": {\n\t\t\"ghcr.io/devcontainers/features/docker-in-docker:2\": {\n\t\t\t\"moby\": true,\n\t\t\t\"azureDnsAutoDetection\": true,\n\t\t\t\"installDockerBuildx\": true,\n\t\t\t\"installDockerComposeSwitch\": true,\n\t\t\t\"version\": \"27.0\"\n\t\t}\n\t},\n\t\"postCreateCommand\": \"pip install langgraph-cli && langgraph up\"\n}\n\nError Message and Stack Trace (if applicable)\n`langgraph up`\n\nStarting LangGraph API server...\nFor local dev, requires env var LANGSMITH_API_KEY with access to LangGraph Cloud closed beta.\nFor production use, requires a license key in env var LANGGRAPH_CLOUD_LICENSE_KEY.\n\\ Building...open /workspaces/temp_no_such_file_issue/-: no such file or directory\nDescription\nI don't know where the issue lies between langgraph, docker, devcontainer... I don't see issues raised in the docker or devcontainer repos related to this yet, which makes me think it could be some strange interaction with langgraph. Feel free to close if this is not relevant enough. Just wanted to post in case others have noticed the same issue.\nThis issue just started happening today, and I cannot understand where it comes from.\nUsing the extremely minimal example of a langgraph app provided (also with a .env file that contains a valid api key), I am able to run langgraph up no problem from wsl2. But trying to do the same from within a devcontainer, I keeep getting the error, no such file or directory with directory name ending with a -.\nThings I have tried:\n\nUsing docker-in-docker devcontainer feature with docker engine versions 26.1, 27.0 (and older, 27.1/2 don't seem to exist)\nUsing docker-out-of-docker devcontainer feature\nUsing langgraph-cli==0.1.50 (and 51 and 52)\nUsing langgraph==0.2.37 (and 38 and 39)\n\nSo, it's not due to a very new release of langgraph, langgraph-cli, or docker engines.\nAlso, my external (wsl2) docker version is 27.2\nI first noticed this issue in my CI workflow today. Re-running a successful one from yesterday still works (but I think my devcontainer build is cached).\nI don't usually use the devcontainer locally, but once my CI failed, I tried locally and it failed the same way. However, even going back to the same git commit from yesterday does not work locally.\nMostly wanted to post this in case anyone else comes across the same issue.\nAlso, deploying to langgraph-cloud still works.\nSystem Info\npip freeze output\nclick==8.1.7\ngitdb==4.0.11\nGitPython==3.1.41\nlanggraph-cli==0.1.52\nsetuptools==69.0.3\nsmmap==5.0.1\n\ndocker version output (from in the devcontainer)\nServer:\n Engine:\n  Version:          26.1.5-1\n  API version:      1.45 (minimum version 1.24)\n  Go version:       go1.21.12\n  Git commit:       411e817ddf710ff8e08fa193da80cb78af708191\n  Built:            Tue Jul 23 19:36:28 2024\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.6.36-1\n  GitCommit:        88c3d9bc5b5a193f40b7c14fa996d23532d6f956\n runc:\n  Version:          1.1.15-1\n  GitCommit:        bc20cb4497af9af01bea4a8044f1678ffca2745c\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n\ndocker version (from wsl2)\nServer: Docker Desktop  ()\nEngine:\n Version:          27.2.0\n API version:      1.47 (minimum version 1.24)\n Go version:       go1.21.13\n Git commit:       3ab5c7d\n Built:            Tue Aug 27 14:15:15 2024\n OS/Arch:          linux/amd64\n Experimental:     false\ncontainerd:\n Version:          1.7.20\n GitCommit:        8fc6bcff51318944179630522a095cc9dbf9f353\nrunc:\n Version:          1.1.13\n GitCommit:        v1.1.13-0-g58aa920\ndocker-init:\n Version:          0.19.0\n GitCommit:        de40ad0\n", "created_at": "2024-10-29", "closed_at": null, "labels": [], "State": "open", "Author": "TimChild"}
{"issue_number": 2220, "issue_title": "ToolNode doesn't detect `InjectedState` annotations for tools with Pydantic `args_schema`", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\n.\nError Message and Stack Trace (if applicable)\n.\nDescription\n.\nSystem Info\n.", "created_at": "2024-10-29", "closed_at": "2024-10-31", "labels": [], "State": "closed", "Author": "vbarda"}
{"issue_number": 2207, "issue_title": "DOC: In the github pages the aupdate_state is not documented", "issue_body": "Issue with current documentation:\n\n Checking the github pages I can not find the method aupdate_state. Nevertheless the library has implemented this method\nIdea or request for content:\nNo response", "created_at": "2024-10-29", "closed_at": "2024-12-20", "labels": [], "State": "closed", "Author": "sruap1214"}
{"issue_number": 2198, "issue_title": "typing.TypedDict is not supported in with pydantic in Python < 3.12", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nAssume installing any template would do the same, as long as you're on the same dependency versions (but I also tried to upgrade to the latest - didn't work).\n\nThe codebase is huge and with a sensitive logic, so I can't upload it here. But you can access the deployment link I provided below to debug.\nError Message and Stack Trace (if applicable)\n27/10/2024, 15:20:20\nStarted server process [1]\n27/10/2024, 15:20:20\nWaiting for application startup.\n27/10/2024, 15:20:21\nPostgres pool stats\n27/10/2024, 15:20:21\nRedis pool stats\n27/10/2024, 15:20:21\nRegistering graph with id 'agent'\n27/10/2024, 15:20:21\nStarting cron scheduler\n27/10/2024, 15:20:21\nApplication startup complete.\n27/10/2024, 15:20:21\nStarting 10 background workers\n27/10/2024, 15:20:21\nUvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)\n27/10/2024, 15:20:22\nQueue stats\n27/10/2024, 15:20:22\nWorker stats\n27/10/2024, 15:20:48\nHTTP Request: GET https://api.smith.langchain.com/auth/public \"HTTP/1.1 200 OK\"\n27/10/2024, 15:20:48\nPOST /assistants/search 200 147ms\n27/10/2024, 15:20:49\nHTTP Request: GET https://api.smith.langchain.com/auth/public \"HTTP/1.1 200 OK\"\n27/10/2024, 15:20:49\nPOST /assistants/search 200 79ms\n27/10/2024, 15:20:49\nHTTP Request: GET https://api.smith.langchain.com/auth/public \"HTTP/1.1 200 OK\"\n27/10/2024, 15:20:49\nHTTP Request: GET https://api.smith.langchain.com/auth/public \"HTTP/1.1 200 OK\"\n27/10/2024, 15:20:49\nHTTP Request: GET https://api.smith.langchain.com/auth/public \"HTTP/1.1 200 OK\"\n27/10/2024, 15:20:49\nGET /assistants/fe096781-5601-53d2-b2f6-0d3403f7e9ca/subgraphs 200 110ms\n27/10/2024, 15:20:49\nGET /assistants/fe096781-5601-53d2-b2f6-0d3403f7e9ca/schemas 200 244ms\n27/10/2024, 15:20:49\nPOST /threads/search 200 247ms\n27/10/2024, 15:20:49\nHTTP Request: GET https://api.smith.langchain.com/auth/public \"HTTP/1.1 200 OK\"\n27/10/2024, 15:20:49\nException in ASGI application\n\n  + Exception Group Traceback (most recent call last):\n  |   File \"/usr/local/lib/python3.11/site-packages/starlette/_utils.py\", line 76, in collapse_excgroups\n  |     yield\n  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/base.py\", line 186, in __call__\n  |     async with anyio.create_task_group() as task_group:\n  |   File \"/usr/local/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 763, in __aexit__\n  |     raise BaseExceptionGroup(\n  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 401, in run_asgi\n    |     result = await app(  # type: ignore[func-returns-value]\n    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n    |     return await self.app(scope, receive, send)\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/applications.py\", line 113, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n    |     raise exc\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    |     await self.app(scope, receive, _send)\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n    |     await self.simple_response(scope, receive, send, request_headers=headers)\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n    |     await self.app(scope, receive, send)\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/base.py\", line 185, in __call__\n    |     with collapse_excgroups():\n    |   File \"/usr/local/lib/python3.11/contextlib.py\", line 158, in __exit__\n    |     self.gen.throw(typ, value, traceback)\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    |     raise exc\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/base.py\", line 187, in __call__\n    |     response = await self.dispatch_func(request, call_next)\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/langgraph_license/middleware.py\", line 20, in dispatch\n    |     response = await call_next(request)\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/base.py\", line 163, in call_next\n    |     raise app_exc\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/base.py\", line 149, in coro\n    |     await self.app(scope, receive_or_disconnect, send_no_error)\n    |   File \"/api/langgraph_api/shared/http_logger.py\", line 58, in __call__\n    |   File \"/api/langgraph_api/shared/http_logger.py\", line 52, in __call__\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 715, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 735, in app\n    |     await route.handle(scope, receive, send)\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 460, in handle\n    |     await self.app(scope, receive, send)\n    |   File \"/api/langgraph_api/auth/middleware.py\", line 36, in __call__\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/authentication.py\", line 48, in __call__\n    |     await self.app(scope, receive, send)\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 715, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 735, in app\n    |     await route.handle(scope, receive, send)\n    |   File \"/api/langgraph_api/shared/route.py\", line 118, in handle\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 288, in handle\n    |     await self.app(scope, receive, send)\n    |   File \"/api/langgraph_api/shared/route.py\", line 37, in app\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/api/langgraph_api/shared/route.py\", line 32, in app\n    |   File \"/usr/local/lib/python3.11/site-packages/langgraph_storage/retry.py\", line 34, in wrapper\n    |     return await func(*args, **kwargs)\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/api/langgraph_api/api/assistants.py\", line 145, in get_assistant_graph\n    |   File \"/usr/local/lib/python3.11/site-packages/langgraph/graph/graph.py\", line 526, in get_graph\n    |     START: graph.add_node(self.get_input_schema(config), START)\n    |                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/langgraph/graph/state.py\", line 501, in get_input_schema\n    |     return _get_schema(\n    |            ^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/langgraph/graph/state.py\", line 820, in _get_schema\n    |     return create_model(\n    |            ^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/langgraph/utils/pydantic.py\", line 24, in create_model\n    |     return create_model_v2(\n    |            ^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/langchain_core/utils/pydantic.py\", line 631, in create_model_v2\n    |     return _create_model_base(\n    |            ^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/pydantic/main.py\", line 1600, in create_model\n    |     return meta(\n    |            ^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py\", line 224, in __new__\n    |     complete_model_class(\n    |   File \"/usr/local/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py\", line 577, in complete_model_class\n    |     schema = cls.__get_pydantic_core_schema__(cls, handler)\n    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/pydantic/main.py\", line 671, in __get_pydantic_core_schema__\n    |     return handler(source)\n    |            ^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/pydantic/_internal/_schema_generation_shared.py\", line 83, in __call__\n    |     schema = self._handler(source_type)\n    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 655, in generate_schema\n    |     schema = self._generate_schema_inner(obj)\n    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 924, in _generate_schema_inner\n    |     return self._model_schema(obj)\n    |            ^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 739, in _model_schema\n    |     {k: self._generate_md_field_schema(k, v, decorators) for k, v in fields.items()},\n    |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 739, in <dictcomp>\n    |     {k: self._generate_md_field_schema(k, v, decorators) for k, v in fields.items()},\n    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 1115, in _generate_md_field_schema\n    |     common_field = self._common_field_schema(name, field_info, decorators)\n    |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 1308, in _common_field_schema\n    |     schema = self._apply_annotations(\n    |              ^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 2107, in _apply_annotations\n    |     schema = get_inner_schema(source_type)\n    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/pydantic/_internal/_schema_generation_shared.py\", line 83, in __call__\n    |     schema = self._handler(source_type)\n    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 2088, in inner_handler\n    |     schema = self._generate_schema_inner(obj)\n    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 929, in _generate_schema_inner\n    |     return self.match_type(obj)\n    |            ^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 999, in match_type\n    |     return self._typed_dict_schema(obj, None)\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py\", line 1461, in _typed_dict_schema\n    |     raise PydanticUserError(\n    | pydantic.errors.PydanticUserError: Please use `typing_extensions.TypedDict` instead of `typing.TypedDict` on Python < 3.12.\n    | \n    | For further information visit https://errors.pydantic.dev/2.9/u/typed-dict-version\n\n\n### Description\n\nHappening on langsmith, when trying to run a langgraph studio on cloud. Deployment URL: https://smith.langchain.com/o/bc95d4c8-cf10-4e47-9b38-c0bc0a195ff6/host/4648030b-6387-4d60-afb2-77b41c2f812a?tab=1&revisionPeek=891430b7-3548-4f46-84c8-7a07a5845351\n\nThis issue seemed to be fixed https://github.com/langchain-ai/langgraph/issues/1856 , but in fact is still ongoing.\n\n### System Info\n\npip freeze\n\nlangchain==0.3.4\nlangchain-community==0.3.3\nlangchain-core==0.3.13\nlangchain-openai==0.2.3\nlangchain-text-splitters==0.3.0\n\n--\n\nOther deps\n\nopenai==1.52.2\npsycopg[pool,binary]==3.1.19\npython-dotenv==1.0.1\nlanggraph==0.2.39\nlangchain-core==0.3.13\nlangchain-openai==0.2.3\n\n--\n\nrunning on linux (your cloud)\npython 3.11.5\n", "created_at": "2024-10-27", "closed_at": "2024-10-31", "labels": [], "State": "closed", "Author": "n-sviridenko"}
{"issue_number": 2209, "issue_title": "Issue: Multiple builds in the single deployment not supported", "issue_body": "Issue you'd like to raise.\nI was hosting my langgraph on langsmith but after some builds it was not updating the code that I wrote in the new build and was still trying to execute the same code from the last build which didn't even existed.\nSuggestion:\nNo response", "created_at": "2024-10-24", "closed_at": "2024-10-29", "labels": [], "State": "closed", "Author": "Pratham271"}
{"issue_number": 2169, "issue_title": "Inconsistency .invoke and .stream for graph's last callback and output_keys affecting streamed values", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom typing import TypedDict\n\nfrom langchain_core.callbacks.base import BaseCallbackHandler\nfrom langchain_core.runnables.config import RunnableConfig\nfrom langgraph.graph import END, StateGraph\nfrom langgraph.graph.graph import START\n\n\nclass CustomCallbackHandler(BaseCallbackHandler):\n    def on_chain_end(self, _outputs, **kwargs):\n        print(\"on_chain_end\", _outputs)\n\n\nclass OutputType(TypedDict):\n    a: int\n\n\nclass State(TypedDict):\n    a: int\n    b: int\n\n\ngraph = StateGraph(State, output=OutputType)\ngraph.add_node(\"node_a\", lambda state: {\"a\": state[\"a\"] + 1})\ngraph.add_node(\"node_b\", lambda state: {\"b\": state[\"b\"] + 1})\n\ngraph.add_edge(START, \"node_a\")\ngraph.add_edge(\"node_a\", \"node_b\")\ngraph.add_edge(\"node_b\", END)\n\ninput = {\"a\": 0, \"b\": 0}\n\nconfig = RunnableConfig(callbacks=[CustomCallbackHandler()])\n\ncompiled_graph = graph.compile()\n\ninvoke_result = compiled_graph.invoke(input, config)\nprint(\"invoke result: \", invoke_result)\n\nprint(\"**************\")\nprint(\"graph without output keys defined\")\nprint(\"**************\")\ngraph_without_output_keys_results = []\nfor stream_output in compiled_graph.stream(input, config):\n    graph_without_output_keys_results.append(stream_output)\n\nprint(\"*** stream outputs ***\")\nfor stream_output in graph_without_output_keys_results:\n    print(stream_output)\n\nprint(\"**************\")\nprint(\"graph with output keys (a) defined\")\nprint(\"**************\")\ngraph_with_output_keys_results = []\nfor stream_output in compiled_graph.stream(input, config, output_keys=[\"a\"]):\n    graph_with_output_keys_results.append(stream_output)\n\nprint(\"*** stream outputs ***\")\nfor stream_output in graph_with_output_keys_results:\n    print(stream_output)\nError Message and Stack Trace (if applicable)\n[Annotated result]\n\non_chain_end {'a': 0, 'b': 0}\non_chain_end {'a': 1}\non_chain_end {'a': 1}\non_chain_end {'b': 1}\non_chain_end {'b': 1}\non_chain_end {'a': 1} <--- This is the graph's final callback that I am tracing\ninvoke result:  {'a': 1}\n**************\ngraph without output keys defined\n**************\non_chain_end {'a': 0, 'b': 0}\non_chain_end {'a': 1}\non_chain_end {'a': 1}\non_chain_end {'b': 1}\non_chain_end {'b': 1}\non_chain_end {'a': 1, 'b': 1} <-- This is the graph's final callback result when using stream\n*** stream outputs ***\n{'node_a': {'a': 1}}\n{'node_b': {'b': 1}}\n**************\ngraph with output keys (a) defined\n**************\non_chain_end {'a': 0, 'b': 0}\non_chain_end {'a': 1}\non_chain_end {'a': 1}\non_chain_end {'b': 1}\non_chain_end {'b': 1}\non_chain_end {'a': 1} <-- This is the graph's final callback result when using stream with output_keys\n*** stream outputs ***\n{'node_a': {'a': 1}}\n{'node_b': None} <-- I expect this not to be None, as I want to stream content, that is not necessarily returned from the graph in the end\nDescription\nHere is a minimal repo to show the encountered inconsistency: https://github.com/kaiwend/langgraph-streaming-inconsistency\nI have been working with langgraph for a while now, mostly invoking my graph using .invoke. Now after some time, I wanted to introduce streaming into my app. The overall goal is to get some internals from the graph execution out earlier to the user and not just after the whole invocation is done, e.g. logging and streaming the output to the user. While doing that, I noticed some inconsistencies on how the streaming behaves in combination with callbacks. I am using tracing that uses the callbacks to collect traces. I am working with quite large amounts of data, so limiting what is coming in and out of nodes is crucial for the task at hand. However when using streaming, I noticed the following difference to the non-streamed version:\n\nI have an output TypedDict, which I assign to my graph StateGraph(State, output=OutputType). When using .invoke, the output type is used and I only get what's defined in OutputType. This also leads to the last callback also being triggered with OutputType.\nWhen using streaming on the other hand, the last callback from the graph get my whole state, which is a problem in regards to the sheer size of my state's contents\nI tried adding output_keys= (keys from my OutputType) to my graph.stream call. This lead to the last callback being triggered correctly, with only returning OutputType. However this also had the side effect of the streamed values becoming limited to those output_keys as well.\n\nTo summarize, I think there are 2 potential issues at hand:\n\nWhen streaming the graph, the final callback is called with the whole state instead of being limited to the type that is defined in output=\nWhen setting output_keys= on graph.stream, the streamed values in between are also filtered by those keys and not only the final output keys, which is the opposite of what I would have expected, but might be intended?\n\nSystem Info\nlanggraph==0.2.39\nlanggraph-checkpoint==2.0.1\nlanggraph-sdk==0.1.33\nmac os\nPython 3.11.6", "created_at": "2024-10-24", "closed_at": null, "labels": [], "State": "open", "Author": "kaiwend"}
{"issue_number": 2870, "issue_title": "Resume value get reused when resuming parent graph which has subgraph with multiple interrupts", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n# graphs\nfrom typing import TypedDict\nfrom langgraph.graph import StateGraph, START, END\nfrom app.ai.agent.base import BaseAgent\nfrom langgraph.types import Checkpointer, interrupt\nfrom langgraph.checkpoint.memory import MemorySaver\n\nclass AgentState(TypedDict):\n    input: str\n\n\ndef node_1(state: AgentState):\n    result = interrupt(\"interrupt node 1\")\n    print(\"result\", result)\n\ndef node_2(state: AgentState):\n    result = interrupt(\"interrupt node 2\")\n    print(\"result\", result)\n\nsub_graph = StateGraph(AgentState).add_node(\"node_1\", node_1).add_node(\"node_2\", node_2).add_edge(START, \"node_1\").add_edge(\"node_1\", \"node_2\").add_edge(\"node_2\", END).compile()\n\n\n\ndef invoke_sub_agent(state: AgentState):\n    sub_graph.invoke(state)\n\n\nparent_agent = StateGraph(AgentState).add_node(\"invoke_sub_agent\", invoke_sub_agent).add_edge(START, \"invoke_sub_agent\").add_edge(\"invoke_sub_agent\", END).compile(checkpointer=MemorySaver())\n\n# invoking the parent graph\nfrom app.ai.agent.test import parent_agent\nimport uuid\n\nthread_id = uuid.uuid4()\nparent_agent.invoke({\"input\": \"test\"}, config={\"configurable\": {\"thread_id\": thread_id}}, subgraphs=True)\n\n# then the graph get interrupted by the first interrupt as expected\n\n# resume from the first interrupt\nparent_agent.invoke(Command(resume=True), config={\"configurable\": {\"thread_id\": thread_id}}, subgraphs=True)\n\n# then not just first interrupt get passed, the second interrupt also get passed with same resume value printed out.\n\nThe code is supposed to be interrupted twice and wait for human input for twice\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nAs described in the Example code section. I have\nparent_graph -> subgraph (two interrupts)\nwhen invoke parent_graph, the parent_graph stops at the first subgraph interrupt as expected. Then I resume, the expected behavior is the graph should pass the first interrupt and stop at the second interrupt, however, both interrupt passed. The second interrupt returns the value I passed for resuming the first interrupt instead of raising a new interrupt.\nSystem Info\nlanggraph==0.2.60", "created_at": "2024-12-24", "closed_at": "2025-01-15", "labels": ["investigate"], "State": "closed", "Author": "chinazhangyujia"}
{"issue_number": 2866, "issue_title": "The difference between two scene processing strategies at the merging node of LangGraph parallel branch.", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport operator\nfrom langgraph.graph import StateGraph\nfrom typing import Annotated\nfrom typing import TypedDict\n\n\nclass AgentState(TypedDict):\n    input: str\n    history: Annotated[list[dict], operator.add]\n\n\ndef getapp():\n    def start(state):\n        print(\"run: START\")\n        return {\"history\": [{}]}\n\n    def end(state):\n        print(\"run: END\")\n        # \u8fd4\u56de\u4ee3\u7406\u7684\u7ed3\u679c\n        return {\"history\": [{}]}\n\n    def run_agent(name):\n        def run(state):\n            print(\"run: \" + name)\n            return {\"history\": [{}]}\n\n        return run\n\n    def conditional(state):\n        # \u8fd4\u56de\u4ee3\u7406\u7684\u7ed3\u679c\n        return [\"agent2\", \"agent3\"]\n\n    workflow = StateGraph(AgentState)\n    # scenario_one(conditional, end, run_agent, start, workflow)\n    scenario_two(conditional, end, run_agent, start, workflow)\n    app = workflow.compile()\n    return app\n\n\ndef scenario_one(conditional, end, run_agent, start, workflow):\n    \"\"\"\n    Scenario 1 graph:\n    start -> agent1 -> agent2 -> agen4 -> end\n                    -> agent3\n    Branch one is agent2.\n    Branch tow is agent2.\n    The number of nodes in branch one is equal to the number of nodes in branch two.\n    \"\"\"\n    workflow.add_node(\"start\", start)\n    workflow.add_node(\"end\", end)\n    workflow.add_node(\"agent1\", run_agent(\"agen1\"))\n    workflow.add_node(\"agent2\", run_agent(\"agen2\"))\n    workflow.add_node(\"agent3\", run_agent(\"agen3\"))\n    workflow.add_node(\"agent4\", run_agent(\"agen4\"))\n    workflow.add_edge(\"start\", \"agent1\")\n    workflow.add_conditional_edges(\"agent1\", conditional, {\"agent2\": \"agent2\", \"agent3\": \"agent3\"})\n    workflow.add_edge(\"agent2\", \"agent4\")\n    workflow.add_edge(\"agent3\", \"agent4\")\n    workflow.add_edge(\"agent3\", \"agent4\")\n    workflow.add_edge(\"agent4\", \"end\")\n    workflow.set_entry_point(\"start\")\n    workflow.set_finish_point(\"end\")\n\n\ndef scenario_two(conditional, end, run_agent, start, workflow):\n    \"\"\"\n    Scenario 1 graph\n    start -> agent1 -> agent2                 -> agen4 -> end\n                    -> agent3 -> agent3.5\n    Branch one is agent2.\n    Branch tow is agent3 and agent3.5.\n    The number of nodes in branch two is greater than that in branch one.\n    \"\"\"\n    workflow.add_node(\"start\", start)\n    workflow.add_node(\"end\", end)\n    workflow.add_node(\"agent1\", run_agent(\"agen1\"))\n    workflow.add_node(\"agent2\", run_agent(\"agen2\"))\n    workflow.add_node(\"agent3\", run_agent(\"agen3\"))\n    workflow.add_node(\"agent3.5\", run_agent(\"agen3.5\"))\n    workflow.add_node(\"agent4\", run_agent(\"agen4\"))\n    workflow.add_edge(\"start\", \"agent1\")\n    workflow.add_conditional_edges(\"agent1\", conditional, {\"agent2\": \"agent2\", \"agent3\": \"agent3\"})\n    workflow.add_edge(\"agent2\", \"agent4\")\n    workflow.add_edge(\"agent3\", \"agent4\")\n    workflow.add_edge(\"agent3\", \"agent3.5\")\n    workflow.add_edge(\"agent3.5\", \"agent4\")\n    workflow.add_edge(\"agent4\", \"end\")\n    workflow.set_entry_point(\"start\")\n    workflow.set_finish_point(\"end\")\n\n\nif __name__ == '__main__':\n    getapp().invoke(input={\"input\": \"\"}, config={\"recursion_limit\": 10})\nError Message and Stack Trace (if applicable)\nScenario 1\uff1a\n\nrun: START\nrun:agen1\nrun:agen2\nrun:agen3\nrun:agen4\nrun: END\n\nScenario 2\uff1a\nrun: START\nrun:agen1\nrun:agen2\nrun:agen3\nrun:agen3.5\nrun:agen4\nrun: END\nrun:agen4\nrun: END\nDescription\nIn the scenario of parallel branching and merging, when the number of nodes in multiple branches is the same, the merging node only runs once, which is in line with the Wait for All Strategy. However, when the number of nodes in the branches is not the same, the merging node and subsequent processes will run multiple times, which is inconsistent with the above strategy and barely belongs to the Parallel Merge Strategy.\nSystem Info\nLangGraph Version Description\uff1a\n", "created_at": "2024-12-24", "closed_at": "2025-01-07", "labels": [], "State": "closed", "Author": "huangyuanyuan000"}
{"issue_number": 2844, "issue_title": "Command: goto argument is not working. TypeError: Command.__init__() got an unexpected keyword argument 'goto'", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nreturn Command(goto=\"multiplication_expert\", update={\"messages\": [ai_msg, tool_msg]})\nThe above code is throwing the error.\nError Message and Stack Trace (if applicable)\nTypeError: Command.__init__() got an unexpected keyword argument 'goto'\nDescription\nWhen I try to implement like below it's working:\nreturn Command(send=Send(goto_node, {\"messages\": state['messages']}), update={\"messages\": [ai_msg, tool_msg]})\nSystem Info\nUsing the latest version of langgraph", "created_at": "2024-12-19", "closed_at": "2024-12-20", "labels": [], "State": "closed", "Author": "Saisiva123"}
{"issue_number": 2841, "issue_title": "Langgraph dev (cli) error with library", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nError is inside an internal import.\nError Message and Stack Trace (if applicable)\nProcess SpawnProcess-1:\nTraceback (most recent call last):\n  File \"C:\\Users\\cwzda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\process.py\", line 314, in _bootstrap\n    self.run()\n  File \"C:\\Users\\cwzda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\multiprocessing\\process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"C:\\Users\\cwzda\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\chat-analyzer-9qU6P2e3-py3.11\\Lib\\site-packages\\uvicorn\\_subprocess.py\", line 80, in subprocess_started\n    target(sockets=sockets)\n  File \"C:\\Users\\cwzda\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\chat-analyzer-9qU6P2e3-py3.11\\Lib\\site-packages\\uvicorn\\server.py\", line 66, in run\n    return asyncio.run(self.serve(sockets=sockets))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\cwzda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\runners.py\", line 190, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\cwzda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\cwzda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 654, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"C:\\Users\\cwzda\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\chat-analyzer-9qU6P2e3-py3.11\\Lib\\site-packages\\uvicorn\\server.py\", line 70, in serve\n    await self._serve(sockets)\n  File \"C:\\Users\\cwzda\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\chat-analyzer-9qU6P2e3-py3.11\\Lib\\site-packages\\uvicorn\\server.py\", line 77, in _serve\n    config.load()\n  File \"C:\\Users\\cwzda\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\chat-analyzer-9qU6P2e3-py3.11\\Lib\\site-packages\\uvicorn\\config.py\", line 435, in load\n    self.loaded_app = import_from_string(self.app)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\cwzda\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\chat-analyzer-9qU6P2e3-py3.11\\Lib\\site-packages\\uvicorn\\importer.py\", line 19, in import_from_string\n    module = importlib.import_module(module_str)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\cwzda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"C:\\Users\\cwzda\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\chat-analyzer-9qU6P2e3-py3.11\\Lib\\site-packages\\langgraph_api\\server.py\", line 12, in <module>\n    from langgraph_api.api import routes\n  File \"C:\\Users\\cwzda\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\chat-analyzer-9qU6P2e3-py3.11\\Lib\\site-packages\\langgraph_api\\api\\__init__.py\", line 7, in <module>\n    from langgraph_api.api.assistants import assistants_routes\n  File \"C:\\Users\\cwzda\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\chat-analyzer-9qU6P2e3-py3.11\\Lib\\site-packages\\langgraph_api\\api\\assistants.py\", line 10, in <module>\n    from langgraph_api.graph import get_assistant_id, get_graph\n  File \"C:\\Users\\cwzda\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\chat-analyzer-9qU6P2e3-py3.11\\Lib\\site-packages\\langgraph_api\\graph.py\", line 24, in <module>\n    from langgraph_api.js.remote import RemotePregel\n  File \"C:\\Users\\cwzda\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\chat-analyzer-9qU6P2e3-py3.11\\Lib\\site-packages\\langgraph_api\\js\\remote.py\", line 30, in <module>\n    from langgraph_api.route import ApiResponse\n  File \"C:\\Users\\cwzda\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\chat-analyzer-9qU6P2e3-py3.11\\Lib\\site-packages\\langgraph_api\\route.py\", line 17, in <module>\n    from langgraph_api.utils import set_auth_ctx\n  File \"C:\\Users\\cwzda\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\chat-analyzer-9qU6P2e3-py3.11\\Lib\\site-packages\\langgraph_api\\utils.py\", line 8, in <module>\n    from langgraph_sdk import Auth\nImportError: cannot import name 'Auth' from 'langgraph_sdk' (C:\\Users\\cwzda\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\chat-analyzer-9qU6P2e3-py3.11\\Lib\\site-packages\\langgraph_sdk\\__init__.py)\nDescription\nI am trying to run langgraph CLI but I am getting Import Errors.\nFor dependency management, I am using poetry. Here is the full requirements:\npython = \"^3.11\"\npandas = \"^2.2.3\"\nopenai = \"^1.57.0\"\nlangchain = \"^0.3.9\"\ntiktoken = \"^0.8.0\"\nlangchain-openai = \"^0.2.11\"\nlangchain-anthropic = \"^0.3.0\"\ninstructor = \"^1.7.0\"\ntavily-python = \"^0.5.0\"\nlanggraph = \"^0.2.56\"\nfastapi = \"^0.115.6\"\ntwilio = \"^9.3.8\"\npython-dotenv = \"^1.0.1\"\nlangchain-community = \"^0.3.10\"\nsupabase = \"^2.10.0\"\nsqlmodel = \"^0.0.22\"\nboto3 = \"^1.35.83\"\nuvicorn = \"^0.34.0\"\npython-multipart = \"^0.0.20\"\nlanggraph-cli = {extras = [\"inem\"], version = \"^0.1.65\"}\nSystem Info\nlanggraph dev", "created_at": "2024-12-19", "closed_at": "2024-12-20", "labels": [], "State": "closed", "Author": "rd-rugg"}
{"issue_number": 2835, "issue_title": "DOC: System Prompt in Create ReAct agent is un effective", "issue_body": "Issue with current documentation:\nIn the nodes and edge definition from How to create a ReAct agent from scratch, we declare a system_prompt variable and pass it to the model.invoke along the state.\nThis has in fact no effect, I replaced the prompt with:\n    system_prompt = SystemMessage(\n        \"Speak in Italian\"\n    )\nWe get an english answer.\nI also noticed that we had some warnings with this\nresponse = model.invoke([system_prompt] + state[\"messages\"], config)\n\nOperator \"+\" not supported for types \"list[SystemMessage]\" and \"Sequence[BaseMessage]\"\n\nIdea or request for content:\nI managed to make it work with this:\n    system_prompt = SystemMessage(\n        \"Speak in Italian\"\n    )\n    state[\"messages\"].insert(0, system_prompt)\n    response = model.invoke(state[\"messages\"], config)\nDoing so I'm assured to have one first message containing the system prompt.\nNoteIf the same node is called several times (which is the case when a tool redirect to this node), the system prompt is injected multiple times... So I guess we can introduce a test checking that the first message is a System prompt.\nWe can imagine something like:\nif not isinstance(state[\"messages\"][0], SystemMessage):\n  state[\"messages\"].insert(0, system_prompt)\n", "created_at": "2024-12-19", "closed_at": "2024-12-19", "labels": [], "State": "closed", "Author": "DucretJe"}
{"issue_number": 2831, "issue_title": "graph stream with stream_mode=updates miss tool messages when using tools that return Command", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport os\n\nfrom langchain_core.messages import ToolMessage\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain_core.tools import tool\nfrom langchain_core.tools.base import InjectedToolCallId\nfrom langgraph.types import Command\n\nfrom typing_extensions import Annotated\n\n\n@tool\ndef add(\n    a: int,\n    b: int,\n    tool_call_id: Annotated[str, InjectedToolCallId],\n    config: RunnableConfig,\n):\n    \"\"\"add two numbers\"\"\"\n\n    result = a + b\n\n    return Command(\n        update={\n            \"messages\": [\n                ToolMessage(f\"add result: {result}\", tool_call_id=tool_call_id)\n            ],\n        }\n    )\n\n\n@tool\ndef sub(\n    a: int,\n    b: int,\n    tool_call_id: Annotated[str, InjectedToolCallId],\n    config: RunnableConfig,\n):\n    \"\"\"sub two numbers\"\"\"\n\n    result = a + b\n\n    return f\"sub result: {result}\"\n\n\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\n\nmodel = ChatOpenAI(\n    model=\"gpt-4o\",\n)\n\nfrom langgraph.checkpoint.memory import MemorySaver\n\nmemory = MemorySaver()\n\ntools = [add, sub]\nagent = create_react_agent(model, tools=tools, checkpointer=memory)\n\nconfig = {\n    \"configurable\": {\"thread_id\": \"1\"},\n}\n\n# use add tool\nfor chunk in agent.stream(\n    input={\n        \"messages\": [\n            (\n                \"user\",\n                \"add(1,1), add(1,2), add(1,3) at once\",\n            ),\n        ]\n    },\n    config=config,\n    stream_mode=\"updates\",\n):\n    for node, values in chunk.items():\n        print(f\"Receiving update from node: '{node}'\")\n        print(values)\n        print(\"\\n\\n\")\n\n# use sub tool\nfor chunk in agent.stream(\n    input={\n        \"messages\": [\n            (\n                \"user\",\n                \"sub(1,1), sub(1,2), sub(1,3) at once\",\n            ),\n        ]\n    },\n    config=config,\n    stream_mode=\"updates\",\n):\n    for node, values in chunk.items():\n        print(f\"Receiving update from node: '{node}'\")\n        print(values)\n        print(\"\\n\\n\")\n\nprint(\"======================message history=================\\n\\n\")\ncur_state = agent.get_state(config)\nmessages = cur_state.values.get(\"messages\", [])\nfor message in messages:\n    message.pretty_print()\nError Message and Stack Trace (if applicable)\nReceiving update from node: 'agent'\n{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_vvaLfOBoaWoxaCtkF0kKAWAJ', 'function': {'arguments': '{\"a\": 1, \"b\": 1}', 'name': 'add'}, 'type': 'function'}, {'id': 'call_aYTuKfiWaF8ldAR4cfqU5VXj', 'function': {'arguments': '{\"a\": 1, \"b\": 2}', 'name': 'add'}, 'type': 'function'}, {'id': 'call_qO6RfVysJhQ8gP6DaSNpSg6v', 'function': {'arguments': '{\"a\": 1, \"b\": 3}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 67, 'prompt_tokens': 85, 'total_tokens': 152, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f3927aa00d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-63311600-238f-4c51-a4cf-70617c0e9da3-0', tool_calls=[{'name': 'add', 'args': {'a': 1, 'b': 1}, 'id': 'call_vvaLfOBoaWoxaCtkF0kKAWAJ', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 1, 'b': 2}, 'id': 'call_aYTuKfiWaF8ldAR4cfqU5VXj', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 1, 'b': 3}, 'id': 'call_qO6RfVysJhQ8gP6DaSNpSg6v', 'type': 'tool_call'}], usage_metadata={'input_tokens': 85, 'output_tokens': 67, 'total_tokens': 152, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n\n\n\nReceiving update from node: 'tools'\n{'messages': [ToolMessage(content='add result: 4', name='add', id='11837479-f7b7-4f4d-b245-8a32e5d02776', tool_call_id='call_qO6RfVysJhQ8gP6DaSNpSg6v')]}\n\n\n\nReceiving update from node: 'agent'\n{'messages': [AIMessage(content='The results of the additions are as follows:\\n- \\\\(1 + 1 = 2\\\\)\\n- \\\\(1 + 2 = 3\\\\)\\n- \\\\(1 + 3 = 4\\\\)', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 183, 'total_tokens': 226, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f3927aa00d', 'finish_reason': 'stop', 'logprobs': None}, id='run-8b12c0d2-9b5a-4a6a-8d5f-56cc773c83a4-0', usage_metadata={'input_tokens': 183, 'output_tokens': 43, 'total_tokens': 226, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n\n\n\nReceiving update from node: 'agent'\n{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_vMYScsWHPGVSLMD9hS8fDA5Q', 'function': {'arguments': '{\"a\": 1, \"b\": 1}', 'name': 'sub'}, 'type': 'function'}, {'id': 'call_nxtqJoafmwh6qtFO9f5lhVBt', 'function': {'arguments': '{\"a\": 1, \"b\": 2}', 'name': 'sub'}, 'type': 'function'}, {'id': 'call_vXrXRc4IClKzKZ3dRmABGISU', 'function': {'arguments': '{\"a\": 1, \"b\": 3}', 'name': 'sub'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 67, 'prompt_tokens': 253, 'total_tokens': 320, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_04751d0b65', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-bb8e32b7-eea5-4d59-a7a1-69973a99a1be-0', tool_calls=[{'name': 'sub', 'args': {'a': 1, 'b': 1}, 'id': 'call_vMYScsWHPGVSLMD9hS8fDA5Q', 'type': 'tool_call'}, {'name': 'sub', 'args': {'a': 1, 'b': 2}, 'id': 'call_nxtqJoafmwh6qtFO9f5lhVBt', 'type': 'tool_call'}, {'name': 'sub', 'args': {'a': 1, 'b': 3}, 'id': 'call_vXrXRc4IClKzKZ3dRmABGISU', 'type': 'tool_call'}], usage_metadata={'input_tokens': 253, 'output_tokens': 67, 'total_tokens': 320, 'input_token_details': {}, 'output_token_details': {}})]}\n\n\n\nReceiving update from node: 'tools'\n{'messages': [ToolMessage(content='sub result: 2', name='sub', id='f7dfa2fb-d578-46b6-a39d-4523ac1abaed', tool_call_id='call_vMYScsWHPGVSLMD9hS8fDA5Q'), ToolMessage(content='sub result: 3', name='sub', id='c3be1d0d-7575-4ec0-be00-4e181964241a', tool_call_id='call_nxtqJoafmwh6qtFO9f5lhVBt'), ToolMessage(content='sub result: 4', name='sub', id='8d5fad1f-9fbb-4acf-be78-a766de53d322', tool_call_id='call_vXrXRc4IClKzKZ3dRmABGISU')]}\n\n\n\nReceiving update from node: 'agent'\n{'messages': [AIMessage(content='The results of the subtractions are as follows:\\n- \\\\(1 - 1 = 0\\\\)\\n- \\\\(1 - 2 = -1\\\\)\\n- \\\\(1 - 3 = -2\\\\)', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 351, 'total_tokens': 395, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f3927aa00d', 'finish_reason': 'stop', 'logprobs': None}, id='run-f94e074c-18ce-4513-9f1a-ff19be9c2532-0', usage_metadata={'input_tokens': 351, 'output_tokens': 44, 'total_tokens': 395, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n\n\n\n======================message history=================\n\n\n================================ Human Message =================================\n\nadd(1,1), add(1,2), add(1,3) at once\n================================== Ai Message ==================================\nTool Calls:\n  add (call_vvaLfOBoaWoxaCtkF0kKAWAJ)\n Call ID: call_vvaLfOBoaWoxaCtkF0kKAWAJ\n  Args:\n    a: 1\n    b: 1\n  add (call_aYTuKfiWaF8ldAR4cfqU5VXj)\n Call ID: call_aYTuKfiWaF8ldAR4cfqU5VXj\n  Args:\n    a: 1\n    b: 2\n  add (call_qO6RfVysJhQ8gP6DaSNpSg6v)\n Call ID: call_qO6RfVysJhQ8gP6DaSNpSg6v\n  Args:\n    a: 1\n    b: 3\n================================= Tool Message =================================\nName: add\n\nadd result: 2\n================================= Tool Message =================================\nName: add\n\nadd result: 3\n================================= Tool Message =================================\nName: add\n\nadd result: 4\n================================== Ai Message ==================================\n\nThe results of the additions are as follows:\n- \\(1 + 1 = 2\\)\n- \\(1 + 2 = 3\\)\n- \\(1 + 3 = 4\\)\n================================ Human Message =================================\n\nsub(1,1), sub(1,2), sub(1,3) at once\n================================== Ai Message ==================================\nTool Calls:\n  sub (call_vMYScsWHPGVSLMD9hS8fDA5Q)\n Call ID: call_vMYScsWHPGVSLMD9hS8fDA5Q\n  Args:\n    a: 1\n    b: 1\n  sub (call_nxtqJoafmwh6qtFO9f5lhVBt)\n Call ID: call_nxtqJoafmwh6qtFO9f5lhVBt\n  Args:\n    a: 1\n    b: 2\n  sub (call_vXrXRc4IClKzKZ3dRmABGISU)\n Call ID: call_vXrXRc4IClKzKZ3dRmABGISU\n  Args:\n    a: 1\n    b: 3\n================================= Tool Message =================================\nName: sub\n\nsub result: 2\n================================= Tool Message =================================\nName: sub\n\nsub result: 3\n================================= Tool Message =================================\nName: sub\n\nsub result: 4\n================================== Ai Message ==================================\n\nThe results of the subtractions are as follows:\n- \\(1 - 1 = 0\\)\n- \\(1 - 2 = -1\\)\n- \\(1 - 3 = -2\\)\nDescription\nI'm trying to use Command in tools to update graph state from tools,\nwhen i call stream func with stream_mode=updates, if the llm call multiple tools at once, only the tool messages related to last tool is stream out, others not output.\nif i define tool without return command, it works.\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP Sat Oct 7 17:52:50 CST 2023\nPython Version:  3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0]\n\nPackage Information\n\nlangchain_core: 0.3.25\nlangsmith: 0.1.140\nlangchain_openai: 0.2.6\nlanggraph_sdk: 0.1.47\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nhttpx: 0.27.0\njsonpatch: 1.33\nopenai: 1.54.3\norjson: 3.10.3\npackaging: 24.0\npydantic: 2.7.4\nPyYAML: 6.0.1\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\n", "created_at": "2024-12-19", "closed_at": "2025-01-02", "labels": [], "State": "closed", "Author": "rayshen92"}
{"issue_number": 2829, "issue_title": "Subgraph not working parallelly", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n# this work as expected in parallel\nimport operator\nimport time\nfrom functools import partial\nfrom typing import Annotated\n\nfrom langgraph.graph.state import END, START, CompiledStateGraph, StateGraph\nfrom typing_extensions import TypedDict\n\nclass ChildState(TypedDict):\n    question: str\n    results: Annotated[list[str], operator.add]\n\n\nclass ChildOutputState(TypedDict):\n    results: Annotated[list[str], operator.add]\n\n\nclass ParentState(TypedDict):\n    question: str\n    results: Annotated[list[str], operator.add]\n\n\ndef child_node(state: ChildState, i: int) -> dict:\n    print(i,\"sleeping,,,\")\n    time.sleep(i * 5)\n    print(i,\"awake,,,\")\n    return {\"results\": [f\"{state['question']} FROM {i}\"]}\n\n\ndef create_child(i: int) -> CompiledStateGraph:\n    child = StateGraph(ChildState, output=ChildOutputState)\n    child.add_node(\"sub_child1\", partial(child_node, i=i))\n    child.add_node(\"sub_child2\", partial(child_node, i=i))\n    child.add_edge(START, \"sub_child1\")\n    child.add_edge(\"sub_child1\", \"sub_child2\")\n    child.add_edge(\"sub_child2\", END)\n    return child.compile()\n\n\nasync def parent_question_generation_node(state: dict) -> dict:\n    return {\"question\": \"what's up?\"}\n\n\nasync def parent_sink_node(state: dict) -> dict:\n    return {\"results\": state[\"results\"]}\n\n\nparent = StateGraph(ParentState)\nparent.add_node(\"question_generation\", parent_question_generation_node)\nparent.add_node(\"child1\", create_child(1))\nparent.add_node(\"child2\", create_child(3))\nparent.add_node(\"sink\", parent_sink_node)\n\nparent.add_edge(START, \"question_generation\")\nparent.add_edge(\"question_generation\", \"child1\")\nparent.add_edge(\"question_generation\", \"child2\")\nparent.add_edge(\"child1\", \"sink\")\nparent.add_edge(\"child2\", \"sink\")\nparent.add_edge(\"sink\", END)\n\nparent_graph = parent.compile()\nstate = ParentState(question=\"hi\", results=[])\n\n\nasync def safe_invoke(parent_graph, state):\n    result = await parent_graph.ainvoke(state)\n    return result\n\n\nimport asyncio\n\nasync def main():\n    result = await safe_invoke(parent_graph, state)\n\nasyncio.run(main())\n\n\n\n# this is not working as expected, it is more like running synchronously\nimport operator\nimport time\nfrom functools import partial\nfrom typing import Annotated\n\nfrom langgraph.graph.state import END, START, CompiledStateGraph, StateGraph\nfrom typing_extensions import TypedDict\n\n\nclass ChildState(TypedDict):\n    question: str\n    results: Annotated[list[str], operator.add]\n\n\nclass ChildOutputState(TypedDict):\n    results: Annotated[list[str], operator.add]\n\n\nclass ParentState(TypedDict):\n    question: str\n    results: Annotated[list[str], operator.add]\n\n\nasync def child_node(state: ChildState, i: int) -> dict:\n    print(i,\"sleeping,,,\")\n    time.sleep(i * 5)\n    print(i,\"awake,,,\")\n    return {\"results\": [f\"{state['question']} FROM {i}\"]}\n\n\ndef create_child(i: int) -> CompiledStateGraph:\n    child = StateGraph(ChildState, output=ChildOutputState)\n    child.add_node(\"sub_child1\", partial(child_node, i=i))\n    child.add_node(\"sub_child2\", partial(child_node, i=i))\n    child.add_edge(START, \"sub_child1\")\n    child.add_edge(\"sub_child1\", \"sub_child2\")\n    child.add_edge(\"sub_child2\", END)\n    return child.compile()\n\n\nasync def parent_question_generation_node(state: dict) -> dict:\n    return {\"question\": \"what's up?\"}\n\n\nasync def parent_sink_node(state: dict) -> dict:\n    return {\"results\": state[\"results\"]}\n\n\nparent = StateGraph(ParentState)\nparent.add_node(\"question_generation\", parent_question_generation_node)\nparent.add_node(\"child1\", create_child(1))\nparent.add_node(\"child2\", create_child(3))\nparent.add_node(\"sink\", parent_sink_node)\n\nparent.add_edge(START, \"question_generation\")\nparent.add_edge(\"question_generation\", \"child1\")\nparent.add_edge(\"question_generation\", \"child2\")\nparent.add_edge(\"child1\", \"sink\")\nparent.add_edge(\"child2\", \"sink\")\nparent.add_edge(\"sink\", END)\n\nparent_graph = parent.compile()\nstate = ParentState(question=\"hi\", results=[])\n\n\nasync def safe_invoke(parent_graph, state):\n    result = await parent_graph.ainvoke(state)\n    return result\n\n\nimport asyncio\n\nasync def main():\n    result = await safe_invoke(parent_graph, state)\n    # print(result)\n\nasyncio.run(main())\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nI'm trying to use LangGraph to implement a parallel running subgraph.\nHowever, when subgraph include \"async def\" the graph is more like running synchronously, not in parallel.\nIn the first script, the output is like:\n13 sleeping,,,\n sleeping,,,\n1 awake,,,\n1 sleeping,,,\n1 awake,,,\n3 awake,,,\n3 sleeping,,,\n3 awake,,,\n\nThe second script looks like this:\n1 sleeping,,,\n1 awake,,,\n3 sleeping,,,\n3 awake,,,\n1 sleeping,,,\n1 awake,,,\n3 sleeping,,,\n3 awake,,,\n\nThis does not like working in parallel.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 22.6.0: Fri Sep 15 13:41:28 PDT 2023; root:xnu-8796.141.3.700.8~1/RELEASE_ARM64_T6000\nPython Version:  3.11.11 (main, Dec 11 2024, 10:25:04) [Clang 14.0.6 ]\n\nPackage Information\n\nlangchain_core: 0.3.27\nlangchain: 0.3.13\nlangchain_community: 0.3.12\nlangsmith: 0.1.147\nlangchain_openai: 0.2.13\nlangchain_text_splitters: 0.3.3\nlanggraph_sdk: 0.1.48\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.11\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.7\nhttpx: 0.27.0\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.58.1\norjson: 3.10.7\npackaging: 24.2\npydantic: 2.8.2\npydantic-settings: 2.7.0\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.36\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.11.0\n", "created_at": "2024-12-19", "closed_at": "2024-12-19", "labels": [], "State": "closed", "Author": "owenzhw"}
{"issue_number": 2804, "issue_title": "Error when using pydantic model in `Command.update`", "issue_body": "Privileged issue\n\n I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.\n\nIssue Content\nfrom pydantic import BaseModel\nfrom langgraph.graph import StateGraph, START\nfrom langgraph.types import Command\n\nclass State(BaseModel):\n    foo: str\n\ndef node_a(state: State):\n    return Command(goto=\"node_b\", update=State(foo=state.foo + \"bar\"))\n\ndef node_b(state: State):\n    return State(foo=state.foo + \"baz\")\n\nbuilder = StateGraph(State)\nbuilder.add_node(node_a)\nbuilder.add_node(node_b)\nbuilder.add_edge(START, \"node_a\")\ngraph = builder.compile()\n\ngraph.invoke(State(foo=\"\"))\nError:\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[26], line 20\n     17 builder.add_edge(START, \"node_a\")\n     18 graph = builder.compile()\n---> 20 graph.invoke(State(foo=\"\"))\n\nFile [~/development/langgraph/libs/langgraph/langgraph/pregel/__init__.py:1936](http://localhost:8888/lab/tree/~/development/langgraph/libs/langgraph/langgraph/pregel/__init__.py#line=1935), in Pregel.invoke(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\n   1934 else:\n   1935     chunks = []\n-> 1936 for chunk in self.stream(\n   1937     input,\n   1938     config,\n   1939     stream_mode=stream_mode,\n   1940     output_keys=output_keys,\n   1941     interrupt_before=interrupt_before,\n   1942     interrupt_after=interrupt_after,\n   1943     debug=debug,\n   1944     **kwargs,\n   1945 ):\n   1946     if stream_mode == \"values\":\n   1947         latest = chunk\n\nFile [~/development/langgraph/libs/langgraph/langgraph/pregel/__init__.py:1655](http://localhost:8888/lab/tree/~/development/langgraph/libs/langgraph/langgraph/pregel/__init__.py#line=1654), in Pregel.stream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\n   1649     get_waiter = None  # type: ignore[assignment]\n   1650 # Similarly to Bulk Synchronous Parallel [/](http://localhost:8888/) Pregel model\n   1651 # computation proceeds in steps, while there are channel updates\n   1652 # channel updates from step N are only visible in step N+1\n   1653 # channels are guaranteed to be immutable for the duration of the step,\n   1654 # with channel updates applied only at the transition between steps\n-> 1655 while loop.tick(input_keys=self.input_channels):\n   1656     for _ in runner.tick(\n   1657         loop.tasks.values(),\n   1658         timeout=self.step_timeout,\n   (...)\n   1661     ):\n   1662         # emit output\n   1663         yield from output()\n\nFile [~/development/langgraph/libs/langgraph/langgraph/pregel/loop.py:394](http://localhost:8888/lab/tree/~/development/langgraph/libs/langgraph/langgraph/pregel/loop.py#line=393), in PregelLoop.tick(self, input_keys)\n    392 # apply writes to managed values\n    393 for key, values in mv_writes.items():\n--> 394     self._update_mv(key, values)\n    395 # produce values output\n    396 self._emit(\n    397     \"values\", map_output_values, self.output_keys, writes, self.channels\n    398 )\n\nFile [~/development/langgraph/libs/langgraph/langgraph/pregel/loop.py:830](http://localhost:8888/lab/tree/~/development/langgraph/libs/langgraph/langgraph/pregel/loop.py#line=829), in SyncPregelLoop._update_mv(self, key, values)\n    829 def _update_mv(self, key: str, values: Sequence[Any]) -> None:\n--> 830     return self.submit(cast(WritableManagedValue, self.managed[key]).update, values)\n\nKeyError: '__root__'\n", "created_at": "2024-12-18", "closed_at": "2025-01-29", "labels": ["maintainer"], "State": "closed", "Author": "vbarda"}
{"issue_number": 2775, "issue_title": "Node skipped during graph invocation", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing import Annotated, TypedDict\nimport operator\n\nfrom langgraph.graph import START, END, StateGraph\n\n\nclass MyState(TypedDict):\n    path: Annotated[list, operator.add]\n\n\nclass PathLoggingNode:\n    def __init__(self, name: str):\n        self._name = name\n\n    def __call__(self, state: MyState):\n        # print(f\"Adding {self._name} to {state['path']}\")\n        return {\"path\": [self._name]}\n\n\ndef get_graph():\n    def router_fn(state: MyState) -> list[str]:\n        return \"d\"\n\n    builder = StateGraph(MyState)\n\n    builder.add_node(\"a\", PathLoggingNode(\"node a\"))\n    builder.add_node(\"b\", PathLoggingNode(\"node b\"))\n    builder.add_node(\"c\", PathLoggingNode(\"node c\"))\n    builder.add_node(\"d\", PathLoggingNode(\"node d\"))\n    builder.add_node(\"e\", PathLoggingNode(\"node e\"))\n\n    builder.add_edge(START, \"a\")\n    builder.add_conditional_edges(\"a\", router_fn, [\"b\", \"c\", \"d\"])\n\n    ## option 1\n    builder.add_edge([\"b\", \"c\", \"d\"], \"e\")\n\n    ## option 2\n    # builder.add_edge(\"b\",\"e\")\n    # builder.add_edge(\"c\",\"e\")\n    # builder.add_edge(\"d\",\"e\")\n\n    builder.add_edge(\"e\", END)\n\n    return builder.compile()\n\n\ngraph1 = get_graph()\nop1= graph1.invoke({\"path\": []})\nprint(op1)\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nExpected output: {'path': ['node a', 'node d', 'node e']}\nInstead, it does: {'path': ['node a', 'node d']}\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP PREEMPT_DYNAMIC Thu May  2 18:50:49 UTC 2024\nPython Version:  3.11.9 (main, Apr 17 2024, 00:00:00) [GCC 13.2.1 20240316 (Red Hat 13.2.1-7)]\n\nPackage Information\n\nlangchain_core: 0.3.21\nlangchain: 0.3.9\nlangchain_community: 0.3.9\nlangsmith: 0.1.147\nlangchain_chroma: 0.1.4\nlangchain_openai: 0.2.10\nlangchain_text_splitters: 0.3.2\nlanggraph_sdk: 0.1.40\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.8\nasync-timeout: Installed. No version info available.\nchromadb: 0.5.21\ndataclasses-json: 0.6.7\nfastapi: 0.115.6\nhttpx: 0.28.0\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.55.3\norjson: 3.10.12\npackaging: 24.2\npydantic: 2.10.2\npydantic-settings: 2.6.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.35\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\n", "created_at": "2024-12-16", "closed_at": "2024-12-16", "labels": [], "State": "closed", "Author": "ashirgao"}
{"issue_number": 2760, "issue_title": "MIssing `tool_use` block for Anthropic when creating a ReAct style agent with structured output", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n# /// script\n# dependencies = [\n#   \"langchain~=0.3.10\",\n#   \"langchain-anthropic~=0.3.0\",\n#   \"langchain-aws~=0.2.7\",\n#   \"langgraph~=0.2.56\",\n#   \"langgraph-checkpoint-sqlite~=2.0.1\",\n#   \"pydantic~=2.10.3\",\n# ]\n# ///\n\n\"\"\"An structured agent.\n\nRun with\n\n    PYTHONINSPECT=1 uv run --python 3.12 mre.py\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport typing as t\nimport uuid\nfrom contextlib import contextmanager\nfrom textwrap import dedent\n\nfrom langchain_aws.chat_models import ChatBedrock\nfrom langchain_core.messages import HumanMessage, SystemMessage, ToolMessage\nfrom langchain_core.tools import tool\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nfrom langgraph.graph import MessagesState, StateGraph, END\nfrom langgraph.prebuilt.tool_node import ToolNode\nfrom pydantic import BaseModel\n\nif t.TYPE_CHECKING:\n    from langgraph.checkpoint.base import BaseCheckpointSaver\n    from langgraph.graph.state import CompiledStateGraph\n\nlogger = logging.getLogger(__name__)\n\npets: dict[str, str] = {\n    \"Nina\": \"dog\",\n    \"Misha\": \"cat\",\n    \"Tobby\": \"dog\",\n    \"Barry\": \"dog\",\n    \"Apollo\": \"cat\",\n    \"Pericles\": \"parrot\",\n}\n\n# Map species to emojis\nspecies_emojis = {\n    \"dog\": \"\ud83d\udc36\",\n    \"cat\": \"\ud83d\udc31\",\n    \"parrot\": \"\ud83e\udd9c\",\n}\n\n\n@tool\ndef get_pet_species(name: str) -> str:\n    \"\"\"Get the species of a pet.\n\n    Args:\n        name: The name of the pet.\n\n    Returns:\n        The species of the pet, or \"unknown\" if the pet is not found.\n    \"\"\"\n    # return pets.get(name, \"unknown\")\n    return pets[name]\n\n\n@tool\ndef get_pet_emoji(species: str) -> str:\n    \"\"\"Get the emoji for a pet species.\n\n    Args:\n        species: The species of the pet.\n\n    Returns:\n        The emoji for the pet species.\n    \"\"\"\n    return species_emojis.get(species, \"\ud83e\udd14\")\n\n\nclass PetResponse(BaseModel):\n    \"\"\"Pet response schema.\"\"\"\n\n    name: str\n    species: str\n    emoji: str\n\n\nPET_TOOLS = [\n    get_pet_species,\n    get_pet_emoji,\n    PetResponse,\n]\n\n\nclass AgentState(MessagesState):\n    \"\"\"State management for the agent workflow.\"\"\"\n\n    final_response: PetResponse\n\n\nclass StructuredPetsAgent:\n    \"\"\"A Cube.dev agent that responds with structured pet data.\"\"\"\n\n    tools = PET_TOOLS\n\n    def __init__(self) -> None:\n        \"\"\"Initialize the agent.\"\"\"\n        self._model = ChatBedrock(\n            model_id=\"anthropic.claude-3-5-sonnet-20241022-v2:0\",\n            model_kwargs={\"temperature\": 0.8},\n        )\n        self.model_with_tools = self._model.bind_tools(self.tools, tool_choice=\"any\")\n\n    @contextmanager\n    def get_memory(self) -> t.Generator[BaseCheckpointSaver, None, None]:\n        \"\"\"Get the memory connection.\"\"\"\n        with SqliteSaver.from_conn_string(\"./edgar-local/checkpoint.db\") as conn:\n            conn.setup()\n            yield conn\n\n    def call_model(self, state: AgentState) -> dict[str, list]:\n        \"\"\"Call the model with the current state.\"\"\"\n        response = self.model_with_tools.invoke(state[\"messages\"])\n        return {\"messages\": [response]}\n\n    @staticmethod\n    def respond(state: AgentState):\n        # Construct the final answer from the arguments of the last tool call\n        response = PetResponse(**state[\"messages\"][-1].tool_calls[0][\"args\"])\n        # We return the final answer\n        return {\"final_response\": response}\n\n    @staticmethod\n    def should_continue(state: AgentState):\n        messages = state[\"messages\"]\n        last_message = messages[-1]\n        # If there is only one tool call and it is the response tool call we respond to the user\n        if (\n            len(last_message.tool_calls) == 1\n            and last_message.tool_calls[0][\"name\"] == \"PetResponse\"\n        ):\n            return \"respond\"\n        # Otherwise we will use the tool node again\n        else:\n            return \"continue\"\n\n    def _build_graph(self, memory: BaseCheckpointSaver) -> CompiledStateGraph:\n        \"\"\"Build the workflow graph.\"\"\"\n        workflow = StateGraph(AgentState)\n\n        # Add nodes\n        workflow.add_node(\"agent\", self.call_model)\n        workflow.add_node(\"respond\", self.respond)\n        workflow.add_node(\"tools\", ToolNode(self.tools))\n\n        # Set the entrypoint as `agent`\n        # This means that this node is the first one called\n        workflow.set_entry_point(\"agent\")\n\n        # We now add a conditional edge\n        workflow.add_conditional_edges(\n            \"agent\",\n            self.should_continue,\n            {\n                \"continue\": \"tools\",\n                \"respond\": \"respond\",\n            },\n        )\n\n        workflow.add_edge(\"tools\", \"agent\")\n        workflow.add_edge(\"respond\", END)\n\n        return workflow.compile(checkpointer=memory, debug=True)\n\n    def process_question(self, question, thread_id, *, system_prompt=None):\n        \"\"\"Process a single question and return the response.\"\"\"\n        config = {\"configurable\": {\"thread_id\": thread_id}}\n        messages = [HumanMessage(content=question)]\n\n        if system_prompt is not None:\n            messages = [SystemMessage(content=system_prompt), *messages]\n\n        with self.get_memory() as conn:\n            graph = self._build_graph(conn)\n            with open(\"edgar-local/chatbot_pets_structured.png\", \"wb\") as f:\n                f.write(graph.get_graph().draw_mermaid_png())\n            response = graph.invoke({\"messages\": messages}, config)\n\n        return response[\"final_response\"]\n\n\nif __name__ == \"__main__\":\n    # Initialize agent\n    agent = StructuredPetsAgent()\n    thread_id = uuid.uuid4().hex\n    print(\"Processing questions in thread:\", thread_id)  # noqa: T201\n\n    # Process questions\n    response = agent.process_question(\n        question=\"My name is John and my pet's name is Pericles. What is my pet's species?\",\n        thread_id=thread_id,\n        system_prompt=dedent(\"\"\"\\\n            You will be asked to provide the species of a pet based on its name.\n\n            Try to answer if you have the information, but it's okay if you don't know the species of a pet,\n            just say \"I don't know\"\n        \"\"\"),\n    )\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/edgarramirez/poc/mre.py\", line 192, in process_question\n    response = graph.invoke({\"messages\": messages}, config)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/edgarramirez/.cache/uv/archive-v0/ZcxRgOxPEKXB6Yso33-_e/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 1936, in invoke\n    for chunk in self.stream(\n                 ^^^^^^^^^^^^\n  File \"/Users/edgarramirez/.cache/uv/archive-v0/ZcxRgOxPEKXB6Yso33-_e/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 1656, in stream\n    for _ in runner.tick(\n             ^^^^^^^^^^^^\n  File \"/Users/edgarramirez/.cache/uv/archive-v0/ZcxRgOxPEKXB6Yso33-_e/lib/python3.12/site-packages/langgraph/pregel/runner.py\", line 167, in tick\n    run_with_retry(\n  File \"/Users/edgarramirez/.cache/uv/archive-v0/ZcxRgOxPEKXB6Yso33-_e/lib/python3.12/site-packages/langgraph/pregel/retry.py\", line 40, in run_with_retry\n    return task.proc.invoke(task.input, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/edgarramirez/.cache/uv/archive-v0/ZcxRgOxPEKXB6Yso33-_e/lib/python3.12/site-packages/langgraph/utils/runnable.py\", line 408, in invoke\n    input = step.invoke(input, config, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/edgarramirez/.cache/uv/archive-v0/ZcxRgOxPEKXB6Yso33-_e/lib/python3.12/site-packages/langgraph/utils/runnable.py\", line 184, in invoke\n    ret = context.run(self.func, input, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/edgarramirez/poc/mre.py\", line 128, in call_model\n    response = self.model_with_tools.invoke(state[\"messages\"])\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/edgarramirez/.cache/uv/archive-v0/ZcxRgOxPEKXB6Yso33-_e/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 5354, in invoke\n    return self.bound.invoke(\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/edgarramirez/.cache/uv/archive-v0/ZcxRgOxPEKXB6Yso33-_e/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 286, in invoke\n    self.generate_prompt(\n  File \"/Users/edgarramirez/.cache/uv/archive-v0/ZcxRgOxPEKXB6Yso33-_e/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 786, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/edgarramirez/.cache/uv/archive-v0/ZcxRgOxPEKXB6Yso33-_e/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 643, in generate\n    raise e\n  File \"/Users/edgarramirez/.cache/uv/archive-v0/ZcxRgOxPEKXB6Yso33-_e/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n    self._generate_with_cache(\n  File \"/Users/edgarramirez/.cache/uv/archive-v0/ZcxRgOxPEKXB6Yso33-_e/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 851, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/Users/edgarramirez/.cache/uv/archive-v0/ZcxRgOxPEKXB6Yso33-_e/lib/python3.12/site-packages/langchain_aws/chat_models/bedrock.py\", line 570, in _generate\n    completion, tool_calls, llm_output = self._prepare_input_and_invoke(\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/edgarramirez/.cache/uv/archive-v0/ZcxRgOxPEKXB6Yso33-_e/lib/python3.12/site-packages/langchain_aws/llms/bedrock.py\", line 841, in _prepare_input_and_invoke\n    raise e\n  File \"/Users/edgarramirez/.cache/uv/archive-v0/ZcxRgOxPEKXB6Yso33-_e/lib/python3.12/site-packages/langchain_aws/llms/bedrock.py\", line 827, in _prepare_input_and_invoke\n    response = self.client.invoke_model(**request_options)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/edgarramirez/.cache/uv/archive-v0/ZcxRgOxPEKXB6Yso33-_e/lib/python3.12/site-packages/botocore/client.py\", line 569, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/edgarramirez/.cache/uv/archive-v0/ZcxRgOxPEKXB6Yso33-_e/lib/python3.12/site-packages/botocore/client.py\", line 1023, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.errorfactory.ValidationException: An error occurred (ValidationException) when calling the InvokeModel operation: messages.6: Did not find 1 `tool_result` block(s) at the beginning of this message. Messages following `tool_use` blocks must begin with a matching number of `tool_result` blocks.\nDuring task with name 'agent' and id '5be10085-a27b-045f-4da0-8df74d0e0fe6'\nDescription\nI expect the example in https://langchain-ai.github.io/langgraph/how-tos/react-agent-structured-output/ to work out of the box.\nThe following workaround solves the issue for me, but I don't know if this is the best possible solution or even if there's an actual bug somewhere in the call chain (langgraph, langchain, boto, etc.).\n          with self.get_memory() as conn:\n              graph = self._build_graph(conn)\n              with open(\"edgar-local/chatbot_pets_structured.png\", \"wb\") as f:\n                  f.write(graph.get_graph().draw_mermaid_png())\n              response = graph.invoke({\"messages\": messages}, config)\n\n+             state = graph.get_state(config)\n+             message = ToolMessage(\n+                 content=\"Here is your structured response\",\n+                 tool_call_id=state.values[\"messages\"][-1].tool_calls[0][\"id\"],\n+             )\n+             graph.update_state(config, {\"messages\": [message]}, as_node=\"tools\")\n+ \n        return response[\"final_response\"]\nI also shared this workaround in the discussion linked to the guide: #1540 (comment).\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.2.0: Fri Dec  6 18:51:28 PST 2024; root:xnu-11215.61.5~2/RELEASE_ARM64_T8112\nPython Version:  3.12.7 (main, Oct  1 2024, 13:06:20) [Clang 16.0.0 (clang-1600.0.26.3)]\n\nPackage Information\nOptional packages not installed\n\nlangsmith\nlangserve\n", "created_at": "2024-12-13", "closed_at": "2024-12-16", "labels": [], "State": "closed", "Author": "edgarrmondragon"}
{"issue_number": 2758, "issue_title": "When the `update` argument for `Command` is provided as a `TypedDict`, mypy raises a complaint", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing import Literal, TypedDict\n\nfrom langgraph.graph import StateGraph\nfrom langgraph.types import Command\n\n\nclass State(TypedDict):\n    foo: str\n\n\ndef node_a(state: State) -> Command[Literal[\"node_b\"]]:\n    return Command(update=State(foo=state[\"foo\"] + \"a\"), goto=\"node_b\")\n\n\ndef node_b(state: State) -> State:\n    return State(foo=state[\"foo\"] + \"b\")\n\n\nbuilder = StateGraph(State)\nbuilder.set_entry_point(node_a.__name__)\nbuilder.add_node(node_a)\nbuilder.add_node(node_b)\ngraph = builder.compile()\nprint(graph.invoke(State(foo=\"\")))\nError Message and Stack Trace (if applicable)\nerror: Argument \"update\" to \"Command\" has incompatible type \"State\"; expected \"dict[str, Any] | Sequence[tuple[str, Any]]\"  [arg-type]\n        return Command(update=State(foo=state[\"foo\"] + \"a\"), goto=\"node_b\")\n                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nFound 1 error in 1 file (checked 1 source file)\nDescription\nThe program runs fine.\nIt's strange that we define the state format using TypedDict, yet receive warnings when using TypedDict as input, right?\nBecause the internal type hints uses dict, mypy complains when receiving a TypedDict.\nI think changing the type hint to Mapping could resolve this.\n\n# before\nupdate: Union[dict[str, Any], Sequence[tuple[str, Any]]] = ()\n\n# after\nupdate: Union[Mapping[str, Any], Sequence[tuple[str, Any]]] = ()\nIf you agree with this change, I can quickly submit a simple PR to address the issue.\nSystem Info\nSystem Information\n\nOS:  Linux\nPython Version:  3.13.1 (main, Dec  4 2024, 08:54:15) [GCC 13.2.0]\n\nPackage Information\n\nlangchain_core: 0.3.24\nlanggraph: 0.2.59\n", "created_at": "2024-12-13", "closed_at": "2024-12-18", "labels": [], "State": "closed", "Author": "gbaian10"}
{"issue_number": 2755, "issue_title": "LangGraph AsyncPostgresSaver throws an psycopg.errors.InvalidSqlStatementName error", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom dotenv import load_dotenv\nload_dotenv()\n\nfrom typing import (\n    Annotated,\n    Sequence,\n    TypedDict,\n)\nfrom langchain_core.messages import BaseMessage\nfrom langgraph.graph.message import add_messages\nfrom langchain_openai import ChatOpenAI\nfrom tools_new import TOOLS\nimport json\nfrom langchain_core.messages import ToolMessage, SystemMessage\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import StateGraph, END\nfrom prompt import prompt\nfrom langchain_core.messages import HumanMessage\nimport asyncio\nfrom psycopg import AsyncConnection\nimport os\nfrom langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\nfrom rich.console import Console\nrich = Console()\n\nconnection_kwargs = {\n    \"autocommit\": True,\n    \"prepare_threshold\": 0,\n}\n\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n\nclass AgentState(TypedDict):\n    \"\"\"The state of the agent.\"\"\"\n\n    messages: Annotated[Sequence[BaseMessage], add_messages]\n\nmodel = model.bind_tools(TOOLS)\ntools_by_name = {tool.name: tool for tool in TOOLS}\n\ndef tool_node(state: AgentState):\n    outputs = []\n    for tool_call in state[\"messages\"][-1].tool_calls:\n        tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n        outputs.append(\n            ToolMessage(\n                content=json.dumps(tool_result),\n                name=tool_call[\"name\"],\n                tool_call_id=tool_call[\"id\"],\n            )\n        )\n    return {\"messages\": outputs}\n\ndef call_model(\n    state: AgentState,\n    config: RunnableConfig,\n):\n    system_prompt = SystemMessage(\n        prompt\n    )\n    response = model.invoke([system_prompt] + state[\"messages\"], config)\n    return {\"messages\": [response]}\n\ndef should_continue(state: AgentState):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if not last_message.tool_calls:\n        return \"end\"\n    else:\n        return \"continue\"\n\n# Define a new graph\nworkflow = StateGraph(AgentState)\n\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"tools\", tool_node)\n\nworkflow.set_entry_point(\"agent\")\n\n# We now add a conditional edge\nworkflow.add_conditional_edges(\n    \"agent\",\n    should_continue,\n    {\n        \"continue\": \"tools\",\n        \"end\": END,\n    },\n)\n\nworkflow.add_edge(\"tools\", \"agent\")\n\nasync def run_graph(user_input: str, thread_id: str):\n    with AsyncConnection.connect(os.getenv(\"DB_URI\"), **connection_kwargs) as conn:\n        checkpointer = AsyncPostgresSaver(conn)\n        await checkpointer.setup()\n        graph = workflow.compile(checkpointer=checkpointer)\n        config = {\"configurable\": {\"thread_id\": thread_id}}\n        async for event in graph.astream_events(\n            {\"messages\": [HumanMessage(content=user_input)]},\n            version = 'v2', stream_mode=\"values\", config=config\n        ):\n            if \"on_chat_model_stream\" == event['event']:\n                if len(event['data'][\"chunk\"].content) > 0:\n                    print(event['data']['chunk'].content, end='', flush=True)\n\nif __name__ == '__main__':\n    print(\"Running model\")\n    asyncio.run(run_graph(user_input=\"How are you?\", thread_id=\"testing5\"))\n\n\n`DB_URI= postgresql://USER:PASSWORD@aws-0-eu-central-1.pooler.supabase.com:6543/postgres?sslmode=disable`\nError Message and Stack Trace (if applicable)\n(langgraph) hasnain@Mac src % python example_3.py\n/Users/hasnain/anaconda3/envs/langgraph/lib/python3.10/site-packages/langgraph/graph/graph.py:36: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n\nFor example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\nwith: `from pydantic import BaseModel`\nor the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n\n  from langgraph.pregel import Channel, Pregel\nTraceback (most recent call last):\n  File \"/Users/hasnain/Documents/react-agent/src/example_3.py\", line 143, in <module>\n    main()\n  File \"/Users/hasnain/Documents/react-agent/src/example_3.py\", line 112, in main\n    checkpointer.setup()\n  File \"/Users/hasnain/anaconda3/envs/langgraph/lib/python3.10/site-packages/langgraph/checkpoint/postgres/__init__.py\", line 89, in setup\n    row = cur.execute(\n  File \"/Users/hasnain/anaconda3/envs/langgraph/lib/python3.10/site-packages/psycopg/cursor.py\", line 97, in execute\n    raise ex.with_traceback(None)\npsycopg.errors.DuplicatePreparedStatement: prepared statement \"_pg3_0\" already exists\nDescription\nI am using above code for react-agent with memory. Without memory, agent is working fine but when I add memory to it then sometime it gives me this error and sometime it works fine. I am not sure what could be wrong. I am using Supabase and above error is quite random, sometime I see that error immediately when I run the code and sometime agent takes an input and throws an error before or after giving the output.\nNOTE: Initially, I was using some tools where I was calling Database to fetch some data and I assumed that could be the cause  but I am getting this error even with TavilySearchResults tool as well.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.1.0: Thu Oct 10 21:03:11 PDT 2024; root:xnu-11215.41.3~2/RELEASE_ARM64_T6020\nPython Version:  3.10.14 (main, May  6 2024, 14:42:37) [Clang 14.0.6 ]\n\nPackage Information\n\nlangchain_core: 0.3.24\nlangchain: 0.3.0\nlangchain_community: 0.3.0\nlangsmith: 0.1.145\nlangchain_anthropic: 0.2.1\nlangchain_experimental: 0.3.0\nlangchain_openai: 0.2.12\nlangchain_text_splitters: 0.3.0\nlangchainhub: 0.1.20\nlangserve: 0.3.0\n\nOther Dependencies\n\naiohttp: 3.9.5\nanthropic: 0.34.2\nasync-timeout: 4.0.3\ndataclasses-json: 0.6.7\ndefusedxml: 0.7.1\nfastapi: 0.112.0\nhttpx: 0.27.0\njsonpatch: 1.33\nnumpy: 1.26.4\nopenai: 1.57.2\norjson: 3.10.6\npackaging: 24.1\npydantic: 2.9.2\npydantic-settings: 2.5.2\nPyYAML: 6.0.1\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.31\nsse-starlette: 1.8.2\ntenacity: 8.5.0\ntiktoken: 0.7.0\ntypes-requests: 2.32.0.20240712\ntyping-extensions: 4.12.2\n", "created_at": "2024-12-13", "closed_at": "2024-12-18", "labels": [], "State": "closed", "Author": "HasnainKhanNiazi"}
{"issue_number": 2749, "issue_title": "unable to access postgresStore from langgraph store", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.store.postgres import PostgresStore\nError Message and Stack Trace (if applicable)\nNo Module found called (PostgresStore) in langgraph.store.postgres\nDescription\nI am writing to bring to your attention an issue I encountered while using the recent version of the LangGraph package (0.2.59).\nIn a previous version(0.2.56 and 0.2.58), I was able to use the following import statement:\n\"from langgraph.store.postgres import PostgresStore\"\nHowever, in the latest version (0.2.59), it seems that the postgres module is no longer available in the langgraph store package.\nSystem Info\nNA", "created_at": "2024-12-13", "closed_at": "2024-12-13", "labels": [], "State": "closed", "Author": "Sharath01zysk"}
{"issue_number": 2745, "issue_title": "Document Pydantic Schema Usage", "issue_body": "Issue with current documentation:\nfrom langgraph.graph import StateGraph\nfrom langgraph.constants import START\nfrom pydantic import BaseModel\n\nclass Foo(BaseModel):\n    x: int\n    y: str\n\ndef node(state: Foo):\n    \"\"\"This parent node will invoke the subgraph.\"\"\"\n    return state\n\nbuilder = StateGraph(Foo)\nbuilder.add_node(\"node\", node)\nbuilder.add_edge(START, \"node\")\ngraph = builder.compile()\nresult = graph.invoke(Foo(x=3, y=\"hello\"))\nFoo(**result)\nIdea or request for content:\nFigure out where to document current behavior + run time coercion", "created_at": "2024-12-12", "closed_at": "2025-03-18", "labels": ["maintainer"], "State": "closed", "Author": "eyurtsev"}
{"issue_number": 2737, "issue_title": "MULTIPLE_SUBGRAPHS exception raised incorrectly", "issue_body": "Privileged issue\n\n I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.\n\nIssue Content\nThis should probably be a different exception -- we're just returning the wrong thing (the subgraph instead of the return value from the subgraph)\nimport uuid\nfrom typing import TypedDict, Optional\n\nfrom langgraph.graph import StateGraph\nfrom langgraph.constants import START \nfrom langgraph.checkpoint.memory import MemorySaver\n\n\nclass State(TypedDict):\n    \"\"\"The graph state.\"\"\"\n    state_counter: int\n\ndef child_node(state: State):\n    return state\n\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(\"child_node\", child_node)\nsubgraph_builder.add_edge(START, \"child_node\")\nsubgraph = subgraph_builder.compile() \n\ndef parent_node(state: State):\n    \"\"\"This parent node will invoke the subgraph.\"\"\"\n    subgraph_state = subgraph.invoke(state)\n    return subgraph # <--- THIS IS THE BUG IN USER CODE (typo), exception claims multiple subgraph calls (based on ast parsing?)\n\ncheckpointer = MemorySaver()\n\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"parent_node\", parent_node)\nbuilder.add_edge(START, \"parent_node\")\n\n# A checkpointer must be enabled for interrupts to work!\ncheckpointer = MemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": uuid.uuid4(),\n    }\n}\n\nfor chunk in graph.stream({\"state_counter\": 1}, config):\n    print(chunk)", "created_at": "2024-12-12", "closed_at": null, "labels": ["maintainer"], "State": "open", "Author": "eyurtsev"}
{"issue_number": 2734, "issue_title": "Different behaviour between tool usage and directly calling a function", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nI was doing a small personal project where the objective is to given a prompt with a country,get an agent to display a map of the country.\nI am using langchain and langgraph to create the agent, earth engine to get the data and geemap to display the map.\nGoing to simplify a bit the code for the sake of the example. The function I was using was something like this:\nimport ee\nimport geemap\nfrom langchain_cohere import ChatCohere\nfrom langchain.tools import tool\nfrom IPython.display import display\nfrom langgraph.prebuilt import create_react_agent\nimport asyncio\n\nee.Authenticate()\nee.Initialize(project=\"unicef-geospatial\")\n\ndef get_uruguay_map():\n    \"\"\"Displays a map of Uruguay\"\"\"\n    country_boundries = ee.FeatureCollection(\"USDOS/LSIB_SIMPLE/2017\")\n    uruguay_boundries = country_boundries.filter(ee.Filter.eq(\"country_na\", \"Uruguay\"))\n\n    country_map = geemap.Map()\n    country_map.center_object(uruguay_boundries)\n    country_map.add_layer(uruguay_boundries, {}, \"Uruguay Boundaries\")\n    display(country_map)\n    return \"success\"\n\nIf I call the function directly, it works as expected.\n\nWhen changing the function to a tool and hooking it up to the agent, it doesn't work as expected. The code was having at the start was:\n@tool\ndef get_uruguay_map():\n    \"\"\"Displays a map of Uruguay\"\"\"\n    country_boundries = ee.FeatureCollection(\"USDOS/LSIB_SIMPLE/2017\")\n    uruguay_boundries = country_boundries.filter(ee.Filter.eq(\"country_na\", \"Uruguay\"))\n\n    country_map = geemap.Map()\n    country_map.center_object(uruguay_boundries)\n    country_map.add_layer(uruguay_boundries, {}, \"Uruguay Boundaries\")\n    display(country_map)\n    return \"success\"\n\nllm = ChatCohere(temperature=0.0)\ntools = [get_uruguay_map]\n\ngraph = create_react_agent(\n    tools=tools,\n    model=llm,\n)\n\n# %%\ninputs = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"Show me the map of Uruguay\",\n        }\n    ]\n}\ngraph.invoke(inputs)\nWith that code, I was getting the following output:\nToolMessage(content='Error: RuntimeError(\"There is no current event loop in thread \\'ThreadPoolExecutor-3_0\\'.\")\\n Please fix your mistakes.', name='get_uruguay_map', id='4bba79a1-c59a-4fbb-b065-f7e51122db5c', tool_call_id='eec8f8104e7a44a4ba63044c6a06dfa5', status='error'),\nWhich I couldn't diagnose but I found that it could be fixed by setting the event loop.\n@tool\ndef get_uruguay_map():\n    \"\"\"Displays a map of Uruguay\"\"\"\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    country_boundries = ee.FeatureCollection(\"USDOS/LSIB_SIMPLE/2017\")\n    uruguay_boundries = country_boundries.filter(ee.Filter.eq(\"country_na\", \"Uruguay\"))\n\n    country_map = geemap.Map()\n    country_map.center_object(uruguay_boundries)\n    country_map.add_layer(uruguay_boundries, {}, \"Uruguay Boundaries\")\n    display(country_map)\n    return \"success\"\n\nllm = ChatCohere(temperature=0.0)\ntools = [get_uruguay_map]\n\ngraph = create_react_agent(\n    tools=tools,\n    model=llm,\n)\n\n# %%\ninputs = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"Show me the map of Uruguay\",\n        }\n    ]\n}\ngraph.invoke(inputs)\nHowever, now instead of getting the map centered on Uruguay, I am getting the map centered on the world, even though the layer is properly displayed.\n\nI am not sure why this is happening or how to properly diagnose/fix the issue.\nAny thoughts?\nSystem Info\nSystem Information\n------------------\n> OS:  Darwin\n> OS Version:  Darwin Kernel Version 24.1.0: Thu Oct 10 21:00:32 PDT 2024; root:xnu-11215.41.3~2/RELEASE_ARM64_T6030\n> Python Version:  3.12.7 (main, Oct  1 2024, 02:05:46) [Clang 15.0.0 (clang-1500.3.9.4)]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.21\n> langchain: 0.3.9\n> langchain_community: 0.3.9\n> langsmith: 0.1.147\n> langchain_cohere: 0.3.3\n> langchain_experimental: 0.3.3\n> langchain_openai: 0.2.11\n> langchain_text_splitters: 0.3.2\n> langgraph_sdk: 0.1.43\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp: 3.11.10\n> async-timeout: Installed. No version info available.\n> cohere: 5.13.3\n> dataclasses-json: 0.6.7\n> httpx: 0.28.1\n> httpx-sse: 0.4.0\n> jsonpatch: 1.33\n> langsmith-pyo3: Installed. No version info available.\n> numpy: 1.26.4\n> openai: 1.57.0\n> orjson: 3.10.12\n> packaging: 24.2\n> pandas: 2.2.3\n> pydantic: 2.10.3\n> pydantic-settings: 2.6.1\n> PyYAML: 6.0.2\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> SQLAlchemy: 2.0.36\n> tabulate: 0.9.0\n> tenacity: 9.0.0\n> tiktoken: 0.8.0\n> typing-extensions: 4.12.2\n", "created_at": "2024-12-12", "closed_at": "2024-12-16", "labels": [], "State": "closed", "Author": "fede-bello"}
{"issue_number": 2733, "issue_title": "msgpack deserialization with strictmap_key=False", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport asyncio\nfrom typing import Annotated, TypedDict\n\nfrom langgraph.constants import END\nfrom langgraph.graph import StateGraph\nfrom pydantic import BaseModel\n\nfrom app.config.settings import get_settings\nfrom app.utils.redis_checkpointer import AsyncRedisSaver\n\nclass CitationBroker(BaseModel):\n    map_idx_to_utt: dict[int, int]\n\nclass AgentState(TypedDict):\n    citation_brokers: list[CitationBroker]\n\n\ndef f1(state):\n    cit_b = CitationBroker(\n    map_idx_to_utt={\n        1: 1,\n        2: 2,\n        3: 3\n    })\n    \"\"\"\n    With \n    map_idx_to_utt={\n        '1': 1,\n        '2': 2,\n        '3': 3\n    })\n    and \n    \n    class CitationBroker(BaseModel):\n    map_idx_to_utt: dict[str, int]\n    \n    works\n    \"\"\"\n    print(str(cit_b)) # not None\n    return {\n        \"citation_brokers\": state.get('citation_brokers', []) + [cit_b],\n    }\n\ndef ask_human_node(state):\n    print(\"get user input\")\n\nbuilder = StateGraph(AgentState)\nbuilder.add_node(\"node_1\", f1)\nbuilder.add_node(\"ask_human_node\", ask_human_node)\nbuilder.set_entry_point(\"node_1\")\nbuilder.add_edge(\"ask_human_node\", \"node_1\")\nbuilder.add_edge(\"node_1\", \"ask_human_node\")\n\nsettings = get_settings()\nasync def main():\n    async with AsyncRedisSaver.from_url(settings.CACHE_REDIS_ENDPOINT) as memory:\n        graph = builder.compile(checkpointer=memory, interrupt_before=[\"ask_human_node\"])\n        thread = {\n            \"configurable\": {\n                \"thread_id\": \"1\"\n            }\n        }\n\n        async for event in graph.astream_events({\n            \"citation_brokers\": [],\n        }, config=thread, version=\"v2\"):\n            pass\n\n        snapshot = await graph.aget_state(thread)\n        print(str(snapshot.values['citation_brokers'][0]))  # None\n\nasyncio.run(main())\nError Message and Stack Trace (if applicable)\nFile \"msgpack\\\\_unpacker.pyx\", line 194, in msgpack._cmsgpack.unpackb ValueError: int is not allowed for map key when strict_map_key=True\nDescription\nI upgraded libraries from langgraph 0.2.19 to 0.2.58 and langgraph-checkpoint from 1.0.9 to 2.0.8.\nI'm using a REDIS checkpointer as detailed in the official guide.\nI'm serializing a TypedDict which contains Pydantic V2 objects as values (keys are strings). Each of this Pydantic V2 objects contains a simple Python dict() (whose keys are numeric).\nWhen I try to deserialize the Pydantic object I get the following error:\n\nFile \"msgpack\\_unpacker.pyx\", line 194, in msgpack._cmsgpack.unpackb ValueError: int is not allowed for map key when strict_map_key=True\n\nSetting strict_map_key=False inside jsonplus.py solves the issue, but this implies cloning jsonplus.py just to set strict_map_key=False.\nIndeed at line 210 of jsonplus.py I find:\nelif type_ == \"msgpack\":\n            return msgpack.unpackb(\n                data_, ext_hook=_msgpack_ext_hook, strict_map_key=False\n            )\n\nbut at line 482 of jsonplus.py:\nelif code == EXT_PYDANTIC_V2:\n        try:\n            tup = msgpack.unpackb(data, ext_hook=_msgpack_ext_hook) # lacks of strict_map_key=False\n            # module, name, kwargs, method\n            cls = getattr(importlib.import_module(tup[0]), tup[1])\n            try:\n                return cls(**tup[2])\n            except Exception:\n                return cls.model_construct(**tup[2])\n        except Exception:\n            return\n\nAny advice on how should I fix the problem? In the meantime I reverted to previous version of libraries (which solves the issue).\nThanks in advance.\nSystem Info\nPython 3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)] on win32", "created_at": "2024-12-12", "closed_at": "2024-12-12", "labels": [], "State": "closed", "Author": "pperliti"}
{"issue_number": 2732, "issue_title": "human in the loop", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\ndef human_review_node(state) -> Command[Literal[\"assistant\", \"sensitive_tools\"]]:\n    last_message = state[\"messages\"][-1]\n    tool_call = last_message.tool_calls[-1]\n\n    # this is the value we'll be providing via Command(resume=<human_review>)\n    human_review = interrupt(\n        {\n            \"question\": \"Is this correct?\",\n            # Surface tool calls for review\n            \"tool_call\": tool_call,\n        }\n    )\n\n    review_action = human_review[\"action\"]\n    review_data = human_review.get(\"data\")\n\n    # if approved, call the tool\n    if review_action == \"continue\":\n        return Command(goto=\"sensitive_tools\")\n\n    # update the AI message AND call tools\n    elif review_action == \"update\":\n        updated_message = {\n            \"role\": \"ai\",\n            \"content\": last_message.content,\n            \"tool_calls\": [\n                {\n                    \"id\": tool_call[\"id\"],\n                    \"name\": tool_call[\"name\"],\n                    # This the update provided by the human\n                    \"args\": review_data,\n                }\n            ],\n            # This is important - this needs to be the same as the message you replacing!\n            # Otherwise, it will show up as a separate message\n            \"id\": last_message.id,\n        }\n        return Command(goto=\"sensitive_tools\", update={\"messages\": [updated_message]})\n\n    # provide feedback to LLM\n    elif review_action == \"feedback\":\n        # NOTE: we're adding feedback message as a ToolMessage\n        # to preserve the correct order in the message history\n        # (AI messages with tool calls need to be followed by tool call messages)\n        tool_message = {\n            \"role\": \"tool\",\n            # This is our natural language feedback\n            \"content\": review_data,\n            \"name\": tool_call[\"name\"],\n            \"tool_call_id\": tool_call[\"id\"],\n        }\n        return Command(goto=\"assistant\", update={\"messages\": [tool_message]})\nError Message and Stack Trace (if applicable)\nhuman_review_node -> {'messages': [HumanMessage(content='je veux r\u00e9server le bureau alain de 12h \u00e0 13h', additional_kwargs={}, response_metadata={}, id='94a98107-f5cb-4ed8-b84e-2f43eea7a471'),\n              AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.3:latest', 'created_at': '2024-12-12T13:28:50.342536747Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4300589516, 'load_duration': 19115485, 'prompt_eval_count': 1339, 'prompt_eval_duration': 1453000000, 'eval_count': 50, 'eval_duration': 2820000000, 'message': Message(role='assistant', content='', images=None, tool_calls=[ToolCall(function=Function(name='create_reservations', arguments={'end_datetime_str': '2024-12-12T13:00:00', 'start_datetime_str': '2024-12-12T12:00:00'}))])}, id='run-2ebb4e90-2d72-4314-b9cf-be499c0793ab-0', tool_calls=[{'name': 'create_reservations', 'args': {'end_datetime_str': '2024-12-12T13:00:00', 'start_datetime_str': '2024-12-12T12:00:00'}, 'id': 'ae703317-b1b5-45aa-baed-9d8e942a244d', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1339, 'output_tokens': 50, 'total_tokens': 1389})],\n 'user_info': {'userId': 2, 'username': 'admin'}}\n[chain/start] [chain:/chat > chain:human_review_node] Entering Chain run with input:\n[inputs]\n[chain/error] [chain:/chat > chain:human_review_node] [4ms] Chain run errored with error:\n\"GraphInterrupt((Interrupt(value={'question': 'Is this correct?', 'tool_call': {'name': 'create_reservations', 'args': {'end_datetime_str': '2024-12-12T13:00:00', 'start_datetime_str': '2024-12-12T12:00:00'}, 'id': 'ae703317-b1b5-45aa-baed-9d8e942a244d', 'type': 'tool_call'}}, resumable=True, ns=['human_review_node:45e45478-3673-4df5-ef4f-f2492bf5fdf5'], when='during'),))\n\n\nTraceback (most recent call last):\\n\\n\\n  File \\\"C:\\\\Users\\\\suchaudn\\\\OneDrive - Legrand France\\\\PYTHON\\\\gimelec_agent\\\\.venv\\\\Lib\\\\site-packages\\\\langgraph\\\\utils\\\\runnable.py\\\", line 445, in ainvoke\\n    input = await step.ainvoke(input, config, **kwargs)\\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"C:\\\\Users\\\\suchaudn\\\\OneDrive - Legrand France\\\\PYTHON\\\\gimelec_agent\\\\.venv\\\\Lib\\\\site-packages\\\\langgraph\\\\utils\\\\runnable.py\\\", line 236, in ainvoke\\n    ret = await asyncio.create_task(coro, context=context)\\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"C:\\\\Users\\\\suchaudn\\\\OneDrive - Legrand France\\\\PYTHON\\\\gimelec_agent\\\\.venv\\\\Lib\\\\site-packages\\\\langchain_core\\\\runnables\\\\config.py\\\", line 588, in run_in_executor\\n    return await asyncio.get_running_loop().run_in_executor(\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"C:\\\\Python312\\\\Lib\\\\concurrent\\\\futures\\\\thread.py\\\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"C:\\\\Users\\\\suchaudn\\\\OneDrive - Legrand France\\\\PYTHON\\\\gimelec_agent\\\\.venv\\\\Lib\\\\site-packages\\\\langchain_core\\\\runnables\\\\config.py\\\", line 579, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"C:\\\\Users\\\\suchaudn\\\\OneDrive - Legrand France\\\\PYTHON\\\\gimelec_agent\\\\backend\\\\assistants\\\\assistant.py\\\", line 87, in human_review_node\\n    human_review = interrupt(\\n                   ^^^^^^^^^^\\n\\n\\n  File \\\"C:\\\\Users\\\\suchaudn\\\\OneDrive - Legrand France\\\\PYTHON\\\\gimelec_agent\\\\.venv\\\\Lib\\\\site-packages\\\\langgraph\\\\types.py\\\", line 390, in interrupt\\n    raise GraphInterrupt(\\n\\n\\nlanggraph.errors.GraphInterrupt: (Interrupt(value={'question': 'Is this correct?', 'tool_call': {'name': 'create_reservations', 'args': {'end_datetime_str': '2024-12-12T13:00:00', 'start_datetime_str': '2024-12-12T12:00:00'}, 'id': 'ae703317-b1b5-45aa-baed-9d8e942a244d', 'type': 'tool_call'}}, resumable=True, ns=['human_review_node:45e45478-3673-4df5-ef4f-f2492bf5fdf5'], when='during'),)\"\n[chain/end] [chain:/chat] [4.37s] Exiting Chain run with output:\n[outputs]\n2024-12-12 14:28:48,773 - langchain_core.callbacks.manager - WARNING - Error in LangChainTracer.on_chain_error callback: TracerException('No indexed run ID f0a5023c-2ff7-4c5e-99cf-447923f7236b.')\n2024-12-12 14:28:48,774 - langchain_core.callbacks.manager - WARNING - Error in ConsoleCallbackHandler.on_chain_error callback: TracerException('No indexed run ID f0a5023c-2ff7-4c5e-99cf-447923f7236b.')\nDescription\ni try to use human in the loop with langgraph but i have the error listed above.\nmy graph is :\n\n\n\n%%{init: {'flowchart': {'curve': 'linear'}}}%%\ngraph TD;\n\t__start__([<p>__start__</p>]):::first\n\tfetch_user_info(fetch_user_info)\n\tassistant(assistant)\n\tsafe_tools(safe_tools)\n\thuman_review_node(human_review_node)\n\tsensitive_tools(sensitive_tools)\n\t__end__([<p>__end__</p>]):::last\n\t__start__ --> fetch_user_info;\n\tfetch_user_info --> assistant;\n\tsafe_tools --> assistant;\n\tsensitive_tools --> assistant;\n\tassistant -.-> safe_tools;\n\tassistant -.-> human_review_node;\n\tassistant -.-> __end__;\n\thuman_review_node -.-> assistant;\n\thuman_review_node -.-> sensitive_tools;\n\tclassDef default fill:#f2f0ff,line-height:1.2\n\tclassDef first fill-opacity:0\n\tclassDef last fill:#bfb6fc\n\n\n\n\n\n\n\n\n Loading\n\n\n\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.19045\nPython Version:  3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.3.24\nlangchain: 0.3.11\nlangchain_community: 0.3.11\nlangsmith: 0.2.3\nlangchain_ollama: 0.2.1\nlangchain_openai: 0.2.11\nlangchain_text_splitters: 0.3.2\nlanggraph_sdk: 0.1.43\nlangserve: 0.3.0\n\nOther Dependencies\n\naiohttp: 3.11.10\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.7\nfastapi: 0.115.6\nhttpx: 0.27.2\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 2.1.3\nollama: 0.4.2\nopenai: 1.57.0\norjson: 3.10.12\npackaging: 24.2\npydantic: 2.10.3\npydantic-settings: 2.6.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.36\nsse-starlette: 2.1.3\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\n", "created_at": "2024-12-12", "closed_at": "2024-12-20", "labels": [], "State": "closed", "Author": "nicho2"}
{"issue_number": 2731, "issue_title": "langgraph dev asks for the installation of inmem which is already there", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n$ langgraph dev\nError Message and Stack Trace (if applicable)\nError: Required package 'langgraph-api-inmem' is not installed.\nPlease install it with:\n\n    pip install -U \"langgraph-cli[inmem]\"\n\nIf you're developing the langgraph-cli package locally, you can install in development mode:\n    pip install -e .\nDescription\nI can't get the langgraph dev command to work. It asks me to install inmem which I already did.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.2.0: Fri Dec  6 18:51:28 PST 2024; root:xnu-11215.61.5~2/RELEASE_ARM64_T8112\nPython Version:  3.10.6 (main, Jul  1 2024, 15:34:22) [Clang 15.0.0 (clang-1500.3.9.4)]\n\nPackage Information\n\nlangchain_core: 0.3.24\nlangchain: 0.3.11\nlangchain_community: 0.3.11\nlangsmith: 0.2.3\nlangchain_anthropic: 0.3.0\nlangchain_text_splitters: 0.3.2\nlanggraph_cli: 0.1.61\nlanggraph_sdk: 0.1.43\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.10\nanthropic: 0.39.0\nasync-timeout: 4.0.3\nclick: 8.1.7\ndataclasses-json: 0.6.7\ndefusedxml: 0.7.1\nhttpx: 0.27.2\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlanggraph-api: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\norjson: 3.10.12\npackaging: 24.2\npydantic: 2.10.3\npydantic-settings: 2.6.1\npython-dotenv: 1.0.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.36\ntenacity: 9.0.0\ntyping-extensions: 4.12.2\n", "created_at": "2024-12-12", "closed_at": "2025-01-10", "labels": [], "State": "closed", "Author": "lfnovo"}
{"issue_number": 2729, "issue_title": "Missing id in tool_calls of AIMessage when using stream_mode=\"messages\" (Issue does not occur with other modes)", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.graph import StateGraph\nfrom langgraph.graph.message import add_messages\nfrom langgraph.graph.state import CompiledStateGraph\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\ndef make_graph(llm: ChatOpenAI, tools=list[StructuredTool]) -> CompiledStateGraph:\n    class State(TypedDict):\n        messages: Annotated[list, add_messages]\n    \n    graph_builder = StateGraph(State)\n    llm_with_tools = llm.bind_tools(tools)\n\n    def chatbot(state: State):\n        response = llm_with_tools.invoke(state[\"messages\"])\n        return {\"messages\": [response]}\n\n    tool_node = ToolNode(tools)\n    graph_builder.add_node(\"tools\", tool_node)\n    graph_builder.add_node(\"chatbot\", chatbot)\n    graph_builder.add_edge(\"tools\", \"chatbot\")\n    graph_builder.add_conditional_edges(\n        \"chatbot\",\n        tools_condition\n    )\n    graph_builder.set_entry_point(\"chatbot\")\n    graph = graph_builder.compile()\n    return graph\n\ndef get_tools() -> list[StructuredTool]:\n    class CalculatorInput(BaseModel):\n        a: int = Field(description=\"first number\")\n        b: int = Field(description=\"second number\")\n\n    def multiply(a: int, b: int) -> int:\n        \"\"\"Multiply two numbers.\"\"\"\n        return a * b\n\n    calculator = StructuredTool.from_function(\n        func=multiply,\n        name=\"Calculator\",\n        description=\"multiply numbers\",\n        args_schema=CalculatorInput,\n        return_direct=True\n    )\n    tools = [calculator]\n    return tools\n# --------------\n# issue occurs when streaming\nfor event in graph.stream({\"messages\": input}, stream_mode=\"messages\"): \n    print(event)\nError Message and Stack Trace (if applicable)\nFile \"/home/app.e0016/pattern_agent/agent.py\", line 174, in stream_graph_updates_test\n    for event in graph.stream({\"messages\": input}, stream_mode=\"messages\"):\n  File \"/home/app.e0016/miniconda3/envs/agent/lib/python3.10/site-packages/langgraph/pregel/__init__.py\", line 1656, in stream\n    for _ in runner.tick(\n  File \"/home/app.e0016/miniconda3/envs/agent/lib/python3.10/site-packages/langgraph/pregel/runner.py\", line 239, in tick\n    _panic_or_proceed(\n  File \"/home/app.e0016/miniconda3/envs/agent/lib/python3.10/site-packages/langgraph/pregel/runner.py\", line 539, in _panic_or_proceed\n    raise exc\n  File \"/home/app.e0016/miniconda3/envs/agent/lib/python3.10/site-packages/langgraph/pregel/executor.py\", line 76, in done\n    task.result()\n  File \"/home/app.e0016/miniconda3/envs/agent/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/home/app.e0016/miniconda3/envs/agent/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/home/app.e0016/miniconda3/envs/agent/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/home/app.e0016/miniconda3/envs/agent/lib/python3.10/site-packages/langgraph/pregel/retry.py\", line 40, in run_with_retry\n    return task.proc.invoke(task.input, config)\n  File \"/home/app.e0016/miniconda3/envs/agent/lib/python3.10/site-packages/langgraph/utils/runnable.py\", line 408, in invoke\n    input = step.invoke(input, config, **kwargs)\n  File \"/home/app.e0016/miniconda3/envs/agent/lib/python3.10/site-packages/langgraph/prebuilt/tool_node.py\", line 247, in invoke\n    return super().invoke(input, config, **kwargs)\n  File \"/home/app.e0016/miniconda3/envs/agent/lib/python3.10/site-packages/langgraph/utils/runnable.py\", line 184, in invoke\n    ret = context.run(self.func, input, **kwargs)\n  File \"/home/app.e0016421/miniconda3/envs/agent/lib/python3.10/site-packages/langgraph/prebuilt/tool_node.py\", line 219, in _func\n    outputs = [\n  File \"/home/app.e0016/miniconda3/envs/agent/lib/python3.10/concurrent/futures/_base.py\", line 621, in result_iterator\n    yield _result_or_cancel(fs.pop())\n  File \"/home/app.e0016/miniconda3/envs/agent/lib/python3.10/concurrent/futures/_base.py\", line 319, in _result_or_cancel\n    return fut.result(timeout)\n  File \"/home/app.e0016/miniconda3/envs/agent/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n    return self.__get_result()\n  File \"/home/app.e0016/miniconda3/envs/agent/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/home/app.e0016/miniconda3/envs/agent/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/home/app.e0016/miniconda3/envs/agent/lib/python3.10/site-packages/langchain_core/runnables/config.py\", line 527, in _wrapped_fn\n    return contexts.pop().run(fn, *args)\n  File \"/home/app.e0016/miniconda3/envs/agent/lib/python3.10/site-packages/langgraph/prebuilt/tool_node.py\", line 341, in _run_one\n    raise TypeError(\nTypeError: Tool Calculator returned unexpected type: <class 'int'>\nDescription\nWhen using stream_mode set to \"messages\", the id field in the tool_calls property of the AIMessage in the input to the tool node's invoke function is missing. How can this issue be fixed? (This issue does not occur with other stream_mode settings.)\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #161-Ubuntu SMP Fri Feb 3 14:49:04 UTC 2023\nPython Version:  3.10.15 (main, Oct  3 2024, 07:27:34) [GCC 11.2.0]\n\nPackage Information\n\nlangchain_core: 0.3.24\nlangchain: 0.3.11\nlangchain_community: 0.3.11\nlangsmith: 0.2.2\nlangchain_openai: 0.2.12\nlangchain_text_splitters: 0.3.2\nlanggraph_sdk: 0.1.43\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.10\nasync-timeout: 4.0.3\ndataclasses-json: 0.6.7\nhttpx: 0.28.0\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.57.0\norjson: 3.10.12\npackaging: 24.2\npydantic: 2.10.3\npydantic-settings: 2.6.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.36\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\n", "created_at": "2024-12-12", "closed_at": "2024-12-18", "labels": [], "State": "closed", "Author": "kangyic116"}
{"issue_number": 2723, "issue_title": "Langgraph 0.2.58 resulted in empty config passed to tools in langgraph", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n# Graph setup  \n    async with get_checkpoints_pool().connection() as conn:\n        checkpointer = AsyncPostgresSaver(conn=conn)  # type: ignore\n        graph.checkpointer = checkpointer\n        runnable_config = RunnableConfig(\n            configurable={\n                \"thread_id\": str(conversation_id),\n                ... # other fields \n\n            },\n            recursion_limit=80,\n        )\n\n        try:\n            events = graph.astream(\n                {\"messages\": (\"user\", message_content)},\n                runnable_config,\n                stream_mode=\"updates\",\n            )\n\n...\n\n# typical tool \n@tool\nasync def some_tool(input: str, config: RunnableConfig):\n    thread_id = config.get(\"configurable\", {}).get(\"thread_id\") # <--- this returns nothing\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nToday, a change to our poetry lock file resulted in our CI installing the latest minor version of langgraph, going from 0.2.39 to 0.2.58.\nOur application passes in the RunnableConfig into each tool, and we use that to get relevant information which is critical for each tool. This worked fine before the upgrade, but after upgrading to 0.2.58 the config passed to our tools was suddenly blank. No configurable, no metadata: {'tags': [], 'metadata': {}, 'callbacks': None, 'recursion_limit': 25, 'configurable': {}}\nWhen going back to 0.2.39 the issue is not there, and the config contains all data we pass in when initializing the graph.\nIsolating only installing either 0.2.39 or 0.2.58 toggles the error, and results in the following dependencies also changing (we suspect it might be related to the checkpointer)\npoetry add langgraph@0.2.58\n\nUpdating dependencies\nResolving dependencies... (2.0s)\n\nPackage operations: 0 installs, 4 updates, 1 removal\n\n  - Removing httpx-sse (0.4.0)\n  - Updating langchain-core (0.3.13 -> 0.3.24)\n  - Updating langgraph-checkpoint (2.0.2 -> 2.0.8)\n  - Updating langgraph-sdk (0.1.35 -> 0.1.43)\n  - Updating langgraph (0.2.39 -> 0.2.58)\n\nWriting lock file\n\nI know the example is not reproducible, but it was too much work to actually set up an example given db dependencies etc, so I hope the provided example illustrates well enough what the issue is. I still wanted to report this however, as it was a huge breaking change for us, which definitely should not be the case with a minor upgrade.\nSystem Info\nSystem Information (0.2.58)\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.6.0: Thu Sep 12 23:35:29 PDT 2024; root:xnu-10063.141.1.701.1~1/RELEASE_ARM64_T6000\nPython Version:  3.10.13 (main, Sep 11 2023, 15:00:52) [Clang 14.0.0 (clang-1400.0.29.202)]\n\nPackage Information\n\nlangchain_core: 0.3.24\nlangchain: 0.3.4\nlangchain_community: 0.3.3\nlangsmith: 0.1.137\nlangchain_anthropic: 0.1.20\nlangchain_openai: 0.2.4\nlangchain_text_splitters: 0.3.0\nlangchainhub: 0.1.21\nlanggraph_sdk: 0.1.43\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.10.10\nanthropic: 0.28.1\nasync-timeout: 4.0.3\ndataclasses-json: 0.6.7\ndefusedxml: 0.7.1\nhttpx: 0.27.2\njsonpatch: 1.33\nnumpy: 1.26.4\nopenai: 1.52.2\norjson: 3.10.10\npackaging: 24.1\npydantic: 2.9.2\npydantic-settings: 2.6.0\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.36\ntenacity: 9.0.0\ntiktoken: 0.7.0\ntypes-requests: 2.32.0.20241016\ntyping-extensions: 4.12.2\n\n\nSystem Information (0.2.39)\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.6.0: Thu Sep 12 23:35:29 PDT 2024; root:xnu-10063.141.1.701.1~1/RELEASE_ARM64_T6000\nPython Version:  3.10.13 (main, Sep 11 2023, 15:00:52) [Clang 14.0.0 (clang-1400.0.29.202)]\n\nPackage Information\n\nlangchain_core: 0.3.13\nlangchain: 0.3.4\nlangchain_community: 0.3.3\nlangsmith: 0.1.137\nlangchain_anthropic: 0.1.20\nlangchain_openai: 0.2.4\nlangchain_text_splitters: 0.3.0\nlangchainhub: 0.1.21\nlanggraph: 0.2.39\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.10.10\nanthropic: 0.28.1\nasync-timeout: 4.0.3\ndataclasses-json: 0.6.7\ndefusedxml: 0.7.1\nhttpx: 0.27.2\njsonpatch: 1.33\nlanggraph-checkpoint: 2.0.2\nlanggraph-sdk: 0.1.35\nnumpy: 1.26.4\nopenai: 1.52.2\norjson: 3.10.10\npackaging: 24.1\npydantic: 2.9.2\npydantic-settings: 2.6.0\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.36\ntenacity: 9.0.0\ntiktoken: 0.7.0\ntypes-requests: 2.32.0.20241016\ntyping-extensions: 4.12.2\n", "created_at": "2024-12-11", "closed_at": "2024-12-11", "labels": [], "State": "closed", "Author": "Oyveloper"}
{"issue_number": 2712, "issue_title": "Get redis key split error on official document about the RedisSaver", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom configs import REDIS_CONFIG\nfrom redis import Redis\nfrom utils import RedisSaver\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n \nmemory = RedisSaver(Redis(**REDIS_CONFIG))\n \nclass SubState(TypedDict):\n    flag: int\n \ndef interrupt_route(state):\n    return \"interrupt\" if state.get(\"flag\") == 1 else \"end\"\n \ndef p2(state):\n    print(\"interrupt\")\n \nsub_workflow = StateGraph(SubState)\nsub_workflow.add_node(\"p1\", lambda state: {\"flag\": state.get(\"flag\", 0) + 1})\nsub_workflow.add_node(\"p2\", p2)\nsub_workflow.add_edge(START, \"p1\")\nsub_workflow.add_conditional_edges(\"p1\", interrupt_route, {\"interrupt\": \"p2\", \"end\": END})\nsub_workflow.add_edge(START, \"p1\")\nsub_workflow.add_edge(\"p2\", \"p1\")\n \nsub_graph = sub_workflow.compile(\n    interrupt_before=[\n        \"p2\",\n    ]\n)\n \nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"123\"\n    }\n}\n \nclass MainState(TypedDict):\n    flag: int\n \nworkflow = StateGraph(MainState)\nworkflow.add_node(\"sub_graph\", sub_graph)\nworkflow.add_edge(START, \"sub_graph\")\nworkflow.add_edge(\"sub_graph\", END)\ngraph = workflow.compile(checkpointer=memory)\n \nfor event, chunk in graph.stream({\"flag\": 0}, config, stream_mode=[\"updates\", \"messages\"]):\n    print(event, chunk)\n \ngraph.get_state(config, subgraphs=True)\nError Message and Stack Trace (if applicable)\n2024-12-11 10:43:52,580 DEBUG [MainProcess.1458:Thread-3 (process_message).140583974635264] [/home/langgraph-redis/kafka_server.py:124] [STAT] {\"logEvent\": \"updateState\", \"dialogId\": \"464bda98-c076-4a7b-85cc-d07416d01d53\", \"requestId\": \"dda1821b-57e2-4f11-a07f-b5be95b09e5b\", \"logMessage\": \"{\\\"__interrupt__\\\": []}\"}\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n['checkpoint', '464bda98-c076-4a7b-85cc-d07416d01d53', '', '1efb7acc-e6cc-667d-8000-4da30d12e037']\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n['checkpoint', '464bda98-c076-4a7b-85cc-d07416d01d53', '', '1efb7acc-f6cc-6860-8002-8ec69e580160']\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n['checkpoint', '464bda98-c076-4a7b-85cc-d07416d01d53', '', '1efb7acc-e6c7-6765-bfff-0b921ffdfbd4']\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n['checkpoint', '464bda98-c076-4a7b-85cc-d07416d01d53', '', '1efb7acc-e935-66ed-8001-078671ffd609']\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n['checkpoint', '464bda98-c076-4a7b-85cc-d07416d01d53', '', '1efb7acc-f6cc-6860-8002-8ec69e580160']\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n['checkpoint', '464bda98-c076-4a7b-85cc-d07416d01d53', '', '1efb7acc-f6cc-6860-8002-8ec69e580160']\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n['checkpoint', '464bda98-c076-4a7b-85cc-d07416d01d53', 'automation', 'd3048595-0167-ff1c-cfca-47e61b889371', '1efb7acd-01d7-6eb7-8001-42a027e47c5f']\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n['checkpoint', '464bda98-c076-4a7b-85cc-d07416d01d53', '', '1efb7acc-e6cc-667d-8000-4da30d12e037']\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n['checkpoint', '464bda98-c076-4a7b-85cc-d07416d01d53', '', '1efb7acc-f6cc-6860-8002-8ec69e580160']\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n['checkpoint', '464bda98-c076-4a7b-85cc-d07416d01d53', '', '1efb7acc-e6c7-6765-bfff-0b921ffdfbd4']\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n['checkpoint', '464bda98-c076-4a7b-85cc-d07416d01d53', '', '1efb7acc-e935-66ed-8001-078671ffd609']\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n['checkpoint', '464bda98-c076-4a7b-85cc-d07416d01d53', '', '1efb7acc-f6cc-6860-8002-8ec69e580160']\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n['checkpoint', '464bda98-c076-4a7b-85cc-d07416d01d53', '', '1efb7acc-f6cc-6860-8002-8ec69e580160']\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n['checkpoint', '464bda98-c076-4a7b-85cc-d07416d01d53', 'automation', 'd3048595-0167-ff1c-cfca-47e61b889371', '1efb7acd-01d7-6eb7-8001-42a027e47c5f']\nException in thread Thread-3 (process_message):\nTraceback (most recent call last):\n  File \"/home/langgraph-redis/kafka_server.py\", line 243, in process_message\n    {\"logEvent\": \"finishState\", \"dialogId\": dialog_id, \"requestId\": request_id, \"logMessage\": jsonify_log({\"messages\": copilot.get_state(config, subgraphs=True).values.get(\"messages\", [])})}))\n                                                                                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 640, in get_state\n    return self._prepare_state_snapshot(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 503, in _prepare_state_snapshot\n    task_states[task.id] = subgraphs[task.name].get_state(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 639, in get_state\n    saved = checkpointer.get_tuple(config)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/langgraph-redis/utils/checkpointers.py\", line 293, in get_tuple\n    checkpoint_key = self._get_checkpoint_key(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/langgraph-redis/utils/checkpointers.py\", line 371, in _get_checkpoint_key\n    latest_key = max(\n                 ^^^^\n  File \"/home/langgraph-redis/utils/checkpointers.py\", line 373, in <lambda>\n    key=lambda k: _parse_redis_checkpoint_key(k.decode())[\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/langgraph-redis/utils/checkpointers.py\", line 64, in _parse_redis_checkpoint_key\n    namespace, thread_id, checkpoint_ns, checkpoint_id = redis_key.split(\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: too many values to unpack (expected 4)\n \nDuring handling of the above exception, another exception occurred:\n \nTraceback (most recent call last):\n  File \"/opt/anaconda3/envs/py311/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n    self.run()\n  File \"/opt/anaconda3/envs/py311/lib/python3.11/threading.py\", line 982, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/langgraph-redis/kafka_server.py\", line 254, in process_message\n    {\"logEvent\": \"finishState\", \"dialogId\": dialog_id, \"requestId\": request_id, \"logMessage\": jsonify_log({\"messages\": copilot.get_state(config, subgraphs=True).values.get(\"messages\", [])})}))\n                                                                                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 640, in get_state\n    return self._prepare_state_snapshot(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 503, in _prepare_state_snapshot\n    task_states[task.id] = subgraphs[task.name].get_state(\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/py311/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 639, in get_state\n    saved = checkpointer.get_tuple(config)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/langgraph-redis/utils/checkpointers.py\", line 293, in get_tuple\n    checkpoint_key = self._get_checkpoint_key(\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/langgraph-redis/utils/checkpointers.py\", line 371, in _get_checkpoint_key\n    latest_key = max(\n                 ^^^^\n  File \"/home/langgraph-redis/utils/checkpointers.py\", line 373, in <lambda>\n    key=lambda k: _parse_redis_checkpoint_key(k.decode())[\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/langgraph-redis/utils/checkpointers.py\", line 64, in _parse_redis_checkpoint_key\n    namespace, thread_id, checkpoint_ns, checkpoint_id = redis_key.split(\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: too many values to unpack (expected 4)\nDescription\nI got errors when using the RedisSaver from the official how-to guide from https://langchain-ai.github.io/langgraph/how-tos/persistence_redis/\nI create a subgraph and set interrupt_node in it. When I trigger the interrupt node in the subgraph, I want to get the state with subgraphs and I got 5 splits on the redis_key, that's why I got the error.\nAny fix solutions for the official RedisSaver?\nSystem Info\nlanggraph==0.2.45", "created_at": "2024-12-11", "closed_at": "2024-12-11", "labels": [], "State": "closed", "Author": "KanaSukita"}
{"issue_number": 2698, "issue_title": "Figure out what we can do to help explain differences between update vs. values", "issue_body": "Privileged issue\n\n I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.\n\nIssue Content\nThere can be some subtle differences when using update vs. values.\nSpecifically, b/c update is prior to the reducer, message coercion never takes place.\nThis means that update and values give different types for the messages!\nOne option is to use the built in message type everywhere\nfrom typing_extensions import TypedDict, Literal\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import MessagesState, StateGraph, START, END\nimport uuid\n\ndef node(state):\n    return {\n        \"messages\": [{\n            \"role\": \"ai\",\n            \"content\": \"hello\"\n        }]\n    }\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"node\", node)\nbuilder.add_edge(START, \"node\")\ngraph = builder.compile()\n\nfor update in graph.stream(\n    {'messages': []},\n    stream_mode='updates',\n):\n    print(update)\n\nprint('----')\n\nfor update in graph.stream(\n    {'messages': []},\n    stream_mode='values',\n):\n    print(update)\n\nOutput from update:\n{'node': {'messages': [{'role': 'ai', 'content': 'hello'}]}}\nOutput from values:\n{'messages': []}\n{'messages': [AIMessage(content='hello', additional_kwargs={}, response_metadata={}, id='2f277693-67e6-4b5a-9ba0-80877b4ba6ce')]}\nNote that it's AIMessage in values, but dict format in update.\nWhile the behavior is as designed, I am not sure if it's as expected by users.", "created_at": "2024-12-10", "closed_at": null, "labels": ["maintainer"], "State": "open", "Author": "eyurtsev"}
{"issue_number": 2677, "issue_title": "Subgraph xx|xx not found", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n\u591a\u7ea7\u56fe\u7684\u4e00\u4e2a\u4efb\u52a1\uff0c\u5176\u4e2d\u5305\u62ec\u4e863\u7ea7\u5b50\u56fe\uff0c\u8ba1\u5212\u670910\u4e2a\u4e09\u7ea7\u56fe\uff0c\u6211\u5728\u8fdb\u884c\u6d4b\u8bd5\u65f6\u524d\u97627\u4e2a\u90fd\u6b63\u5e38\u5230\u4e86\u7b2c8\u4e2a\u5c31\u62a5\u9519\u4e86,\u7ed3\u6784\u90fd\u5dee\u4e0d\u591a\u3002\nError Message and Stack Trace (if applicable)\n2024-12-08 11:04:24 | DEBUG | chat_agent.process_state_level:178 - [Main]\u68c0\u6d4b\u5230\u5b50\u56fe\u72b6\u6001: None\n2024-12-08 11:04:24 | DEBUG | chat_agent.process_state_level:183 - [Main]\u66f4\u65b0\u4efb\u52a1\u914d\u7f6e\uff1a{'configurable': {'thread_id': 'api_test1', 'checkpoint_ns': 'KBQA:50fff8b3-16c1-c5a1-691c-cd2769c3b57c|AbnormalNoiseTraction:ba80f259-831b-770b-a710-1bac29d20f3f', 'checkpoint_id': '1efb553f-327d-6438-8006-c4eed2333d6e', 'checkpoint_map': {'': '1efb553a-b6fb-616d-8001-ff17f253d729', 'KBQA:50fff8b3-16c1-c5a1-691c-cd2769c3b57c': '1efb553e-6793-685a-8007-38a447a35f3d', 'KBQA:50fff8b3-16c1-c5a1-691c-cd2769c3b57c|AbnormalNoiseTraction:ba80f259-831b-770b-a710-1bac29d20f3f': '1efb553f-327d-6438-8006-c4eed2333d6e'}}}\n2024-12-08 11:04:24 | DEBUG | chat_agent.run:222 - [Main]\u66f4\u65b0\u4efb\u52a1\u914d\u7f6e\uff1a{'configurable': {'thread_id': 'api_test1', 'checkpoint_ns': 'KBQA:50fff8b3-16c1-c5a1-691c-cd2769c3b57c|AbnormalNoiseTraction:ba80f259-831b-770b-a710-1bac29d20f3f', 'checkpoint_id': '1efb553f-327d-6438-8006-c4eed2333d6e', 'checkpoint_map': {'': '1efb553a-b6fb-616d-8001-ff17f253d729', 'KBQA:50fff8b3-16c1-c5a1-691c-cd2769c3b57c': '1efb553e-6793-685a-8007-38a447a35f3d', 'KBQA:50fff8b3-16c1-c5a1-691c-cd2769c3b57c|AbnormalNoiseTraction:ba80f259-831b-770b-a710-1bac29d20f3f': '1efb553f-327d-6438-8006-c4eed2333d6e'}}}\n2024-12-08 11:04:24 | WARNING | chat_agent.run:233 - [Main]\u5904\u7406\u5bf9\u8bdd\u65f6\u51fa\u73b0\u5f02\u5e38\uff1aSubgraph KBQA|AbnormalNoiseTraction not found\nDescription\n\u62a5\u9519\u4e09\u7ea7\u5b50\u56fe\u7684\u7ed3\u6784\n\nSystem Info\nlangchain                0.3.8\nlangchain-core           0.3.21\nlangchain-openai         0.2.9\nlangchain-text-splitters 0.3.2\nlanggraph                0.2.53\nlanggraph-checkpoint     2.0.5\nlanggraph-sdk            0.1.36\nlangsmith                0.1.145", "created_at": "2024-12-08", "closed_at": "2024-12-20", "labels": ["invalid"], "State": "closed", "Author": "deku0818"}
{"issue_number": 2671, "issue_title": "Parameter Name Mismatch in run_server() Function for langgraph-cli", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n# install langgraph-cli=0.1.61\npip install langgraph-cli -U\n\n# cli cmd\nlanggraph dev --config langgraph.json --host 127.0.0.1 --port 2024 --debug-port 5678 --no-browser\n\n# here is the langgraph.json\n{\n    \"graphs\": {\"jockey\": \"jockey/app.py:jockey\"},\n    \"env\": \".env\",\n    \"dependencies\": [\".\"],\n}\nError Message and Stack Trace (if applicable)\nFile \"venv/lib/python3.11/site-packages/langgraph_cli/cli.py\", line 605, in dev\n    run_server(\nTypeError: run_server() got an unexpected keyword argument 'env'\nDescription\nI encountered a bug in the run_server() function where the parameter names env_file, store, and wait_for_client are mismatched or not behaving as expected. This issue is present in the file langgraph_cli/cli.py, specifically in the dev function where run_server() is called.\nIn langgraph_cli/cli.py the params specified do not match the params defined:\n    run_server(\n        host,\n        port,\n        not no_reload,\n        graphs,\n        n_jobs_per_worker=n_jobs_per_worker,\n        open_browser=not no_browser,\n        debug_port=debug_port,\n        env=config_json.get(\"env\"),\n        store=config_json.get(\"store\"),\n        wait_for_client=wait_for_client,\n    )\nHowever, the function run_server() is defined as follows in langgraph_api/cli.py:\ndef run_server(\n    host: str = \"127.0.0.1\",\n    port: int = 2024,\n    reload: bool = False,\n    graphs: dict | None = None,\n    n_jobs_per_worker: int | None = None,\n    env_file: str | None = None,\n    open_browser: bool = False,\n    debug_port: int | None = None,\n):\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.6.0: Thu Sep 12 23:35:10 PDT 2024; root:xnu-10063.141.1.701.1~1/RELEASE_ARM64_T6030\nPython Version:  3.11.10 (main, Sep 30 2024, 15:19:48) [Clang 15.0.0 (clang-1500.3.9.4)]\n\nPackage Information\n\nlangchain_core: 0.3.19\nlangchain: 0.3.7\nlangsmith: 0.1.129\nlangchain_openai: 0.2.9\nlangchain_text_splitters: 0.3.2\nlanggraph: 0.2.56\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.10.8\nasync-timeout: Installed. No version info available.\nhttpx: 0.27.2\njsonpatch: 1.33\nlanggraph-checkpoint: 2.0.7\nlanggraph-sdk: 0.1.42\nnumpy: 1.26.4\nopenai: 1.54.5\norjson: 3.10.7\npackaging: 24.1\npydantic: 2.9.2\nPyYAML: 6.0.2\nrequests: 2.32.3\nSQLAlchemy: 2.0.35\ntenacity: 8.5.0\ntiktoken: 0.7.0\ntyping-extensions: 4.12.2\n", "created_at": "2024-12-06", "closed_at": "2024-12-19", "labels": [], "State": "closed", "Author": "kingsotn-twelve"}
{"issue_number": 2666, "issue_title": "langgraph-checkpoint-postgres fails to persist state fields with Pydantic model types", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nclass XOutput(BaseModel):\n    x: str = Field(description=\"Some variable x\")\n    y: str = Field(description=\"Some variable y\")\n\n\nclass AgentState(BaseModel):\n    class Config:\n        arbitrary_types_allowed = True\n\n    tool_caller: Optional[str] = Field(default=None)\n    messages: Annotated[List[BaseMessage], add_messages] = Field(default_factory=list)\n    output: Optional[XOutput]\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nI'm using an AgentState similar to the example code. While streaming the graph with stream mode set to \"debug\" and using the postgres-checkpointer, I correctly get the value of 'output' state field.\nBut when running app.get_state_history(config), with the correct config set, I'm getting null value for output. other fields with str types are returned correctly though.\nDo langgraph-checkpoint-postgres not support fields with pydantic model type?\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.6.0: Mon Jul 29 21:16:46 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T8112\nPython Version:  3.11.9 (main, Aug 12 2024, 16:41:06) [Clang 15.0.0 (clang-1500.3.9.4)]\n\nPackage Information\n\nlangchain_core: 0.3.21\nlangchain: 0.3.9\nlangchain_community: 0.3.8\nlangsmith: 0.1.142\nlangchain_anthropic: 0.3.0\nlangchain_aws: 0.2.7\nlangchain_google_genai: 2.0.6\nlangchain_google_vertexai: 2.0.8\nlangchain_groq: 0.2.1\nlangchain_openai: 0.2.10\nlangchain_text_splitters: 0.3.2\nlanggraph_sdk: 0.1.35\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.10.3\nanthropic: 0.39.0\nanthropic[vertexai]: Installed. No version info available.\nasync-timeout: Installed. No version info available.\nboto3: 1.34.158\ndataclasses-json: 0.6.7\ndefusedxml: 0.7.1\nfiletype: 1.2.0\ngoogle-cloud-aiplatform: 1.72.0\ngoogle-cloud-storage: 2.18.2\ngoogle-generativeai: 0.8.3\ngroq: 0.9.0\nhttpx: 0.27.0\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlangchain-mistralai: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.54.4\norjson: 3.10.7\npackaging: 24.1\npydantic: 2.9.2\npydantic-settings: 2.6.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.32\ntenacity: 8.5.0\ntiktoken: 0.7.0\ntyping-extensions: 4.12.2\n", "created_at": "2024-12-06", "closed_at": "2024-12-18", "labels": ["invalid"], "State": "closed", "Author": "shivangag"}
{"issue_number": 2664, "issue_title": "Can't the run_name when running graph", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n# when i def the chatbot using follow code,i cant get the name when run the workflow\nasync def chatbot1(state: State, config: RunnableConfig):\n        config[\"run_name\"] = \"chatbot1\"\n        return {\"messages\": [await llm1.ainvoke(state[\"messages\"], config)]}\n\n# here i cant get name\nasync def message_generator(user_input: str):\n    \"\"\"Generate streaming messages from the graph.\"\"\"\n    inputs = [HumanMessage(content=user_input)]\n\n    async for msg, metadata in graph.astream(\n        {\"messages\": inputs}, stream_mode=\"messages\"\n    ):\n        # here is none.\n        # but i can get it when i using second way\n        print(msg.name)\n        yield json.dumps({\"content\": msg.content})\n\n# but when using follow code,i can\n    async def chatbot1(state: State, config: RunnableConfig):\n        RunnableConfig(run_name=\"chatbot1\")\n        return {\"messages\": [await llm1.ainvoke(state[\"messages\"], RunnableConfig(run_name=\"chatbot1\"))]}\n\n# that's why? is it a bug?\nError Message and Stack Trace (if applicable)\nit's not a Error ,it's a phenomenon\nDescription\nwhen i use first way to def the run_name,i cant get name in workflow processing ,but i can get it using seconds code!\nSystem Info\npass", "created_at": "2024-12-06", "closed_at": "2025-01-15", "labels": [], "State": "closed", "Author": "Daniel-963"}
{"issue_number": 2647, "issue_title": "Cannot run Langgraph dev for langgraph.js: AttributeError: 'str' object has no attribute 'parent'", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nhttps://github.com/debkanchan/langgraph-repro.git\nError Message and Stack Trace (if applicable)\n(.venv) workspace \ue0a0 chain-of-thought \u276f langgraph dev                                                                        \u2739 \u272d\nTraceback (most recent call last):\n  File \"/Users/debkanchan/Code/paralegal-ai/workspace/.venv/bin/langgraph\", line 8, in <module>\n    sys.exit(cli())\n             ^^^^^\n  File \"/Users/debkanchan/Code/paralegal-ai/workspace/.venv/lib/python3.11/site-packages/click/core.py\", line 1157, in __call__\n    return self.main(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/debkanchan/Code/paralegal-ai/workspace/.venv/lib/python3.11/site-packages/click/core.py\", line 1078, in main\n    rv = self.invoke(ctx)\n         ^^^^^^^^^^^^^^^^\n  File \"/Users/debkanchan/Code/paralegal-ai/workspace/.venv/lib/python3.11/site-packages/click/core.py\", line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/debkanchan/Code/paralegal-ai/workspace/.venv/lib/python3.11/site-packages/click/core.py\", line 1434, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/debkanchan/Code/paralegal-ai/workspace/.venv/lib/python3.11/site-packages/click/core.py\", line 783, in invoke\n    return __callback(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/debkanchan/Code/paralegal-ai/workspace/.venv/lib/python3.11/site-packages/langgraph_cli/analytics.py\", line 96, in decorator\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/debkanchan/Code/paralegal-ai/workspace/.venv/lib/python3.11/site-packages/langgraph_cli/cli.py\", line 595, in dev\n    config_json = langgraph_cli.config.validate_config_file(config)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/debkanchan/Code/paralegal-ai/workspace/.venv/lib/python3.11/site-packages/langgraph_cli/config.py\", line 154, in validate_config_file\n    package_json_path = config_path.parent / \"package.json\"\n                        ^^^^^^^^^^^^^^^^^^\nAttributeError: 'str' object has no attribute 'parent'\nDescription\n\nI am trying to run langgraph studio locally through langgraph cli\nI expect it to start langgraph studio\nInstead I get the error\n\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.1.0: Thu Oct 10 21:05:14 PDT 2024; root:xnu-11215.41.3~2/RELEASE_ARM64_T8103\nPython Version:  3.11.6 (v3.11.6:8b6ee5ba3b, Oct  2 2023, 11:18:21) [Clang 13.0.0 (clang-1300.0.29.30)]\n\nPackage Information\n\nlangchain_core: 0.3.21\nlangsmith: 0.1.147\nlanggraph_api: 0.0.6\nlanggraph_cli: 0.1.61\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.43\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nclick: 8.1.7\ncryptography: 43.0.3\nhttpx: 0.28.0\njsonpatch: 1.33\njsonschema-rs: 0.25.1\nlanggraph: 0.2.56\nlanggraph-checkpoint: 2.0.8\nlangsmith-pyo3: Installed. No version info available.\norjson: 3.10.12\npackaging: 24.2\npydantic: 2.10.3\npyjwt: 2.10.1\npython-dotenv: 1.0.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nsse-starlette: 2.1.3\nstarlette: 0.41.3\nstructlog: 24.4.0\ntenacity: 8.5.0\ntyping-extensions: 4.12.2\nuvicorn: 0.32.1\nwatchfiles: 1.0.0\n", "created_at": "2024-12-05", "closed_at": "2024-12-18", "labels": [], "State": "closed", "Author": "debkanchan"}
{"issue_number": 2644, "issue_title": "langgraph.errors.InvalidUpdateError: Must write to at least one of ['foo']", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing_extensions import TypedDict\nfrom IPython.display import Image, display\nfrom langgraph.graph import StateGraph, START, END\nclass State(TypedDict):\n    foo: int\nclass TmpState(TypedDict):\n    zoo: int\nclass node1:\n    def __call__(self, state: State) -> TmpState:\n        print(\"---Node 1---\")\n        return {\"zoo\": state['foo'] + 1}\nclass node2:\n    def __call__(self, state: TmpState) -> State:\n        print(\"---Node 2---\")\n        return {\"foo\": state['zoo'] + 1}\n# Build graph\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", node1())\nbuilder.add_node(\"node_2\", node2())\n# Logic\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\nbuilder.add_edge(\"node_2\", END)\n# Add\ngraph = builder.compile()\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\ngraph.invoke({\"foo\" : 1})\nError Message and Stack Trace (if applicable)\n<IPython.core.display.Image object>\n---Node 1---\nTraceback (most recent call last):\n  File \"d:\\Software\\anaconda3\\Lib\\runpy.py\", line 198, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Software\\anaconda3\\Lib\\runpy.py\", line 88, in _run_code\n    exec(code, run_globals)\n  File \"c:\\Users\\f\\.vscode\\extensions\\ms-python.debugpy-2024.12.0-win32-x64\\bundled\\libs\\debugpy\\adapter/../..\\debugpy\\launcher/../..\\debugpy\\__main__.py\", line 71, in <module>\n    cli.main()\n  File \"c:\\Users\\f\\.vscode\\extensions\\ms-python.debugpy-2024.12.0-win32-x64\\bundled\\libs\\debugpy\\adapter/../..\\debugpy\\launcher/../..\\debugpy/..\\debugpy\\server\\cli.py\", line 501, in main\n    run()\n  File \"c:\\Users\\f\\.vscode\\extensions\\ms-python.debugpy-2024.12.0-win32-x64\\bundled\\libs\\debugpy\\adapter/../..\\debugpy\\launcher/../..\\debugpy/..\\debugpy\\server\\cli.py\", line 351, in run_file    runpy.run_path(target, run_name=\"__main__\")\n  File \"c:\\Users\\f\\.vscode\\extensions\\ms-python.debugpy-2024.12.0-win32-x64\\bundled\\libs\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_runpy.py\", line 310, in run_path\n    return _run_module_code(code, init_globals, run_name, pkg_name=pkg_name, script_name=fname)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\f\\.vscode\\extensions\\ms-python.debugpy-2024.12.0-win32-x64\\bundled\\libs\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_runpy.py\", line 127, in _run_module_code\n    _run_code(code, mod_globals, init_globals, mod_name, mod_spec, pkg_name, script_name)\n  File \"c:\\Users\\f\\.vscode\\extensions\\ms-python.debugpy-2024.12.0-win32-x64\\bundled\\libs\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_runpy.py\", line 118, in _run_code\n    exec(code, run_globals)\n  File \"D:\\1\u6bd5\u8bbe\\code\\My\\2.py\", line 28, in <module>\n    graph.invoke({\"foo\" : 1})\n  File \"d:\\Software\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 1551, in invoke\n    for chunk in self.stream(\n  File \"d:\\Software\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 1290, in stream\n    for _ in runner.tick(\n  File \"d:\\Software\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\runner.py\", line 56, in tick\n    run_with_retry(t, retry_policy)\n  File \"d:\\Software\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\retry.py\", line 29, in run_with_retry\n    task.proc.invoke(task.input, config)\n  File \"d:\\Software\\anaconda3\\Lib\\site-packages\\langgraph\\utils\\runnable.py\", line 387, in invoke\n    input = context.run(step.invoke, input, config)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Software\\anaconda3\\Lib\\site-packages\\langgraph\\utils\\runnable.py\", line 159, in invoke\n    ret = context.run(self.func, input, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Software\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\write.py\", line 85, in _write\n    self.do_write(\n  File \"d:\\Software\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\write.py\", line 138, in do_write\n    raise InvalidUpdateError(\nlanggraph.errors.InvalidUpdateError: Must write to at least one of ['foo']\nDescription\nI'm trying to use the class node (instead of function node) to multiple schemas. I expect to see {'foo': 3}. Instead, it raise Exception.\nSystem Info\npython -m langchain_core.sys_info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.19045\nPython Version:  3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.3.12\nlangchain: 0.3.4\nlangchain_community: 0.3.3\nlangsmith: 0.1.128\nlangchain_anthropic: 0.1.23\nlangchain_cohere: 0.3.0\nlangchain_experimental: 0.3.1\nlangchain_openai: 0.2.3\nlangchain_text_splitters: 0.3.0\nlangchainhub: 0.1.21\nlanggraph: 0.2.27\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.9.3\nanthropic: 0.34.2\nasync-timeout: Installed. No version info available.\ncohere: 5.9.4\ndataclasses-json: 0.6.7\ndefusedxml: 0.7.1\nhttpx: 0.27.2\njsonpatch: 1.33\nlanggraph-checkpoint: 1.0.9\nnumpy: 1.26.4\nopenai: 1.52.2\norjson: 3.10.7\npackaging: 23.2\npandas: 2.1.4\npydantic: 2.9.2\npydantic-settings: 2.5.2\nPyYAML: 6.0.1\nrequests: 2.31.0\nSQLAlchemy: 2.0.25\ntabulate: 0.9.0\ntenacity: 8.5.0\ntiktoken: 0.7.0\ntypes-requests: 2.32.0.20240914\ntyping-extensions: 4.12.2\n", "created_at": "2024-12-05", "closed_at": "2024-12-05", "labels": [], "State": "closed", "Author": "starrystar"}
{"issue_number": 2618, "issue_title": "call the tool not work", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nI create tool calls, but find that they often don't work, especially after using MEMORY, the first one is fine, and then I don't call the tool later on\n\n        memory = MemorySaver()\n\n        self.myapp = create_react_agent(\n            self.llm,\n            self.tools,\n            messages_modifier=agent_prompt_template,\n            checkpointer=memory\n        )\nError Message and Stack Trace (if applicable)\nChecking memory usage, but not calling the tool\n{\n  \"name\": \"run_shell_command\",\n  \"parameters\": {\n    \"__arg1\": \"free -h\"\n  }\n}\nDescription\nIf you add memory\uff0cGood for a first time\uff0c The tool will not be called in the future\nSystem Info\nlangchain==0.3.9\nlangchain-community==0.3.7\nlangchain-core==0.3.21\nlangchain-experimental==0.3.3\nlangchain-google-community==2.0.2\nlangchain-huggingface==0.1.2\nlangchain-openai==0.2.8\nlangchain-text-splitters==0.3.2", "created_at": "2024-12-04", "closed_at": "2024-12-16", "labels": ["invalid"], "State": "closed", "Author": "jason571"}
{"issue_number": 2610, "issue_title": "Tool calls not working as expected when are called in parallel", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\n## tool fucntion\ndef publish_final_answer(\n        final_answer: str,\n        index: int,\n    ):\n\n        print(\"\\n\\t\\tMy index is\", index, \"\\n\")\n\n        is_final_answer = True\n        try:\n            response = json.loads(final_answer, strict=False)\n        except Exception as e:\n            response = f\"Tool Error: Final answer is not a valid JSON: {e}\"\n            is_final_answer = False\n\n        tool_response = {\n            \"message\": response,\n            \"is_final_answer\": is_final_answer,\n            TRACE_ID: index,\n        }\n\n        print(\"Tool response:\", tool_response)\n\n        return json.dumps(tool_response)\nError Message and Stack Trace (if applicable)\nNo response\nDescription\n\nThis is my graph, I am executing in parallel two agents (sql_ret_0 and sql_ret_1), each calls a tool inside ToolNode sql_ret_tools_0 and sql_ret_tools_1 respectively, to identify each instance of my agent I use a idx (0/1), so when I call a tool I pass the index  to the tool, and this is working correctly since the tool calls look like this:\nINFO:root:Tools calls: [{'name': 'publish_final_answer', 'args': {'index': 0, 'final_answer': ''}, 'id': '071ca8f1-8ae0-43ae-8e01-9e018995df33', 'type': 'tool_call'}]\nINFO:root:Tools calls: [{'name': 'publish_final_answer', 'args': {'index': 1, 'final_answer': ''}, 'id': 'f36a865c-94c5-4141-8a3b-bbfefe8c7b76', 'type': 'tool_call'}]\nBut the tools is being called twice only from the second node (sql_ret_1) because I added a print inside the tool to print the index and it shows twice the same\n\n                My index is 1 \n\nTool response: {'message': {'answer': \"I'm here to help\"}, 'is_final_answer': True, 'trace_id': 1}\n\n                My index is 1 \n\nTool response: {'message': {'answer': \"I'm here to help\"}, 'is_final_answer': True, 'trace_id': 1}\n\n\nThe graph seeem to be built correctly as you can see from the diagram, also it works perfectly when I instantiate only one node sql_ret.\nSystem Info\nlangchain==0.3.1\nlangchain-anthropic==0.2.1\nlangchain-core==0.3.21\nlangchain-openai==0.2.1\nlangchain-text-splitters==0.3.0\nplatform mac\npython version 3.11", "created_at": "2024-12-03", "closed_at": "2025-01-15", "labels": [], "State": "closed", "Author": "henryf3"}
{"issue_number": 2609, "issue_title": "Postgres checkpointer async persistence errors", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom psycopg.rows import dict_row\nfrom psycopg_pool import AsyncConnectionPool\nfrom langgraph.store.postgres import AsyncPostgresStore\nfrom langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\nfrom langgraph.store.memory import BaseStore\nfrom langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\nfrom IPython.display import Image, display\nfrom langchain_core.runnables import RunnableConfig\nfrom Chatbot.tools.GeneralTools.DbSessionTools import GetTenantConnectionString\n\n# a way to store state\nclass State(TypedDict):\n    s: str\n    \ndef a(state: State, config: RunnableConfig, store: \"BaseStore\") -> State:\n    return {\"s\": f\"{state}\"}\n\ndef b(state: State, config: RunnableConfig, store: \"BaseStore\"):\n    return {\"s\": f\"{state}\"}\n\n# Build the graph\nbuilder = StateGraph(State)\nbuilder.add_node(\"a\", a)\nbuilder.add_node(\"b\", b)\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"b\", END)\n\n# Database but just localy\ntenantId = \"myTenant\"\n\nasync with AsyncConnectionPool(\n            conninfo=GetTenantConnectionString(f'tenant-{tenantId}'),\n            max_size=5,\n            kwargs={\n                \"autocommit\": True,\n                \"prepare_threshold\": 0,\n                \"options\": \"-c search_path=chatbot\",\n                \"row_factory\": dict_row,\n            },\n        ) as pool:\n    checkpointer = AsyncPostgresSaver(pool)\n    store = AsyncPostgresStore(pool)\n            \n    graph = builder.compile(checkpointer=checkpointer, store=store)\n    display(Image(graph.get_graph().draw_mermaid_png()))\n    \n    config={\"configurable\": {\n            \"thread_id\": \"9\",\n            \"checkpoint_ns\": None,\n            }\n        }\n    v = graph.astream({\"s\": \"x\"}, config=config)\n    async for i in v:\n        print(i)\nError Message and Stack Trace (if applicable)\n## Errors while running:\nerror ignored terminating <psycopg.AsyncPipeline [IDLE, pipeline=ON] (host=localhost port=5433 user=admin database=tenant-myTenant) at 0x12f51e410>: pipeline aborted\nerror ignored terminating <psycopg.AsyncPipeline [IDLE, pipeline=ON] (host=localhost port=5433 user=admin database=tenant-myTenant) at 0x12f51ea50>: pipeline aborted\nerror ignored terminating <psycopg.AsyncPipeline [IDLE, pipeline=ON] (host=localhost port=5433 user=admin database=tenant-myTenant) at 0x12f714b10>: pipeline aborted\n\n## Streamed values:\n{'a': {'s': \"{'s': 'x'}\"}}\n\n## Exception trace at the end\nCell In[24], line 57\n     51 config={\"configurable\": {\n     52         \"thread_id\": \"9\",\n     53         \"checkpoint_ns\": None,\n     54         }\n     55     }\n     56 v = graph.astream({\"s\": \"x\"}, config=config)\n---> 57 async for i in v:\n     58     print(i)\n\nFile ~/Documents/Code/RevVue/neate-chatbot/src/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1823, in Pregel.astream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\n   1817 if \"custom\" in stream_modes:\n   1818     config[CONF][CONFIG_KEY_STREAM_WRITER] = (\n   1819         lambda c: aioloop.call_soon_threadsafe(\n   1820             stream.put_nowait, ((), \"custom\", c)\n   1821         )\n   1822     )\n-> 1823 async with AsyncPregelLoop(\n   1824     input,\n   1825     stream=StreamProtocol(stream.put_nowait, stream_modes),\n   1826     config=config,\n   1827     store=store,\n   1828     checkpointer=checkpointer,\n   1829     nodes=self.nodes,\n   1830     specs=self.channels,\n   1831     output_keys=output_keys,\n   1832     stream_keys=self.stream_channels_asis,\n   1833     interrupt_before=interrupt_before_,\n   1834     interrupt_after=interrupt_after_,\n   1835     manager=run_manager,\n   1836     debug=debug,\n   1837 ) as loop:\n   1838     # create runner\n   1839     runner = PregelRunner(\n   1840         submit=loop.submit,\n   1841         put_writes=loop.put_writes,\n   (...)\n   1844         node_finished=config[CONF].get(CONFIG_KEY_NODE_FINISHED),\n   1845     )\n   1846     # enable subgraph streaming\n\nFile ~/Documents/Code/RevVue/neate-chatbot/src/.venv/lib/python3.11/site-packages/langgraph/pregel/loop.py:1011, in AsyncPregelLoop.__aexit__(self, exc_type, exc_value, traceback)\n   1004 async def __aexit__(\n   1005     self,\n   1006     exc_type: Optional[Type[BaseException]],\n   (...)\n   1009 ) -> Optional[bool]:\n   1010     # unwind stack\n-> 1011     return await asyncio.shield(\n   1012         self.stack.__aexit__(exc_type, exc_value, traceback)\n   1013     )\n\nFile /opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:745, in AsyncExitStack.__aexit__(self, *exc_details)\n    741 try:\n    742     # bare \"raise exc_details[1]\" replaces our carefully\n    743     # set-up context\n    744     fixed_ctx = exc_details[1].__context__\n--> 745     raise exc_details[1]\n    746 except BaseException:\n    747     exc_details[1].__context__ = fixed_ctx\n\nFile /opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:728, in AsyncExitStack.__aexit__(self, *exc_details)\n    726     cb_suppress = cb(*exc_details)\n    727 else:\n--> 728     cb_suppress = await cb(*exc_details)\n    730 if cb_suppress:\n    731     suppressed_exc = True\n\nFile ~/Documents/Code/RevVue/neate-chatbot/src/.venv/lib/python3.11/site-packages/langgraph/pregel/executor.py:191, in AsyncBackgroundExecutor.__aexit__(self, exc_type, exc_value, traceback)\n    189 try:\n    190     if exc := task.exception():\n--> 191         raise exc\n    192 except asyncio.CancelledError:\n    193     pass\n\nFile ~/Documents/Code/RevVue/neate-chatbot/src/.venv/lib/python3.11/site-packages/langgraph/pregel/loop.py:931, in AsyncPregelLoop._checkpointer_put_after_previous(self, prev, config, checkpoint, metadata, new_versions)\n    929 try:\n    930     if prev is not None:\n--> 931         await prev\n    932 finally:\n    933     await cast(BaseCheckpointSaver, self.checkpointer).aput(\n    934         config, checkpoint, metadata, new_versions\n    935     )\n\nFile ~/Documents/Code/RevVue/neate-chatbot/src/.venv/lib/python3.11/site-packages/langgraph/pregel/loop.py:933, in AsyncPregelLoop._checkpointer_put_after_previous(self, prev, config, checkpoint, metadata, new_versions)\n    931         await prev\n    932 finally:\n--> 933     await cast(BaseCheckpointSaver, self.checkpointer).aput(\n    934         config, checkpoint, metadata, new_versions\n    935     )\n\nFile ~/Documents/Code/RevVue/neate-chatbot/src/.venv/lib/python3.11/site-packages/langgraph/checkpoint/postgres/aio.py:258, in AsyncPostgresSaver.aput(self, config, checkpoint, metadata, new_versions)\n    249 copy = checkpoint.copy()\n    250 next_config = {\n    251     \"configurable\": {\n    252         \"thread_id\": thread_id,\n   (...)\n    255     }\n    256 }\n--> 258 async with self._cursor(pipeline=True) as cur:\n    259     await cur.executemany(\n    260         self.UPSERT_CHECKPOINT_BLOBS_SQL,\n    261         await asyncio.to_thread(\n   (...)\n    267         ),\n    268     )\n    269     await cur.execute(\n    270         self.UPSERT_CHECKPOINTS_SQL,\n    271         (\n   (...)\n    278         ),\n    279     )\n\nFile /opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:217, in _AsyncGeneratorContextManager.__aexit__(self, typ, value, traceback)\n    215 if typ is None:\n    216     try:\n--> 217         await anext(self.gen)\n    218     except StopAsyncIteration:\n    219         return False\n\nFile ~/Documents/Code/RevVue/neate-chatbot/src/.venv/lib/python3.11/site-packages/langgraph/checkpoint/postgres/aio.py:339, in AsyncPostgresSaver._cursor(self, pipeline)\n    335 elif pipeline:\n    336     # a connection not in pipeline mode can only be used by one\n    337     # thread/coroutine at a time, so we acquire a lock\n    338     if self.supports_pipeline:\n--> 339         async with (\n    340             self.lock,\n    341             conn.pipeline(),\n    342             conn.cursor(binary=True, row_factory=dict_row) as cur,\n    343         ):\n    344             yield cur\n    345     else:\n    346         # Use connection's transaction context manager when pipeline mode not supported\n\nFile /opt/homebrew/Cellar/python@3.11/3.11.10/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:217, in _AsyncGeneratorContextManager.__aexit__(self, typ, value, traceback)\n    215 if typ is None:\n    216     try:\n--> 217         await anext(self.gen)\n    218     except StopAsyncIteration:\n    219         return False\n\nFile ~/Documents/Code/RevVue/neate-chatbot/src/.venv/lib/python3.11/site-packages/psycopg/connection_async.py:398, in AsyncConnection.pipeline(self)\n    395         pipeline = self._pipeline = AsyncPipeline(self)\n    397 try:\n--> 398     async with pipeline:\n    399         yield pipeline\n    400 finally:\n\nFile ~/Documents/Code/RevVue/neate-chatbot/src/.venv/lib/python3.11/site-packages/psycopg/_pipeline.py:266, in AsyncPipeline.__aexit__(self, exc_type, exc_val, exc_tb)\n    264         logger.warning(\"error ignored terminating %r: %s\", self, exc2)\n    265     else:\n--> 266         raise exc2.with_traceback(None)\n    267 finally:\n    268     self._exit(exc_val)\n\nNotNullViolation: null value in column \"checkpoint_ns\" of relation \"checkpoint_blobs\" violates not-null constraint\nDETAIL:  Failing row contains (9, null, __start__, 00000000000000000000000000000001.0.19938872030577381, msgpack, \\x81a173a178).\nDescription\nI am using Postgres checkpointer async persistence, and when I use async stream, there are some unexpected errors in the terminal. After values are streamed I am getting an exception of Not null violation.\nSystem Info\nlangchain==0.3.9\nlangchain-community==0.3.9\nlangchain-core==0.3.21\nlangchain-openai==0.2.10\nlangchain-postgres==0.0.12\nlangchain-text-splitters==0.3.2\nlanggraph==0.2.53\nlanggraph-checkpoint==2.0.8\nlanggraph-checkpoint-postgres==2.0.7\nlanggraph-sdk==0.1.40\npsycopg==3.2.3\npsycopg-pool==3.2.4", "created_at": "2024-12-03", "closed_at": "2024-12-03", "labels": [], "State": "closed", "Author": "BadrElfarri"}
{"issue_number": 2607, "issue_title": "Graph Display not working for over 3 levels nested graph and when more than one node", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom langchain_core.messages import AIMessage\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import MessagesState\n\n# ********************************************************************************************************************\n\ndef get_weather(state: MessagesState):\n    return None\n\ndef will_it_rain(state: MessagesState):\n    return None\n\nworkflow = StateGraph(MessagesState)\nworkflow.add_node(\"will_it_rain\", will_it_rain)\nworkflow.add_node(\"get_weather\", get_weather) # Just adding this additional node throws error\n\nworkflow.add_edge(START, \"will_it_rain\")\nworkflow.add_edge(\"will_it_rain\", \"get_weather\")\nworkflow.add_edge(\"get_weather\", END)\nexecutor_agent = workflow.compile()\n\n# ********************************************************************************************************************\n\ndef testingSubSubGraphNode1(state: MessagesState):\n    return {\n        \"messages\": [AIMessage(content=\"Testing Sub Sub Node 1\")]\n    }\n\nworkflow = StateGraph(MessagesState)\nworkflow.add_node(\"testingSubSubGraphNode1\", testingSubSubGraphNode1)\nworkflow.add_node(\"executor_agent\", executor_agent)\n\nworkflow.add_edge(START, \"testingSubSubGraphNode1\")\nworkflow.add_edge(\"testingSubSubGraphNode1\", \"executor_agent\")\nworkflow.add_edge(\"executor_agent\", END)\nsubSubGraph = workflow.compile()\n\n\n# ********************************************************************************************************************\n\ndef testingSubGraphNode(state: MessagesState):\n    return {\n        \"messages\": [AIMessage(content=\"Testing Sub Node 1\")]\n    }\n\nworkflow = StateGraph(MessagesState)\nworkflow.add_node(\"testingSubGraphNode1\", testingSubGraphNode)\nworkflow.add_node(\"testingSubGraphNode2\", subSubGraph)\nworkflow.add_edge(START, \"testingSubGraphNode1\")\nworkflow.add_edge(\"testingSubGraphNode1\", \"testingSubGraphNode2\")\nworkflow.add_edge(\"testingSubGraphNode2\", END)\nsubGraph = workflow.compile()\n\n\n# ********************************************************************************************************************\n\n\ndef testingParentNode(state: MessagesState):\n    return {\n        \"messages\": [AIMessage(content=\"Testing Parent Node\")]\n    }\n\nworkflow = StateGraph(MessagesState)\nworkflow.add_node(\"testingParentNode1\", testingParentNode)\nworkflow.add_node(\"testingParentNode2\", subGraph)\nworkflow.add_edge(START, \"testingParentNode1\")\nworkflow.add_edge(\"testingParentNode1\", \"testingParentNode2\")\nworkflow.add_edge(\"testingParentNode2\", END)\nparentGraph = workflow.compile()\n\nif __name__ == \"__main__\": \n\n\n    subSubGraphImage = subSubGraph.get_graph(xray=True).draw_mermaid_png() # This works\n    \n    #print(subSubGraph.get_graph(xray=True).draw_ascii())\n    #with open(\"SubSubGraph.png\", \"wb\") as f:\n    #    f.write(subSubGraphImage)\n    print(\"created sub sub graph image\") \n    print(\"***********************************************************************************\")\n\n\n\n    subGraphImage = subGraph.get_graph(xray=True).draw_mermaid_png() # This works\n    \n    #print(subGraph.get_graph(xray=True).draw_ascii())\n    #with open(\"SubGraph.png\", \"wb\") as f:\n    #    f.write(subGraphImage)\n    print(\"created sub graph image\")\n    print(\"***********************************************************************************\")\n\n\n\n    #graphJson = parentGraph.get_graph(xray=True).to_json()\n    #print(str(graphJson))\n    \n    # This doesn't work when there are more than 1 node in the executor_agent\n    parentImage = parentGraph.get_graph(xray=True).draw_mermaid_png() \n    \n    #print(parentGraph.get_graph(xray=True).draw_ascii())\n    #with open(\"parentImage.png\", \"wb\") as f:\n    #    f.write(parentImage)\n    print(\"created parent graph image\")\nError Message and Stack Trace (if applicable)\nValueError: Found duplicate subgraph 'executor_agent' -- this likely means that you're reusing a subgraph node with the same name. Please adjust your graph to have subgraph nodes with unique names.\nDescription\nI am trying to create nested graphs and using 2 nodes in the fourth level. But when displaying the graph, i get error message. I have tried to simplify the code as much as possible to illustrate the problem.\nThis was started as an issue with create_react_agent, but later observed to be different issue altogether and the example code is updated with more simplified version without using create_react_agent.\nSystem Info\nlangchain==0.3.7\nlangchain-core==0.3.18\nlangchain-community==0.3.7\nlanggraph==0.2.48", "created_at": "2024-12-03", "closed_at": "2025-03-11", "labels": ["bug"], "State": "closed", "Author": "kmprasad4u"}
{"issue_number": 2604, "issue_title": "Checkpoint_NS not populating in the database when passed in config in stream method", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\ndef start(self, message = \"\"):\n        thread_id = stripped_uuid4()\n        config: RunnableConfig = RunnableConfig(\n            configurable={\n                \"thread_id\": thread_id,\n                \"checkpoint_ns\": \"story_promo\"\n            }   \n        )\n        initial_state = WorkflowState(\n            messages=[],\n            template=1,\n            context=\"\",\n            product_count=0,\n            product_name=\"\",\n            nodes=[], edges=[], cards=[], file=\"file\",\n        )\n        latest_event = None\n        for event in self.workflow_instance.stream(initial_state, config=config, stream_mode=\"values\"):\n            latest_event = event\n        return thread_id, latest_event\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nIn sample code above, the checkpoint with the corrrect threadId and initialised. But checkpoint_ns is still empty in the database. Tried to debug more, the checkpointer.postgres.init file the put function I presume is what is being used to store it in the database. The configurable object passed to the stream method is not getting passed to there correctly, although threadId is still being passed somehow and being used. Super confused how this is happening. I dont think the stream method in langgraph is using the put at all. Cant figure out how the stream method is calling the checkpointer at all\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.6.0: Fri Jul  5 17:56:39 PDT 2024; root:xnu-10063.141.1~2/RELEASE_ARM64_T8122\nPython Version:  3.12.4 (main, Jun  6 2024, 18:26:44) [Clang 15.0.0 (clang-1500.3.9.4)]\n\nPackage Information\n\nlangchain_core: 0.3.19\nlangchain: 0.3.7\nlangchain_community: 0.3.7\nlangsmith: 0.1.142\nlangchain_huggingface: 0.1.2\nlangchain_openai: 0.2.6\nlangchain_text_splitters: 0.3.2\nlanggraph: 0.2.45\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.0\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.7\nhttpx: 0.27.2\nhttpx-sse: 0.4.0\nhuggingface-hub: 0.26.2\njsonpatch: 1.33\nlanggraph-checkpoint: 2.0.2\nlanggraph-sdk: 0.1.35\nnumpy: 1.26.4\nopenai: 1.54.3\norjson: 3.10.11\npackaging: 24.2\npydantic: 2.9.2\npydantic-settings: 2.6.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nsentence-transformers: 3.2.1\nSQLAlchemy: 2.0.35\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntokenizers: 0.20.3\ntransformers: 4.46.2\ntyping-extensions: 4.12.2\n", "created_at": "2024-12-03", "closed_at": "2024-12-04", "labels": [], "State": "closed", "Author": "ishaan812"}
{"issue_number": 2599, "issue_title": "INVALID_GRAPH_NODE_RETURN_VALUE on conditional node return", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\n# Define state for application\nclass State(TypedDict):\n    question: str\n    context: List[Document]\n    answer: str\n    uploaded: str\n\ndef evaluate_answer(state: State):\n    #if generated RAG answer is not good then generate only from LLM.\n    if  \"I don't know\" in state[\"answer\"]:\n        print(\"generate instead\")\n        return \"this is not useful\"\n    return \"this is useful\"\n\n\n\n\ndef lang_graph_init():\n    # Compile application and test\n    # graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n    # graph_builder.add_edge(START, \"retrieve\")\n    \n    # graph = graph_builder.compile()\n\n    ###\n    graph_builder = StateGraph(State)\n    graph_builder.add_node(\"retrieve\", retrieve) # retrieve\n    graph_builder.set_entry_point(\"retrieve\")\n    graph_builder.add_node(\"generate\", generate) # generatae\n    graph_builder.add_node(\"evaluate\", evaluate_answer) # grade documents\n    graph_builder.add_node(\"generate_llm_response\", generate_llm_response) # generatae\n\n    graph_builder.add_edge(\"retrieve\",\"generate\")\n    graph_builder.add_edge(\"generate\",\"evaluate\")\n    graph_builder.add_conditional_edges(\n    \"evaluate\",\n    evaluate_answer,{\"this is not useful\":\"generate_llm_response\", \"this is useful\":END })\n    graph_builder.add_edge(\"generate_llm_response\",END)\n\n    graph = graph_builder.compile()\n    mermaid_png_data=graph.get_graph().draw_mermaid_png()\n\n    with open('my_graph.png', 'wb') as f:\n        f.write(mermaid_png_data)\nError Message and Stack Trace (if applicable)\nExpected dict, got this is useful For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE\n\nor\n\nExpected dict, got this is not useful For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE\nDescription\nWith the following langgraph, evaluate is a conditional_node that is suppose to return the next node.\nIf \"this is useful\" then go to END\nIf \"this is not useful\" then go to generate_llm_response\nHowever getting the errors as indicated. Why is it expecting a dictionary return of the from State?\nSystem Info\nlangchain==0.3.8\nlangchain-community==0.3.8\nlangchain-core==0.3.21\nlangchain-google-community==2.0.2\nlangchain-google-vertexai==2.0.3\nlangchain-huggingface==0.1.2\nlangchain-openai==0.2.10\nlangchain-text-splitters==0.3.2\nlanggraph==0.2.53\nlanggraph-checkpoint==2.0.6\nlanggraph-sdk==0.1.36\nlangsmith==0.1.146", "created_at": "2024-12-03", "closed_at": "2024-12-03", "labels": [], "State": "closed", "Author": "kevin-liangit"}
{"issue_number": 2595, "issue_title": " ReAct Agent doesn't recognize ALL the tool responses made after a single AI (aka assistant) message contained several tool calls", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nimport os\nimport logging\nfrom langchain_core.messages import SystemMessage, HumanMessage\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_openai import ChatOpenAI\nfrom .toolkit import CustomSQLDatabaseToolkit\nfrom .prompts import PROMPTS\n\nclass Agent():\n    \"\"\"\n    Agent: \n    Usage example: \n\n    \"\"\"\n    def __init__(self, db, user_prompt):\n        self.db = db\n        self.user_prompt = user_prompt\n\n        self.llm = ChatOpenAI(model=GPT_MODEL,\n                        temperature=0,\n                        max_retries=MAX_RETRIES,\n                        request_timeout=TIMEOUT_IN_SECONDS\n                    )\n        \n        # Initialize the toolkit\n        toolkit = CustomSQLDatabaseToolkit(db=self.db, llm=self.llm)\n        # Get the tools from the toolkit\n        self.tools = toolkit.get_tools()\n\n        # Create the Agent\n        self.sys_msg_dataquery = SystemMessage(content=\"<some prompt>\")\n        self.agent_executor = create_react_agent(self.llm, self.tools, state_modifier=self.sys_msg_dataquery)\n\n    def gen_response_dataquery_table(self):\n        messages = []\n        inputs = {\"messages\": [HumanMessage(content=self.user_prompt)]}\n\n        for s in self.agent_executor.stream(inputs, stream_mode=\"values\"):\n\n            message = s[\"messages\"][-1]\n            if isinstance(message, tuple):\n                logger.info(message)\n            else:\n                message.pretty_print()\n\n            messages.append(message)\nError Message and Stack Trace (if applicable)\n[2024-11-27 21:43:22,239: WARNING/ForkPoolWorker-1] ================================ Human Message =================================\n\nShow me the data of Italy\n[2024-11-27 21:43:23,253: INFO/ForkPoolWorker-1] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n[2024-11-27 21:43:31,124: WARNING/ForkPoolWorker-1] ================================== Ai Message ==================================\nTool Calls:\n  sql_db_query_checker (call_RGzcL13f3gicJYM8PV9mRFtM)\n Call ID: call_RGzcL13f3gicJYM8PV9mRFtM\n  Args:\n    query: SELECT * FROM country_codes WHERE Country_name = 'Italy'\n  sql_db_query_checker (call_2tHWQsf8JzFRTwUcwQmhDUGX)\n Call ID: call_2tHWQsf8JzFRTwUcwQmhDUGX\n  Args:\n    query: SELECT * FROM geo_data WHERE Country = 'ITA'\n  sql_db_query_checker (call_0mZ8gcfp8Ax7sntdwNcDu9AM)\n Call ID: call_0mZ8gcfp8Ax7sntdwNcDu9AM\n  Args:\n    query: SELECT * FROM italian_cities WHERE Regione = 'Lazio' OR Regione = 'Lombardia' OR Regione = 'Campania' OR Regione = 'Sicilia' OR Regione = 'Veneto' OR Regione = 'Emilia-Romagna' OR Regione = 'Piemonte' OR Regione = 'Toscana' OR Regione = 'Liguria' OR Regione = 'Marche' OR Regione = 'Abruzzo' OR Regione = 'Puglia' OR Regione = 'Calabria' OR Regione = 'Sardegna' OR Regione = 'Friuli-Venezia Giulia' OR Regione = 'Umbria' OR Regione = 'Basilicata' OR Regione = 'Molise' OR Regione = 'Aosta Valley' OR Regione = 'Trentino-Alto Adige' OR Regione = 'Apulia' OR Regione = 'Lazio' OR Regione = 'Lombardia' OR Regione = 'Campania' OR Regione = 'Sicilia' OR Regione = 'Veneto' OR Regione = 'Emilia-Romagna' OR Regione = 'Piemonte' OR Regione = 'Toscana' OR Regione = 'Liguria' OR Regione = 'Marche' OR Regione = 'Abruzzo' OR Regione = 'Puglia' OR Regione = 'Calabria' OR Regione = 'Sardegna' OR Regione = 'Friuli-Venezia Giulia' OR Regione = 'Umbria' OR Regione = 'Basilicata' OR Regione = 'Molise' OR Regione = 'Aosta Valley' OR Regione = 'Trentino-Alto Adige'\n  sql_db_query_checker (call_3nQeSZ7vJCr5GAoCI6vVPfx7)\n Call ID: call_3nQeSZ7vJCr5GAoCI6vVPfx7\n  Args:\n    query: SELECT * FROM italian_cities\n[2024-11-27 21:43:31,878: INFO/ForkPoolWorker-1] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n[2024-11-27 21:43:31,932: INFO/ForkPoolWorker-1] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n[2024-11-27 21:43:33,373: INFO/ForkPoolWorker-1] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n[2024-11-27 21:43:35,026: INFO/ForkPoolWorker-1] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n[2024-11-27 21:43:35,033: WARNING/ForkPoolWorker-1] ================================= Tool Message =================================\nName: sql_db_query_checker\n\nsql\nSELECT * FROM italian_cities\n\n[2024-11-27 21:43:46,796: WARNING/ForkPoolWorker-1] ================================== Ai Message ==================================\nsql\nSELECT * FROM country_codes WHERE Country_name = 'Italy';\n\n[2024-11-27 21:43:47,205: INFO/ForkPoolWorker-1] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n[2024-11-27 21:43:47,214: INFO/ForkPoolWorker-1] Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_RGzcL13f3gicJYM8PV9mRFtM, call_2tHWQsf8JzFRTwUcwQmhDUGX, call_0mZ8gcfp8Ax7sntdwNcDu9AM\", 'type': 'invalid_request_error', 'param': 'messages.[7].role', 'code': None}}\nDescription\nHi everyone, I am working on a Django app that uses Celery for processing tasks in the background. I am calling a wrapper function of the ReAct agent (create_react_agent) and when the agent performs multiple tool calls inside a single assistant/AI message, it only detects the last tool call and breaks the app with a 400 Error code at the end of the agent streaming. How to solve this? i.e. getting the agent to recognize all the tool calls.\nWhen I run it in a Jupyter notebook the cell is executed without any problem while when I run it in the Django app, it breaks the app. Do you have any leads to at least avoid breaking the app?\nThanks in advance\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP Tue Nov 5 00:21:55 UTC 2024\nPython Version:  3.11.9 (main, May 13 2024, 11:06:43) [GCC 9.4.0]\n\nPackage Information\n\nlangchain_core: 0.2.43\nlangchain: 0.2.16\nlangchain_community: 0.2.17\nlangsmith: 0.1.143\nlangchain_anthropic: 0.1.23\nlangchain_openai: 0.1.10\nlangchain_text_splitters: 0.2.4\nlanggraph: 0.2.50\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.6\nanthropic: 0.39.0\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.7\ndefusedxml: 0.7.1\nhttpx: 0.27.2\njsonpatch: 1.33\nlanggraph-checkpoint: 2.0.5\nlanggraph-sdk: 0.1.36\nnumpy: 1.26.4\nopenai: 1.28.1\norjson: 3.10.11\npackaging: 24.2\npydantic: 2.9.2\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.36\ntenacity: 8.5.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\n", "created_at": "2024-12-02", "closed_at": "2024-12-03", "labels": [], "State": "closed", "Author": "daqo98"}
{"issue_number": 2586, "issue_title": "Why isn\u2019t the state updated to the last executed node on interrupt?", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom pydantic import BaseModel\nfrom langgraph.graph import START, END, StateGraph\nfrom langgraph.checkpoint.memory import MemorySaver\n\n\nclass OverallState(BaseModel):\n  input_message: str = Field(default=\"\")\n  middle_way: str = Field(default=\"\")\n\ng_sub = StateGraph(OverallState)\ng_sub.add_edge(START, \"node1\")\ng_sub.add_node(\"node1\", lambda state: {\"middle_way\": \"Hello \" + state.input_message})\ng_sub.add_edge(\"node1\", \"node2\")\ng_sub.add_node(\"node2\", lambda state: {\"middle_way\": \"\"})\ng_sub.add_edge(\"node2\", END)\n\ngraph_sub = g_sub.compile(checkpointer=MemorySaver(), interrupt_after=[\"node1\"])\n\ng = StateGraph(OverallState)\ng.add_edge(START, \"graph_sub\")\ng.add_node(\"graph_sub\", graph_sub)\ng.add_edge(\"graph_sub\", END)\n\ngraph = g.compile(checkpointer=MemorySaver())\n\nconfig = {\"configurable\": {\"thread_id\": 2}}\n\noutput_middle_way = graph.invoke({\"input_message\": \"Minki\", \"middle_way\": \"\"}, config)\nprint(\"output_middle_way:\", output_middle_way[\"middle_way\"], \".\")\n# printed <output_middle_way:  . > whereas I expect <output_middle_way:  Hello Minki.>\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nI expected the interrupt method to return the state updated up to the point of the last executed node. However, based on the example code I provided, this doesn\u2019t seem to be the case when the node that is interrupted is in subgraph. Could you help clarify this behavior?\nIf I\u2019ve misunderstood or misused the feature, please let me know. I\u2019d appreciate any guidance on the correct usage.\nThank you for your support and for building LangGraph!\nSystem Info\nlanggraph==0.2.53\nlanggraph-api-inmem==0.0.4\nlanggraph-checkpoint==2.0.6\nlanggraph-checkpoint-sqlite==1.0.4\nlanggraph-cli==0.1.55\nlanggraph-sdk==0.1.36", "created_at": "2024-11-30", "closed_at": "2025-01-15", "labels": [], "State": "closed", "Author": "minki-j"}
{"issue_number": 2583, "issue_title": "Cannot run Langgraph up", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nlanggraph up\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nI get this problem when running \"langgraph up\"\nI have developer license, I think.  But where do I find the Langsmith_API_KEY ?  is this problem related to the key missing ?\n\nSystem Info\n\"langgraph up\"", "created_at": "2024-11-30", "closed_at": "2024-12-16", "labels": [], "State": "closed", "Author": "shanumas"}
{"issue_number": 2582, "issue_title": "Multi-agent supervisor gives json related error", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nhttps://colab.research.google.com/drive/1QszbxpiFJkhWWYhBpSmDCX_3MMMeHVdd?usp=sharing#scrollTo=UiWxjTxDNNOe\n\n\n    class RouteResponse(TypedDict):\n        \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n        next: Union[Literal[\"FINISH\"], Literal[\"Rag_agent\"], Literal[\"Sql_agent\"]]\nError Message and Stack Trace (if applicable)\nOutputParserException: Function RouteResponse arguments:\n\n     {\n       next: \"Rag_agent\"\n     }\n\n\n     are not valid JSON. Received JSONDecodeError Expecting property name enclosed in double quotes: \n     line 2 column 3 (char 4)\n     For troubleshooting, visit: \n     https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE\n     For troubleshooting, visit: \n     https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE\nDescription\ni used the multi agent supervisor code: #683\nand i created similar agentic workflow with two agents (rag and sql agent). My code is here:\nhttps://colab.research.google.com/drive/1QszbxpiFJkhWWYhBpSmDCX_3MMMeHVdd?usp=sharing#scrollTo=XbyYy3HU3J--\nand i get the error related to RouteResponse argument not generating in a json format.\neven the\n class Router(TypedDict):\n       \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n       next: Literal[*options]\n\ngives me syntax error because of asterisk (*) which i had to replace it with\n class RouteResponse(TypedDict):\n      \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n      next: Union[Literal[\"FINISH\"], Literal[\"Rag_agent\"], Literal[\"Sql_agent\"]]\n\ni replicated the github code as is and i got same error. So the router function instead of generating next action in json like\n{'supervisor': {'next': 'Rag_agent'}})\nIt generates {'next': 'Rag_agent'} which is a non json format\n\nSystem Info\nlangchain                                0.3.7\nlangchain-anthropic                      0.3.0\nlangchain-community                      0.3.7\nlangchain-core                           0.3.19\nlangchain-experimental                   0.3.3\nlangchain-openai                         0.2.9\nlangchain-text-splitters                 0.3.2\nlanggraph                                0.2.52\nlanggraph-checkpoint                     2.0.4\nlanggraph-sdk                            0.1.36\nlangsmith                                0.1.143", "created_at": "2024-11-29", "closed_at": "2024-12-18", "labels": [], "State": "closed", "Author": "sfallahpour"}
{"issue_number": 2581, "issue_title": "Fan-out/in fails with subgraphs", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nimport operator\nfrom langgraph.graph import START, StateGraph\nfrom typing import TypedDict, Annotated\nfrom langgraph.types import Send\n\n# Define subgraph\nclass SubgraphState(TypedDict):\n    father: str\n\ndef get_kids(state: SubgraphState):\n    return {\"kids\": [state[\"father\"] + \" Junior\"]}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(get_kids)\nsubgraph_builder.add_edge(START, \"get_kids\")\nsubgraph = subgraph_builder.compile()\n\n\n# Define parent graph\nclass ParentState(TypedDict):\n    fathers: list[str]\n    kids: Annotated[list[str], operator.add]\n\ndef parent_node_1(state: ParentState):\n    pass\n\ndef parent_fanout(state: ParentState):\n    return [\n        Send(\n            \"child_node\",\n            {\n                \"father\": father,\n            },\n        ) for father in state[\"fathers\"]\n    ]\n\ndef parent_node_2(state: ParentState):\n    pass\n\n\nbuilder = StateGraph(ParentState)\nbuilder.add_node(\"parent_node_1\", parent_node_1)\nbuilder.add_node(\"child_node\", subgraph)\n# Replacing subgraph with functions makes it work as expected\n# builder.add_node(\"child_node\", get_kids)\nbuilder.add_node(\"parent_node_2\", parent_node_2)\nbuilder.add_edge(START, \"parent_node_1\")\nbuilder.add_conditional_edges(\"parent_node_1\", parent_fanout, [\"child_node\"])\nbuilder.add_edge(\"child_node\", \"parent_node_2\")\ngraph = builder.compile()\n\n\n# Run graph\ncontext = {\"fathers\": [\"Jon\", \"Matt\"]}\nprint(graph.invoke(context))\nError Message and Stack Trace (if applicable)\n/Users/user/Projects/private/llm-playground/.venv/bin/python /Users/user/Projects/private/llm-playground/script.py \nTraceback (most recent call last):\n  File \"/Users/user/Projects/private/llm-playground/script.py\", line 55, in <module>\n    print(graph.invoke(context))\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/user/Projects/private/llm-playground/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 1927, in invoke\n    for chunk in self.stream(\n                 ^^^^^^^^^^^^\n  File \"/Users/user/Projects/private/llm-playground/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 1647, in stream\n    for _ in runner.tick(\n             ^^^^^^^^^^^^\n  File \"/Users/user/Projects/private/llm-playground/.venv/lib/python3.12/site-packages/langgraph/pregel/runner.py\", line 159, in tick\n    _panic_or_proceed(\n  File \"/Users/user/Projects/private/llm-playground/.venv/lib/python3.12/site-packages/langgraph/pregel/runner.py\", line 367, in _panic_or_proceed\n    raise exc\n  File \"/Users/user/Projects/private/llm-playground/.venv/lib/python3.12/site-packages/langgraph/pregel/executor.py\", line 70, in done\n    task.result()\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/user/Projects/private/llm-playground/.venv/lib/python3.12/site-packages/langgraph/pregel/retry.py\", line 40, in run_with_retry\n    task.proc.invoke(task.input, config)\n  File \"/Users/user/Projects/private/llm-playground/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py\", line 410, in invoke\n    input = context.run(step.invoke, input, config, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/user/Projects/private/llm-playground/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 1927, in invoke\n    for chunk in self.stream(\n                 ^^^^^^^^^^^^\n  File \"/Users/user/Projects/private/llm-playground/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 1647, in stream\n    for _ in runner.tick(\n             ^^^^^^^^^^^^\n  File \"/Users/user/Projects/private/llm-playground/.venv/lib/python3.12/site-packages/langgraph/pregel/runner.py\", line 104, in tick\n    run_with_retry(t, retry_policy, writer=writer)\n  File \"/Users/user/Projects/private/llm-playground/.venv/lib/python3.12/site-packages/langgraph/pregel/retry.py\", line 40, in run_with_retry\n    task.proc.invoke(task.input, config)\n  File \"/Users/user/Projects/private/llm-playground/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py\", line 412, in invoke\n    input = context.run(step.invoke, input, config)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/user/Projects/private/llm-playground/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py\", line 176, in invoke\n    ret = context.run(self.func, input, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/user/Projects/private/llm-playground/.venv/lib/python3.12/site-packages/langgraph/pregel/write.py\", line 85, in _write\n    self.do_write(\n  File \"/Users/user/Projects/private/llm-playground/.venv/lib/python3.12/site-packages/langgraph/pregel/write.py\", line 130, in do_write\n    write.mapper(write.value) if write.mapper is not None else write.value\n    ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/user/Projects/private/llm-playground/.venv/lib/python3.12/site-packages/langgraph/graph/state.py\", line 635, in _get_state_key\n    raise InvalidUpdateError(\nlanggraph.errors.InvalidUpdateError: Expected node father to update at least one of ['father'], got {'kids': ['Jon Junior']}\n\nProcess finished with exit code 1\nDescription\nWhen fanning in subgraphs LangGraph fails to merge subgraphs state into the receiving graph state.\nI belive the code explains the issue best TLDR of what would be expected:\n\nParentGraph receives list of names\nParentGraph sends Send event per each name to SubGraph (when sending to function of ParentGraph it works as expected)\nSubGraph returns list of kids names for specific father\nParentGraph aggregates kids names to return as single list (this step fails)\n\nIt works perfectly fine without subgraph but I have pretty complex logic that would be nasty to move to a single node.\nSystem Info\nlangchain==0.3.7\nlangchain-anthropic==0.3.0\nlangchain-community==0.3.7\nlangchain-core==0.3.18\nlangchain-groq==0.2.1\nlangchain-ollama==0.2.0\nlangchain-openai==0.2.8\nlangchain-text-splitters==0.3.2\nmac\nPython 3.12.7", "created_at": "2024-11-29", "closed_at": "2024-11-30", "labels": [], "State": "closed", "Author": "Karjan1"}
{"issue_number": 2576, "issue_title": "Intermittent InvalidSqlStatementName: prepared statement \"_pg3_4\" does not exist Error with AsyncPostgresSaver in LangGraph", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nimport os\nfrom dotenv import load_dotenv\n\nfrom pydantic import BaseModel, ConfigDict, Field\nfrom typing import Annotated, Optional, Dict, Any, List\nfrom typing_extensions import TypedDict\nimport traceback\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage, AIMessage, ToolMessage\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.tools import BaseTool\n\nfrom langgraph.graph.message import AnyMessage\nfrom langgraph.graph import StateGraph, END\nfrom langgraph.graph.graph import CompiledGraph\nfrom langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\n\nimport json\nfrom uuid import uuid4\n\n# Load environment variables\nload_dotenv(override=True)\n\nclass State(TypedDict):\n    query: Annotated[str, \"The user's query\"] = \"\"\n    messages: Annotated[list[AnyMessage], \"The messages exchanged with the assistant\"] = []\n    intent_response: Annotated[Optional[Dict[str, Any]], \"The response from the intent assistant\"] = None\n    response_sent: Annotated[bool, \"Flag to indicate if the response has been sent.\"] = False\n\nclass ProductAssistant(BaseModel):\n    name: str\n    builder: Optional[StateGraph] = Field(default=None, exclude=True)\n    graph: Optional[CompiledGraph] = None\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.builder = StateGraph(State)\n\n        # Adding nodes to the graph\n        self.builder.add_node(\"main_assistant\", self.main_assistant)\n        self.builder.add_node(\"intent_assistant\", self.intent_assistant)\n        self.builder.add_node(\"generic_reply_assistant\", self.generic_reply_assistant)\n        self.builder.add_node(\"single_product_assistant\", self.single_product_assistant)\n        self.builder.add_node(\"multiple_products_assistant\", self.multiple_products_assistant)\n        self.builder.add_node(\"call_tool\", self.call_tool)\n        self.builder.add_node(\"clean_and_respond\", self.clean_and_respond)\n\n        self.builder.set_entry_point(\"main_assistant\")\n\n        self.builder.add_conditional_edges(\n            \"main_assistant\",\n            self.should_call_tool,\n            {\n                \"continue\": \"call_tool\",\n                \"end\": \"intent_assistant\"\n            }\n        )\n\n        self.builder.add_conditional_edges(\n            \"call_tool\",\n            self.should_call_tool,\n            {\n                \"continue\": \"call_tool\",\n                \"end\": \"main_assistant\",\n            }\n        )\n\n        self.builder.add_conditional_edges(\n            \"intent_assistant\",\n            self.assistant_to_call,\n        )\n\n        self.builder.add_edge(\"single_product_assistant\", \"clean_and_respond\")\n        self.builder.add_edge(\"multiple_products_assistant\", \"clean_and_respond\")\n\n        self.builder.add_edge(\"clean_and_respond\", END)\n\n    def main_assistant(self, state: State, config):\n        messages = state[\"messages\"]\n\n        # Build the prompt (implementation omitted for brevity)\n        prompt_builder: ChatPromptTemplate = ...  # Your prompt setup here\n        prompt = prompt_builder.invoke(state)\n\n        model = ChatOpenAI(model=\"gpt-4o-mini\")\n\n        tools = self.create_tools()\n\n        model = model.bind_tools(tools, parallel_tool_calls=False)\n\n        response = model.invoke(prompt)\n\n        if hasattr(response, \"tool_calls\") and response.tool_calls:\n            messages.append(response)\n\n            tool_names = [tool_call[\"name\"] for tool_call in response.tool_calls]\n            print(f\"Tool detected and appended: {tool_names}\")\n        else:\n            print(\"No tool detected.\")\n\n        return {\"messages\": messages}\n\n    # ... [Other methods omitted for brevity] ...\n\n    def create_tools(\n        self,\n    ) -> List[BaseTool]:\n        # Tools are created here (implementation omitted)\n        return [fetch_products, fetch_product_details, fetch_terms_and_conditions]\n\n    async def invoke(\n        self,\n        query: str,\n        user_metadata: Optional[Dict[str, Any]] = None,\n        search_config: Optional[Dict[str, Any]] = None,\n    ):\n        thread_id = uuid4().hex\n\n        config = {\n            \"configurable\": {\"thread_id\": thread_id},\n            \"user_metadata\": user_metadata or None,\n            \"search_config\": search_config or None,\n        }\n\n        initial_state: State = {\n            \"messages\": [HumanMessage(content=query)],\n            \"query\": query,\n            \"response_sent\": False,\n        }\n\n        async with AsyncPostgresSaver.from_conn_string(os.getenv(\"DATABASE_URL\", \"\")) as async_memory:\n            await async_memory.setup()\n            self.graph = self.builder.compile(checkpointer=async_memory)\n\n            try:\n                async for event in self.graph.astream_events(initial_state, config, version=\"v2\"):\n                    event_type = event.get('event')\n                    event_data = event.get('data')\n\n                    if event_type == \"on_chain_stream\":\n                        chunk = event_data.get('chunk', {})\n                        response_sent = chunk.get('response_sent', False)\n\n                        messages = chunk.get('messages', [])\n                        if messages:\n                            last_ai_message = next(\n                                (msg.content for msg in reversed(messages) if isinstance(msg, AIMessage)), None\n                            )\n\n                            if last_ai_message and response_sent:\n                                if last_ai_message and isinstance(last_ai_message, str):\n                                    try:\n                                        structured_response_dict = json.loads(last_ai_message)\n                                        yield json.dumps(structured_response_dict)\n                                    except json.JSONDecodeError:\n                                        last_ai_message = last_ai_message.replace('\"', \"'\")\n                                        yield json.dumps({\"message\": last_ai_message})\n\n            except Exception as e:\n                print(f\"Error in invoking the assistant: {str(e)}\")\n                traceback.print_exc()\nError Message and Stack Trace (if applicable)\nError in invoking the assistant: prepared statement \"_pg3_4\" does not exist\nTraceback (most recent call last):\n  File \"/path/to/product_assistant.py\", line 442, in invoke\n    async for event in self.graph.astream_events(initial_state, config, version=\"v2\"):\n  File \"/path/to/langchain_core/runnables/base.py\", line 1388, in astream_events\n    async for event in event_stream:\n  File \"/path/to/langchain_core/tracers/event_stream.py\", line 1012, in _astream_events_implementation_v2\n    await task\n  File \"/path/to/langchain_core/tracers/event_stream.py\", line 967, in consume_astream\n    async for _ in event_streamer.tap_output_aiter(run_id, stream):\n  File \"/path/to/langchain_core/tracers/event_stream.py\", line 203, in tap_output_aiter\n    async for chunk in output:\n  File \"/path/to/langgraph/pregel/__init__.py\", line 1823, in astream\n    async with AsyncPregelLoop(\n               ^^^^^^^^^^^^^^^^\n  File \"/path/to/langgraph/pregel/loop.py\", line 1011, in __aexit__\n    return await asyncio.shield(\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/contextlib.py\", line 754, in __aexit__\n    raise exc_details[1]\n  File \"/usr/local/lib/python3.12/contextlib.py\", line 737, in __aexit__\n    cb_suppress = await cb(*exc_details)\n                  ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/path/to/langgraph/pregel/executor.py\", line 191, in __aexit__\n    raise exc\n  File \"/path/to/langgraph/checkpoint/postgres/aio.py\", line 320, in aput_writes\n    async with self._cursor(pipeline=True) as cur:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/contextlib.py\", line 217, in __aexit__\n    await anext(self.gen)\n  File \"/path/to/langgraph/checkpoint/postgres/aio.py\", line 349, in _cursor\n    async with self.lock, conn.pipeline(), conn.cursor(\n                          ^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/contextlib.py\", line 217, in __aexit__\n    await anext(self.gen)\n  File \"/path/to/psycopg/connection_async.py\", line 398, in pipeline\n    async with pipeline:\n               ^^^^^^^^\n  File \"/path/to/psycopg/_pipeline.py\", line 266, in __aexit__\n    raise exc2.with_traceback(None)\npsycopg.errors.InvalidSqlStatementName: prepared statement \"_pg3_4\" does not exist\nDescription\nI'm trying to use LangGraph with Supabase PostgreSQL as my database. I have an assistant implemented in product_assistant.py that uses AsyncPostgresSaver for asynchronous database operations. My assistant works fine most of the time, but sometimes it throws the following error:\npsycopg.errors.InvalidSqlStatementName: prepared statement \"_pg3_4\" does not exist\n\nThis error seems to occur intermittently, and I haven't been able to identify a consistent pattern.\nAdditional Information:\n\n\nMy tools, which are invoked by the assistant, also make database calls using SQLAlchemy (synchronous operations).\n\n\nI suspect that mixing asynchronous database operations (via AsyncPostgresSaver in LangGraph) and synchronous database operations (via SQLAlchemy in my tools) might be causing conflicts or unexpected behavior.\n\n\nI have tried removing any explicit DEALLOCATE ALL statements, but the error still occurs.\n\n\nOccasionally, I also encounter another error:\npsycopg.OperationalError: the connection is closed\n\nThis error also happens intermittently, similar to the InvalidSqlStatementName error.\n\n\nQuestion:\nHow can I resolve this intermittent InvalidSqlStatementName error? Is there a recommended way to handle database operations in tools when using LangGraph, especially when both the assistant and the tools make database calls? Should I refactor my tools to use asynchronous database sessions? Any guidance on how to solve this issue would be greatly appreciated.\nSystem Info\n\nLangGraph Version: 0.2.53\nPython Version: 3.12\nLanggraph Checkpoint Postgres Version: 2.0.3\nOperating System: MacOS\n", "created_at": "2024-11-29", "closed_at": "2024-12-18", "labels": [], "State": "closed", "Author": "Zolastic"}
{"issue_number": 2574, "issue_title": "adispatch_custom_event not emitted in time when run via langgraph up (works on LangGraph cloud)", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\n# This snippet is from the CopilotKit <> LangGraph integration\n# We are using copilotkit_emit_state to send state updates to CopilotKit while\n# a node is running.\n# copilotkit_emit_state is just a thin wrapper around adispatch_custom_event:\n# https://github.com/CopilotKit/CopilotKit/blob/3b362e5858f193c56e0e3baf184451eb1d6170fe/sdk-python/copilotkit/langchain.py#L120\n\nasync def search_node(state: AgentState, config: RunnableConfig):\n    \"\"\"\n    The search node is responsible for searching the internet for resources.\n    \"\"\"\n    ai_message = cast(AIMessage, state[\"messages\"][-1])\n\n    state[\"resources\"] = state.get(\"resources\", [])\n    state[\"logs\"] = state.get(\"logs\", [])\n    queries = ai_message.tool_calls[0][\"args\"][\"queries\"]\n\n    for query in queries:\n        state[\"logs\"].append({\n            \"message\": f\"Search for {query}\",\n            \"done\": False\n        })\n\n    await copilotkit_emit_state(config, state)\n\n    search_results = []\n\n    for i, query in enumerate(queries):\n        response = tavily_client.search(query)\n        search_results.append(response)\n        state[\"logs\"][i][\"done\"] = True\n        await copilotkit_emit_state(config, state)\n\n    config = copilotkit_customize_config(\n        config,\n        emit_intermediate_state=[{\n            \"state_key\": \"resources\",\n            \"tool\": \"ExtractResources\",\n            \"tool_argument\": \"resources\",\n        }],\n    )\n\n    model = get_model(state)\n    ainvoke_kwargs = {}\n    if model.__class__.__name__ in [\"ChatOpenAI\"]:\n        ainvoke_kwargs[\"parallel_tool_calls\"] = False\n\n    # figure out which resources to use\n    response = await model.bind_tools(\n        [ExtractResources],\n        tool_choice=\"ExtractResources\",\n        **ainvoke_kwargs\n    ).ainvoke([\n        SystemMessage(\n            content=\"\"\"\n            You need to extract the 3-5 most relevant resources from the following search results.\n            \"\"\"\n        ),\n        *state[\"messages\"],\n        ToolMessage(\n        tool_call_id=ai_message.tool_calls[0][\"id\"],\n        content=f\"Performed search: {search_results}\"\n    )\n    ], config)\n\n    state[\"logs\"] = []\n    await copilotkit_emit_state(config, state)\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nOn cloud, the custom event is immediately sent. With the local setup via langgraph up, the events are sent with a delay and arrive in the wrong order (so the user can see the current status of the searches being performed)\nSystem Info\nCLI\n\n$ langgraph --version\nLangGraph CLI, version 0.1.60\n\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.0.0: Fri Sep 15 14:41:43 PDT 2023; root:xnu-10002.1.13~1/RELEASE_ARM64_T6000\nPython Version:  3.11.9 (main, Apr  2 2024, 08:25:04) [Clang 15.0.0 (clang-1500.3.9.4)]\n\nPackage Information\n\nlangchain_core: 0.3.19\nlangchain: 0.3.7\nlangsmith: 0.1.143\nlangchain_openai: 0.2.9\nlangchain_text_splitters: 0.3.2\nlanggraph: 0.2.52\nlangserve: 0.0.41\n\nOther Dependencies\n\naiohttp: 3.10.5\nasync-timeout: 4.0.2\nfastapi: 0.115.0\nhttpx: 0.27.2\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlanggraph-checkpoint: 2.0.5\nlanggraph-sdk: 0.1.36\nnumpy: 1.26.4\nopenai: 1.54.5\norjson: 3.10.7\npackaging: 24.1\npydantic: 2.9.2\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.35\nsse-starlette: 2.1.3\ntenacity: 8.5.0\ntiktoken: 0.7.0\ntyping-extensions: 4.12.2\n", "created_at": "2024-11-28", "closed_at": "2024-12-02", "labels": [], "State": "closed", "Author": "mme"}
{"issue_number": 2570, "issue_title": "langgraph-checkpoint-postgres: Calls to postgres async checkpointer setup() fail on new postgres db", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\n# set POSTGRES_URI=...\nclass CheckpointerManager:\n    \"\"\"\n    A manager class to handle checkpointer initialization and lifecycle.\n    \"\"\"\n    def __init__(self, conninfo: str, max_pool_size: int = 20):\n        self.conninfo = conninfo\n        self.max_pool_size = max_pool_size\n        self.pool = None\n        self.checkpointer = None\n\n    async def setup(self):\n        \"\"\"\n        Initialize the connection pool and checkpointer.\n        \"\"\"\n        self.pool = AsyncConnectionPool(conninfo=self.conninfo,\n                                        max_size=self.max_pool_size,\n                                        open=False,timeout=5)\n        await self.pool.open(wait=True, timeout=5)\n        self.checkpointer = AsyncPostgresSaver(conn=self.pool)\n        try:\n            await self.checkpointer.setup()\n        except Exception as e:\n            print(f\"Error setting up checkpointer: {e}\")\n            await self.close()\n            raise e\n        return self\n\n    async def close(self):\n        \"\"\"\n        Close the connection pool and cleanup resources.\n        \"\"\"\n        if self.pool:\n            await self.pool.close()\n\n    def get_checkpointer(self):\n        \"\"\"\n        Get the initialized checkpointer.\n        \"\"\"\n        if not self.checkpointer:\n            raise RuntimeError(\"Checkpointer has not been initialized. Call setup() first.\")\n        return self.checkpointer\ncheckpointer_manager = CheckpointerManager(os.getenv(POSTGRES_URI))\ncheckpointer_manager = asyncio.run(checkpointer_manager.setup())\n\n\n### Error Message and Stack Trace (if applicable)\n\n```shell\nsetting up checkpointer: current transaction is aborted, commands ignored until end of transaction block\nTraceback (most recent call last):\n  File \"/home/tai/project/main.py\", line 91, in <module>\n    main()\n  File \"/home/tai/project/main.py\", line 49, in main\n    checkpointer_manager = asyncio.run(checkpointer_manager.setup())\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/home/tai/project/checkpointer/__init__.py\", line 31, in setup\n    raise e\n  File \"/home/tai/project/checkpointer/__init__.py\", line 27, in setup\n    await self.checkpointer.setup()\n  File \"/home/tai/project/.venv/lib/python3.12/site-packages/langgraph/checkpoint/postgres/aio.py\", line 98, in setup\n    await cur.execute(migration)\n  File \"/home/tai/project/.venv/lib/python3.12/site-packages/psycopg/cursor_async.py\", line 97, in execute\n    raise ex.with_traceback(None)\npsycopg.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block\n\nDescription\nWhen running the async postgres setup() function, the first execution for SELECT fails, but the cursor is reused in the except block, so postgres ignores it. The CREATE IF NOT EXISTS statement should be hoisted up and out of the main block such that it runs in a separate transaction context, or a least before the SELECT statement runs. For now, I've got it fixed by running this manually:\nclass CheckpointerManager:\n    \"\"\"\n    A manager class to handle checkpointer initialization and lifecycle.\n    \"\"\"\n    initialization = \"\"\"CREATE TABLE IF NOT EXISTS checkpoint_migrations (\n    v INTEGER PRIMARY KEY\n);\"\"\"\n\n    def __init__(self, conninfo: str, max_pool_size: int = 20):\n        self.conninfo = conninfo\n        self.max_pool_size = max_pool_size\n        self.pool = None\n        self.checkpointer = None\n\n\n    async def setup(self):\n        \"\"\"\n        Initialize the connection pool and checkpointer.\n        \"\"\"\n        self.pool = AsyncConnectionPool(conninfo=self.conninfo,\n                                        max_size=self.max_pool_size,\n                                        open=False,timeout=5)\n        await self.pool.open(wait=True, timeout=5)\n        self.checkpointer = AsyncPostgresSaver(conn=self.pool)\n        async with self.pool.connection() as conn:\n            await conn.execute(self.initialization)\n        try:\n            await self.checkpointer.setup()\n        except Exception as e:\n            print(f\"Error setting up checkpointer: {e}\")\n            await self.close()\n            raise e\n        return self\n\n    async def close(self):\n        \"\"\"\n        Close the connection pool and cleanup resources.\n        \"\"\"\n        if self.pool:\n            await self.pool.close()\n\n    def get_checkpointer(self):\n        \"\"\"\n        Get the initialized checkpointer.\n        \"\"\"\n        if not self.checkpointer:\n            raise RuntimeError(\"Checkpointer has not been initialized. Call setup() first.\")\n        return self.checkpointer\ncheckpointer_manager = CheckpointerManager(os.getenv(POSTGRES_URI))\ncheckpointer_manager = asyncio.run(checkpointer_manager.setup())\n\n(note the async with self.pool.connection() as conn: ; await conn.execute(self.initialization) before the .setup() call. This fixes it.\nSystem Info\npip freeze | grep langgraph\nlanggraph==0.2.53\nlanggraph-checkpoint==2.0.6\nlanggraph-checkpoint-postgres==2.0.4\nlanggraph-checkpoint-sqlite==2.0.1\nlanggraph-sdk==0.1.36", "created_at": "2024-11-28", "closed_at": "2024-12-03", "labels": [], "State": "closed", "Author": "taigrr"}
{"issue_number": 2559, "issue_title": "Stack trace needs more details", "issue_body": "Privileged issue\n\n I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.\n\nIssue Content\nThe RemotException that's surfaced does not have enough information to identify where the error is / which state update fails (InvalidUpdateError)\n---------------------------------------------------------------------------\nRemoteException                           Traceback (most recent call last)\nCell In[1], line 38\n     13 remote_graph = RemoteGraph(graph_name, url=url)\n     15 schema = {\n     16     \"title\": \"People Name Extractor\",\n     17     \"description\": \"Extracts names of people from text.\",\n   (...)\n     35     },\n     36 }\n---> 38 value = remote_graph.invoke(\n     39     {\n     40         \"url\": \"https://www.apple.com/leadership/\",\n     41         \"json_schema\": schema,\n     42         \"examples\": [],\n     43     }\n     44 )\n\nFile ~/.pyenv/versions/3.11.4/envs/graph_3_11_4/lib/python3.11/site-packages/langgraph/pregel/remote.py:772, in RemoteGraph.invoke(self, input, config, interrupt_before, interrupt_after)\n    749 def invoke(\n    750     self,\n    751     input: Union[dict[str, Any], Any],\n   (...)\n    755     interrupt_after: Optional[Union[All, Sequence[str]]] = None,\n    756 ) -> Union[dict[str, Any], Any]:\n    757     \"\"\"Create a run, wait until it finishes and return the final state.\n    758 \n    759     This method calls `POST [/threads/](http://localhost:8890/threads/){thread_id}[/runs/wait](http://localhost:8890/runs/wait)` if a `thread_id`\n   (...)\n    770         The output of the graph.\n    771     \"\"\"\n--> 772     for chunk in self.stream(\n    773         input,\n    774         config=config,\n    775         interrupt_before=interrupt_before,\n    776         interrupt_after=interrupt_after,\n    777         stream_mode=\"values\",\n    778     ):\n    779         pass\n    780     try:\n\nFile [~/.pyenv/versions/3.11.4/envs/graph_3_11_4/lib/python3.11/site-packages/langgraph/pregel/remote.py:630](http://localhost:8890/home/eugene/.pyenv/versions/3.11.4/envs/graph_3_11_4/lib/python3.11/site-packages/langgraph/pregel/remote.py#line=629), in RemoteGraph.stream(self, input, config, stream_mode, interrupt_before, interrupt_after, subgraphs)\n    628         raise GraphInterrupt(chunk.data[INTERRUPT])\n    629 elif chunk.event.startswith(\"error\"):\n--> 630     raise RemoteException(chunk.data)\n    631 # filter for what was actually requested\n    632 if mode not in requested:\n\nRemoteException: {'error': 'InvalidUpdateError', 'message': \"Expected node t", "created_at": "2024-11-27", "closed_at": null, "labels": ["maintainer"], "State": "open", "Author": "eyurtsev"}
{"issue_number": 2557, "issue_title": "DOC: serialization exception", "issue_body": "Issue with current documentation:\n\n\nException needs an error code and documentation about how to implement custom serializer.\nServer side log that shows what failed to serialize?\n\nIdea or request for content:\nNo response", "created_at": "2024-11-27", "closed_at": null, "labels": ["maintainer"], "State": "open", "Author": "eyurtsev"}
{"issue_number": 2555, "issue_title": "Using Pydantic state with aliased fields", "issue_body": "Privileged issue\n\n I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.\n\nIssue Content\nMRE\nfrom pydantic import BaseModel, Field\nfrom langgraph.graph import StateGraph\n\nclass Foo(BaseModel):\n    foo: str = Field(alias='bar')\n\n# Define a new graph\ngraph_builder = StateGraph(\n    Foo\n)\n\n\ndef node(foo: Foo):\n    print(foo)\n    return None\n\n# Add the node to the graph\ngraph_builder.add_node(\"node\", node)\n# Set the entrypoint as `call_model`\ngraph_builder.add_edge(\"__start__\", \"node\")\n\n# Compile the workflow into an executable graph\ngraph = graph_builder.compile()\n\n\ngraph.invoke({'bar': 'hello'})\nExpected behavior\nThe alias should be respected and the field foo should be only updateable by alias.\nObserved behavior\nValidation error.\nContext\n\nMostly a problem for IO boundaries (since that's where I actually use pydantic models)\nalias is needed in some cases to  avoid name collisions (e.g., schema field is already taken by pydantic)\n", "created_at": "2024-11-27", "closed_at": null, "labels": ["maintainer"], "State": "open", "Author": "eyurtsev"}
{"issue_number": 2550, "issue_title": "The node is followed by one conditional_edges, and the state update is returned at the node. As a result, conditional_edges are updated twice after the return", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\n# code likes like this\n\ndef add_message(left_message, right_message):\n    print(\"add_message: \", left_message, right_message)\n    if isinstance(left_message, list) and isinstance(right_message, list):\n        left_message.extend(right_message)\n    elif isinstance(left_message, list):\n        left_message.append(right_message)\n    else:\n        left_message = right_message\n    print(\"add_message end: \", left_message)\n    return left_message\n\n\n\nclass MainState(TypedDict):\n    history_diag: Annotated[List[Union[UserMessage, SystemMessage, str, int]], add_message]\n\n\n\nasync def build_graph(self):\n    ********some node************\n\n    self.workflow.add_node(\"node 1\", self.node_1)\n    self.workflow.add_node(\"daozhen_node_get_next_question_wrap\", self.node_get_next_question_wrap)\n    self.workflow.add_edge(\"daozhen_node_get_next_question\", \"daozhen_node_get_next_question_wrap\")\n\n    self.workflow.add_conditional_edges(\"daozhen_node_get_next_question_wrap\", self.edge_conditional_res_route, [\"node 1\",\"node 2\"])\n    return self\n\n\n\nasync def edge_conditional_res_route(self, state: MainState):\n    if is_end or jump_out:\n        return \"node 1\"\n    else:\n        return \"node 2\"\n\n\n\nasync def node_get_next_question_wrap(self, state: MainState):\n    append_message = [1, 2]\n    return {\"history_diag\": append_message}\n\n\n\nasync def node_1(self, state: MainState):\n    print(state[\"history_diag\"])\n    # get [1, 2, 1, 2], bug just update once\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nOne node is called node1. Following the node, conditional_edges returns to update one field of the state. As a result, conditional_edges returns are updated twice. But if I insert a node between node1 and conditional_edges that doesn't return anything, it won't repeat\nSystem Info\nmacbook m3\nlang version:\nlangchain                     0.3.7\nlangchain-anthropic           0.2.3\nlangchain-core                0.3.15\nlangchain-text-splitters      0.3.2\nlanggraph                     0.2.53\nlanggraph-checkpoint          2.0.6\nlanggraph-checkpoint-postgres 2.0.2\nlanggraph-sdk                 0.1.35\nlangsmith                     0.1.137", "created_at": "2024-11-27", "closed_at": "2025-01-29", "labels": ["invalid"], "State": "closed", "Author": "Panda-eat-meat"}
{"issue_number": 2549, "issue_title": "I want to deploy the langgraph platform on the intranet. Can I do this without entering the langsmith key?", "issue_body": "Issue with current documentation:\nNo response\nIdea or request for content:\nNo response", "created_at": "2024-11-27", "closed_at": "2024-12-18", "labels": [], "State": "closed", "Author": "LiChengChen666"}
{"issue_number": 2538, "issue_title": "Pydantic objects with all None properties converted to empty dict internally instead of dict with None values", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\nfrom pydantic import BaseModel\nfrom langgraph.graph import StateGraph, START, END\nfrom langchain_core.output_parsers import PydanticOutputParser\n\n\n# Define the state schema with an optional string key \"foo\"\nclass OverallState(BaseModel):\n    foo: str | None  # Optional string\n\nclass InputState(BaseModel):\n    bar: str | None  # Optional string\n\nclass OutputState(BaseModel):\n    baz: str | None  # Optional string\n\n# Define the node function that sets \"foo\" to None\ndef set_foo_to_none(state: InputState) -> OverallState:\n    return OverallState(foo=None)\n\ndef set_baz(state: OverallState) -> OutputState:\n    return OutputState(baz=state.foo)\n\n# Build the state graph\nbuilder = StateGraph(OverallState, input=InputState, output=OutputState)  # Initialize the graph\nbuilder.add_node(\"set_foo_to_none\", set_foo_to_none)  # Add the node\nbuilder.add_node(\"set_baz\", set_baz)  # Add the node\n\nbuilder.add_edge(START, \"set_foo_to_none\")  # Start the graph with the node\nbuilder.add_edge(\"set_foo_to_none\", \"set_baz\")  # Connect the nodes\nbuilder.add_edge(\"set_baz\", END)  # End the graph after the node\n\n# Compile the graph\ngraph = builder.compile()\n\n\nif __name__ == \"__main__\":\n    # Invoke the graph with an initial state\n    response = graph.invoke({\"bar\": \"initial value bar\"})\n    print(f\"Response: \" + str(response))\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\n  File \"/Users/gillesmajor/dev/lizy-ai/projects/picture_categorization/graph_v1.py\", line 38, in <module>\n    response = graph.invoke({\"bar\": \"initial value bar\"})\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/gillesmajor/Library/Caches/pypoetry/virtualenvs/lizy-ai-4pk-LIdT-py3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 1884, in invoke\n    for chunk in self.stream(\n                 ^^^^^^^^^^^^\n  File \"/Users/gillesmajor/Library/Caches/pypoetry/virtualenvs/lizy-ai-4pk-LIdT-py3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 1611, in stream\n    while loop.tick(input_keys=self.input_channels):\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/gillesmajor/Library/Caches/pypoetry/virtualenvs/lizy-ai-4pk-LIdT-py3.12/lib/python3.12/site-packages/langgraph/pregel/loop.py\", line 421, in tick\n    self.tasks = prepare_next_tasks(\n                 ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/gillesmajor/Library/Caches/pypoetry/virtualenvs/lizy-ai-4pk-LIdT-py3.12/lib/python3.12/site-packages/langgraph/pregel/algo.py\", line 361, in prepare_next_tasks\n    if task := prepare_single_task(\n               ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/gillesmajor/Library/Caches/pypoetry/virtualenvs/lizy-ai-4pk-LIdT-py3.12/lib/python3.12/site-packages/langgraph/pregel/algo.py\", line 631, in prepare_single_task\n    val = next(\n          ^^^^^\n  File \"/Users/gillesmajor/Library/Caches/pypoetry/virtualenvs/lizy-ai-4pk-LIdT-py3.12/lib/python3.12/site-packages/langgraph/pregel/algo.py\", line 769, in _proc_input\n    val = proc.mapper(val)\n          ^^^^^^^^^^^^^^^^\n  File \"/Users/gillesmajor/Library/Caches/pypoetry/virtualenvs/lizy-ai-4pk-LIdT-py3.12/lib/python3.12/site-packages/langgraph/graph/state.py\", line 814, in _coerce_state\n    return schema(**input)\n           ^^^^^^^^^^^^^^^\n  File \"/Users/gillesmajor/Library/Caches/pypoetry/virtualenvs/lizy-ai-4pk-LIdT-py3.12/lib/python3.12/site-packages/pydantic/main.py\", line 212, in __init__\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 1 validation error for OverallState\nfoo\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\nDescription\nThis bug has been confirmed by Isaac Herhenson.\nBasically, internally, langgraph is converting a CertainPydanticObjetct(prop1=None, prop2=None) to '{}' (empty dict) when passing states around nodes. Instead of passing {\"prop1\": None, \"Prop2\": None}, it passes {} to the next node, leading to various internal errors in various contexts. Making nodes return a dict with pydantic_object.model_dump() solves the issue temporarily.\nSystem Info\n[tool.poetry.dependencies]\npython = \">=3.12,<3.13\"\nawscli = \"^1.36.4\" # required as a normal dependency to install common at runtime\nboto3 = \"==1.35.63\"\nbotocore = \"==1.35.63\"\nbs4 = \"==0.0.2\"\nchromadb = \"==0.5.18\"\nlanggraph = \"==0.2.50\"\nlangsmith = \"==0.1.143\"\nlangchain-community = \"==0.3.7\"\nlangchain-chroma = \"==0.1.4\"\nlangchain-core = \"==0.3.19\"\nlangchain-openai = \"==0.2.8\"\ngoogle-cloud-documentai = \"==3.0.1\"\nhtml2text = \"==2024.2.26\"\nphonenumberslite = \"==8.12.48\" # leave at this version or common will not install dynamically\npydantic = { extras = [\"email\"], version = \"==2.9.2\" }\nsentry-sdk = \"==2.14.0\" # leave at this version or common will not install dynamically\nredis = \"==5.2.0\"\nring = \"==0.10.1\"\nrequests = \"==2.32.3\"\nrequests-aws4auth = \"==1.3.1\"\nsetuptools = \"==74.1.1\" # leave at this version or common will not install dynamically\nssm-cache = \"==2.10\"\nvininfo = \"==1.8.0\"\n[tool.poetry.group.dev.dependencies]\nboto3-stubs = \"==1.35.45\"\ncoveralls = \"==4.0.1\"\ncoverage = \"==7.6.1\"\nmypy = \"==1.12.1\"\npre-commit = \"==3.8.0\"\npytest = \"==7.4.4\"\npytest-asyncio = \"==0.23.7\"\npytest-cov = \"==5.0.0\"\npytest-mock = \"==3.12.0\"\npytest-socket = \"==0.7.0\"\npytest-xdist = \"==3.5.0\"\ntypes-requests = \"==2.31.0.6\"\ntypes-python-dateutil = \"^2.9.0.20241003\"\ntypes-pytz = \"==2024.2.0.20240913\"\nruff = \"==0.7.0\"\nvulture = \"==2.11\"", "created_at": "2024-11-26", "closed_at": null, "labels": ["investigate"], "State": "open", "Author": "majorgilles"}
{"issue_number": 2537, "issue_title": "DOC:  Fix Typographical Error in \"with with\" in Documentation/Comments", "issue_body": "Issue with current documentation:\nPurpose\nTo correct a grammatical issue in the documentation or code comment for improved clarity and professionalism.\nOutcome\nA clear and professional statement without repetitive wording.\nBackground\nThe current statement includes a redundant \"with with,\" which is a typographical error that could confuse readers or appear unpolished.\nOriginal statement:\nNow, we can just manually update our graph state with with the user input -\n\nAfter Step-1 in\nLocation: https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/wait-user-input/#simple-usage\nIdea or request for content:\nActions\n\nLocate the source of the statement in the documentation or code comments.\nUpdate the statement to remove the redundant \"with\" for clarity:\nNow, we can just manually update our graph state with the user input -\n\n\nReview the surrounding text to ensure no additional issues are present.\nCommit the corrected text and reference this issue for tracking purposes.\n", "created_at": "2024-11-26", "closed_at": "2024-12-18", "labels": [], "State": "closed", "Author": "nirnay01"}
{"issue_number": 2530, "issue_title": "Dataclass coercion ", "issue_body": "Privileged issue\n\n I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.\n\nIssue Content\nfrom dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass Foo:\n    \"\"\"Represents an item with an index.\"\"\"\n    index: int\n\n@dataclass\nclass State:\n    foos: List[Foo]\n\ndef map_to_node(state: State):\n    \"\"\"Prepare all items in the state for processing.\"\"\"\n    return [describe_foo(foo) for foo in state.foos]\n\ndef describe_foo(foo: Foo): # <-- foo is a DICT here instead of a dataclass\n    \"\"\"Raise a TypeError if foo is not a dataclass instance.\"\"\"\n    raise TypeError(f\"Expected Foo, got {type(foo).__name__}\")  # Assume foo is a dict\n\nSystem Information\n\nOS:  Linux\nOS Version:  #49~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Nov  6 17:42:15 UTC 2\nPython Version:  3.11.4 (main, Sep 25 2023, 10:06:23) [GCC 11.4.0]\n\nPackage Information\n\nlangchain_core: 0.3.19\nlangsmith: 0.1.144\nlangchain_openai: 0.2.9\nlanggraph: 0.2.53\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nhttpx: 0.27.2\njsonpatch: 1.33\nlanggraph-checkpoint: 2.0.5\nlanggraph-sdk: 0.1.36\nopenai: 1.55.0\norjson: 3.10.11\npackaging: 24.2\npydantic: 2.10.0\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\n", "created_at": "2024-11-25", "closed_at": null, "labels": ["maintainer"], "State": "open", "Author": "eyurtsev"}
{"issue_number": 2528, "issue_title": "Should the comments here be updated?", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangGraph/LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangGraph/LangChain rather than my code.\n I am sure this is better as an issue rather than a GitHub discussion, since this is a LangGraph bug and not a design question.\n\nExample Code\ndef attach_edge(self, starts: Union[str, Sequence[str]], end: str) -> None:\n        if isinstance(starts, str):\n            if starts == START:\n                channel_name = f\"start:{end}\"\n                # register channel\n                self.channels[channel_name] = EphemeralValue(Any)\n                # subscribe to channel\n                self.nodes[end].triggers.append(channel_name)\n                # publish to channel\n                self.nodes[START] |= ChannelWrite(\n                    [ChannelWriteEntry(channel_name, START)], tags=[TAG_HIDDEN]\n                )\n            elif end != END:\n                # subscribe to start channel\n                self.nodes[end].triggers.append(starts)\n        elif end != END:\n            channel_name = f\"join:{'+'.join(starts)}:{end}\"\n            # register channel\n            self.channels[channel_name] = NamedBarrierValue(str, set(starts))\n            # subscribe to channel\n            self.nodes[end].triggers.append(channel_name)\n            # publish to channel\n            for start in starts:\n                self.nodes[start] |= ChannelWrite(\n                    [ChannelWriteEntry(channel_name, start)], tags=[TAG_HIDDEN]\n                )\nError Message and Stack Trace (if applicable)\n# publish to channel \nthis comments is not Precisely \uff0cMapBe should be \"use channel to update\"?\nDescription\npublish to channel\nthis comments is not Precisely \uff0cMapBe should be \"use channel to update\"?\nSystem Info\npublish to channel\nthis comments is not Precisely \uff0cMapBe should be \"use channel to update\"?", "created_at": "2024-11-25", "closed_at": "2024-11-27", "labels": [], "State": "closed", "Author": "leild"}
{"issue_number": 3199, "issue_title": "run does not change config based on assistant_id", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n\"\"\"\nLanggraph Server code\n\"\"\"\n\nfrom dataclasses import dataclass, field, fields\nfrom typing import Annotated, Dict, List, Optional, Sequence, cast\nfrom langchain_core.messages import AIMessage\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import StateGraph\nfrom typing import Sequence\nfrom react_agent.state import InputState, State\nfrom react_agent.tools import TOOLS\nfrom react_agent.utils import load_chat_model\nfrom langchain_core.runnables import RunnableConfig, ensure_config\nfrom langchain_core.messages import AnyMessage\nfrom langgraph.graph import add_messages\n\n\n@dataclass\nclass State(InputState):\n    messages: Annotated[Sequence[AnyMessage], add_messages] = field(\n        default_factory=list\n    )\n\n\n@dataclass(kw_only=True)\nclass Configuration:\n    prompt: str = field(default=\"Just reply with MISSING_PROMPT\")\n\n    @classmethod\n    def from_runnable_config(\n        cls, config: Optional[RunnableConfig] = None\n    ) -> \"Configuration\":\n        \"\"\"Create a Configuration instance from a RunnableConfig object.\"\"\"\n        config = ensure_config(config)\n        configurable = config.get(\"configurable\") or {}\n        _fields = {f.name for f in fields(cls) if f.init}\n        return cls(**{k: v for k, v in configurable.items() if k in _fields})\n\n\nasync def call_model(\n    state: State, config: RunnableConfig\n) -> Dict[str, List[AIMessage]]:\n    configuration = Configuration.from_runnable_config(config)\n    model = load_chat_model(\"anthropic/claude-3-5-sonnet-20240620\")\n    system_message = configuration.prompt\n    response = cast(\n        AIMessage,\n        await model.ainvoke(\n            [{\"role\": \"system\", \"content\": system_message}, *state.messages], config\n        ),\n    )\n    return {\"messages\": [response]}\n\n\nbuilder = StateGraph(State, input=InputState, config_schema=Configuration)\nbuilder.add_node(call_model)\nbuilder.add_edge(\"__start__\", \"call_model\")\nbuilder.add_edge(\"call_model\", \"__end__\")\n\ngraph = builder.compile()\nError Message and Stack Trace (if applicable)\nFrom a jupyter notebook:\n%pip install langgraph-sdk\n\n\n    Requirement already satisfied: langgraph-sdk in /Users/josh/dev/langgraph-jupyter/env/lib/python3.13/site-packages (0.1.51)\n    Requirement already satisfied: httpx>=0.25.2 in /Users/josh/dev/langgraph-jupyter/env/lib/python3.13/site-packages (from langgraph-sdk) (0.28.1)\n    Requirement already satisfied: orjson>=3.10.1 in /Users/josh/dev/langgraph-jupyter/env/lib/python3.13/site-packages (from langgraph-sdk) (3.10.15)\n    Requirement already satisfied: anyio in /Users/josh/dev/langgraph-jupyter/env/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk) (4.8.0)\n    Requirement already satisfied: certifi in /Users/josh/dev/langgraph-jupyter/env/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk) (2024.12.14)\n    Requirement already satisfied: httpcore==1.* in /Users/josh/dev/langgraph-jupyter/env/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk) (1.0.7)\n    Requirement already satisfied: idna in /Users/josh/dev/langgraph-jupyter/env/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk) (3.10)\n    Requirement already satisfied: h11<0.15,>=0.13 in /Users/josh/dev/langgraph-jupyter/env/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk) (0.14.0)\n    Requirement already satisfied: sniffio>=1.1 in /Users/josh/dev/langgraph-jupyter/env/lib/python3.13/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk) (1.3.1)\n    Note: you may need to restart the kernel to use updated packages.\n\n\n\n\nimport getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"LANGSMITH_API_KEY\")\n\n\n\n\n\nfrom langgraph_sdk import get_client\n\nURL=\"http://localhost:2024\"\nclient = get_client(url=URL,api_key=os.getenv('LANGSMITH_API_KEY'))\nassistant_id = \"agent\"\nthread = await client.threads.create()\n\n\n\n\nassistant_one = await client.assistants.create(\n    graph_id=\"agent\",\n    config={\"configurable\": {\"prompt\": \"just respond with ASSISTANT ONE\"}},\n    assistant_id=\"11111111-1111-1111-1111-111111111111\",\n    if_exists=\"do_nothing\",\n    name=\"asssistant one\"\n)\n\n\n\n\nassistant_two = await client.assistants.create(\n    graph_id=\"agent\",\n    config={\"configurable\": {\"prompt\": \"just respond with ASSISTANT TWO\"}},\n    assistant_id=\"22222222-2222-2222-2222-222222222222\",\n    if_exists=\"do_nothing\",\n    name=\"asssistant two\"\n)\n\n\n\n\nthread = await client.threads.create(\n    metadata={\"number\":1},\n    if_exists=\"raise\"\n)\n\n\n\n\nthread\n\n\n\n\n\n    {'thread_id': 'fba20607-a345-4c52-99e2-270122b5604c',\n     'created_at': '2025-01-24T18:39:41.939930+00:00',\n     'updated_at': '2025-01-24T18:39:41.939934+00:00',\n     'metadata': {'number': 1},\n     'status': 'idle',\n     'config': {},\n     'values': None}\n\n\n\n\n\nresult_a = await client.runs.wait(\n    thread_id=thread[\"thread_id\"],\n    assistant_id=assistant_one[\"assistant_id\"],\n    input={\"messages\": [{\"role\": \"user\", \"content\": \"hello\"}]},\n)\nresult_a[\"messages\"][1][\"content\"]\n\n\n\n\n\n    'ASSISTANT ONE'\n\n\n\n\n\nresult_b = await client.runs.wait(\n    thread_id=thread[\"thread_id\"],\n    assistant_id=assistant_two[\"assistant_id\"],\n    input={\"messages\": [{\"role\": \"user\", \"content\": \"hello\"}]},\n)\nresult_b[\"messages\"][1][\"content\"]\n\n\n\n\n\n    'ASSISTANT ONE'\nDescription\nI have two assistants. I start a thread and complete a run with the first assistant. Then I submit a run with the second assistant. The config provided to the node is always from the first assistant. Notice in the output from my notebook above that it prints \"ASSISTANT ONE\" twice even though the second run is using assistant_two which should print \"ASSISTANT TWO\". If I change the order and call assistant_two first then it will print \"ASSISTANT TWO\" twice.\nI expected that the assistant_id I pass in with the run would cause the configuration for that assistant to be provided to the node.\nI notice in LangGraph Studio I can pick an assistant for each run and it will actually work. But looking at the network request I can see Studio is passing in a config parameter with all the configuration of the selected assistant including the prompt. I don't want to have to do this and it seems counter-intuitive that I should have to. I expected I could use the assistant_id for this.\nThanks!\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.5.0: Wed May  1 20:12:58 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T6000\nPython Version:  3.11.11 (main, Jan 18 2025, 10:11:10) [Clang 16.0.0 (clang-1600.0.26.6)]\n\nPackage Information\n\nlangchain_core: 0.3.30\nlangchain: 0.3.14\nlangchain_community: 0.3.14\nlangsmith: 0.2.11\nlangchain_anthropic: 0.3.3\nlangchain_fireworks: 0.2.6\nlangchain_openai: 0.3.0\nlangchain_text_splitters: 0.3.5\nlanggraph_api: 0.0.16\nlanggraph_cli: 0.1.67\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.51\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.11\nanthropic: 0.43.1\nasync-timeout: Installed. No version info available.\nclick: 8.1.8\ncryptography: 43.0.3\ndataclasses-json: 0.6.7\ndefusedxml: 0.7.1\nfireworks-ai: 0.15.11\nhttpx: 0.28.1\nhttpx-sse: 0.4.0\njsonpatch: 1.33\njsonschema-rs: 0.25.1\nlanggraph: 0.2.64\nlanggraph-checkpoint: 2.0.10\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.59.8\norjson: 3.10.15\npackaging: 24.2\npydantic: 2.10.5\npydantic-settings: 2.7.1\npyjwt: 2.10.1\npython-dotenv: 1.0.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.37\nsse-starlette: 2.1.3\nstarlette: 0.45.2\nstructlog: 24.4.0\ntenacity: 8.5.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\nuvicorn: 0.34.0\nwatchfiles: 1.0.4\nzstandard: Installed. No version info available.\n", "created_at": "2025-01-24", "closed_at": "2025-04-15", "labels": [], "State": "closed", "Author": "weinberg"}
{"issue_number": 3193, "issue_title": "AsyncConnectionPool AsyncPostgresSaver cannot send pipeline when not in pipeline mode", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n#main.py\n @asynccontextmanager\n    async def lifespan(self, app: FastAPI):\n        try:\n            async with AsyncConnectionPool(\n                conninfo=DB_URI, #connection information    \n                kwargs={\n                    \"autocommit\": True,\n                    \"prepare_threshold\": 0,\n                    \"row_factory\": dict_row\n                },\n                min_size=5, #minimum size of the pool\n                max_size=15, #maximum size of the pool\n            ) as pool, pool.connection() as conn:\n                await AsyncPostgresSaver(conn).setup()\n                yield {\"conn\": conn}\n        except Exception as e:\n            logger.error(f\"Error setting up connection pool: {e}\")\n            raise\n        finally:\n            logger.info(f\"End of lifespan\")\n\n#chat.py\n# Use the connection directly from request.state.pool\n            async with request.state.conn as conn:  # Use 'conn' directly\n                checkpointer = AsyncPostgresSaver(conn)\n\n                tools = [\n                    GetInformationTool(\n                        metadata={\"information\": only_structure}),\n                    ValidateInformationTool(\n                        metadata={\"information\": only_structure}),\n                    RegisterInformationTool(metadata={\n                        \"information\": only_structure,\n                        \"user_id\": user_id,\n                        \"family_id\": family_id,\n                        \"organization_id\": organization_id,\n                        \"uuid\": uuid,\n                        \"session_id\": session_id\n                    })\n                ]\n\n                graph_builder = RegistrationGraphBuilder(model=model, prompt=registration_prompt, system_prompt=system_message,\n                                                            member_name=member_name, language=\"Japanese\", tools=tools, checkpointer=checkpointer)\n                graph = graph_builder.build()\n\n                config = {\"configurable\": {\"thread_id\": thread_id}}\n                result = await graph.ainvoke(\n                    {\n                        \"messages\": [HumanMessage(content=user_message)]\n                    },\n                    config\n                )\n\n                content = self.handle_result(result)\n                return ResponseHandler.ok(message=VALID_RESPONSE_MESSAGE, body=content)\nError Message and Stack Trace (if applicable)\nOperationalError('sending prepared query failed: cannot send pipeline when not in pipeline mode\\nanother command is already in progress\\ncannot exit pipeline mode while busy\\ncannot enter pipeline mode, connection not idle\\ncannot enter pipeline mode, connection not idle\\ncannot enter pipeline mode, connection not idle\\nanother command is already in progress\\nanother command is already in progress')Traceback (most recent call last):\n\n\n  File \"/usr/local/lib/python3.10/site-packages/langgraph/pregel/__init__.py\", line 1836, in astream\n    async with AsyncPregelLoop(\n\n\n  File \"/usr/local/lib/python3.10/site-packages/langgraph/pregel/loop.py\", line 988, in __aenter__\n    saved = await self.checkpointer.aget_tuple(self.checkpoint_config)\n\n\n  File \"/usr/local/lib/python3.10/site-packages/langgraph/checkpoint/postgres/aio.py\", line 186, in aget_tuple\n    await cur.execute(\n\n\n  File \"/usr/local/lib/python3.10/site-packages/psycopg/cursor_async.py\", line 97, in execute\n    raise ex.with_traceback(None)\n\n\npsycopg.OperationalError: sending prepared query failed: cannot send pipeline when not in pipeline mode\nanother command is already in progress\ncannot exit pipeline mode while busy\ncannot enter pipeline mode, connection not idle\ncannot enter pipeline mode, connection not idle\ncannot enter pipeline mode, connection not idle\nanother command is already in progress\nanother command is already in progress\nDescription\nI'm developing a chatbot with FastAPI and using LangGraph with prebuilt create_react_agent\nI'm using AsyncConnectionPool and AsyncPostgresSaver\nI have a problem with managing the connection pool of Postgres with LangGraph, I saw this error: cannot send pipeline when not in pipeline mode\nCould you help me to explain what happened and if I missed something, many thanks.\nI appreciate that.\nSystem Info\nfastapi==0.115.6\nlangchain==0.3.14\nlanggraph==0.2.62\nlanggraph-checkpoint-postgres==2.0.10\npsycopg==3.2.3\npsycopg-pool==3.2.4", "created_at": "2025-01-24", "closed_at": "2025-01-24", "labels": [], "State": "closed", "Author": "lam-dm"}
{"issue_number": 3175, "issue_title": "Missing Nodes Arrows In LangGraph Platform", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\nclass GraphState(TypedDict):\n    query: str\n    documents: list\n    attempts: int\n\ndef retrieve_docs(state: GraphState):\n    documents = []\n    return {\n        \"documents\": documents,\n        \"attempts\": state[\"attempts\"] + 1\n    }\n\ndef grade_data(state: GraphState):\n    return {}\n\nworkflow1 = StateGraph(GraphState)\nworkflow1.add_node(\"retrieve_docs\", retrieve_docs)\nworkflow1.add_node(\"grade_data\", grade_data)\nworkflow1.add_edge(START, \"retrieve_docs\")\nworkflow1.add_edge(\"retrieve_docs\", \"grade_data\")\nworkflow1.add_edge(\"grade_data\", END)\n\nworkflow2 = StateGraph(GraphState)\nworkflow2.add_node(\"retrieve docs\", retrieve_docs)\nworkflow2.add_node(\"grade_data\", grade_data)\nworkflow2.add_edge(START, \"retrieve docs\")\nworkflow2.add_edge(\"retrieve docs\", \"grade_data\")\nworkflow2.add_edge(\"grade_data\", END)\n\nworks = workflow1.compile()\nfails = workflow2.compile()\nError Message and Stack Trace (if applicable)\n\nDescription\nThe arrows are missing in the fails graph as shown in the pics:\nOK\n\nNOK\n\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Mon Jan  6 21:59:47 PST 2025; root:xnu-11215.81.4~9/RELEASE_ARM64_T6000\nPython Version:  3.12.3 | packaged by Anaconda, Inc. | (main, May  6 2024, 14:46:42) [Clang 14.0.6 ]\n\nPackage Information\n\nlangchain_core: 0.3.31\nlangchain: 0.3.14\nlangchain_community: 0.3.14\nlangsmith: 0.2.10\nlangchain_ollama: 0.2.2\nlangchain_openai: 0.3.1\nlangchain_qdrant: 0.2.0\nlangchain_text_splitters: 0.3.5\nlanggraph_api: 0.0.15\nlanggraph_cli: 0.1.65\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.48\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.11\nasync-timeout: Installed. No version info available.\nclick: 8.1.8\ncryptography: 43.0.3\ndataclasses-json: 0.6.7\nfastembed: Installed. No version info available.\nhttpx: 0.27.2\nhttpx-sse: 0.4.0\njsonpatch: 1.33\njsonschema-rs: 0.25.1\nlanggraph: 0.2.61\nlanggraph-checkpoint: 2.0.9\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 2.2.1\nollama: 0.4.5\nopenai: 1.60.0\norjson: 3.10.14\npackaging: 24.2\npydantic: 2.10.4\npydantic-settings: 2.7.1\npyjwt: 2.10.1\npython-dotenv: 1.0.1\nPyYAML: 6.0.2\nqdrant-client: 1.13.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.36\nsse-starlette: 2.1.3\nstarlette: 0.45.2\nstructlog: 24.4.0\ntenacity: 8.5.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\nuvicorn: 0.34.0\nwatchfiles: 1.0.3\nzstandard: Installed. No version info available.\n", "created_at": "2025-01-23", "closed_at": null, "labels": ["bug"], "State": "open", "Author": "soufiene-slimi"}
{"issue_number": 3168, "issue_title": "Anthropic API error when using prebuilt create_react_agent after model ends turn with empty content message", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport os\nfrom typing import Literal\nfrom dotenv import load_dotenv\nfrom pprint import pprint\n\nfrom langchain_core.tools import tool\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langchain_anthropic import ChatAnthropic\nfrom langgraph.prebuilt import create_react_agent\n\nload_dotenv()\n\n@tool(\"NameThread\")\ndef name_thread_tool(name: str):\n    \"\"\"\n    Gives this message thread a nice name to show the customer. Keep it to a few words. Quick, witty, specific.\n    \"\"\"\n    return name\n\n\nprompt = \"\"\"\nYou are a helpful assistant that can chat with a user.\nYou are communicating via text message, so make sure your responses are appropriate for that format: brief and VERY casual.\n\nNAMING THE THREAD: \nCall the NameThread tool to give the thread a name to show the user. \nMake sure to ALWAYS give the thread a name after the very first message from the user!\nAs the user gives you more information and the conversation changes, make sure to update the thread name.\nDO NOT continue responding to the user after naming the thread until they respond with a new message.\n\"\"\"\n\ntools = [name_thread_tool]\nmodel = ChatAnthropic(\n    model=\"claude-3-5-sonnet-latest\",\n    temperature=0,\n)\n\nmemory = MemorySaver()\ngraph = create_react_agent(model, tools=tools, checkpointer=memory, state_modifier=prompt)\nconfig = {\"configurable\": {\"thread_id\": \"123\"}}\n\npprint(graph.invoke({\"messages\": [(\"user\", \"Hi, I'm max.\")]}, config))\nprint(\"\\n##########################\\n\")\npprint(graph.invoke({\"messages\": [(\"user\", \"How are you doing?\")]}, config))\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\n  File \"/Users/max/code/langchain_issue_demo/test.py\", line 55, in <module>\n    pprint(graph.invoke({\"messages\": [(\"user\", \"How are you doing?\")]}, config))\n  File \"/Users/max/code/langchain_issue_demo/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py\", line 1961, in invoke\n    for chunk in self.stream(\n  File \"/Users/max/code/langchain_issue_demo/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py\", line 1670, in stream\n    for _ in runner.tick(\n  File \"/Users/max/code/langchain_issue_demo/.venv/lib/python3.10/site-packages/langgraph/pregel/runner.py\", line 231, in tick\n    run_with_retry(\n  File \"/Users/max/code/langchain_issue_demo/.venv/lib/python3.10/site-packages/langgraph/pregel/retry.py\", line 40, in run_with_retry\n    return task.proc.invoke(task.input, config)\n  File \"/Users/max/code/langchain_issue_demo/.venv/lib/python3.10/site-packages/langgraph/utils/runnable.py\", line 462, in invoke\n    input = step.invoke(input, config, **kwargs)\n  File \"/Users/max/code/langchain_issue_demo/.venv/lib/python3.10/site-packages/langgraph/utils/runnable.py\", line 218, in invoke\n    ret = context.run(self.func, *args, **kwargs)\n  File \"/Users/max/code/langchain_issue_demo/.venv/lib/python3.10/site-packages/langgraph/prebuilt/chat_agent_executor.py\", line 628, in call_model\n    response = model_runnable.invoke(state, config)\n  File \"/Users/max/code/langchain_issue_demo/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 3022, in invoke\n    input = context.run(step.invoke, input, config)\n  File \"/Users/max/code/langchain_issue_demo/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 5352, in invoke\n    return self.bound.invoke(\n  File \"/Users/max/code/langchain_issue_demo/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 286, in invoke\n    self.generate_prompt(\n  File \"/Users/max/code/langchain_issue_demo/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 790, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n  File \"/Users/max/code/langchain_issue_demo/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 647, in generate\n    raise e\n  File \"/Users/max/code/langchain_issue_demo/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 637, in generate\n    self._generate_with_cache(\n  File \"/Users/max/code/langchain_issue_demo/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 855, in _generate_with_cache\n    result = self._generate(\n  File \"/Users/max/code/langchain_issue_demo/.venv/lib/python3.10/site-packages/langchain_anthropic/chat_models.py\", line 796, in _generate\n    data = self._client.messages.create(**payload)\n  File \"/Users/max/code/langchain_issue_demo/.venv/lib/python3.10/site-packages/anthropic/_utils/_utils.py\", line 275, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/max/code/langchain_issue_demo/.venv/lib/python3.10/site-packages/anthropic/resources/messages/messages.py\", line 904, in create\n    return self._post(\n  File \"/Users/max/code/langchain_issue_demo/.venv/lib/python3.10/site-packages/anthropic/_base_client.py\", line 1282, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n  File \"/Users/max/code/langchain_issue_demo/.venv/lib/python3.10/site-packages/anthropic/_base_client.py\", line 959, in request\n    return self._request(\n  File \"/Users/max/code/langchain_issue_demo/.venv/lib/python3.10/site-packages/anthropic/_base_client.py\", line 1063, in _request\n    raise self._make_status_error_from_response(err.response) from None\nanthropic.BadRequestError: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'messages.3: all messages must have non-empty content except for the optional final assistant message'}}\nDescription\nWhen using the prebuilt create_react_agent with Anthropic models, in some cases the agent will end a turn with a message with empty content. For example, my demo code produces the following output after the first user message:\n{'messages': [HumanMessage(content=\"Hi, I'm max.\", additional_kwargs={}, response_metadata={}, id='e869f686-d31a-4716-beed-c00608fd0469'),\n              AIMessage(content=[{'text': 'Hey Max! Let me give this chat a name to start us off.', 'type': 'text'}, {'id': 'toolu_01PY3e9tqPGcCs7tfk5qV1C2', 'input': {'name': 'Meeting Max! \ud83d\udc4b'}, 'name': 'NameThread', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_011EGXw5nitFBxkmLN4NymqV', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 527, 'output_tokens': 74}}, id='run-23cfb30e-7269-4ae3-a97e-45166f57c44f-0', tool_calls=[{'name': 'NameThread', 'args': {'name': 'Meeting Max! \ud83d\udc4b'}, 'id': 'toolu_01PY3e9tqPGcCs7tfk5qV1C2', 'type': 'tool_call'}], usage_metadata={'input_tokens': 527, 'output_tokens': 74, 'total_tokens': 601, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}}),\n              ToolMessage(content='Meeting Max! \ud83d\udc4b', name='NameThread', id='b37f9855-bc9d-450f-ab90-df49e5b91d55', tool_call_id='toolu_01PY3e9tqPGcCs7tfk5qV1C2'),\n              AIMessage(content=[], additional_kwargs={}, response_metadata={'id': 'msg_01TeW2BazQH6PS7X7rbb6vEp', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 618, 'output_tokens': 3}}, id='run-2a669331-b4c3-40fe-bb0e-b20736c45d94-0', usage_metadata={'input_tokens': 618, 'output_tokens': 3, 'total_tokens': 621, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})]}\n\nNote the empty content in the final AIMessage. On the next turn, this message is included in the input to the agent, and causes Anthropic to return an error:\nanthropic.BadRequestError: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'messages.3: all messages must have non-empty content except for the optional final assistant message'}}\n\nI can filter these empty messages out manually, but I would expect this behavior to be included in the prebuilt workflow. Additionally, filtering out empty messages in this example will cause errors when using OpenAI models, so in order to be model-agnostic, I need to carefully special-case my handling.\nExpected Result: No error, agent continues as normal\nObserved Result: Error is thrown on any subsequent message sent to this thread\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.6.0: Wed Jul 31 20:48:04 PDT 2024; root:xnu-10063.141.1.700.5~1/RELEASE_ARM64_T6030\nPython Version:  3.11.3 (main, Apr  7 2023, 21:05:46) [Clang 14.0.0 (clang-1400.0.29.202)]\n\nPackage Information\n\nlangchain_core: 0.3.31\nlangchain: 0.3.15\nlangchain_community: 0.3.15\nlangsmith: 0.3.1\nlangchain_anthropic: 0.3.3\nlangchain_openai: 0.3.1\nlangchain_text_splitters: 0.3.5\nlanggraph_sdk: 0.1.51\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.11\nanthropic: 0.44.0\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.7\ndefusedxml: 0.7.1\nhttpx: 0.28.1\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.60.0\norjson: 3.10.15\npackaging: 24.2\npydantic: 2.10.5\npydantic-settings: 2.7.1\npytest: Installed. No version info available.\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrich: Installed. No version info available.\nSQLAlchemy: 2.0.37\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\nzstandard: 0.23.0\n", "created_at": "2025-01-23", "closed_at": "2025-02-06", "labels": [], "State": "closed", "Author": "maxmamis"}
{"issue_number": 3164, "issue_title": "ERROR:  extension \"vector\" is not available", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nlanggraph up\nError Message and Stack Trace (if applicable)\nStarting LangGraph API server...\nFor local dev, requires env var LANGSMITH_API_KEY with access to LangGraph Cloud closed beta.\nFor production use, requires a license key in env var LANGGRAPH_CLOUD_LICENSE_KEY.\n| Starting...#0 building with \"desktop-linux\" instance using docker driver\n\n#1 [langgraph-api internal] load build definition from Dockerfile\n#1 transferring dockerfile: 541B done\n#1 DONE 0.0s\n\n#2 [langgraph-api internal] load metadata for docker.io/langchain/langgraph-api:3.11\n#2 DONE 0.0s\n\n#3 [langgraph-api internal] load .dockerignore\n#3 transferring context: 2B done\n#3 DONE 0.0s\n\n#4 [langgraph-api internal] load build context\n#4 DONE 0.0s\n\n#5 [langgraph-api 1/4] FROM docker.io/langchain/langgraph-api:3.11@sha256:f8a020d5d61fa4b19f276b635542b25faea7d71239a903ffca3e729b34a2aea8\n#5 resolve docker.io/langchain/langgraph-api:3.11@sha256:f8a020d5d61fa4b19f276b635542b25faea7d71239a903ffca3e729b34a2aea8\n#5 ...\n\n#6 [langgraph-api auth] langchain/langgraph-api:pull token for registry-1.docker.io\n#6 DONE 0.0s\n\n#5 [langgraph-api 1/4] FROM docker.io/langchain/langgraph-api:3.11@sha256:f8a020d5d61fa4b19f276b635542b25faea7d71239a903ffca3e729b34a2aea8\n#5 resolve docker.io/langchain/langgraph-api:3.11@sha256:f8a020d5d61fa4b19f276b635542b25faea7d71239a903ffca3e729b34a2aea8 0.8s done\n#5 DONE 0.8s\n\n#5 [langgraph-api 1/4] FROM docker.io/langchain/langgraph-api:3.11@sha256:f8a020d5d61fa4b19f276b635542b25faea7d71239a903ffca3e729b34a2aea8\n#5 DONE 0.9s\n\n#4 [langgraph-api internal] load build context\n#4 transferring context: 283.50kB 0.1s done\n#4 DONE 0.1s\n\n#7 [langgraph-api 2/4] ADD . /deps/memory-template-fablr\n#7 DONE 0.5s\n\n#8 [langgraph-api 3/4] RUN PYTHONDONTWRITEBYTECODE=1 pip install --no-cache-dir -c /api/constraints.txt -e /deps/*\n#8 0.240 Obtaining file:///deps/memory-template-fablr\n#8 0.242   Installing build dependencies: started\n#8 1.178   Installing build dependencies: finished with status 'done'\n#8 1.179   Checking if build backend supports build_editable: started\n#8 1.238   Checking if build backend supports build_editable: finished with status 'done'\n#8 1.238   Getting requirements to build editable: started\n#8 1.333   Getting requirements to build editable: finished with status 'done'\n#8 1.334   Preparing editable metadata (pyproject.toml): started\n#8 1.421   Preparing editable metadata (pyproject.toml): finished with status 'done'\n#8 1.433 Requirement already satisfied: langgraph<0.3.0,>=0.2.53 in /usr/local/lib/python3.11/site-packages (from memory-graph==0.0.1) (0.2.66)\n#8 1.433 Requirement already satisfied: langgraph-checkpoint>=2.0.8 in /usr/local/lib/python3.11/site-packages (from memory-graph==0.0.1) (2.0.10)\n#8 1.499 Collecting langchain-openai>=0.2.1 (from memory-graph==0.0.1)\n#8 1.574   Downloading langchain_openai-0.3.1-py3-none-any.whl.metadata (2.7 kB)\n#8 1.596 Collecting langchain-anthropic>=0.2.1 (from memory-graph==0.0.1)\n#8 1.614   Downloading langchain_anthropic-0.3.3-py3-none-any.whl.metadata (2.3 kB)\n#8 1.700 Collecting langchain>=0.3.8 (from memory-graph==0.0.1)\n#8 1.719   Downloading langchain-0.3.15-py3-none-any.whl.metadata (7.1 kB)\n#8 1.736 Collecting python-dotenv>=1.0.1 (from memory-graph==0.0.1)\n#8 1.753   Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n#8 1.762 Requirement already satisfied: langgraph-sdk>=0.1.40 in /usr/local/lib/python3.11/site-packages (from memory-graph==0.0.1) (0.1.51)\n#8 1.780 Collecting trustcall>=0.0.21 (from memory-graph==0.0.1)\n#8 1.799   Downloading trustcall-0.0.28-py3-none-any.whl.metadata (29 kB)\n#8 1.812 Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/site-packages (from langchain>=0.3.8->memory-graph==0.0.1) (6.0.2)\n#8 1.954 Collecting SQLAlchemy<3,>=1.4 (from langchain>=0.3.8->memory-graph==0.0.1)\n#8 1.975   Downloading SQLAlchemy-2.0.37-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (9.6 kB)\n#8 2.139 Collecting aiohttp<4.0.0,>=3.8.3 (from langchain>=0.3.8->memory-graph==0.0.1)\n#8 2.158   Downloading aiohttp-3.11.11-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (7.7 kB)\n#8 2.160 Requirement already satisfied: langchain-core<0.4.0,>=0.3.31 in /usr/local/lib/python3.11/site-packages (from langchain>=0.3.8->memory-graph==0.0.1) (0.3.31)\n#8 2.174 Collecting langchain-text-splitters<0.4.0,>=0.3.3 (from langchain>=0.3.8->memory-graph==0.0.1)\n#8 2.194   Downloading langchain_text_splitters-0.3.5-py3-none-any.whl.metadata (2.3 kB)\n#8 2.195 Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/site-packages (from langchain>=0.3.8->memory-graph==0.0.1) (0.3.1)\n#8 2.287 Collecting numpy<2,>=1.22.4 (from langchain>=0.3.8->memory-graph==0.0.1)\n#8 2.306   Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (62 kB)\n#8 2.315      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 62.3/62.3 kB 8.6 MB/s eta 0:00:00\n#8 2.321 Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/site-packages (from langchain>=0.3.8->memory-graph==0.0.1) (2.10.5)\n#8 2.321 Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/site-packages (from langchain>=0.3.8->memory-graph==0.0.1) (2.32.3)\n#8 2.322 Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/site-packages (from langchain>=0.3.8->memory-graph==0.0.1) (9.0.0)\n#8 2.348 Collecting anthropic<1,>=0.41.0 (from langchain-anthropic>=0.2.1->memory-graph==0.0.1)\n#8 2.366   Downloading anthropic-0.44.0-py3-none-any.whl.metadata (23 kB)\n#8 2.382 Collecting defusedxml<0.8.0,>=0.7.1 (from langchain-anthropic>=0.2.1->memory-graph==0.0.1)\n#8 2.402   Downloading defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n#8 2.444 Collecting openai<2.0.0,>=1.58.1 (from langchain-openai>=0.2.1->memory-graph==0.0.1)\n#8 2.461   Downloading openai-1.60.0-py3-none-any.whl.metadata (27 kB)\n#8 2.485 Collecting tiktoken<1,>=0.7 (from langchain-openai>=0.2.1->memory-graph==0.0.1)\n#8 2.507   Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.6 kB)\n#8 2.514 Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/site-packages (from langgraph-checkpoint>=2.0.8->memory-graph==0.0.1) (1.1.0)\n#8 2.516 Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/site-packages (from langgraph-sdk>=0.1.40->memory-graph==0.0.1) (0.28.1)\n#8 2.517 Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/site-packages (from langgraph-sdk>=0.1.40->memory-graph==0.0.1) (3.10.15)\n#8 2.602 Collecting dydantic<1.0.0,>=0.0.7 (from trustcall>=0.0.21->memory-graph==0.0.1)\n#8 2.625   Downloading dydantic-0.0.7-py3-none-any.whl.metadata (3.6 kB)\n#8 2.626 Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/site-packages (from trustcall>=0.0.21->memory-graph==0.0.1) (1.33)\n#8 2.651 Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain>=0.3.8->memory-graph==0.0.1)\n#8 2.669   Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n#8 2.687 Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain>=0.3.8->memory-graph==0.0.1)\n#8 2.707   Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n#8 2.727 Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain>=0.3.8->memory-graph==0.0.1)\n#8 2.746   Downloading attrs-24.3.0-py3-none-any.whl.metadata (11 kB)\n#8 2.785 Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain>=0.3.8->memory-graph==0.0.1)\n#8 2.804   Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (13 kB)\n#8 2.883 Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain>=0.3.8->memory-graph==0.0.1)\n#8 2.901   Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (5.0 kB)\n#8 2.924 Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain>=0.3.8->memory-graph==0.0.1)\n#8 2.944   Downloading propcache-0.2.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (9.2 kB)\n#8 3.080 Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain>=0.3.8->memory-graph==0.0.1)\n#8 3.098   Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (69 kB)\n#8 3.102      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 69.2/69.2 kB 21.0 MB/s eta 0:00:00\n#8 3.108 Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/site-packages (from anthropic<1,>=0.41.0->langchain-anthropic>=0.2.1->memory-graph==0.0.1) (4.8.0)\n#8 3.126 Collecting distro<2,>=1.7.0 (from anthropic<1,>=0.41.0->langchain-anthropic>=0.2.1->memory-graph==0.0.1)\n#8 3.142   Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n#8 3.174 Collecting jiter<1,>=0.4.0 (from anthropic<1,>=0.41.0->langchain-anthropic>=0.2.1->memory-graph==0.0.1)\n#8 3.192   Downloading jiter-0.8.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (5.2 kB)\n#8 3.194 Requirement already satisfied: sniffio in /usr/local/lib/python3.11/site-packages (from anthropic<1,>=0.41.0->langchain-anthropic>=0.2.1->memory-graph==0.0.1) (1.3.1)\n#8 3.194 Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/site-packages (from anthropic<1,>=0.41.0->langchain-anthropic>=0.2.1->memory-graph==0.0.1) (4.12.2)\n#8 3.202 Requirement already satisfied: certifi in /usr/local/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.40->memory-graph==0.0.1) (2024.12.14)\n#8 3.203 Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.40->memory-graph==0.0.1) (1.0.7)\n#8 3.203 Requirement already satisfied: idna in /usr/local/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.40->memory-graph==0.0.1) (3.10)\n#8 3.206 Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk>=0.1.40->memory-graph==0.0.1) (0.14.0)\n#8 3.208 Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->trustcall>=0.0.21->memory-graph==0.0.1) (3.0.0)\n#8 3.211 Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.31->langchain>=0.3.8->memory-graph==0.0.1) (24.2)\n#8 3.219 Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain>=0.3.8->memory-graph==0.0.1) (1.0.0)\n#8 3.220 Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain>=0.3.8->memory-graph==0.0.1) (0.23.0)\n#8 3.255 Collecting tqdm>4 (from openai<2.0.0,>=1.58.1->langchain-openai>=0.2.1->memory-graph==0.0.1)\n#8 3.273   Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n#8 3.276      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 57.7/57.7 kB 38.1 MB/s eta 0:00:00\n#8 3.284 Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.3.8->memory-graph==0.0.1) (0.7.0)\n#8 3.284 Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.3.8->memory-graph==0.0.1) (2.27.2)\n#8 3.292 Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.3.8->memory-graph==0.0.1) (3.4.1)\n#8 3.293 Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.3.8->memory-graph==0.0.1) (2.3.0)\n#8 3.393 Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain>=0.3.8->memory-graph==0.0.1)\n#8 3.412   Downloading greenlet-3.1.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (3.8 kB)\n#8 3.547 Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain-openai>=0.2.1->memory-graph==0.0.1)\n#8 3.566   Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (40 kB)\n#8 3.569      \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 40.5/40.5 kB 14.8 MB/s eta 0:00:00\n#8 3.669 Downloading langchain-0.3.15-py3-none-any.whl (1.0 MB)\n#8 3.715    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.0/1.0 MB 23.2 MB/s eta 0:00:00\n#8 3.734 Downloading langchain_anthropic-0.3.3-py3-none-any.whl (22 kB)\n#8 3.754 Downloading langchain_openai-0.3.1-py3-none-any.whl (54 kB)\n#8 3.755    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 54.3/54.3 kB 400.6 MB/s eta 0:00:00\n#8 3.773 Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n#8 3.790 Downloading trustcall-0.0.28-py3-none-any.whl (24 kB)\n#8 3.808 Downloading aiohttp-3.11.11-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.7 MB)\n#8 3.841    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.7/1.7 MB 59.9 MB/s eta 0:00:00\n#8 3.862 Downloading anthropic-0.44.0-py3-none-any.whl (208 kB)\n#8 3.866    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 208.6/208.6 kB 217.8 MB/s eta 0:00:00\n#8 3.884 Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n#8 3.907 Downloading dydantic-0.0.7-py3-none-any.whl (8.6 kB)\n#8 3.929 Downloading langchain_text_splitters-0.3.5-py3-none-any.whl (31 kB)\n#8 3.955 Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (14.2 MB)\n#8 4.086    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14.2/14.2 MB 129.0 MB/s eta 0:00:00\n#8 4.106 Downloading openai-1.60.0-py3-none-any.whl (456 kB)\n#8 4.111    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 456.1/456.1 kB 162.7 MB/s eta 0:00:00\n#8 4.131 Downloading SQLAlchemy-2.0.37-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (3.2 MB)\n#8 4.161    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.2/3.2 MB 112.7 MB/s eta 0:00:00\n#8 4.182 Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.1 MB)\n#8 4.200    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.1/1.1 MB 76.8 MB/s eta 0:00:00\n#8 4.219 Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n#8 4.239 Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n#8 4.264 Downloading attrs-24.3.0-py3-none-any.whl (63 kB)\n#8 4.269    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 63.4/63.4 kB 93.2 MB/s eta 0:00:00\n#8 4.288 Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n#8 4.311 Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (276 kB)\n#8 4.315    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 276.4/276.4 kB 201.5 MB/s eta 0:00:00\n#8 4.337 Downloading greenlet-3.1.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (640 kB)\n#8 4.343    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 640.4/640.4 kB 117.7 MB/s eta 0:00:00\n#8 4.362 Downloading jiter-0.8.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (335 kB)\n#8 4.369    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 335.5/335.5 kB 243.8 MB/s eta 0:00:00\n#8 4.387 Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (131 kB)\n#8 4.389    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 131.1/131.1 kB 457.2 MB/s eta 0:00:00\n#8 4.407 Downloading propcache-0.2.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (232 kB)\n#8 4.410    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 232.1/232.1 kB 334.9 MB/s eta 0:00:00\n#8 4.433 Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (792 kB)\n#8 4.439    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 792.1/792.1 kB 207.5 MB/s eta 0:00:00\n#8 4.458 Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n#8 4.460    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.5/78.5 kB 250.6 MB/s eta 0:00:00\n#8 4.481 Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (340 kB)\n#8 4.483    \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 340.6/340.6 kB 389.2 MB/s eta 0:00:00\n#8 4.531 Building wheels for collected packages: memory-graph\n#8 4.532   Building editable for memory-graph (pyproject.toml): started\n#8 4.641   Building editable for memory-graph (pyproject.toml): finished with status 'done'\n#8 4.641   Created wheel for memory-graph: filename=memory_graph-0.0.1-0.editable-py3-none-any.whl size=12058 sha256=231700013530c5ec53d51858097ee84bff1feaa31e80e3767011fdb6bb1da20f\n#8 4.641   Stored in directory: /tmp/pip-ephem-wheel-cache-4xloqqw2/wheels/c5/7a/de/6394e8973710799d9374a0b7e96b0800e1979dc4e97f5a93ea\n#8 4.643 Successfully built memory-graph\n#8 4.769 Installing collected packages: tqdm, regex, python-dotenv, propcache, numpy, multidict, jiter, greenlet, frozenlist, distro, defusedxml, attrs, aiohappyeyeballs, yarl, tiktoken, SQLAlchemy, aiosignal, openai, dydantic, anthropic, aiohttp, langchain-text-splitters, langchain-openai, langchain-anthropic, langchain, trustcall, memory-graph\n#8 6.783 Successfully installed SQLAlchemy-2.0.37 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 anthropic-0.44.0 attrs-24.3.0 defusedxml-0.7.1 distro-1.9.0 dydantic-0.0.7 frozenlist-1.5.0 greenlet-3.1.1 jiter-0.8.2 langchain-0.3.15 langchain-anthropic-0.3.3 langchain-openai-0.3.1 langchain-text-splitters-0.3.5 memory-graph-0.0.1 multidict-6.1.0 numpy-1.26.4 openai-1.60.0 propcache-0.2.1 python-dotenv-1.0.1 regex-2024.11.6 tiktoken-0.8.0 tqdm-4.67.1 trustcall-0.0.28 yarl-1.18.3\n#8 6.858 \n#8 6.858 [notice] A new release of pip is available: 24.0 -> 24.3.1\n#8 6.858 [notice] To update, run: pip install --upgrade pip\n#8 DONE 7.3s\n\n#9 [langgraph-api 4/4] WORKDIR /deps/memory-template-fablr\n#9 DONE 0.0s\n\n#10 [langgraph-api] exporting to image\n#10 exporting layers\n#10 exporting layers 2.2s done\n#10 exporting manifest sha256:39ad610bb2738b5579213c7f4df8a3abe393a77b5676a48ff06ea3fda76b8e59\n#10 exporting manifest sha256:39ad610bb2738b5579213c7f4df8a3abe393a77b5676a48ff06ea3fda76b8e59 done\n#10 exporting config sha256:156a740bbc10092b69f2a576f17bd4aacc4276526d318850caf7e3f4537ad0bd done\n#10 exporting attestation manifest sha256:5e28adc6229387a563d26e2124efd582fcb5c23461b4961b7d903f2dcc949e05 done\n#10 exporting manifest list sha256:2255a8f540d49db3441f4866eae7bcee9836d959d9c3edfcc3ec3b83e7213a8e done\n#10 naming to docker.io/library/memory-template-fablr-langgraph-api:latest done\n#10 unpacking to docker.io/library/memory-template-fablr-langgraph-api:latest\n#10 unpacking to docker.io/library/memory-template-fablr-langgraph-api:latest 0.7s done\n#10 DONE 3.0s\n\n#11 [langgraph-api] resolving provenance for metadata file\n#11 DONE 0.0s\nAttaching to langgraph-api-1, langgraph-postgres-1, langgraph-redis-1\nlanggraph-redis-1     | 1:C 23 Jan 2025 12:50:31.408 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\nlanggraph-redis-1     | 1:C 23 Jan 2025 12:50:31.408 # Redis version=6.2.16, bits=64, commit=00000000, modified=0, pid=1, just started\nlanggraph-redis-1     | 1:C 23 Jan 2025 12:50:31.408 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\nlanggraph-redis-1     | 1:M 23 Jan 2025 12:50:31.408 * monotonic clock: POSIX clock_gettime\nlanggraph-redis-1     | 1:M 23 Jan 2025 12:50:31.408 * Running mode=standalone, port=6379.\nlanggraph-redis-1     | 1:M 23 Jan 2025 12:50:31.408 # Server initialized\nlanggraph-redis-1     | 1:M 23 Jan 2025 12:50:31.410 * Loading RDB produced by version 6.2.16\nlanggraph-redis-1     | 1:M 23 Jan 2025 12:50:31.410 * RDB age 568754 seconds\nlanggraph-redis-1     | 1:M 23 Jan 2025 12:50:31.410 * RDB memory usage when created 0.81 Mb\nlanggraph-redis-1     | 1:M 23 Jan 2025 12:50:31.410 # Done loading RDB, keys loaded: 0, keys expired: 0.\nlanggraph-redis-1     | 1:M 23 Jan 2025 12:50:31.410 * DB loaded from disk: 0.000 seconds\nlanggraph-redis-1     | 1:M 23 Jan 2025 12:50:31.410 * Ready to accept connections\nlanggraph-postgres-1  | \nlanggraph-postgres-1  | PostgreSQL Database directory appears to contain a database; Skipping initialization\nlanggraph-postgres-1  | \nlanggraph-api-1       | 2025-01-23T12:50:37.367224Z [info     ] Using auth of type=noop        [langgraph_api.auth.middleware] api_revision=bbed8a5 api_variant=local\nlanggraph-api-1       | 2025-01-23T12:50:37.368050Z [info     ] Started server process [1]     [uvicorn.error] api_revision=bbed8a5 api_variant=local color_message='Started server process [\\x1b[36m%d\\x1b[0m]'\nlanggraph-api-1       | 2025-01-23T12:50:37.368114Z [info     ] Waiting for application startup. [uvicorn.error] api_revision=bbed8a5 api_variant=local\nlanggraph-api-1       | 2025-01-23T12:50:37.368281Z [warning  ] No license key found, running in test mode with LangSmith API key. For production use, set LANGGRAPH_CLOUD_LICENSE_KEY in environment. [langgraph_license.validation] api_revision=bbed8a5 api_variant=local\nlanggraph-api-1       | 2025-01-23T12:50:37.522194Z [info     ] HTTP Request: GET https://api.smith.langchain.com/auth?langgraph-api=true \"HTTP/1.1 200 OK\" [httpx] api_revision=bbed8a5 api_variant=local\nlanggraph-api-1       | 2025-01-23T12:50:37.562562Z [warning  ] /api/langgraph_api/graph.py:470: LangChainBetaWarning: The function `init_embeddings` is in beta. It is actively being worked on, so the API may change.\nlanggraph-api-1       |  [py.warnings] api_revision=bbed8a5 api_variant=local\nlanggraph-api-1       | 2025-01-23T12:50:37.782794Z [info     ] Setting up vector index        [langgraph_storage.database] api_revision=bbed8a5 api_variant=local store_config={'index': {'dims': 1536, 'embed': OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0xffffab5c8390>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0xffffab580150>, model='text-embedding-3-small', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)}}\nlanggraph-api-1       | 2025-01-23T12:50:37.787183Z [error    ] Traceback (most recent call last):\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 693, in lifespan\nlanggraph-api-1       |     async with self.lifespan_context(app) as maybe_state:\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/contextlib.py\", line 210, in __aenter__\nlanggraph-api-1       |     return await anext(self.gen)\nlanggraph-api-1       |            ^^^^^^^^^^^^^^^^^^^^^\nlanggraph-api-1       |   File \"/api/langgraph_api/lifespan.py\", line 29, in lifespan\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/langgraph_storage/database.py\", line 149, in start_pool\nlanggraph-api-1       |     await migrate_vector_index()\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/langgraph_storage/database.py\", line 138, in migrate_vector_index\nlanggraph-api-1       |     await lg_store.setup_vector_index(store)\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/langgraph_storage/store.py\", line 94, in setup_vector_index\nlanggraph-api-1       |     await cur.execute(sql)\nlanggraph-api-1       |   File \"/usr/local/lib/python3.11/site-packages/psycopg/cursor_async.py\", line 97, in execute\nlanggraph-api-1       |     raise ex.with_traceback(None)\nlanggraph-api-1       | psycopg.errors.FeatureNotSupported: extension \"vector\" is not available\nlanggraph-api-1       | DETAIL:  Could not open extension control file \"/usr/share/postgresql/16/extension/vector.control\": No such file or directory.\nlanggraph-api-1       | HINT:  The extension must first be installed on the system where PostgreSQL is running.\nlanggraph-api-1       |  [uvicorn.error] api_revision=bbed8a5 api_variant=local\nlanggraph-api-1       | 2025-01-23T12:50:37.787265Z [error    ] Application startup failed. Exiting. [uvicorn.error] api_revision=bbed8a5 api_variant=local\nlanggraph-api-1 exited with code 3\n Service langgraph-api  Building\n Service langgraph-api  Built\n Container memory-template-fablr-langgraph-postgres-1  Created\n Container memory-template-fablr-langgraph-redis-1  Created\n Container memory-template-fablr-langgraph-api-1  Recreate\n Container memory-template-fablr-langgraph-api-1  Recreated\nlanggraph-postgres-1  | 2025-01-23 12:50:31.428 UTC [1] LOG:  starting PostgreSQL 16.6 (Debian 16.6-1.pgdg120+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit\nlanggraph-postgres-1  | 2025-01-23 12:50:31.428 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\nlanggraph-postgres-1  | 2025-01-23 12:50:31.428 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\nlanggraph-postgres-1  | 2025-01-23 12:50:31.432 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\nlanggraph-postgres-1  | 2025-01-23 12:50:31.437 UTC [29] LOG:  database system was shut down at 2025-01-16 22:51:17 UTC\nlanggraph-postgres-1  | 2025-01-23 12:50:31.441 UTC [1] LOG:  database system is ready to accept connections\nlanggraph-postgres-1  | 2025-01-23 12:50:37.786 UTC [41] ERROR:  extension \"vector\" is not available\nlanggraph-postgres-1  | 2025-01-23 12:50:37.786 UTC [41] DETAIL:  Could not open extension control file \"/usr/share/postgresql/16/extension/vector.control\": No such file or directory.\nlanggraph-postgres-1  | 2025-01-23 12:50:37.786 UTC [41] HINT:  The extension must first be installed on the system where PostgreSQL is running.\nlanggraph-postgres-1  | 2025-01-23 12:50:37.786 UTC [41] STATEMENT:  \nlanggraph-postgres-1  | \tCREATE EXTENSION IF NOT EXISTS vector;\nlanggraph-postgres-1  | \t\nAborting on container exit...\n Container memory-template-fablr-langgraph-api-1  Stopping\n Container memory-template-fablr-langgraph-api-1  Stopped\n Container memory-template-fablr-langgraph-postgres-1  Stopping\n Container memory-template-fablr-langgraph-redis-1  Stopping\n Container memory-template-fablr-langgraph-postgres-1  Stopped\n Container memory-template-fablr-langgraph-redis-1  Stopped\nDescription\nTrying to load the using Langgraph CLI. This vector error was just fixed with version .35 of the Langgraph Studio client but it's still seems to be present in the CLI version of Langgraph.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.2.0: Fri Dec  6 19:03:40 PST 2024; root:xnu-11215.61.5~2/RELEASE_ARM64_T6041\nPython Version:  3.11.0 (v3.11.0:deaf509e8f, Oct 24 2022, 14:43:23) [Clang 13.0.0 (clang-1300.0.29.30)]\n\nPackage Information\n\nlangchain_core: 0.3.29\nlangchain: 0.3.14\nlangsmith: 0.2.10\nlangchain_anthropic: 0.3.1\nlangchain_openai: 0.3.0\nlangchain_text_splitters: 0.3.5\nlanggraph_api: 0.0.16\nlanggraph_cli: 0.1.67\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.51\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.11\nanthropic: 0.42.0\nasync-timeout: Installed. No version info available.\nclick: 8.1.8\ncryptography: 43.0.3\ndefusedxml: 0.7.1\nhttpx: 0.28.1\njsonpatch: 1.33\njsonschema-rs: 0.25.1\nlanggraph: 0.2.62\nlanggraph-checkpoint: 2.0.9\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.59.7\norjson: 3.10.14\npackaging: 24.2\npydantic: 2.10.5\npyjwt: 2.10.1\npython-dotenv: 1.0.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.37\nsse-starlette: 2.1.3\nstarlette: 0.45.2\nstructlog: 24.4.0\ntenacity: 8.5.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\nuvicorn: 0.34.0\nwatchfiles: 1.0.4\nzstandard: Installed. No version info available.\n", "created_at": "2025-01-23", "closed_at": "2025-01-25", "labels": [], "State": "closed", "Author": "daflood"}
{"issue_number": 3153, "issue_title": "No command in response", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langchain_ollama import ChatOllama\n\n#llm = OpenAI(temperature=0)\n\n\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\n\n\nfrom typing import List, Optional, Literal\nfrom langchain_core.language_models.chat_models import BaseChatModel\n\nfrom langgraph.graph import StateGraph, MessagesState, START, END\nfrom langgraph.types import Command\nfrom langchain_core.messages import HumanMessage, trim_messages\n\n\nclass State(MessagesState):\n    next: str\n\n\ndef make_supervisor_node(llm: BaseChatModel, members: list[str]) -> str:\n    options =  members + [\"FINISH\"]\n    system_prompt = (\n        \"You are a supervisor tasked with managing a conversation between the\"\n        f\" following workers: {members}. Given the following user request,\"\n        \" respond with the worker to act next. Each worker will perform a\"\n        \" task and respond with their results and status. When finished,\"\n        \" respond with FINISH.\"\n    )\n\n    class Router(TypedDict):\n        \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n\n        next: Literal[*options]\n\n    def supervisor_node(state: State) -> Command[Literal[*members, \"__end__\"]]:\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},\n        ] + state[\"messages\"]\n        response = llm.with_structured_output(Router).invoke(messages)\n\n        goto = response[\"next\"]\n        if goto == \"FINISH\":\n            goto = END\n\n        return Command(goto=goto, update={\"next\": goto})\n\n    return supervisor_node\n\nllm = ChatOllama(\n  model= \"llama3.2\",\n    temperature=0\n)\n\nsearch_agent = create_react_agent(llm, tools=[tavily_tool], state_modifier=\"You are a researcher. DO NOT do any math. And always return the state\")\n\n\ndef search_node(state: State) -> Command[Literal[\"supervisor\"]]:\n    result = search_agent.invoke(state)\n    print(\"here\")\n    update = HumanMessage(content=result[\"messages\"][-1].content, name=\"search\")\n\n    command = Command(\n        update={\n            \"messages\": [\n                HumanMessage(content=result[\"messages\"][-1].content, name=\"search\")\n            ]\n        },\n        # We want our workers to ALWAYS \"report back\" to the supervisor when done\n        goto=\"supervisor\",\n    )\n    return command\n\n\nweb_scraper_agent = create_react_agent(llm, tools=[scrape_webpages])\n\n\ndef web_scraper_node(state: State) -> Command[Literal[\"supervisor\"]]:\n    result = web_scraper_agent.invoke(state)\n    return Command(\n        update={\n            \"messages\": [\n                \nfrom typing import Annotated, List\n\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_core.tools import tool\nfrom langchain_ollama import ChatOllama\nfrom langchain_core.messages import HumanMessage\nfrom langgraph.prebuilt import create_react_agent\n\nllm = ChatOllama(\n  model= \"llama3.2\",\n    temperature=0\n)\n\n@tool\ndef scrape_webpages(urls: List[str]) -> str:\n    \"\"\"Use requests and bs4 to scrape the provided web pages for detailed information.\"\"\"\n    loader = WebBaseLoader(urls)\n    docs = loader.load()\n    return \"\\n\\n\".join(\n        [\n            f'<Document name=\"{doc.metadata.get(\"title\", \"\")}\">\\n{doc.page_content}\\n</Document>'\n            for doc in docs\n        ]\n    )\n\n\n\nsearch_agent = create_react_agent(llm, tools=[tavily_tool], state_modifier=\"You are a researcher. DO NOT do any math. And always return the state\")\n\n\ndef search_node(state: State) -> Command[Literal[\"supervisor\"]]:\n    result = search_agent.invoke(state)\n    print(\"here\")\n    update = HumanMessage(content=result[\"messages\"][-1].content, name=\"search\")\n\n    command = Command(\n        update={\n            \"messages\": [\n                HumanMessage(content=result[\"messages\"][-1].content, name=\"search\")\n            ]\n        },\n        # We want our workers to ALWAYS \"report back\" to the supervisor when done\n        goto=\"supervisor\",\n    )\n    return command\n\n\nweb_scraper_agent = create_react_agent(llm, tools=[scrape_webpages])\n\n\ndef web_scraper_node(state: State) -> Command[Literal[\"supervisor\"]]:\n    result = web_scraper_agent.invoke(state)\n    return Command(\n        update={\n            \"messages\": [\n                HumanMessage(content=result[\"messages\"][-1].content, name=\"web_scraper\")\n            ]\n        },\n        # We want our workers to ALWAYS \"report back\" to the supervisor when done\n        goto=\"supervisor\",\n    )\n\n\nresearch_supervisor_node = make_supervisor_node(llm, [\"search\", \"web_scraper\"])\n\nresearch_builder = StateGraph(State)\nresearch_builder.add_node(\"supervisor\", research_supervisor_node)\nresearch_builder.add_node(\"search\", search_node)\nresearch_builder.add_node(\"web_scraper\", web_scraper_node)\n\nresearch_builder.add_edge(START, \"supervisor\")\nresearch_graph = research_builder.compile()\n\nfor s in research_graph.stream(\n    {\"messages\": [(\"user\", \"when is the water temperature in Copenhagen today?\")]},\n    {\"recursion_limit\": 100},\n):\n    print(s)\n    print(\"---\")\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[118], line 1\n----> 1 for s in research_graph.stream(\n      2     {\"messages\": [(\"user\", \"when is the water temperature in Copenhagen today?\")]},\n      3     {\"recursion_limit\": 100},\n      4 ):\n      5     print(s)\n      6     print(\"---\")\n\nFile ~/.local/pipx/venvs/jupyter/lib/python3.13/site-packages/langgraph/pregel/__init__.py:1670, in Pregel.stream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\n   1664     # Similarly to Bulk Synchronous Parallel / Pregel model\n   1665     # computation proceeds in steps, while there are channel updates.\n   1666     # Channel updates from step N are only visible in step N+1\n   1667     # channels are guaranteed to be immutable for the duration of the step,\n   1668     # with channel updates applied only at the transition between steps.\n   1669     while loop.tick(input_keys=self.input_channels):\n-> 1670         for _ in runner.tick(\n   1671             loop.tasks.values(),\n   1672             timeout=self.step_timeout,\n   1673             retry_policy=self.retry_policy,\n   1674             get_waiter=get_waiter,\n   1675         ):\n   1676             # emit output\n   1677             yield from output()\n   1678 # emit output\n\nFile ~/.local/pipx/venvs/jupyter/lib/python3.13/site-packages/langgraph/pregel/runner.py:231, in PregelRunner.tick(self, tasks, reraise, timeout, retry_policy, get_waiter)\n    229 t = tasks[0]\n    230 try:\n--> 231     run_with_retry(\n    232         t,\n    233         retry_policy,\n    234         configurable={\n    235             CONFIG_KEY_SEND: partial(writer, t),\n    236             CONFIG_KEY_CALL: partial(call, t),\n    237         },\n    238     )\n    239     self.commit(t, None)\n    240 except Exception as exc:\n\nFile ~/.local/pipx/venvs/jupyter/lib/python3.13/site-packages/langgraph/pregel/retry.py:40, in run_with_retry(task, retry_policy, configurable)\n     38     task.writes.clear()\n     39     # run the task\n---> 40     return task.proc.invoke(task.input, config)\n     41 except ParentCommand as exc:\n     42     ns: str = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n\nFile ~/.local/pipx/venvs/jupyter/lib/python3.13/site-packages/langgraph/utils/runnable.py:462, in RunnableSeq.invoke(self, input, config, **kwargs)\n    458 config = patch_config(\n    459     config, callbacks=run_manager.get_child(f\"seq:step:{i + 1}\")\n    460 )\n    461 if i == 0:\n--> 462     input = step.invoke(input, config, **kwargs)\n    463 else:\n    464     input = step.invoke(input, config)\n\nFile ~/.local/pipx/venvs/jupyter/lib/python3.13/site-packages/langgraph/utils/runnable.py:226, in RunnableCallable.invoke(self, input, config, **kwargs)\n    224 else:\n    225     context.run(_set_config_context, config)\n--> 226     ret = context.run(self.func, *args, **kwargs)\n    227 if isinstance(ret, Runnable) and self.recurse:\n    228     return ret.invoke(input, config)\n\nCell In[114], line 34, in make_supervisor_node.<locals>.supervisor_node(state)\n     29 messages = [\n     30     {\"role\": \"system\", \"content\": system_prompt},\n     31 ] + state[\"messages\"]\n     32 response = llm.with_structured_output(Router).invoke(messages)\n---> 34 goto = response[\"next\"]\n     35 if goto == \"FINISH\":\n     36     goto = END\n\nTypeError: 'NoneType' object is not subscriptable\nDuring task with name 'supervisor' and id 'ce47368d-11a1-1e6a-b74b-31245d84ce0f'\nDescription\nI'm trying to run the Multi-agent example with Ollama - LLama3.2 - and it crashed due to an empty response.\nI do get some responses sometimes but then it crashes.\nSystem Info\nPackage Information\n\nlangchain_core: 0.3.30\nlangsmith: 0.2.10\nlangchain_ollama: 0.2.2\n", "created_at": "2025-01-22", "closed_at": "2025-01-23", "labels": ["question"], "State": "closed", "Author": "LouiseAbela"}
{"issue_number": 3120, "issue_title": "Thread runs sometimes throws HttpException, Graph 'xxx' not found", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport requests\nimport time\nimport logging\n\n# Configure logging\nlogging.basicConfig(\n    filename='thread_process.log',\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\n\nconsole = logging.StreamHandler()\nconsole.setLevel(logging.INFO)\nformatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\nconsole.setFormatter(formatter)\nlogging.getLogger('').addHandler(console)\n\ndef log_request_response(response):\n    logging.info(f\"Request URL: {response.request.url}\")\n    logging.info(f\"Request Method: {response.request.method}\")\n    logging.info(f\"Request Headers: {response.request.headers}\")\n    if response.request.body:\n        logging.info(f\"Request Body: {response.request.body}\")\n    logging.info(f\"Response Status Code: {response.status_code}\")\n    logging.info(f\"Response Headers: {response.headers}\")\n    logging.info(f\"Response Body: {response.text}\")\n\nimport uuid\n\ndef create_thread():\n    # thread_id = str(uuid.uuid4())  # Generate a UUID for thread_id\n    url = \"http://127.0.0.1:8123/threads\"\n    payload = {\n    }\n    response = requests.post(url, json=payload)\n    log_request_response(response)\n    response.raise_for_status()\n    thread_id = response.json()[\"thread_id\"]\n    logging.info(f\"Thread created with ID: {thread_id}\")\n    return thread_id\n\ndef run_thread(thread_id):\n    url = f\"http://127.0.0.1:8123/threads/{thread_id}/runs/wait\"\n    payload = {\n        \"assistant_id\": \"0676914a-25a7-595a-a130-6f9e1ad87f7d\",\n        \"input\": {\n            \"knowledge\": \"Camera\",\n            \"node\": \"Rule\"\n        },\n        \"after_seconds\": 1\n    }\n    response = requests.post(url, json=payload)\n    log_request_response(response)\n    response.raise_for_status()\n    run_id = response.json()[\"run_id\"]\n    logging.info(f\"Thread run initiated with Run ID: {run_id}\")\n    return run_id\n\ndef get_thread_status(thread_id):\n    url = f\"http://127.0.0.1:8123/threads/{thread_id}/runs\"\n    response = requests.get(url)\n    log_request_response(response)\n    response.raise_for_status()\n    status = response.json()[0][\"status\"]\n    logging.info(f\"Current status: {status}\")\n    return status\n\ndef get_thread_result(thread_id):\n    url = f\"http://127.0.0.1:8123/threads/{thread_id}\"\n    response = requests.get(url)\n    log_request_response(response)\n    response.raise_for_status()\n    result = response.json()\n    logging.info(f\"Thread result: {result}\")\n    return result\n\ndef execute_test():\n    try:\n        thread_id = create_thread()\n        retry_count = 0\n        run_id = run_thread(thread_id)\n        status = get_thread_status(thread_id)\n        while status != \"success\":\n            if status == \"error\":\n                retry_count += 1\n                if retry_count > 3:\n                    logging.error(\"Thread run failed after 3 retries.\")\n                    return False\n                logging.warning(f\"Thread run encountered an error. Retrying {retry_count}/3...\")\n                run_id = run_thread(thread_id)  # Retry running the thread\n            # time.sleep(2)\n            status = get_thread_status(thread_id)\n        result = get_thread_result(thread_id)\n        return True\n    except Exception as e:\n        logging.error(f\"Exception occurred: {e}\")\n        return False\n\ndef main():\n    success_count = 0\n    total_runs = 10\n    for i in range(total_runs):\n        logging.info(f\"Starting test run {i+1}/{total_runs}\")\n        if execute_test():\n            success_count += 1\n        logging.info(f\"Test run {i+1}/{total_runs} completed\")\n\n    success_rate = (success_count / total_runs) * 100\n    logging.info(f\"Success rate: {success_rate}%\")\n\nif __name__ == \"__main__\":\n    main()\nError Message and Stack Trace (if applicable)\n2025-01-21 12:33:33,813 - INFO - Request URL: http://127.0.0.1:8123/threads\n2025-01-21 12:33:33,813 - INFO - Request Method: POST\n2025-01-21 12:33:33,813 - INFO - Request Headers: {'User-Agent': 'python-requests/2.32.3', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'Content-Length': '2', 'Content-Type': 'application/json'}\n2025-01-21 12:33:33,813 - INFO - Request Body: b'{}'\n2025-01-21 12:33:33,813 - INFO - Response Status Code: 200\n2025-01-21 12:33:33,813 - INFO - Response Headers: {'date': 'Tue, 21 Jan 2025 04:33:33 GMT', 'server': 'uvicorn', 'content-length': '220', 'content-type': 'application/json'}\n2025-01-21 12:33:33,813 - INFO - Response Body: {\"thread_id\":\"2a6608cc-ec49-41f6-8926-a249dda7ff97\",\"created_at\":\"2025-01-21T12:33:34.919186+08:00\",\"updated_at\":\"2025-01-21T12:33:34.919186+08:00\",\"metadata\":{},\"status\":\"idle\",\"config\":{},\"values\":null,\"interrupts\":{}}\n2025-01-21 12:33:33,813 - INFO - Thread created with ID: 2a6608cc-ec49-41f6-8926-a249dda7ff97\n2025-01-21 12:33:35,551 - INFO - Request URL: http://127.0.0.1:8123/threads/2a6608cc-ec49-41f6-8926-a249dda7ff97/runs/wait\n2025-01-21 12:33:35,551 - INFO - Request Method: POST\n2025-01-21 12:33:35,552 - INFO - Request Headers: {'User-Agent': 'python-requests/2.32.3', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'Content-Length': '126', 'Content-Type': 'application/json'}\n2025-01-21 12:33:35,552 - INFO - Request Body: b'{\"assistant_id\": \"0676914a-25a7-595a-a130-6f9e1ad87f7d\", \"input\": {\"knowledge\": \"Camera\", \"node\": \"Rule\"}, \"after_seconds\": 1}'\n2025-01-21 12:33:35,552 - INFO - Response Status Code: 200\n2025-01-21 12:33:35,552 - INFO - Response Headers: {'date': 'Tue, 21 Jan 2025 04:33:33 GMT', 'server': 'uvicorn', 'location': '/threads/2a6608cc-ec49-41f6-8926-a249dda7ff97/runs/1efd7b0d-fa56-665c-a80c-a655ff544eea/join', 'content-type': 'application/json', 'transfer-encoding': 'chunked'}\n2025-01-21 12:33:35,553 - INFO - Response Body: {\"__error__\":{\"error\":\"HTTPException\",\"message\":\"404: Graph 'rule' not found\"}}\n2025-01-21 12:33:35,553 - ERROR - Exception occurred: 'run_id'\nDescription\nAfter I finished deploying using Docker, the Runs API sometimes throws a Graph 'rule' not found error, with an error rate of about 20%. Is there a problem somewhere, and how should it be resolved?\nhttps://langchain-ai.github.io/langgraph/how-tos/deploy-self-hosted/#using-docker\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP Tue Nov 5 00:21:55 UTC 2024\nPython Version:  3.11.11 (main, Dec  4 2024, 08:55:08) [GCC 9.4.0]\n\nPackage Information\n\nlangchain_core: 0.3.30\nlangchain: 0.3.12\nlangchain_community: 0.3.10\nlangsmith: 0.1.147\nlangchain_anthropic: 0.3.0\nlangchain_fireworks: 0.2.5\nlangchain_openai: 0.3.0\nlangchain_text_splitters: 0.3.3\nlangchainhub: 0.1.21\nlanggraph_api: 0.0.15\nlanggraph_cli: 0.1.65\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.48\nlanggraph_storage: Installed. No version info available.\nlangserve: 0.3.0\n\nOther Dependencies\n\naiohttp: 3.11.10\nanthropic: 0.40.0\nasync-timeout: Installed. No version info available.\nclick: 8.1.7\ncryptography: 43.0.3\ndataclasses-json: 0.6.7\ndefusedxml: 0.7.1\nfastapi: 0.115.6\nfireworks-ai: 0.15.10\nhttpx: 0.28.1\nhttpx-sse: 0.4.0\njsonpatch: 1.33\njsonschema-rs: 0.25.1\nlanggraph: 0.2.63\nlanggraph-checkpoint: 2.0.10\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.59.7\norjson: 3.10.12\npackaging: 24.2\npydantic: 2.10.3\npydantic-settings: 2.6.1\npyjwt: 2.10.1\npython-dotenv: 1.0.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.36\nsse-starlette: 2.1.3\nstarlette: 0.41.3\nstructlog: 24.4.0\ntenacity: 8.5.0\ntiktoken: 0.8.0\ntypes-requests: 2.32.0.20241016\ntyping-extensions: 4.12.2\nuvicorn: 0.32.1\nwatchfiles: 1.0.0\n", "created_at": "2025-01-21", "closed_at": "2025-02-11", "labels": [], "State": "closed", "Author": "Jackoder"}
{"issue_number": 3115, "issue_title": "Issue in Stream Execution When Using Command within a Subgraph", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing import TypedDict\n\nfrom langgraph.graph import END, START, StateGraph\nfrom langgraph.types import Command\nfrom typing_extensions import Literal\n\n\nclass State(TypedDict):\n    node_name: str\n    foo: str\n\n\ndef subgraph_node_1(state: State) -> Command[Literal[\"subgraph_node_2\"]]:\n    return Command(\n        goto=\"subgraph_node_2\",\n        update={\n            \"node_name\": \"subgraph_node_1\",\n            \"foo\": \"Update at subgraph_node_1!\",\n        },\n    )\n\n\ndef subgraph_node_2(state: State) -> Command:\n    return Command(\n        goto=\"node_3\",\n        update={\"node_name\": \"subgraph_node_2\"},\n        graph=Command.PARENT,\n    )\n\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_2\", END)\nsubgraph = subgraph_builder.compile()\n\n\n# Define main graph\ndef node_1(state: State) -> Command[Literal[\"node_2\"]]:\n    return Command(\n        goto=\"node_2\",\n        update={\"node_name\": \"node_1\"},\n    )\n\n\ndef node_3(state: State) -> Command[Literal[\"__end__\"]]:\n    return Command(\n        goto=END,\n        update={\"node_name\": \"node_3\"},\n    )\n\n\nmain_builder = StateGraph(State)\nmain_builder.add_node(\"node_1\", node_1)\nmain_builder.add_node(\"node_2\", subgraph)\nmain_builder.add_node(\"node_3\", node_3)\nmain_builder.add_edge(START, \"node_1\")\nmain_builder.add_edge(\"node_2\", \"node_3\")\nmain_graph = main_builder.compile()\n\n\n# Build subgraph\nwith open(\"graph.md\", \"w\") as file:\n    file.write(f\"\\n{main_graph.get_graph(xray=1).draw_mermaid()}\")\n\ninitial = {\"node_name\": [\"__start__\"]}\nfor chunk in main_graph.stream(initial, stream_mode=\"values\", subgraphs=True):\n    print(chunk)\nError Message and Stack Trace (if applicable)\n\nDescription\nWhile using the LangGraph library and attempting stream execution with a Command within a subgraph, I encountered the following two issues:\n\nPart of the subgraph's state is not passed to the parent graph.\n\n'foo': 'Update at subgraph_node_1!' is not displayed in the parent graph.\n\n\nThe output of the subgraph's last node is not displayed.\n\nThe execution result at subgraph_node_2 is not shown.\n\n\n\n\n\u3010Output\u3011\n((), {'node_name': ['__start__']})\n((), {'node_name': 'node_1'})\n(('node_2:d41305f4-c2ba-78d9-5d9b-ed1c6b8549da',), {'node_name': 'node_1'})\n(('node_2:d41305f4-c2ba-78d9-5d9b-ed1c6b8549da',), {'node_name': 'subgraph_node_1', 'foo': 'Update at subgraph_node_1!'})\n((), {'node_name': 'subgraph_node_2'})\n((), {'node_name': 'node_3'})\n\n\nWhen not using Command, the result was as expected.\n\u3010Expected Output\u3011\n((), {'node_name': ['__start__']})\n((), {'node_name': 'node_1'})\n(('node_2:cd87a0ec-b602-da30-ffca-48950974937f',), {'node_name': 'node_1'})\n(('node_2:cd87a0ec-b602-da30-ffca-48950974937f',), {'node_name': 'subgraph_node_1', 'foo': 'Update at subgraph_node_1!'})\n(('node_2:cd87a0ec-b602-da30-ffca-48950974937f',), {'node_name': 'subgraph_node_2', 'foo': 'Update at subgraph_node_1!'})\n((), {'node_name': 'subgraph_node_2', 'foo': 'Update at subgraph_node_1!'})\n((), {'node_name': 'node_3', 'foo': 'Update at subgraph_node_1!'})\n\n\u3010Expected Output Code\u3011\nfrom typing import TypedDict\n\nfrom langgraph.graph import END, START, StateGraph\n\n\nclass State(TypedDict):\n    node_name: str\n    foo: str\n\n\ndef subgraph_node_1(state: State):\n    return {\n        \"node_name\": \"subgraph_node_1\",\n        \"foo\": \"Update at subgraph_node_1!\",\n    }\n\n\ndef subgraph_node_2(state: State):\n    return {\"node_name\": \"subgraph_node_2\"}\n\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph_builder.add_edge(\"subgraph_node_2\", END)\nsubgraph = subgraph_builder.compile()\n\n\n# Define main graph\ndef node_1(state: State):\n    return {\"node_name\": \"node_1\"}\n\n\ndef node_3(state: State):\n    return {\"node_name\": \"node_3\"}\n\n\nmain_builder = StateGraph(State)\nmain_builder.add_node(\"node_1\", node_1)\nmain_builder.add_node(\"node_2\", subgraph)\nmain_builder.add_node(\"node_3\", node_3)\nmain_builder.add_edge(START, \"node_1\")\nmain_builder.add_edge(\"node_1\", \"node_2\")\nmain_builder.add_edge(\"node_2\", \"node_3\")\nmain_builder.add_edge(\"node_3\", END)\nmain_graph = main_builder.compile()\n\n\n# Build subgraph\nwith open(\"graph.md\", \"w\") as file:\n    file.write(f\"```mermaid\\n{main_graph.get_graph(xray=1).draw_mermaid()}```\")\n\ninitial = {\"node_name\": [\"__start__\"]}\nfor chunk in main_graph.stream(initial, stream_mode=\"values\", subgraphs=True):\n    print(chunk)\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #54~20.04.1-Ubuntu SMP Fri Oct 6 22:04:33 UTC 2023\nPython Version:  3.11.9 | packaged by conda-forge | (main, Apr 19 2024, 18:36:13) [GCC 12.3.0]\n\nPackage Information\n\nlangchain_core: 0.3.31\nlangchain: 0.3.14\nlangsmith: 0.2.3\nlangchain_aws: 0.2.3\nlangchain_openai: 0.2.4\nlangchain_text_splitters: 0.3.5\nlanggraph_sdk: 0.1.43\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.10\nasync-timeout: Installed. No version info available.\nboto3: 1.35.37\nhttpx: 0.28.1\njsonpatch: 1.33\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.53.0\norjson: 3.10.12\npackaging: 23.2\npydantic: 2.10.3\nPyYAML: 6.0.1\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.36\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\n", "created_at": "2025-01-21", "closed_at": "2025-01-31", "labels": ["investigate"], "State": "closed", "Author": "yamato0811"}
{"issue_number": 3104, "issue_title": "Confusing behavior when using `add_conditional_edges` with `BaseModel`", "issue_body": "Discussed in #2226\n\nOriginally posted by observerw October 30, 2024\nLet's say we have the following graph:\nclass State(TypedDict):\n    a: int\n\n\nclass NodeAInput(State):\n    private_b: int | None\n\n\nclass NodeAOutput(TypedDict):\n    private_a: int\n\n\nclass NodeBInput(State):\n    private_a: int\n\n\nclass NodeBOutput(TypedDict):\n    private_b: int\n\n\ndef node_a(input: NodeAInput) -> NodeAOutput:\n    return NodeAOutput(private_a=1)\n\n\ndef node_b(input: NodeBInput) -> NodeBOutput:\n    return NodeBOutput(private_b=2)\n\n# !!! Confusing behavior happens here\ndef node_b_edge(input: Any):\n    print(f\"input type: {type(input)}, value: {input}\")\n\n    return END\n\n\ngraph = StateGraph(State)\ngraph.add_node(node_a)\ngraph.add_node(node_b)\n\ngraph.add_edge(\"node_a\", \"node_b\")\ngraph.add_conditional_edges(\"node_b\", node_b_edge)\n\ngraph.set_entry_point(\"node_a\")\n\ngraph = graph.compile()\n\ngraph.invoke(State(a=1))\nThe output is:\ninput type: <class '__main__.NodeBInput'>, value: a=1 private_a=1\nAfter I added node_b_edge after node_b, I was expecting this function to accept a State with a private_b field, or at least a complete State. However, the function actually accepts a parameter of type NodeBInput instead of State! This is very confusing and results in an error if I wish to use the private_b field to determine the output of node_b_edge.\nNonetheless, when I ditched the BaseModel and used TypedDict, the run results were:\ninput type: <class 'dict'>, value: {'private_b': 2, 'a': 1, 'private_a': 1}\nWell, at least I was able to access the private_b, So I guess the problems here associated with the use of BaseModel.\nIs this behavior expected? How can I access private_b when using BaseModel?", "created_at": "2025-01-20", "closed_at": "2025-01-23", "labels": ["question"], "State": "closed", "Author": "observerw"}
{"issue_number": 3102, "issue_title": "\u8def\u7531\u51fd\u6570\u6b63\u5e38 \u4f46\u662f\u56fe\u7684\u8fd0\u884c\u987a\u5e8f\u51fa\u9519", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nworkflow.add_conditional_edges(\n    \"tools\",\n    check_ToM,\n    {\n        # \u8fbe\u5230\u6307\u5b9a\u6b21\u6570\uff0c\u66f4\u65b0ToM\n        \"tom\": \"tom\",\n        # \u8fd4\u56deagent\n        \"agent\": \"agent\",\n    },\n)\nError Message and Stack Trace (if applicable)\n\nDescription\n\u3001\n\u6211\u7684\u8def\u7531\u51fd\u6570\u90fd\u8f93\u51fa\u4e86\u6b63\u786e\u7684\u8def\u7531\u4e3a\u4ec0\u4e48\u6ca1\u6709\u6309\u7167\u9884\u671f\u7684\u987a\u5e8f\u8fd0\u884c\uff1f\nSystem Info\n\u56e0\u4e3a\u9519\u8bef\u7684\u6a21\u578b\u8fd0\u884c\u987a\u5e8f\u5bfc\u81f4\u4e86\u9519\u8bef\u5982\u4e0b\nValueError: {'message': 'Provider returned error', 'code': 400, 'metadata': {'raw': {'__kind': 'OK', 'data': '{\\n  \"error\": {\\n    \"message\": \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_Rad8sO1nwvG0fRKww4DlSJBM\",\\n    \"type\": \"invalid_request_error\",\\n    \"param\": \"messages.[15].role\",\\n    \"code\": null\\n  }\\n}'}, 'provider_name': 'OpenAI'}}", "created_at": "2025-01-19", "closed_at": "2025-01-31", "labels": ["invalid"], "State": "closed", "Author": "The-uyu"}
{"issue_number": 3100, "issue_title": "react-agent-python template does not work", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nlanggraph new path/to/your/app --template react-agent-python\nError Message and Stack Trace (if applicable)\nFile \"C:\\Python311\\Lib\\site-packages\\starlette\\routing.py\", line 693, in lifespan\n    async with self.lifespan_context(app) as maybe_state:\n  File \"C:\\Python311\\Lib\\contextlib.py\", line 204, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\site-packages\\langgraph_api\\lifespan.py\", line 30, in lifespan\n    await collect_graphs_from_env(True)\n  File \"C:\\Python311\\Lib\\site-packages\\langgraph_api\\graph.py\", line 257, in collect_graphs_from_env\n    graph = await run_in_executor(None, _graph_from_spec, spec)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\config.py\", line 588, in run_in_executor\n    return await asyncio.get_running_loop().run_in_executor(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\concurrent\\futures\\thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\config.py\", line 579, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Python311\\Lib\\site-packages\\langgraph_api\\graph.py\", line 295, in _graph_from_spec\n    modspec.loader.exec_module(module)\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"D:\\projects\\forexdropoff\\react-agent\\./src/react_agent/graph.py\", line 14, in <module>\n    from react_agent.configuration import Configuration\nModuleNotFoundError: No module named 'react_agent'\nCould not import python module for graph:\nGraphSpec(id='agent', path='./src/react_agent/graph.py', module=None, variable='graph', config=None)\nThis error likely means you haven't installed your project and its dependencies yet. Before running the server, install your project:\n\nIf you are using requirements.txt:\npython -m pip install -r requirements.txt\n\nIf you are using pyproject.toml or setuptools:\npython -m pip install -e .\n\nMake sure to run this command from your project's root directory (where your setup.py or pyproject.toml is located)\n [uvicorn.error] api_variant=local_dev\n2025-01-19T02:33:06.962480Z [error    ] Application startup failed. Exiting. [uvicorn.error] api_variant=local_dev\nDescription\nI am trying to run the template on the local server.\nDespite following the docs the same error is triggering. I have tried to fix the packages but have not been able to go around.\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.22631\nPython Version:  3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.3.30\nlangchain: 0.3.14\nlangchain_community: 0.3.14\nlangsmith: 0.2.11\nlangchain_anthropic: 0.3.3\nlangchain_fireworks: 0.2.6\nlangchain_openai: 0.3.0\nlangchain_text_splitters: 0.3.5\nlanggraph_sdk: 0.1.51\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.11\nanthropic: 0.43.1\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.7\ndefusedxml: 0.7.1\nfireworks-ai: 0.15.11\nhttpx: 0.28.1\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.59.8\norjson: 3.10.15\npackaging: 24.2\npydantic: 2.10.5\npydantic-settings: 2.7.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.37\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\nzstandard: Installed. No version info available.\n", "created_at": "2025-01-19", "closed_at": "2025-01-27", "labels": ["question"], "State": "closed", "Author": "viren-vii"}
{"issue_number": 3099, "issue_title": "DOC: Should the Quickstart be upgraded from model=\"claude-3-5-sonnet-20240620\"?", "issue_body": "Issue with current documentation:\nI am a beginner to both LangGraph and Anthropic/Claude. Please excuse my n00b question if this is off base or I could raise this in a more appropriate forum.\nCurrently (January 18, 2025), the LangGraph Quickstart tutorial at https://langchain-ai.github.io/langgraph/tutorials/introduction/ uses\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\nIs it time to upgrade this to a newer model, such as\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-20241022\") ?\nI asked Perplexity Pro, which told me that the 20241022 model is now probably better for this kind of activity.\nIdea or request for content:\nNo response", "created_at": "2025-01-19", "closed_at": "2025-01-20", "labels": [], "State": "closed", "Author": "royseto"}
{"issue_number": 3098, "issue_title": "DOC: minimal functioning example of agent inbox usage", "issue_body": "Issue with current documentation:\nHello\nI've been trying desperately for roughly the last 12 hours to get Agent Inbox working. Nothing I've tried actually succeeds at making a message show up in the inbox.\nCan someone please provide a minimal but working repository to plug into langgraph, that I can ping via the langgraph server and just see messages show up?\nIdea or request for content:\nNo response", "created_at": "2025-01-18", "closed_at": "2025-01-28", "labels": [], "State": "closed", "Author": "SethTurin"}
{"issue_number": 3097, "issue_title": "DeepSeek V3 API Call Does Not Stop Automatically in LangGraph with ChatOpenAI()", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n!pip install -qU langchain-openai\n!pip install -qU langchain_community\n!pip install -qU langchain_experimental\n!pip install -qU langgraph\n!pip install -qU duckduckgo-search\n\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_community.tools import DuckDuckGoSearchRun\nfrom kaggle_secrets import UserSecretsClient\n\n# the code is run on the kaggle notebook\nllm_api_key = UserSecretsClient().get_secret(\"api-key-deepseek\")\nmodel = ChatOpenAI(model=\"deepseek-chat\", temperature=0, openai_api_key=llm_api_key, openai_api_base='https://api.deepseek.com')\n\n# Initialize the search tool\nsearch = DuckDuckGoSearchRun()\n\ndef search_web(query: str) -> str:\n    '''Perform a web search using DuckDuckGo and return the results.'''\n    return search.invoke(query)\n\ntools = [search_web]\ngraph = create_react_agent(model, tools=tools)\n\ninputs = {\"messages\": [(\"user\", \"Search for the latest news on AI advancements\")]}\nfor s in graph.stream(inputs, stream_mode=\"values\"):\n    message = s[\"messages\"][-1]\n    if isinstance(message, tuple):\n        print(message)\n    else:\n        message.pretty_print()\nError Message and Stack Trace (if applicable)\n**Actual Behavior:**  \n\nThe API call continues indefinitely, making repeated tool calls and generating new search queries without stopping. This results in an infinite loop of search queries and responses.\n\n**Example Output:**\n\n\n================================ Human Message =================================\n\nSearch for the latest news on AI advancements\n================================== Ai Message ==================================\nTool Calls:\n  search_web (call_0_bbac5567-4a8c-4e13-8752-ad37a91f6f27)\n Call ID: call_0_bbac5567-4a8c-4e13-8752-ad37a91f6f27\n  Args:\n    query: latest news on AI advancements 2023\n================================= Tool Message =================================\nName: search_web\n\nWith broad AI comes broad risks in everything from misinformation to AI-related privacy risks, failures, and mistakes. 2023 saw an increasing flow of regulation, from the United States AI Bill of ... 2022 was the year that generative artificial intelligence (AI) exploded into the public consciousness, and 2023 was the year it began to take root in the business world. 2024 thus stands to be a pivotal year for the future of AI, as researchers and enterprises seek to establish how this evolutionary leap in technology can be most practically integrated into our everyday lives. While overall AI private investment decreased in 2023, funding for generative AI sharply increased. The sector attracted $25.2 billion last year, nearly nine times the investment of 2022 and about 30 times the amount in 2019. Generative AI accounted for over a quarter of all AI-related private investment in 2023. Artificial intelligence. Download RSS feed: News Articles / In the Media / Audio. Displaying 1 - 15 of 1293 news articles related to this topic. Show: News Articles. In the Media. Audio. Explained: Generative AI's environmental impact. Rapid development and deployment of powerful generative AI models comes with environmental consequences ... GPU chips play a pivotal role in enhancing overall performance for genAI applications, underscoring their significance for future advancements. Defining AI trends for 2023. Amid the myriad of AI trends and themes that characterized 2023, four major themes emerged, shaping the narrative and capturing widespread attention:\n================================== Ai Message ==================================\nTool Calls:\n  search_web (call_0_744fb5ec-24f1-4e8d-b3d0-5704b72b60a7)\n Call ID: call_0_744fb5ec-24f1-4e8d-b3d0-5704b72b60a7\n  Args:\n    query: latest AI advancements 2023\n================================= Tool Message =================================\nName: search_web\n\nUncover the transformative AI achievements of 2023, with a focus on NLP, computer vision, robotics, voice recognition, and HCI. This comprehensive overview highlights the latest advancements and their far-reaching impacts on technology, industry, and society. Top AI achievements of 2023. AI achievements in 2023 were quite phenomenal. March 30, 2023: GitHub presents AutoGTP based on ChatGPT-4.; May 12, 2023: OpenAI releases ChatGPT Plugins; May 25, 2023: Nvidia's stock price tripled in less than eight months, surging by 27% overnight and approaching a market cap of $1 trillion, driven by the skyrocketing demand for GPUs due to the growing interest in AI.; May 25, 2023: Google launches Google Search Generative Experience. The year 2023 marked a significant period in the evolution of artificial intelligence (AI), characterized by groundbreaking advancements across various domains. From the development of ... 2022 was the year that generative artificial intelligence (AI) exploded into the public consciousness, and 2023 was the year it began to take root in the business world. 2024 thus stands to be a pivotal year for the future of AI, as researchers and enterprises seek to establish how this evolutionary leap in technology can be most practically integrated into our everyday lives. AI Highlights from May 2023. Advancements in robotics took center stage in May 2023 as an AI system enabled robots to autonomously conduct scientific experiments. While Sanctuary AI's Phoenix Robot became the world's first commercially available general-purpose humanoid robot, Tesla's Optimus Bot also showed significant progress. OpenAI ...\n================================== Ai Message ==================================\nTool Calls:\n  search_web (call_0_ba4b87e9-2096-49f2-840c-f1790cacc0f4)\n Call ID: call_0_ba4b87e9-2096-49f2-840c-f1790cacc0f4\n  Args:\n    query: AI advancements 2023\n================================= Tool Message =================================\nName: search_web\n\nThe report, which is in its seventh edition, covers trends such as technical advancements in AI, public perceptions of the technology and the geopolitical dynamics surrounding its development. Emerging Technologies ... In 2023, 25 AI-related regulations were enacted in the US, growing the total number by 56.3%. Compare that to 2016, when just ... 2023 AI Recap: Innovations in AI Technology. OpenAI's GPT-4: Revolutionized user interactions with AI through advanced natural language understanding, setting a new standard in generative models.; Multimodal Capabilities: GPT-4's ability to process various types of data, including text and images, marked a significant step towards more versatile AI systems. This year's AI Index \u2014 a 500-page report tracking 2023's worldwide trends in AI \u2014 is out.. The index is an independent initiative at the Stanford Institute for Human-Centered Artificial Intelligence (HAI), led by the AI Index Steering Committee, an interdisciplinary group of experts from across academia and industry. This year's report covers the rise of multimodal foundation models ... 2022 was the year that generative artificial intelligence (AI) exploded into the public consciousness, and 2023 was the year it began to take root in the business world. 2024 thus stands to be a pivotal year for the future of AI, as researchers and enterprises seek to establish how this evolutionary leap in technology can be most practically integrated into our everyday lives. GPU chips play a pivotal role in enhancing overall performance for genAI applications, underscoring their significance for future advancements. Defining AI trends for 2023. Amid the myriad of AI trends and themes that characterized 2023, four major themes emerged, shaping the narrative and capturing widespread attention:\n================================== Ai Message ==================================\nTool Calls:\n  search_web (call_0_9d944700-80fc-4092-a60c-d75e36d351d1)\n Call ID: call_0_9d944700-80fc-4092-a60c-d75e36d351d1\n  Args:\n    query: AI trends 2023\n================================= Tool Message =================================\nName: search_web\n\nThis year's AI Index \u2014 a 500-page report tracking 2023's worldwide trends in AI \u2014 is out.. The index is an independent initiative at the Stanford Institute for Human-Centered Artificial Intelligence (HAI), led by the AI Index Steering Committee, an interdisciplinary group of experts from across academia and industry. This year's report covers the rise of multimodal foundation models ... Defining AI trends for 2023. Amid the myriad of AI trends and themes that characterized 2023, four major themes emerged, shaping the narrative and capturing widespread attention: Business adoption: Although consumer genAI applications grabbed headlines, 2023 also saw increased business experimentation. Enterprise adoption, startup activity and ... Get 90+ pages of charts and data detailing the latest venture trends in AI. First name. Last name. Email. ... In 2023, AI startups raised $42.5B across 2,500 equity rounds. Although down 10% year-over-year (YoY), AI funding fell far less than broader venture funding (-42% in 2023). AI deal volume decreased by 24% YoY \u2014 also less than the ... 2022 was the year that generative artificial intelligence (AI) exploded into the public consciousness, and 2023 was the year it began to take root in the business world. 2024 thus stands to be a pivotal year for the future of AI, as researchers and enterprises seek to establish how this evolutionary leap in technology can be most practically integrated into our everyday lives. March 30, 2023: GitHub presents AutoGTP based on ChatGPT-4.; May 12, 2023: OpenAI releases ChatGPT Plugins; May 25, 2023: Nvidia's stock price tripled in less than eight months, surging by 27% overnight and approaching a market cap of $1 trillion, driven by the skyrocketing demand for GPUs due to the growing interest in AI.; May 25, 2023: Google launches Google Search Generative Experience.\n================================== Ai Message ==================================\nTool Calls:\n  search_web (call_0_3a3d86af-8b05-482a-ac02-4fc1c4437cfc)\n Call ID: call_0_3a3d86af-8b05-482a-ac02-4fc1c4437cfc\n  Args:\n    query: AI regulations 2023\n================================= Tool Message =================================\nName: search_web\n\nNew Framework Advances AI Innovation While Protecting U.S. National Security. Washington, D.C. \u2014 Today, the Department of Commerce's Bureau of Industry and Security (BIS) announced controls on advanced computing chips and certain closed artificial intelligence (AI) model weights, alongside new license exceptions and updates to the Data Center Validated End User (VEU) authorization. Section 1. Purpose. Artificial intelligence (AI) is a defining technology of our era. ... and Executive Order 14110 of November 1, 2023 (Safe, Secure, and Trustworthy Development and Use of ... In October 2023, President Joe Biden issued a broad executive order on AI (AI Order), which the administration touted as a vehicle to establish AI safety and security standards while protecting privacy, advancing civil rights and promoting innovation. However, most of the AI Order was a series of directives to various federal agencies to study ... In October 2023, President Joe Biden issued a broad executive order on AI ... we expect to see states take an even more active role in enacting state-specific AI regulations. Monitor Legal and Regulatory Developments: Stay informed about new state and federal AI regulations, along with the developing case law related to AI and ERISA litigation. Establish a process for ...\n================================== Ai Message ==================================\nTool Calls:\n  search_web (call_0_10269960-464f-4abd-a458-b863da1710b4)\n Call ID: call_0_10269960-464f-4abd-a458-b863da1710b4\n  Args:\n    query: AI investment trends 2023\n  search_web (call_1_97eabda5-9144-4a28-93f9-e21ec851b5ba)\n Call ID: call_1_97eabda5-9144-4a28-93f9-e21ec851b5ba\n  Args:\n    query: AI private investment 2023\n================================= Tool Message =================================\nName: search_web\n\nPrivate equity and venture capital firms poured more than twice as much money into generative AI companies in 2023 compared with the prior year, and dealmaking got off to a fast start in 2024. ... Incera said. Private equity-backed investment peaked at $927.7 million in the third quarter of 2023, up from $121.5 million recorded in the prior ... The volume of private investment in artificial intelligence (AI) worldwide in 2023 had its largest share concentrated in the United States of America, with approximately 67.2 billion U.S. Generative AI accounted for over a quarter of all AI-related private investments in 2023. U.S. Wins $$ Race. And again, in 2023 the United States dominates in AI private investment. In 2023, the $67.2 billion invested in the U.S. was roughly 8.7 times greater than the amount invested in the next highest country, China, and 17.8 times the amount ... The graph below shows the yearly number of private equity platform and add-on investments in AI. Quick stats. 315 Private equity firms with current investments in the Artificial Intelligence and Machine Learning industry. 573 current private equity investments in AI/ML companies (2013 - 2023, 52 exits) Download our free list As from 2013 to 2023, the sum of private investments in artificial intelligence (AI) worldwide has had its largest share coming from the United States of America, accounting for over 336 billion U.S.\n================================== Ai Message ==================================\nTool Calls:\n  search_web (call_0_1667e946-5452-48ab-b5e2-d6e3fb9a526b)\n Call ID: call_0_1667e946-5452-48ab-b5e2-d6e3fb9a526b\n  Args:\n    query: AI investment trends 2023\n  search_web (call_1_d7bdf09b-5892-4e60-91a8-63a4b898a209)\n Call ID: call_1_d7bdf09b-5892-4e60-91a8-63a4b898a209\n  Args:\n    query: AI private investment 2023\n================================= Tool Message =================================\nName: search_web\n\nPrivate equity and venture capital firms poured more than twice as much money into generative AI companies in 2023 compared with the prior year, and dealmaking got off to a fast start in 2024. ... Incera said. Private equity-backed investment peaked at $927.7 million in the third quarter of 2023, up from $121.5 million recorded in the prior ... The volume of private investment in artificial intelligence (AI) worldwide in 2023 had its largest share concentrated in the United States of America, with approximately 67.2 billion U.S. Generative AI accounted for over a quarter of all AI-related private investments in 2023. U.S. Wins $$ Race. And again, in 2023 the United States dominates in AI private investment. In 2023, the $67.2 billion invested in the U.S. was roughly 8.7 times greater than the amount invested in the next highest country, China, and 17.8 times the amount ... Figure 3 shows that no country, at least on a private investment basis, is spending anything close to what the U.S. is spending, which likely means it will be difficult to challenge U.S. leadership in this area. Figure 3: Private Investment in AI by Geographic Area, 2023. 2023 Was a Year of Building AI Applications As from 2013 to 2023, the sum of private investments in artificial intelligence (AI) worldwide has had its largest share coming from the United States of America, accounting for over 336 billion U.S.\n================================== Ai Message ==================================\nTool Calls:\n  search_web (call_0_cb9e01b0-7850-4038-a727-aa7086c97191)\n Call ID: call_0_cb9e01b0-7850-4038-a727-aa7086c97191\n  Args:\n    query: AI investment trends 2023\n  search_web (call_1_d658f5fe-7db0-42e1-81a6-d003f231efe6)\n Call ID: call_1_d658f5fe-7db0-42e1-81a6-d003f231efe6\n  Args:\n    query: AI private investment 2023\n  search_web (call_2_7ccf4fdf-3921-41b9-b805-9157d25998be)\n Call ID: call_2_7ccf4fdf-3921-41b9-b805-9157d25998be\n  Args:\n    query: AI regulations 2023\n  search_web (call_3_df9e132a-1328-4db8-8984-3051c59dcee2)\n Call ID: call_3_df9e132a-1328-4db8-8984-3051c59dcee2\n  Args:\n    query: AI advancements 2023\n  search_web (call_4_46a83a96-205c-47c0-8c67-94d07ac91894)\n Call ID: call_4_46a83a96-205c-47c0-8c67-94d07ac91894\n  Args:\n    query: latest AI advancements 2023\n  search_web (call_5_d506554b-9025-407b-8cdc-9154d547ab88)\n Call ID: call_5_d506554b-9025-407b-8cdc-9154d547ab88\n  Args:\n    query: latest news on AI advancements 2023\n================================= Tool Message =================================\nName: search_web\n\nWith broad AI comes broad risks in everything from misinformation to AI-related privacy risks, failures, and mistakes. 2023 saw an increasing flow of regulation, from the United States AI Bill of ... Artificial intelligence. Download RSS feed: News Articles / In the Media / Audio. Displaying 1 - 15 of 1293 news articles related to this topic. Show: News Articles. In the Media. Audio. Explained: Generative AI's environmental impact. Rapid development and deployment of powerful generative AI models comes with environmental consequences ... March 30, 2023: GitHub presents AutoGTP based on ChatGPT-4.; May 12, 2023: OpenAI releases ChatGPT Plugins; May 25, 2023: Nvidia's stock price tripled in less than eight months, surging by 27% overnight and approaching a market cap of $1 trillion, driven by the skyrocketing demand for GPUs due to the growing interest in AI.; May 25, 2023: Google launches Google Search Generative Experience. GPU chips play a pivotal role in enhancing overall performance for genAI applications, underscoring their significance for future advancements. Defining AI trends for 2023. Amid the myriad of AI trends and themes that characterized 2023, four major themes emerged, shaping the narrative and capturing widespread attention: AI Highlights from May 2023. Advancements in robotics took center stage in May 2023 as an AI system enabled robots to autonomously conduct scientific experiments. While Sanctuary AI's Phoenix Robot became the world's first commercially available general-purpose humanoid robot, Tesla's Optimus Bot also showed significant progress. OpenAI ...\nDescription\nDescribe the bug\nWhen using ChatOpenAI() in LangChain with a custom tool (e.g., DuckDuckGoSearchRun), the API call does not terminate automatically after completing the task. Instead, it continues to make repeated tool calls indefinitely, leading to an infinite loop of search queries and responses.\nExpected behavior\nThe API call should terminate after completing the task (e.g., retrieving search results for the query) and return the final response.\nSystem Info\nEnvironment:\n\nPython 3.x (Kaggle Notebook)\nLibraries: langchain-openai, langchain-community, langchain_experimental, langgraph\nModel: deepseek-chat (via DeepSeek API)\n", "created_at": "2025-01-18", "closed_at": "2025-01-20", "labels": [], "State": "closed", "Author": "ksmooi"}
{"issue_number": 3096, "issue_title": "Multi-agent supervisor", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nI add agent, run shell \nclass Router(TypedDict):\n    \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n    next: Literal[\"researcher\", \"coder\", \"shell\", \"FINISH\"]\n\n    def supervisor_node(self, state: State) -> Command[Literal[\"researcher\", \"coder\", \"shell\", \"__end__\"]]:\n        messages = [\n                       {\"role\": \"system\", \"content\": self.system_prompt},\n                   ] + state[\"messages\"]\n        response = self.llm.with_structured_output(Router).invoke(messages)\n        goto = response[\"next\"]\n        if goto == \"FINISH\":\n            goto = END\n\n        mylogging.vip(response)\n        return Command(goto=goto, update={\"next\": goto})\n\nQuestion: check  Memory Usage\nTASK can't FINISH, it keeps executing the shell, I've checked response = self.llm.invoke(messages)\nresponse is FINISH. \nbut self.llm.with_structured_output(Router).invoke(messages) response  is {'next': 'shell'}, this is bug.\n\n\nAgent: [2025-01-18 14:04:30,491]-VIP-[agentSupervisor.py:41]: {'next': 'shell'}\n[2025-01-18 14:04:30,498]-VIP-[supervisorProcess.py:97]: shell->supervisor\nStarting tool: shell_tool with inputs: {'command': 'free -m'}\n[2025-01-18 14:04:31,890]-INFO-[tools.py:24]: executing shell output:               total        used        free      shared  buff/cache   available\nMem:           32064        1075       31085           3         298       30988\nSwap:           8192           0        8192\n\n\nDone tool: shell_tool\nThe memory usage is as follows:\n- Total memory: 32064 MB\n- Used memory: 1075 MB\n- Free memory: 31085 MB\n- Shared memory: 3 MB\n- Buffer/cache memory: 298 MB\n- Available memory: 30988 MB\n\nThe swap memory usage is as follows:\n- Total swap: 8192 MB\n- Used swap: 0 MB\n- Free swap: 8192 MB[2025-01-18 14:04:35,914]-VIP-[agentSupervisor.py:41]: {'next': 'shell'}\n[2025-01-18 14:04:35,940]-VIP-[supervisorProcess.py:97]: shell->supervisor\nStarting tool: shell_tool with inputs: {'command': 'free -m'}\n[2025-01-18 14:04:37,599]-INFO-[tools.py:24]: executing shell output:               total        used        free      shared  buff/cache   available\nMem:           32064        1077       31083           3         298       30986\nSwap:           8192           0        8192\n\n\nDone tool: shell_tool\nThe memory usage is as follows:\n- Total memory: 32064 MB\n- Used memory: 1077 MB\n- Free memory: 31083 MB\n- Shared memory: 3 MB\n- Buffer/cache memory: 298 MB\n- Available memory: 30986 MB\n\nThe swap memory usage is as follows:\n- Total swap: 8192 MB\n- Used swap: 0 MB\n- Free swap: 8192 MB[2025-01-18 14:04:40,450]-VIP-[agentSupervisor.py:41]: {'next': 'shell'}\n[2025-01-18 14:04:40,504]-VIP-[supervisorProcess.py:97]: shell->supervisor\n[2025-01-18 14:04:41,771]-INFO-[tools.py:24]: executing shell output:               total        used        free      shared  buff/cache   available\nMem:           32064        1079       31081           3         298       30985\nSwap:           8192           0        8192\n\n\nStarting tool: shell_tool with inputs: {'command': 'free -m'}\nDone tool: shell_tool\nThe memory usage is as follows:\n- Total memory: 32064 MB\n- Used memory: 1079 MB\n- Free memory: 31081 MB\n- Shared memory: 3 MB\n- Buffer/cache memory: 298 MB\n- Available memory: 30985 MB\n\nThe swap memory usage is as follows:\n- Total swap: 8192 MB\n- Used swap: 0 MB\n- Free swap: 8192 MB[2025-01-18 14:04:44,322]-VIP-[agentSupervisor.py:41]: {'next': 'shell'}\n[2025-01-18 14:04:44,500]-VIP-[supervisorProcess.py:97]: shell->supervisor\nStarting tool: shell_tool with inputs: {'command': 'free -m'}\n[2025-01-18 14:04:45,320]-INFO-[tools.py:24]: executing shell output:               total        used        free      shared  buff/cache   available\nMem:           32064        1079       31081           3         298       30984\nSwap:           8192           0        8192\n\n\nDone tool: shell_tool\n[2025-01-18 14:04:45,845]-INFO-[_base_client.py:1087]: Retrying request to /chat/completions in 0.483649 seconds\nThe memory usage is as follows:\n- Total memory: 32064 MB\n- Used memory: 1079 MB\n- Free memory: 31081 MB\n- Shared memory: 3 MB\n- Buffer/cache memory: 298 MB\n- Available memory: 30984 MB\n\nThe swap memory usage is as follows:\n- Total swap: 8192 MB\n- Used swap: 0 MB\n- Free swap: 8192 MB[2025-01-18 14:04:49,860]-VIP-[agentSupervisor.py:41]: {'next': 'shell'}\n[2025-01-18 14:04:50,079]-VIP-[supervisorProcess.py:97]: shell->supervisor\n[2025-01-18 14:04:50,978]-INFO-[tools.py:24]: executing shell output:               total        used        free      shared  buff/cache   available\nMem:           32064        1080       31080           3         298       30983\nSwap:           8192           0        8192\nError Message and Stack Trace (if applicable)\n[2025-01-18 14:04:35,940]-VIP-[supervisorProcess.py:97]: shell->supervisor\nStarting tool: shell_tool with inputs: {'command': 'free -m'}\n[2025-01-18 14:04:37,599]-INFO-[tools.py:24]: executing shell output:               total        used        free      shared  buff/cache   available\nMem:           32064        1077       31083           3         298       30986\nSwap:           8192           0        8192\n\n\nDone tool: shell_tool\nThe memory usage is as follows:\n- Total memory: 32064 MB\n- Used memory: 1077 MB\n- Free memory: 31083 MB\n- Shared memory: 3 MB\n- Buffer/cache memory: 298 MB\n- Available memory: 30986 MB\n\nThe swap memory usage is as follows:\n- Total swap: 8192 MB\n- Used swap: 0 MB\n- Free swap: 8192 MB[2025-01-18 14:04:40,450]-VIP-[agentSupervisor.py:41]: {'next': 'shell'}\n[2025-01-18 14:04:40,504]-VIP-[supervisorProcess.py:97]: shell->supervisor\n[2025-01-18 14:04:41,771]-INFO-[tools.py:24]: executing shell output:               total        used        free      shared  buff/cache   available\nMem:           32064        1079       31081           3         298       30985\nSwap:           8192           0        8192\n\n\nStarting tool: shell_tool with inputs: {'command': 'free -m'}\nDone tool: shell_tool\nThe memory usage is as follows:\n- Total memory: 32064 MB\n- Used memory: 1079 MB\n- Free memory: 31081 MB\n- Shared memory: 3 MB\n- Buffer/cache memory: 298 MB\n- Available memory: 30985 MB\n\nThe swap memory usage is as follows:\n- Total swap: 8192 MB\n- Used swap: 0 MB\n- Free swap: 8192 MB[2025-01-18 14:04:44,322]-VIP-[agentSupervisor.py:41]: {'next': 'shell'}\n[2025-01-18 14:04:44,500]-VIP-[supervisorProcess.py:97]: shell->supervisor\nStarting tool: shell_tool with inputs: {'command': 'free -m'}\n[2025-01-18 14:04:45,320]-INFO-[tools.py:24]: executing shell output:               total        used        free      shared  buff/cache   available\nMem:           32064        1079       31081           3         298       30984\nSwap:           8192           0        8192\n\n\nDone tool: shell_tool\n[2025-01-18 14:04:45,845]-INFO-[_base_client.py:1087]: Retrying request to /chat/completions in 0.483649 seconds\nThe memory usage is as follows:\n- Total memory: 32064 MB\n- Used memory: 1079 MB\n- Free memory: 31081 MB\n- Shared memory: 3 MB\n- Buffer/cache memory: 298 MB\n- Available memory: 30984 MB\n\nThe swap memory usage is as follows:\n- Total swap: 8192 MB\n- Used swap: 0 MB\n- Free swap: 8192 MB[2025-01-18 14:04:49,860]-VIP-[agentSupervisor.py:41]: {'next': 'shell'}\n[2025-01-18 14:04:50,079]-VIP-[supervisorProcess.py:97]: shell->supervisor\n[2025-01-18 14:04:50,978]-INFO-[tools.py:24]: executing shell output:               total        used        free      shared  buff/cache   available\nMem:           32064        1080       31080           3         298       30983\nSwap:           8192           0        8192\nDescription\nQuestion: check  Memory Usage\nTASK can't FINISH, it keeps executing the shell, I've checked response = self.llm.invoke(messages)\nresponse is FINISH.\nSystem Info\nI add agent, run shell\nclass Router(TypedDict):\n\"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\nnext: Literal[\"researcher\", \"coder\", \"shell\", \"FINISH\"]\ndef supervisor_node(self, state: State) -> Command[Literal[\"researcher\", \"coder\", \"shell\", \"__end__\"]]:\n    messages = [\n                   {\"role\": \"system\", \"content\": self.system_prompt},\n               ] + state[\"messages\"]\n    response = self.llm.with_structured_output(Router).invoke(messages)\n    goto = response[\"next\"]\n    if goto == \"FINISH\":\n        goto = END\n\n    mylogging.vip(response)\n    return Command(goto=goto, update={\"next\": goto})\n\nQuestion: check  Memory Usage\nTASK can't FINISH, it keeps executing the shell, I've checked response = self.llm.invoke(messages)\nresponse is FINISH.\nbut self.llm.with_structured_output(Router).invoke(messages) response  is {'next': 'shell'}, this is bug.", "created_at": "2025-01-18", "closed_at": "2025-01-31", "labels": ["invalid"], "State": "closed", "Author": "jason571"}
{"issue_number": 3088, "issue_title": "Subgraph `checkpointer=True` does not work for async invoke", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import END, START, StateGraph\nfrom rich import get_console\nfrom typing_extensions import TypedDict\n\n\nclass SubGraphState(TypedDict, total=False):\n    run_idx: int\n    sub_counter: int\n\n\nasync def subgraph_accumulator(state: SubGraphState) -> SubGraphState:\n    get_console().print(\"---subgraph counter node---\")\n    get_console().print(f\"{state = }\")\n\n    sub_counter = state[\"sub_counter\"] + 1 if \"sub_counter\" in state else 0\n    return {\"sub_counter\": sub_counter}\n\n\nsub_graph = (\n    StateGraph(SubGraphState)\n    .add_node(subgraph_accumulator)\n    .add_edge(START, subgraph_accumulator.__name__)\n    .add_edge(subgraph_accumulator.__name__, END)\n    .compile(checkpointer=True)  # enable subgraph checkpointer for follow-up invokes\n)\nsub_graph.name = \"sub\"\n\n\nclass ParentGraphState(TypedDict, total=False):\n    run_idx: int\n    parent_counter: int\n\n\nasync def parent_graph_accumulator(state: ParentGraphState) -> ParentGraphState:\n    print(\"---parent counter node---\")\n    get_console().print(f\"{state = }\")\n    parent_counter = state[\"parent_counter\"] + 1 if \"parent_counter\" in state else 0\n\n    return {\n        \"parent_counter\": parent_counter,\n    }\n\n\nasync def check_after_sub_node(state: ParentGraphState) -> None:\n    print(\"---parent check after sub node---\")\n    get_console().print(f\"{state = }\")\n    pass\n\n\nparent_agent = (\n    StateGraph(ParentGraphState)\n    .add_node(parent_graph_accumulator)\n    .add_node(sub_graph)\n    .add_node(check_after_sub_node)\n    .add_edge(START, parent_graph_accumulator.__name__)\n    .add_edge(parent_graph_accumulator.__name__, sub_graph.get_name())\n    .add_edge(sub_graph.get_name(), check_after_sub_node.__name__)\n    .add_edge(check_after_sub_node.__name__, END)\n    .compile(checkpointer=MemorySaver())\n)\n\n\nasync def main():\n    config: RunnableConfig = {\"configurable\": {\"thread_id\": \"42\"}}\n\n    for i in range(3):\n        print(f\"---Run Index {i}---\")\n        async for event in parent_agent.astream(\n            ParentGraphState(run_idx=i),\n            config,\n            stream_mode=\"values\",\n            subgraphs=True,\n        ):\n            print(event)\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(main())\nError Message and Stack Trace (if applicable)\nNo errors.\nA output is attached.\n\nThe issue is\n- The subgraph counter should increase together with run_idx but it does not.\n\n\n---Run Index 0---\n((), {'run_idx': 0})\n---parent counter node---\nstate = {'run_idx': 0}\n((), {'run_idx': 0, 'parent_counter': 0})\n---subgraph counter node---\nstate = {'run_idx': 0}\n(('call_subgraph:240f14d1-f0c8-7f2f-018c-a10c655385cf',), {'run_idx': 0})\n(('call_subgraph:240f14d1-f0c8-7f2f-018c-a10c655385cf',), {'run_idx': 0, 'sub_counter': 0})\n---parent check after sub node---\nstate = {'run_idx': 0, 'parent_counter': 0}\n---Run Index 1---\n((), {'run_idx': 1, 'parent_counter': 0})\n---parent counter node---\nstate = {'run_idx': 1, 'parent_counter': 0}\n((), {'run_idx': 1, 'parent_counter': 1})\n---subgraph counter node---\nstate = {'run_idx': 1}\n(('call_subgraph:65507cf5-16ed-1a02-96cf-49a48899a65b',), {'run_idx': 1})\n(('call_subgraph:65507cf5-16ed-1a02-96cf-49a48899a65b',), {'run_idx': 1, 'sub_counter': 0})\n---parent check after sub node---\nstate = {'run_idx': 1, 'parent_counter': 1}\n---Run Index 2---\n((), {'run_idx': 2, 'parent_counter': 1})\n---parent counter node---\nstate = {'run_idx': 2, 'parent_counter': 1}\n((), {'run_idx': 2, 'parent_counter': 2})\n---subgraph counter node---\nstate = {'run_idx': 2}\n(('call_subgraph:e9ae413d-d6b8-d2a6-4732-072e73341a90',), {'run_idx': 2})\n(('call_subgraph:e9ae413d-d6b8-d2a6-4732-072e73341a90',), {'run_idx': 2, 'sub_counter': 0})\n---parent check after sub node---\nstate = {'run_idx': 2, 'parent_counter': 2}\nDescription\nThis is a follow-up issue of #3020, where checkpointer=True (see #3055) is introduced in 0.2.63  to enable subgraph to carry its state to new invoke from previous invoke.\nThe bug is\n\ncheckpointer=True does not work when parent graph is invoked asynchronously.\n\nLangGraph version\n0.2.63\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #135~20.04.1-Ubuntu SMP Mon Oct 7 13:56:22 UTC 2024\nPython Version:  3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0]\n\nPackage Information\n\nlangchain_core: 0.3.29\nlangchain: 0.3.11\nlangsmith: 0.2.10\nlangchain_openai: 0.2.12\nlangchain_text_splitters: 0.3.5\nlanggraph_sdk: 0.1.51\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.11\nasync-timeout: Installed. No version info available.\nhttpx: 0.28.1\njsonpatch: 1.33\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.59.7\norjson: 3.10.14\npackaging: 24.2\npydantic: 2.10.3\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.37\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\nzstandard: Installed. No version info available.\n", "created_at": "2025-01-17", "closed_at": "2025-01-17", "labels": ["investigate"], "State": "closed", "Author": "shengbo-ma"}
{"issue_number": 3080, "issue_title": "ERROR: column cw.task_path does not exist in LangGraph Checkpoint Postgres >=2.0.12 (Creating an issue with a solution)", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nclass ChatState(MessagesState):\n    pass\n\nasync def chat_node(state: ChatState, *, config: Optional[RunnableConfig] = None) -> ChatState:\n    system_message = \"\"\n    llm  = call_llm()\n\n    system_message = (\"You are GPT-4o with training data up to Oct 2023.\\n\"\n                       \"You are a very careful thinker. Think step by step before answering.\\n\")\n\n    messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n\n    response = await llm.ainvoke(messages)\n    return {\"messages\": [response]}\n\n\nasync def build_and_run_graph(message: str, config: RunnableConfig, run_method):\n    connection_kwargs = {\n        \"autocommit\": True,\n        \"prepare_threshold\": 0,\n    }\n    print(\"Building Agent\")\n    async with AsyncConnectionPool(\n            conninfo=PG_CONNECTIONSTRING,\n            max_size=5,\n            kwargs=connection_kwargs,\n    ) as pool:\n        checkpointer = AsyncPostgresSaver(pool)\n        graph = await create_graph(config)\n        compiled_graph = graph.compile(checkpointer=checkpointer)\n\n        message_input = {\"messages\": [HumanMessage(content=message)]}\n\n        return await run_method(compiled_graph, message_input, config)\n\nasync def stream_graph(compiled_graph, message_input, config):\n    ui_message = cl.Message(content=\"\")\n    async for event in compiled_graph.astream_events(message_input, config=config, version=\"v1\"):\n        if event[\"event\"] == \"on_chat_model_stream\" and event[\"name\"] == \"chatgpt\":\n            content = event[\"data\"][\"chunk\"].content or \"\"\n            await ui_message.stream_token(token=content)\n    await ui_message.send()\n\nasync def run_agent(message: str, config: RunnableConfig = None):\n    await build_and_run_graph(message, config, stream_graph)\n\nasync def create_graph(config: RunnableConfig):\n    print(\"Initializing Graph\")\n    graph = StateGraph(ChatState)\n    graph.add_node(\"chat\", chat_node)\n    graph.add_edge(START, \"chat\")\n    graph.add_edge(\"chat\", END)\n    return graph\nError Message and Stack Trace (if applicable)\nERROR: column cw.task_path does not exist\nLINE 27: ...array_agg(array[cw.type::bytea, cw.blob] order by cw.task_pa...\n                                                              ^\nTraceback (most recent call last):\n  File \"/home/ubuntu/miniforge3/envs/python3.12/lib/python3.12/site-packages/chainlit/utils.py\", line 45, in wrapper\n    return await user_function(**params_values)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/miniforge3/envs/python3.12/lib/python3.12/site-packages/chainlit/callbacks.py\", line 121, in with_parent_id\n    await func(message)\n  File \"/mnt/c/Github/chainlit/app.py\", line 193, in on_message\n    await run_agent(message.content, config=config)\n  File \"/mnt/c/Github/chainlit/agent/run_graph.py\", line 41, in run_agent\n    await build_and_run_graph(message, config, stream_graph)\n  File \"/mnt/c/Github/chainlit/agent/run_graph.py\", line 26, in build_and_run_graph\n    return await run_method(compiled_graph, message_input, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Github/chainlit/agent/run_graph.py\", line 30, in stream_graph\n    async for event in compiled_graph.astream_events(message_input, config=config, version=\"v1\"):\n  File \"/home/ubuntu/miniforge3/envs/python3.12/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 1386, in astream_events\n    async for event in event_stream:\n  File \"/home/ubuntu/miniforge3/envs/python3.12/lib/python3.12/site-packages/langchain_core/tracers/event_stream.py\", line 781, in _astream_events_implementation_v1\n    async for log in _astream_log_implementation(  # type: ignore[misc]\n  File \"/home/ubuntu/miniforge3/envs/python3.12/lib/python3.12/site-packages/langchain_core/tracers/log_stream.py\", line 675, in _astream_log_implementation\n    await task\n  File \"/home/ubuntu/miniforge3/envs/python3.12/lib/python3.12/site-packages/langchain_core/tracers/log_stream.py\", line 629, in consume_astream\n    async for chunk in runnable.astream(input, config, **kwargs):\n  File \"/home/ubuntu/miniforge3/envs/python3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 1846, in astream\n    async with AsyncPregelLoop(\n  File \"/home/ubuntu/miniforge3/envs/python3.12/lib/python3.12/site-packages/langgraph/pregel/loop.py\", line 1033, in __aenter__\n    saved = await self.checkpointer.aget_tuple(self.checkpoint_config)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/miniforge3/envs/python3.12/lib/python3.12/site-packages/langgraph/checkpoint/postgres/aio.py\", line 186, in aget_tuple\n    await cur.execute(\n  File \"/home/ubuntu/miniforge3/envs/python3.12/lib/python3.12/site-packages/psycopg/cursor_async.py\", line 97, in execute\n    raise ex.with_traceback(None)\npsycopg.errors.UndefinedColumn: column cw.task_path does not exist\nLINE 27: ...array_agg(array[cw.type::bytea, cw.blob] order by cw.task_pa...\nDescription\nLangGraph CheckPoint Postgres 2.0.12 and above is referencing a column that doesn't exist.\nSolution is to ensure checkpointer.setup() is called to create the new column.\nSystem Info\nSystem Information\nOS: Linux\nOS Version: langchain-ai/langchain#1 SMP Tue Nov 5 00:21:55 UTC 2024\nPython Version: 3.12.0 | packaged by conda-forge | (main, Oct 3 2023, 08:43:22) [GCC 12.3.0]\nPackage Information\nlangchain_core: 0.3.29\nlangchain: 0.3.14\nlangchain_community: 0.3.14\nlangsmith: 0.2.10\nlangchain_openai: 0.3.0\nlangchain_postgres: 0.0.12\nlangchain_sdk: 0.1.5\nlangchain_text_splitters: 0.3.5\nlangchain_unstructured: 0.1.6\nlanggraph_sdk: 0.1.51\nOptional packages not installed\nlangserve\nOther Dependencies\naiohttp: 3.11.11\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.7\nhttpx: 0.28.1\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlangchain_core>=0.3.0: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nonnxruntime: 1.19.2\nopenai: 1.59.7\norjson: 3.10.14\npackaging: 23.2\npgvector: 0.2.5\npsycopg: 3.2.3\npsycopg-pool: 3.2.4\npydantic: 2.9.2\npydantic-settings: 2.7.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.37\nsqlalchemy: 2.0.37\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\nunstructured-client: 0.27.0\nunstructured[all-docs]: Installed. No version info available.\nzstandard: Installed. No version info available.", "created_at": "2025-01-17", "closed_at": "2025-01-17", "labels": [], "State": "closed", "Author": "GhimBoon"}
{"issue_number": 3072, "issue_title": "Interrupt() when invoked for the second time, failed to wait for the user input", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n@tool\ndef book_appointment(first_name: str, last_name: str, email: str, doctor_name: str, time: str, tool_call_id: Annotated[str, InjectedToolCallId] ):\n    '''This is responsible to book an appointment using the first name, last name, email, doctor name and appointment time.'''\n\n    if first_name and last_name and email and doctor_name and time:\n        api_response = {'success': True, 'message': 'Successfully booked the appointment'}\n\n        return ToolMessage(content=api_response.get('message', ''), name=\"Book_appointment_tool\", tool_call_id = tool_call_id)\n\n    else:\n        return ToolMessage(content='Not able to book the appointment', name=\"Book_appointment_tool\",\n                           tool_call_id=tool_call_id)\n\n@tool\ndef collect_information(tool_call_id: Annotated[str, InjectedToolCallId]): # This acts like transfer tool that transfers to ask_human_node\n    '''This is responsible to collect the necessary information like the first name, last name, email, doctor name and appointment time from the user.'''\n\n    return Command(goto='ask_human_node', update={'messages': [\n        ToolMessage(content=\"Collecting required information from the user\", tool_call_id=tool_call_id)]\n    })\n\ndef call_node(state: MessagesState) -> Command[Literal['ask_human_node', '__end__']]:\n    prompt = '''You are an appointment booking agent who will be responsible to collect the necessary information from the user while booking the appointment.\n    \n    You would be always require to have following details to book an appointment:\n    => First name, last name, email, doctor name and appointment time.\n    '''\n    tools = [book_appointment]\n    model = ChatOpenAI(model=\"gpt-4o\", openai_api_key=os.getenv(\"OPEN_AI_API_KEY\")).bind_tools(tools)\n\n    messages = [SystemMessage(content=prompt)] + state['messages']\n\n    response = model.invoke(messages)\n\n    results = []\n\n    if len(response.tool_calls) > 0:\n        tool_names = {tool.name: tool for tool in tools}\n\n        for tool_call in response.tool_calls:\n            tool_ = tool_names[tool_call[\"name\"]]\n            tool_input_fields = tool_.get_input_schema().model_json_schema()[\n                \"properties\"\n            ]\n            if \"state\" in tool_input_fields:\n                tool_call = {**tool_call, \"args\": {**tool_call[\"args\"], \"state\": state}}\n\n            tool_response = tool_.invoke(tool_call)\n            results.append(tool_response)\n\n        if len(results) > 0:\n            return results\n        else:\n            return Command(goto='call_node', update={'messages': [AIMessage(content=str(results))]})\n\n    return Command(update={'messages': [response]})\n\n\ndef ask_human_node(state: MessagesState) -> Command[Literal['call_node']]:\n    last_message = state['messages'][-1]\n\n    user_response = interrupt({\n        'id': str(uuid.uuid4()),\n        'request': last_message\n    })\n\n    if user_response:\n        return Command(goto='call_node',\n                       resume={'messages': [HumanMessage(content=user_response, name=\"User_Response\")] },\n                       update={'messages': [HumanMessage(content=user_response, name=\"User_Response\")] })\n\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node('call_node', call_node)\nbuilder.add_node('ask_human_node', ask_human_node)\n\nbuilder.add_edge(START, 'call_node')\nbuilder.add_edge('call_node', END)\nError Message and Stack Trace (if applicable)\n\nDescription\nI'm trying to collect the information from the user to book an appointment using some details that needs to be passed to the Book Appointment API.\nDuring the graph execution, the interrupt() method within the ask_human_node is triggered for the first time to request details from the user. After the user submits the details, if any required information is missing, the ask_human_node attempts to gather the missing details using interrupt(). However, at this point, it does not pause the execution and instead continues using the previously cached value.\nAs mentioned in the documentation I know its the default behavior, but I REQUEST to please let us know how to make interrupt wait for the second time as well.\n\nSystem Info\npython -m langchain_core.sys_info", "created_at": "2025-01-16", "closed_at": "2025-01-22", "labels": [], "State": "closed", "Author": "Saisiva123"}
{"issue_number": 3071, "issue_title": "Getting error when trying to stream events from prebuit ReAct agent", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.graph import MessagesState\nfrom utils.tools_utils import tools\nfrom utils.network_agent_utils import manage_memory\nfrom llms.llms import llama_chatmodel_react\nimport utils.memory_checkpoint as memory\n\nasync def initialize_agent():\n    checkpointer = await memory.get_checkpointer()\n    # Create the agent with the initialized checkpointer\n    langgraph_agent = create_react_agent(\n        model=llama_chatmodel_react,\n        tools=tools,\n        checkpointer=checkpointer,  # Pass the actual checkpointer instance\n        state_modifier=manage_memory\n    )\n\n    return langgraph_agent\n\n\nasync def process_chat_stream(message: str, thread_id: str):\n    \"\"\"\n    Process chat messages using langgraph with streaming implementation that properly\n    handles ToolMessage events and appends sources as markdown HTML anchor tags after\n    the model stream completes.\n\n    The function processes events in this sequence:\n    1. Streams model content in real-time\n    2. Collects sources from ToolMessage events during streaming\n    3. Appends formatted source links after content streaming ends\n\n    Args:\n        message (str): The input message to process\n        thread_id (str): Unique identifier for the chat thread\n\n    Yields:\n        str: Content chunks including model output and source information\n    \"\"\"\n    langgraph_agent = await initialize_agent()\n\n    # Initialize our source collection to gather sources during streaming\n    sources = []\n    is_streaming_complete = False\n\n    async for event in langgraph_agent.astream_events(\n            {\"messages\": [HumanMessage(content=message)]},\n            {\"recursion_limit\": 10, \"configurable\": {\"thread_id\": thread_id}},\n            version=\"v2\"\n    ):\n        await process_chunks(event)\n        event_type = event[\"event\"]\n\n        if event_type == \"on_chat_model_stream\":\n            # Handle the main content streaming from the language model\n            chunk = event[\"data\"][\"chunk\"]\n            content = chunk.content if hasattr(chunk, \"content\") else str(chunk)\n\n            if content:\n                # Stream the content chunk immediately\n                yield content\n\n        elif event_type == \"on_tool_end\":\n            # Extract source information from ToolMessage\n            tool_output = event[\"data\"][\"output\"]\n\n            # Handle ToolMessage specific format\n            if isinstance(tool_output, ToolMessage):\n                # Extract content from ToolMessage\n                tool_content = tool_output.content\n\n                # Process the content string to extract file information\n                if isinstance(tool_content, str):\n                    # Split content into lines, handling potential line breaks\n                    lines = tool_content.split('\\n')\n                    current_file = None\n                    current_url = None\n\n                    for line in lines:\n                        # Extract file name and URL using string operations\n                        if line.startswith('file_name ::'):\n                            current_file = line.replace('file_name ::', '').strip()\n                        #elif line.startswith('s3_source_URL ::'):\n                            #current_url = line.replace('s3_source_URL ::', '').strip()\n\n                        # When we have file name, store them\n                        if current_file:\n                            source_tuple = (current_file)\n                            if source_tuple not in sources:\n                                sources.append(source_tuple)\n                                # Reset for next pair\n                                current_file = None\n                                current_url = None\n\n        elif event_type == \"on_chain_end\":\n            # Mark streaming as complete when the chain ends\n            is_streaming_complete = True\n\n    # After all streaming is complete, append sources if we have any\n    if is_streaming_complete and sources:\n        # Add formatting for the sources section\n        yield \"\\n\\n\"  # Add visual separation\n        yield \"**Sources:**\\n\"  # Add header in bold\n\n        # Generate and yield source links individually\n        for file_name in sources:\n            # Format each source as a markdown bullet point with HTML anchor tag\n            source_link = f'\u2022 <a href=\"{file_name}\" target=\"_blank\">{file_name}</a>\\n'\n            yield source_link\nError Message and Stack Trace (if applicable)\n|   File \"C:\\Users\\0047YN744\\PycharmProjects\\dish-ran-ym\\routers\\agentic_ran.py\", line 129, in process_chat_stream\n    |     async for event in langgraph_agent.astream_events(\n    |   File \"C:\\Users\\0047YN744\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 1386, in astream_events\n    |     async for event in event_stream:\n    |   File \"C:\\Users\\0047YN744\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\tracers\\event_stream.py\", line 1012, in _astream_events_implementation_v2\n    |     await task\n    |   File \"C:\\Users\\0047YN744\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\tracers\\event_stream.py\", line 967, in consume_astream\n    |     async for _ in event_streamer.tap_output_aiter(run_id, stream):\n    |   File \"C:\\Users\\0047YN744\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\tracers\\event_stream.py\", line 203, in tap_output_aiter\n    |     async for chunk in output:\n    |   File \"C:\\Users\\0047YN744\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langgraph\\pregel\\__init__.py\", line 1878, in astream     \n    |     async for _ in runner.atick(\n    |   File \"C:\\Users\\0047YN744\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langgraph\\pregel\\runner.py\", line 362, in atick\n    |     await arun_with_retry(\n    |   File \"C:\\Users\\0047YN744\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langgraph\\pregel\\retry.py\", line 127, in arun_with_retry \n    |     async for _ in task.proc.astream(task.input, config):\n    |   File \"C:\\Users\\0047YN744\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langgraph\\utils\\runnable.py\", line 568, in astream       \n    |     async for chunk in aiterator:\n    |   File \"C:\\Users\\0047YN744\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\tracers\\event_stream.py\", line 180, in tap_output_aiter\n    |     first = await py_anext(output, default=sentinel)\n    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"C:\\Users\\0047YN744\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\utils\\aiter.py\", line 76, in anext_impl   \n    |     return await __anext__(iterator)\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"C:\\Users\\0047YN744\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 1453, in atransform\n    |     async for ichunk in input:\n    |   File \"C:\\Users\\0047YN744\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 1453, in atransform\n    |     async for ichunk in input:\n    |   File \"C:\\Users\\0047YN744\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 1016, in astream \n    |     yield await self.ainvoke(input, config, **kwargs)\n    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"C:\\Users\\0047YN744\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langgraph\\prebuilt\\tool_node.py\", line 253, in ainvoke   \n    |     return await super().ainvoke(input, config, **kwargs)\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"C:\\Users\\0047YN744\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langgraph\\utils\\runnable.py\", line 236, in ainvoke       \n    |     ret = await asyncio.create_task(coro, context=context)\n    |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"C:\\Users\\0047YN744\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langgraph\\prebuilt\\tool_node.py\", line 267, in _afunc    \n    |     outputs = await asyncio.gather(\n    |               ^^^^^^^^^^^^^^^^^^^^^\n    |   File \"C:\\Users\\0047YN744\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langgraph\\prebuilt\\tool_node.py\", line 350, in _arun_one \n    |     if invalid_tool_message := self._validate_tool_call(call):\n    |                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"C:\\Users\\0047YN744\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langgraph\\prebuilt\\tool_node.py\", line 436, in _validate_tool_call\n    |     return ToolMessage(\n    |            ^^^^^^^^^^^^\n    |   File \"C:\\Users\\0047YN744\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\messages\\tool.py\", line 139, in __init__  \n    |     super().__init__(content=content, **kwargs)\n    |   File \"C:\\Users\\0047YN744\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\messages\\base.py\", line 76, in __init__   \n    |     super().__init__(content=content, **kwargs)\n    |   File \"C:\\Users\\0047YN744\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\load\\serializable.py\", line 125, in __init__\n    |     super().__init__(*args, **kwargs)\n    |   File \"C:\\Users\\0047YN744\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydantic\\main.py\", line 212, in __init__\n    |     validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n    |                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    | pydantic_core._pydantic_core.ValidationError: 1 validation error for ToolMessage\n    | tool_call_id\n    |   Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    |     For further information visit https://errors.pydantic.dev/2.9/v/string_type\n    | During task with name 'tools' and id '0ea8bd9b-7b55-a42d-658c-e9d8d69205e7'\n    +------------------------------------\nDescription\ni am using belwo llm provider interface\nfrom langchain_ibm import ChatWatsonx\nSystem Info\nlanggraph==0.2.60\nlangchain==0.3.14\nlangchain_ibm==0.3.5\nibm_cloud_sdk_core==3.22.0\npsycopg==3.2.3\npsycopg_pool==3.2.4\npsycopg-binary==3.2.3\nlanggraph-checkpoint==2.0.9\nlanggraph-checkpoint-postgres==2.0.9\nlanggraph-sdk==0.1.48\npsycopg2-binary==2.9.10\nlangchain_community==0.3.14", "created_at": "2025-01-16", "closed_at": "2025-01-31", "labels": ["invalid"], "State": "closed", "Author": "nikhil23011996"}
{"issue_number": 3068, "issue_title": "DOCS: Document checkpointer=True mode", "issue_body": "Privileged issue\n\n I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.\n\nIssue Content\n#3055", "created_at": "2025-01-16", "closed_at": null, "labels": ["maintainer"], "State": "open", "Author": "eyurtsev"}
{"issue_number": 3062, "issue_title": "getting double repetative Output from agents tools langraph", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\ni have 2 chain bing chain & gk chain (using langchain) im calling this inside tools\n\n\n\n        all_tools = [image_tool, bing_tool,gk_tool]\n\n    def setup_langgraph_workflow(self):\n        \"\"\"Set up the langgraph workflow.\"\"\"\n        logger.info(\"lanngraph workflow has started..\")\n        # Define the tools for langgraph\n        self.tools = self.define_tools()\n        tool_node = ToolNode(self.tools)\n\n        llm_graph = self.main_llm\n\n        # Bind tools to LLM\n        model_with_tools = llm_graph.bind_tools(self.tools)\n        # Create the workflow\n        workflow = StateGraph(MessagesState)\n\n\n        # Define the function that determines whether to continue or not\n        def should_continue(state: MessagesState):\n            print(\"should continue using\")\n            messages = state[\"messages\"]\n            last_message = messages[-1]\n            # If there is no function call, then we finish\n            if not last_message.tool_calls:\n                return END\n            # Otherwise if there is, we continue\n            else:\n                return \"tools\"\n\n\n        def call_model(state: MessagesState):\n            messages = state[\"messages\"]\n            response = model_with_tools.invoke(messages)\n            return {\"messages\": [response]}\n\n        def call_tool(state: MessagesState):\n            tools_by_name = {\n                tool.name: tool for tool in self.define_tools()\n            }  # Dynamically map tools by name\n            messages = state[\"messages\"]\n            last_message = messages[-1]\n            output_messages = []\n            for tool_call in last_message.tool_calls:\n                try:\n                    tool_result = tools_by_name[tool_call[\"name\"]].invoke(\n                        tool_call[\"args\"]\n                    )\n                    output_messages.append(\n                        ToolMessage(\n                            content=json.dumps(tool_result),\n                            name=tool_call[\"name\"],\n                            tool_call_id=tool_call[\"id\"],\n                        )\n                    )\n                except Exception as e:\n                    # Return the error if the tool call fails\n                    output_messages.append(\n                        ToolMessage(\n                            content=\"\",\n                            name=tool_call[\"name\"],\n                            tool_call_id=tool_call[\"id\"],\n                            additional_kwargs={\"error\": e},\n                        )\n                    )\n            return {\"messages\": output_messages}\n\n        def should_fallback(\n            state: MessagesState,\n        ) -> Literal[\"agent\", \"remove_failed_tool_call_attempt\"]:\n\n            messages = state[\"messages\"]\n            failed_tool_messages = [\n                msg\n                for msg in messages\n                if isinstance(msg, ToolMessage)\n                and msg.additional_kwargs.get(\"error\") is not None\n            ]\n            if failed_tool_messages:\n                return \"remove_failed_tool_call_attempt\"\n            return \"agent\"\n\n        def remove_failed_tool_call_attempt(state: MessagesState):\n            messages = state[\"messages\"]\n\n            # instance of AIMessage onwards.\n            last_ai_message_index = next(\n                i\n                for i, msg in reversed(list(enumerate(messages)))\n                if isinstance(msg, AIMessage)\n            )\n            messages_to_remove = messages[last_ai_message_index:]\n            return {\"messages\": [RemoveMessage(id=m.id) for m in messages_to_remove]}\n\n        # Fallback to a better model if a tool call fails\n        def call_fallback_model(state: MessagesState):\n            messages = state[\"messages\"]\n            response = model_with_tools.invoke(messages)\n            return {\"messages\": [response]}\n\n        workflow.add_node(\"agent\", call_model)\n        workflow.add_node(\"tools\", call_tool)\n        workflow.add_node(\n            \"remove_failed_tool_call_attempt\", remove_failed_tool_call_attempt\n        )\n        workflow.add_node(\"fallback_agent\", call_fallback_model)\n\n        workflow.add_edge(START, \"agent\")\n        workflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])\n        workflow.add_conditional_edges(\"tools\", should_fallback)\n        workflow.add_edge(\"remove_failed_tool_call_attempt\", \"fallback_agent\")\n        workflow.add_edge(\"fallback_agent\", \"tools\")\n\n        # Ensure Redis connection\n        self.ensure_connection()\n\n        with RedisSaver.from_conn_url(REDIS_URL) as checkpointer:\n            self.checkpointer = checkpointer\n            self.workflow_app = workflow.compile(checkpointer=self.checkpointer)\n\n\n    def store_thread_id(self, session_id: str, thread_id: str):\n        \"\"\"Store the thread_id in Redis for a given session_id.\"\"\"\n        redis_key = f\"langgraph_thread_id_{session_id}\"\n        try:\n            r = redis.Redis.from_url(url=REDIS_URL, decode_responses=True)\n            r.set(redis_key, thread_id)\n        except Exception as e:\n            logger.error(f\"Error saving thread ID to Redis: {e}\")\n\n    def get_thread_id(self, session_id: str) -> Optional[str]:\n        \"\"\"Retrieve the thread_id from Redis for a given session_id.\"\"\"\n        redis_key = f\"langgraph_thread_id_{session_id}\"\n        try:\n            r = redis.Redis.from_url(url=REDIS_URL, decode_responses=True)\n            print(\"redis used dad\")\n            return r.get(redis_key)\n        except Exception as e:\n            logger.error(f\"Error retrieving thread ID from Redis: {e}\")\n            return None\n\n\n\n\n    def get_answer_from_langgraph(self, query: str, session_id: str) -> str:\n        \"\"\"Fetch answer using LangGraph with streaming support.\"\"\"\n        logger.info(\"Fetching answer using LangGraph with streaming\")\n        try:\n            if not self.workflow_app:\n                raise ValueError(\"LangGraph workflow is not initialized.\")\n\n            thread_id = self.get_thread_id(self.session_id)\n            if not thread_id:\n                thread_id = f\"thread-{session_id}\"\n                self.store_thread_id(session_id, thread_id)\n\n            logger.info(f\"Using thread ID: {thread_id}\")\n            config = {\"configurable\": {\"thread_id\": thread_id}}\n            logger.debug(f\"Workflow config: {config}\")\n\n            inputs = [(\"user\", query)]\n            first = True\n            gathered_response = None\n\n            # Stream tokens as they are generated\n            for msg, metadata in self.workflow_app.stream({\"messages\": inputs}, config, stream_mode=\"messages\"):\n                if msg.content and not isinstance(msg, HumanMessage):\n                    # Print to console for visibility (optional)\n                    print(msg.content, end=\"\", flush=True)\n                    # Gather the AI message chunks\n                    if isinstance(msg, AIMessageChunk):\n                        if first:\n                            gathered_response = msg\n                            first = False\n                            print(\"first msg firsts\",gathered_response)\n                        else:\n                            gathered_response = gathered_response + msg\n\n                        # Handle tool calls if present in the chunk\n                        if msg.tool_call_chunks:\n\n                            print(gathered_response.tool_calls)\n\n            # Finalize and return the full response\n            if gathered_response:\n                print(\"gathered responses is\",gathered_response)\n                return gathered_response.content\n            else:\n                logger.warning(\"No AI response received from LangGraph\")\n                return \"No response received from LangGraph.\"\n        except Exception as e:\n            logger.error(f\"Error in LangGraph response fetching: {e}\")\n            return \"An error occurred while processing your request.\"\nError Message and Stack Trace (if applicable)\nanswer_helper.py          : __init__            : 217 : Main LLM is callbacks=[<__main__.StreamHandler object at 0x75fb1835d930>] client=<openai.resources.chat.completions.Completions object at 0x75fa8816cb80> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x75fa8816ec50> root_client=<openai.OpenAI object at 0x75fa88156aa0> root_async_client=<openai.AsyncOpenAI object at 0x75fa8816cbe0> model_name='gpt-4o' temperature=0.01 model_kwargs={} openai_api_key=SecretStr('**********') openai_organization='org-RK0NQC1sHHggOsWSbEz4HqjF' streaming=True max_tokens=4096\n     answer_helper.py          : __init__            : 253 : Initializing LangGraph workflow for Universal AI ok .\n     answer_helper.py          : setup_langgraph_workflow: 651 : lanngraph workflow has started..\n     answer_helper.py          : __init__            : 256 : LangGraph workflow initialized successfully.\nwe here in universal ai \n     answer_helper.py          : get_answer_from_langgraph: 981 : Fetching answer using LangGraph with streaming\n:    answer_helper.py          : get_answer_from_langgraph: 998 : Workflow config: {'configurable': {'thread_id': 'thread-26669966', 'recursion_limit': 2}}\nINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\nHTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\nshould continue using\n     get_bing_chains.py        : get_answer          : 47  : Executing get_answer with query: latest world news June 2025\n     get_bing_chains.py        : get_documents       : 56  : get documents from bing search api\n     get_bing_chains.py        : get_documents       : 67  : time taken for get results from solr: 1.4878 seconds\n     get_bing_chains.py        : prepare_final_response: 94  : Generating answer from Bing search API and LLM\nNG:  get_bing_chains.py        : prepare_final_response: 125 : Memory is not initialized. Using empty chat history.\nINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\nHTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n     get_bing_chains.py        : prepare_final_response: 156 : Time taken in LLM (bing_chain): 9.8183 seconds\nINFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\nHTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\nshould continue using\n     answer_helper.py          : compose_final_answer: 1230: compose_final_answer -> {'text': \"# Latest World News - June 2025\\n\\nTo provide you with the most recent and relevant updates from around the globe, here are some key highlights from June 2025:\\n\\n## Middle East\\n- **Israel-Hamas Ceasefire**: A ceasefire agreement has been confirmed between Israel and Hamas, facilitated by the US and Qatar. This includes the release of some hostages, marking a significant development in the ongoing conflict in the Gaza Strip. [Source: AP News](https://apnews.com/world-news)\\n\\n## Europe\\n- **Prince William's Activities**: Prince William, the Prince of Wales, has been actively participating in public events, including attending the Together At Christmas carol service at Westminster Abbey. [Source: Hindustan Times](https://www.hindustantimes.com/world-news/world-news-live-latest-updates-on-politics-economy-conflicts-climate-change-today-january-9-2025-101736365588453.html)\\n\\n## United States\\n- **California Wildfires**: Governor Gavin Newsom has been addressing the aftermath of wildfires in Pacific Palisades, working closely with CalFire and local officials to assess and manage the damage. [Source: Hindustan Times](https://www.hindustantimes.com/world-news/world-news-live-latest-updates-on-politics-economy-conflicts-climate-change-today-january-10-2025-101736449130295.html)\\n\\n## Global Economy\\n- **Russia's Economic Forecast**: The Russian central bank has projected economic growth of 0.5 to 1.5 percent for 2025, a decrease from the previous year's growth rate, indicating a slowdown following a wartime economic boom. [Source: CFR](https://www.cfr.org/article/what-were-watching-around-globe-2025)\\n\\nThese updates provide a snapshot of significant global events and developments as of June 2025. For more detailed information, you can explore the provided sources.Here are some key highlights from the latest world news in June 2025:\\n\\n### Middle East\\n- **Israel-Hamas Ceasefire**: A ceasefire agreement has been confirmed between Israel and Hamas, facilitated by the US and Qatar. This includes the release of some hostages, marking a significant development in the ongoing conflict in the Gaza Strip. [Read more](https://apnews.com/world-news)\\n\\n### Europe\\n- **Prince William's Activities**: Prince William, the Prince of Wales, has been actively participating in public events, including attending the Together At Christmas carol service at Westminster Abbey. [Read more](https://www.hindustantimes.com/world-news/world-news-live-latest-updates-on-politics-economy-conflicts-climate-change-today-january-9-2025-101736365588453.html)\\n\\n### United States\\n- **California Wildfires**: Governor Gavin Newsom has been addressing the aftermath of wildfires in Pacific Palisades, working closely with CalFire and local officials to assess and manage the damage. [Read more](https://www.hindustantimes.com/world-news/world-news-live-latest-updates-on-politics-economy-conflicts-climate-change-today-january-10-2025-101736449130295.html)\\n\\n### Global Economy\\n- **Russia's Economic Forecast**: The Russian central bank has projected economic growth of 0.5 to 1.5 percent for 2025, a decrease from the previous year's growth rate, indicating a slowdown following a wartime economic boom. [Read more](https://www.cfr.org/article/what-were-watching-around-globe-2025)\\n\\nThese updates provide a snapshot of significant global events and developments as of June 2025.\", 'found_an_answer': True, 'related_articles': [], 'llm_error_occurred': False}\n\n\n# Latest World News - June 2025\n\nTo provide you with the most recent and relevant updates from around the globe, here are some key highlights from June 2025:\n\n## Middle East\n- **Israel-Hamas Ceasefire**: A ceasefire agreement has been confirmed between Israel and Hamas, facilitated by the US and Qatar. This includes the release of some hostages, marking a significant development in the ongoing conflict in the Gaza Strip. [Source: AP News](https://apnews.com/world-news)\n\n## Europe\n- **Prince William's Activities**: Prince William, the Prince of Wales, has been actively participating in public events, including attending the Together At Christmas carol service at Westminster Abbey. [Source: Hindustan Times](https://www.hindustantimes.com/world-news/world-news-live-latest-updates-on-politics-economy-conflicts-climate-change-today-january-9-2025-101736365588453.html)\n\n## United States\n- **California Wildfires**: Governor Gavin Newsom has been addressing the aftermath of wildfires in Pacific Palisades, working closely with CalFire and local officials to assess and manage the damage. [Source: Hindustan Times](https://www.hindustantimes.com/world-news/world-news-live-latest-updates-on-politics-economy-conflicts-climate-change-today-january-10-2025-101736449130295.html)\n\n## Global Economy\n- **Russia's Economic Forecast**: The Russian central bank has projected economic growth of 0.5 to 1.5 percent for 2025, a decrease from the previous year's growth rate, indicating a slowdown following a wartime economic boom. [Source: CFR](https://www.cfr.org/article/what-were-watching-around-globe-2025)\n\nThese updates provide a snapshot of significant global events and developments as of June 2025. For more detailed information, you can explore the provided sources.Here are some key highlights from the latest world news in June 2025:\n\n### Middle East\n- **Israel-Hamas Ceasefire**: A ceasefire agreement has been confirmed between Israel and Hamas, facilitated by the US and Qatar. This includes the release of some hostages, marking a significant development in the ongoing conflict in the Gaza Strip. [Read more](https://apnews.com/world-news)\n\n### Europe\n- **Prince William's Activities**: Prince William, the Prince of Wales, has been actively participating in public events, including attending the Together At Christmas carol service at Westminster Abbey. [Read more](https://www.hindustantimes.com/world-news/world-news-live-latest-updates-on-politics-economy-conflicts-climate-change-today-january-9-2025-101736365588453.html)\n\n### United States\n- **California Wildfires**: Governor Gavin Newsom has been addressing the aftermath of wildfires in Pacific Palisades, working closely with CalFire and local officials to assess and manage the damage. [Read more](https://www.hindustantimes.com/world-news/world-news-live-latest-updates-on-politics-economy-conflicts-climate-change-today-january-10-2025-101736449130295.html)\n\n### Global Economy\n- **Russia's Economic Forecast**: The Russian central bank has projected economic growth of 0.5 to 1.5 percent for 2025, a decrease from the previous year's growth rate, indicating a slowdown following a wartime economic boom. [Read more](https://www.cfr.org/article/what-were-watching-around-globe-2025)\n\nThese updates provide a snapshot of significant global events and developments as of June 2025.\nDescription\ni m building langgraph based multiaget system & the output im getting is sometimes doubled so not sure where is issue?\nissue with agents streaming or logic .why its calling double output\nSystem Info\nubantu 22.04 local system", "created_at": "2025-01-16", "closed_at": "2025-03-05", "labels": ["invalid"], "State": "closed", "Author": "akashAD98"}
{"issue_number": 3059, "issue_title": "Unable to run langgraph docker container when using existing postgres db and redis", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\ndocker run \\\n    --env-file .env \\\n    -p 8123:8000 \\\n    -e REDIS_URI=\"redacted\" \\\n    -e DATABASE_URI=\"redacted\" \\\n    -e LANGSMITH_API_KEY=\"redacted\" \\\n    my-image\nError Message and Stack Trace (if applicable)\n2025-01-15 21:38:28 2025-01-16T02:38:28.085675Z [info     ] Using auth of type=noop        [langgraph_api.auth.middleware] api_revision=939d51f api_variant=local\n2025-01-15 21:38:28 2025-01-16T02:38:28.087168Z [info     ] Started server process [1]     [uvicorn.error] api_revision=939d51f api_variant=local color_message=Started server process [%d]\n2025-01-15 21:38:28 2025-01-16T02:38:28.087305Z [info     ] Waiting for application startup. [uvicorn.error] api_revision=939d51f api_variant=local\n2025-01-15 21:38:28 2025-01-16T02:38:28.087515Z [warning  ] No license key found, running in test mode with LangSmith API key. For production use, set LANGGRAPH_CLOUD_LICENSE_KEY in environment. [langgraph_license.validation] api_revision=939d51f api_variant=local\n2025-01-15 21:38:28 2025-01-16T02:38:28.282471Z [info     ] HTTP Request: GET https://api.smith.langchain.com/auth?langgraph-api=true \"HTTP/1.1 200 OK\" [httpx] api_revision=939d51f api_variant=local\n2025-01-15 21:38:28 2025-01-16T02:38:28.327079Z [warning  ] error connecting in 'pool-1': connection failed: connection to server at \"192.168.1.95\", port 5432 failed: server closed the connection unexpectedly\n2025-01-15 21:38:28     This probably means the server terminated abnormally\n2025-01-15 21:38:28     before or while processing the request. [psycopg.pool] api_revision=939d51f api_variant=local\n2025-01-15 21:38:29 2025-01-16T02:38:29.298065Z [warning  ] error connecting in 'pool-1': connection failed: connection to server at \"192.168.1.95\", port 5432 failed: server closed the connection unexpectedly\n2025-01-15 21:38:29     This probably means the server terminated abnormally\n2025-01-15 21:38:29     before or while processing the request. [psycopg.pool] api_revision=939d51f api_variant=local\n2025-01-15 21:38:31 2025-01-16T02:38:31.233615Z [warning  ] error connecting in 'pool-1': connection failed: connection to server at \"192.168.1.95\", port 5432 failed: server closed the connection unexpectedly\n2025-01-15 21:38:31     This probably means the server terminated abnormally\n2025-01-15 21:38:31     before or while processing the request. [psycopg.pool] api_revision=939d51f api_variant=local\n2025-01-15 21:38:35 2025-01-16T02:38:35.103750Z [warning  ] error connecting in 'pool-1': connection failed: connection to server at \"192.168.1.95\", port 5432 failed: server closed the connection unexpectedly\n2025-01-15 21:38:35     This probably means the server terminated abnormally\n2025-01-15 21:38:35     before or while processing the request. [psycopg.pool] api_revision=939d51f api_variant=local\n2025-01-15 21:38:42 2025-01-16T02:38:42.837522Z [warning  ] error connecting in 'pool-1': connection failed: connection to server at \"192.168.1.95\", port 5432 failed: server closed the connection unexpectedly\n2025-01-15 21:38:42     This probably means the server terminated abnormally\n2025-01-15 21:38:42     before or while processing the request. [psycopg.pool] api_revision=939d51f api_variant=local\n2025-01-15 21:38:58 2025-01-16T02:38:58.299583Z [warning  ] error connecting in 'pool-1': connection failed: connection to server at \"192.168.1.95\", port 5432 failed: server closed the connection unexpectedly\n2025-01-15 21:38:58     This probably means the server terminated abnormally\n2025-01-15 21:38:58     before or while processing the request. [psycopg.pool] api_revision=939d51f api_variant=local\n2025-01-15 21:38:58 2025-01-16T02:38:58.325300Z [error    ] Traceback (most recent call last):\n2025-01-15 21:38:58   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 693, in lifespan\n2025-01-15 21:38:58     async with self.lifespan_context(app) as maybe_state:\n2025-01-15 21:38:58   File \"/usr/local/lib/python3.11/contextlib.py\", line 210, in __aenter__\n2025-01-15 21:38:58     return await anext(self.gen)\n2025-01-15 21:38:58            ^^^^^^^^^^^^^^^^^^^^^\n2025-01-15 21:38:58   File \"/api/langgraph_api/lifespan.py\", line 29, in lifespan\n2025-01-15 21:38:58   File \"/usr/local/lib/python3.11/site-packages/langgraph_storage/database.py\", line 146, in start_pool\n2025-01-15 21:38:58     await _pg_pool.open(wait=True)\n2025-01-15 21:38:58   File \"/usr/local/lib/python3.11/site-packages/psycopg_pool/pool_async.py\", line 387, in open\n2025-01-15 21:38:58     await self.wait(timeout=timeout)\n2025-01-15 21:38:58   File \"/usr/local/lib/python3.11/site-packages/psycopg_pool/pool_async.py\", line 174, in wait\n2025-01-15 21:38:58     raise PoolTimeout(f\"pool initialization incomplete after {timeout} sec\")\n2025-01-15 21:38:58 psycopg_pool.PoolTimeout: pool initialization incomplete after 30.0 sec\n2025-01-15 21:38:58  [uvicorn.error] api_revision=939d51f api_variant=local\n2025-01-15 21:38:58 2025-01-16T02:38:58.325529Z [error    ] Application startup failed. Exiting. [uvicorn.error] api_revision=939d51f api_variant=local\nDescription\nI am trying to run langgraph against existing postgres and redis containers. The redis and database containers start up fine and ready to receive connections.  However, when I run the langgraph server, it fails to connect to database.\nThe database  and redis uris passed to the docker run command are:\npostgres://postgres:postgres@192.168.1.95:5432/postgres?sslmode=disable\nredis://192.168.1.95:6379\nNote that the dabase and redis are started from a separate docker compose and both containers are running fine.\nI have tested running the database, redis and langgraph from the same docker-compose file and that seems to work ok.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.6.0: Mon Jul 29 21:13:00 PDT 2024; root:xnu-10063.141.2~1/RELEASE_X86_64\nPython Version:  3.11.7 (main, Dec  4 2023, 18:10:11) [Clang 15.0.0 (clang-1500.1.0.2.5)]\n\nPackage Information\n\nlangchain_core: 0.3.29\nlangsmith: 0.2.10\nlangchain_openai: 0.3.0\nlanggraph_cli: 0.1.67\nlanggraph_sdk: 0.1.51\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nclick: 8.1.8\nhttpx: 0.28.1\njsonpatch: 1.33\nlanggraph-api: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nopenai: 1.59.7\norjson: 3.10.14\npackaging: 24.2\npydantic: 2.10.5\npython-dotenv: Installed. No version info available.\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\nzstandard: Installed. No version info available.\n", "created_at": "2025-01-16", "closed_at": "2025-01-17", "labels": [], "State": "closed", "Author": "magallardo"}
{"issue_number": 3020, "issue_title": "Subgraph forgets its state of the first run when it is invoked the second time in a parent graph", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nDescription\nNoteThis issue has updates in comments, for TL;DR, see this summary comment.\n\nI am building a nested graph multi-agent system for question and answer style chat service.\nIn this scenario, I will run the same parent graph multiple times, since new follow-up questions comes and trigger a new graph run. Each follow-up question in based on context of the previous runs.\nI observed this bug:\n\nThe parent graph works well, all states in previous runs are there. However, the subgraph totally forgets the previous run. The agent state of a subgraph is flushed and it starts from empty.\n\nLangGraph Version\nlanggraph: 0.2.61\nExample Code\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import END, START, StateGraph\nfrom rich import get_console\nfrom typing_extensions import TypedDict\n\n\nclass SubGraphState(TypedDict, total=False):\n    run_idx: int\n    sub_counter: int\n\n\ndef subgraph_accumulator(state: SubGraphState) -> SubGraphState:\n    get_console().print(\"---subgraph counter node---\")\n    get_console().print(f\"{state = }\")\n\n    sub_counter = state[\"sub_counter\"] + 1 if \"sub_counter\" in state else 0\n    return {\"sub_counter\": sub_counter}\n\n\nsub_graph = (\n    StateGraph(SubGraphState)\n    .add_node(subgraph_accumulator)\n    .add_edge(START, subgraph_accumulator.__name__)\n    .add_edge(subgraph_accumulator.__name__, END)\n    .compile()\n)\nsub_graph.name = \"sub\"\n\n\nclass ParentGraphState(TypedDict, total=False):\n    run_idx: int\n    parent_counter: int\n\n\ndef parent_graph_accumulator(state: ParentGraphState) -> ParentGraphState:\n    print(\"---parent counter node---\")\n    get_console().print(f\"{state = }\")\n    parent_counter = state[\"parent_counter\"] + 1 if \"parent_counter\" in state else 0\n\n    return {\n        \"parent_counter\": parent_counter,\n    }\n\n\nparent_agent = (\n    StateGraph(ParentGraphState)\n    .add_node(parent_graph_accumulator)\n    .add_node(sub_graph)\n    .add_edge(START, parent_graph_accumulator.__name__)\n    .add_edge(parent_graph_accumulator.__name__, sub_graph.get_name())\n    .add_edge(sub_graph.get_name(), END)\n    .compile(checkpointer=MemorySaver())\n)\n\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"42\"}}\n\nfor i in range(3):\n    print(f\"---Run Index {i}---\")\n    for event in parent_agent.stream(\n        ParentGraphState(run_idx=i),\n        config,\n        stream_mode=\"values\",\n        subgraphs=True,\n    ):\n        print(event)\n\n\n\n%%{init: {'flowchart': {'curve': 'linear'}}}%%\ngraph TD;\n        __start__([<p>__start__</p>]):::first\n        parent_graph_accumulator(parent_graph_accumulator)\n        sub(sub)\n        __end__([<p>__end__</p>]):::last\n        __start__ --> parent_graph_accumulator;\n        parent_graph_accumulator --> sub;\n        sub --> __end__;\n        classDef default fill:#f2f0ff,line-height:1.2\n        classDef first fill-opacity:0\n        classDef last fill:#bfb6fc\n\n\n\n\n\n\n\n\n Loading\n\n\n\nError Message and Stack Trace (if applicable)\nNo error thrown. Below is the output of the example code.\nParent graph counter increases correctly along with run index, while subgraph counter sub_counter gets reset to 0 on each follow-up run. Subgraph counter should be increasing with the run_idx.\n---Run Index 0---\n((), {'run_idx': 0})\n---parent counter node---\nstate = {'run_idx': 0}\n((), {'run_idx': 0, 'parent_counter': 0})\n(('sub:45ab0cd7-e60d-1a76-fdec-c4a0b972639f',), {'run_idx': 0})\n---subgraph counter node---\nstate = {'run_idx': 0}\n(('sub:45ab0cd7-e60d-1a76-fdec-c4a0b972639f',), {'run_idx': 0, 'sub_counter': 0})\n((), {'run_idx': 0, 'parent_counter': 0})\n---Run Index 1---\n((), {'run_idx': 1, 'parent_counter': 0})\n---parent counter node---\nstate = {'run_idx': 1, 'parent_counter': 0}\n((), {'run_idx': 1, 'parent_counter': 1})\n(('sub:ad3a2d91-7d1c-b791-f8a1-3e6213226d3c',), {'run_idx': 1})\n---subgraph counter node---\nstate = {'run_idx': 1}\n(('sub:ad3a2d91-7d1c-b791-f8a1-3e6213226d3c',), {'run_idx': 1, 'sub_counter': 0})\n((), {'run_idx': 1, 'parent_counter': 1})\n---Run Index 2---\n((), {'run_idx': 2, 'parent_counter': 1})\n---parent counter node---\nstate = {'run_idx': 2, 'parent_counter': 1}\n((), {'run_idx': 2, 'parent_counter': 2})\n---subgraph counter node---\n(('sub:21723217-8cf8-4856-863a-8c8f9b9b9351',), {'run_idx': 2})\nstate = {'run_idx': 2}\n(('sub:21723217-8cf8-4856-863a-8c8f9b9b9351',), {'run_idx': 2, 'sub_counter': 0})\n((), {'run_idx': 2, 'parent_counter': 2})\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #135~20.04.1-Ubuntu SMP Mon Oct 7 13:56:22 UTC 2024\nPython Version:  3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0]\n\nPackage Information\n\nlangchain_core: 0.3.29\nlangchain: 0.3.11\nlangsmith: 0.2.10\nlangchain_openai: 0.2.12\nlangchain_text_splitters: 0.3.5\nlanggraph_sdk: 0.1.51\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.11\nasync-timeout: Installed. No version info available.\nhttpx: 0.28.1\njsonpatch: 1.33\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.59.7\norjson: 3.10.14\npackaging: 24.2\npydantic: 2.10.3\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.37\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\nzstandard: Installed. No version info available.\n", "created_at": "2025-01-14", "closed_at": "2025-01-16", "labels": [], "State": "closed", "Author": "shengbo-ma"}
{"issue_number": 3005, "issue_title": "DOC: API reference doesn't mention the \u201cmessages\u201d  stream mode in stream_mode definition.", "issue_body": "Issue with current documentation:\nstream_mode (Optional[Union[StreamMode, list[StreamMode]]], default: None ) \u2013 The mode to stream output, defaults to self.stream_mode. Options are 'values', 'updates', and 'debug'. values: Emit the current values of the state for each step. updates: Emit only the updates to the state for each step. Output is a dict with the node name as key and the updated values as value. debug: Emit debug events for each step.\nFor example, this is the definition of stream_mode in the async meth under the CompiledStateGraph  url with mention for  'values', 'updates', and 'debug' but no 'messages'. I thought it was deprecated or something. But I tried it out and it works.\nIdea or request for content:\nadd the 'messages' stream mode and it's definition to the stream_mode definition.", "created_at": "2025-01-13", "closed_at": null, "labels": ["documentation"], "State": "open", "Author": "habib-source"}
{"issue_number": 3003, "issue_title": "Update_State not working well during interruption while using Command function to navigate graph but works well with add_edge functionality.  ", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n\nimport random\nfrom typing_extensions import TypedDict, Literal\n\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.types import Command\nfrom langgraph.checkpoint.memory import MemorySaver\n\n\n\nclass State(TypedDict):\n    input: str\n\n\ndef step_1(state:State) -> Command[Literal[\"step_2\"]]:\n    print(\"---Step 1---\")\n    return Command(goto=\"step_2\",update={\"input\":\"step1\"})\n\n\ndef step_2(state:State) -> Command[Literal[\"step_3\"]]:\n    print(\"---Step 2---\")\n    return Command(goto=\"step_3\",update={\"input\":\"step2\"})\n\n\ndef step_3(state:State) -> Command[Literal[END]]:\n    print(\"---Step 3---\")\n    return Command(goto=END, update={\"input\":\"step2\"})\n\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"step_1\", step_1)\nbuilder.add_node(\"step_2\", step_2)\nbuilder.add_node(\"step_3\", step_3)\nbuilder.add_edge(START, \"step_1\")\n\n\n# Set up memory\nmemory = MemorySaver()\n\n# Add\ngraph = builder.compile(checkpointer=memory, interrupt_before=[\"step_2\"])\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n\ninitial_input = {\"input\": \"hello world\"}\n\n# Thread\nthread = {\"configurable\": {\"thread_id\": \"1\"}}\n\ngraph.invoke(initial_input,thread)\ngraph.get_state(thread).next # will have some value\ngraph.update_state(thread, {\"input\":\"outer_input\"})\ngraph.get_state(thread).next                # will be empty---issue\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nHi Harrison\nI hope you are doing well. I came across a problem while using Lang graph. I would directly jump into it.\nI made few nodes for my project and I used interrupt before to get input from human to edit node. I also used command function's goto rather than using add_edge function to jump from one  node to another. When I update the state in between two nodes while being in interrupted state, it returns graph.get_state().next to be empty.\nWhen I use add_edge functionality to navigate the graph, and update the state in between while in interrupted state, It works fine. Most probably command(goto=\"some_node\",update=...) returns the next state dynamically which the update_state is not providing/returning. If there's any way we could tell the graph to edit the graph's next state while using update_state function, please let me know.\nPlease have a look into it.\nBest regards,\nDaksh Arora\nSystem Info\ngoogle colab\npython -m langchain_core.sys_info", "created_at": "2025-01-13", "closed_at": "2025-01-16", "labels": ["investigate"], "State": "closed", "Author": "DAKSH1-HUB"}
{"issue_number": 2992, "issue_title": "When the graph includes an async `BaseCheckpointSaver` and uses `get_state_history`, the program hangs", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport asyncio\n\nfrom langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\nfrom langgraph.graph import END, START, MessagesState, StateGraph\n\n\nasync def main() -> None:\n    builder = StateGraph(MessagesState)\n    builder.add_node(\"foo\", lambda _: None)\n    builder.add_edge(START, \"foo\")\n    builder.add_edge(\"foo\", END)\n\n    async with AsyncSqliteSaver.from_conn_string(\":memory:\") as checkpointer:\n        graph = builder.compile(checkpointer=checkpointer)\n        config = {\"configurable\": {\"thread_id\": \"1\"}}\n        await graph.ainvoke({\"messages\": []}, config)\n        for state in graph.get_state_history(config):\n            print(state)\n\n        # async for state in graph.aget_state_history(config):\n        #     print(state)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nDescription\nWhen using a graph with async checkpointer(sqlite / postgres) and calling sync method get_state_history, the program blocks and cannot continue running.\nI expect the sync method to either throw a NotImplementedError exception or continue running, but without leveraging the benefits of async.\nSystem Info\nSystem Information\n\nOS:  Linux\nPython Version:  3.13.1 (main, Dec  4 2024, 08:54:15) [GCC 13.2.0]\n\nPackage Information\n\nlangchain_core: 0.3.29\nlanggraph: 0.2.62\nlanggraph-checkpoint-sqlite: 2.0.1\nlanggraph-checkpoint-postgres: 2.0.10\n", "created_at": "2025-01-10", "closed_at": "2025-01-14", "labels": [], "State": "closed", "Author": "gbaian10"}
{"issue_number": 2991, "issue_title": "Failed to batch ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError('422 Client Error: unknown for url: https://api.smith.langchain.com/runs/batch', '{\"detail\":\"Invalid identifiers received for run_id:...", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nLANGCHAIN_TRACING_V2=\"true\"\nLANGCHAIN_API_KEY=\"<key starting with lsv2>\"\nLANGCHAIN_PROJECT=\"my-project\"\n\n\n// \"langgraph.json\"\n\n{\n  \"dependencies\": [\".\"],\n  \"graphs\": {\n    \"telescope_v4\": \"./path_to_graph/graph.py:app\"\n  },\n  \"env\": \"./.env\"\n}\nError Message and Stack Trace (if applicable)\nFailed to batch ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError('422 Client Error: unknown for url: https://api.smith.langchain.com/runs/batch', '{\"detail\":\"Invalid identifiers received for run_id:2ff3004e-72d4-42ff-a4b1-9c8111a63ebf trace_id:UUID(\\'1efcf892-f23f-6856-8e1b-96da72ff5d2a\\') dotted_order:20250110T192918543695Z1efcf892-f23f-6856-8e1b-96da72ff5d2a.20250110T192918579965Z16f0f4e7-6bd4-4ea1-b6d2-b4652d1b7122.20250110T192918598421Z76086b6b-2754-4bf4-a06b-35886f709bbf.20250110T192919677228Z2ff3004e-72d4-42ff-a4b1-9c8111a63ebf parent_run_id:76086b6b-2754-4bf4-a06b-35886f709bbf\"}')\nDescription\nHello,\nI'm using Langgraph and langgraph-cli to develop an agent, but I have issues when looking for traces in Langsmith. The batch ingest calls for run traces all fail, and I checked the api key 10 times but I still get the issue. I increased my usage limits as well, to no avail.\nThis issue seems to come from langgraph-cli, since from the same .env I have a fastapi server using langgraph (another graph), and this one does send logs to langsmith properly.\nAny ideas what could be the issue?\nCheers, Olivier\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 22.5.0: Mon Apr 24 20:52:24 PDT 2023; root:xnu-8796.121.2~5/RELEASE_ARM64_T6000\nPython Version:  3.11.9 (main, Sep 23 2024, 23:17:37) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n\nPackage Information\n\nlangchain_core: 0.3.29\nlangchain: 0.3.14\nlangchain_community: 0.3.1\nlangsmith: 0.1.125\nlangchain_anthropic: 0.2.1\nlangchain_google_vertexai: 2.0.1\nlangchain_openai: 0.2.1\nlangchain_text_splitters: 0.3.5\nlanggraph_api: 0.0.15\nlanggraph_cli: 0.1.65\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.48\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.10.5\nanthropic: 0.34.2\nanthropic[vertexai]: Installed. No version info available.\nasync-timeout: 4.0.3\nclick: 8.1.7\ncryptography: 43.0.3\ndataclasses-json: 0.5.7\ndefusedxml: 0.7.1\ngoogle-cloud-aiplatform: 1.67.1\ngoogle-cloud-storage: 2.17.0\nhttpx: 0.27.2\nhttpx-sse: 0.4.0\njsonpatch: 1.33\njsonschema-rs: 0.25.0\nlangchain-mistralai: Installed. No version info available.\nlanggraph: 0.2.61\nlanggraph-checkpoint: 2.0.9\nnumpy: 1.25.2\nopenai: 1.46.0\norjson: 3.10.7\npackaging: 23.2\npydantic: 2.9.2\npydantic-settings: 2.5.2\npyjwt: 2.10.1\npython-dotenv: 0.21.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nSQLAlchemy: 2.0.35\nsse-starlette: 2.1.3\nstarlette: 0.38.6\nstructlog: 24.4.0\ntenacity: 8.5.0\ntiktoken: 0.7.0\ntyping-extensions: 4.12.2\nuvicorn: 0.30.6\nwatchfiles: 1.0.3\n", "created_at": "2025-01-10", "closed_at": "2025-01-13", "labels": [], "State": "closed", "Author": "Layvier"}
{"issue_number": 2983, "issue_title": "Langgraph does not save memory into PostgresStore but instead .langgrapi_api folder", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nworkflow = StateGraph(AgentState, input=InputState, output=OutputState)\n\nconnection = POSTGRES_DB_STORE_CONNECTION //Postgres db connection string\nwith PostgresStore.from_conn_string(conn_string=connection) as store:\n    store.setup()\n    app = workflow.compile(store=store)\n\n//saving memory\ndef write_memory(state, config: RunnableConfig, store: BaseStore):\n\n    \"\"\"Reflect on the chat history and save a memory to the store.\"\"\"\n    \n    # Get the user ID from the config\n    user_id = config[\"configurable\"][\"user_id\"]\n    print(f\"user id: {user_id}\")\n\n    # Retrieve existing memory from the store\n    namespace = (\"memory\", user_id)\n    existing_memory = store.get(namespace, \"user_memory\")\n        \n    # Extract the memory\n    if existing_memory:\n        existing_memory_content = existing_memory.value.get('memory')\n    else:\n        existing_memory_content = \"No existing memory found.\"\n\n    # Format the memory in the system prompt\n    system_msg = CREATE_MEMORY_INSTRUCTION.format(memory=existing_memory_content)\n    new_memory = get_llm().invoke([SystemMessage(content=system_msg)]+state['messages'])\n\n    # Overwrite the existing memory in the store \n    key = \"user_memory\"\n    print(new_memory)\n    # Write value as a dictionary with a memory key\n    store.put(namespace, key, {\"memory\": new_memory.content})\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nThere is no exception. The store.setup() does create the schema (store table) in the postgres db. But when I save memory, it saves into the .langgraph_api folder instead of the postgres db.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.2.0: Fri Dec  6 19:02:12 PST 2024; root:xnu-11215.61.5~2/RELEASE_ARM64_T6031\nPython Version:  3.11.10 (main, Sep  7 2024, 01:03:31) [Clang 15.0.0 (clang-1500.3.9.4)]\n\nPackage Information\n\nlangchain_core: 0.3.22\nlangchain: 0.3.10\nlangchain_community: 0.3.10\nlangsmith: 0.1.147\nlangchain_chroma: 0.1.4\nlangchain_openai: 0.2.11\nlangchain_postgres: 0.0.12\nlangchain_text_splitters: 0.3.2\nlanggraph_api: 0.0.15\nlanggraph_cli: 0.1.65\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.48\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.10\nasync-timeout: Installed. No version info available.\nchromadb: 0.5.23\nclick: 8.1.7\ncryptography: 43.0.3\ndataclasses-json: 0.6.7\nfastapi: 0.115.6\nhttpx: 0.28.1\nhttpx-sse: 0.4.0\njsonpatch: 1.33\njsonschema-rs: 0.25.1\nlanggraph: 0.2.56\nlanggraph-checkpoint: 2.0.8\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.57.0\norjson: 3.10.12\npackaging: 24.2\npgvector: 0.2.5\npsycopg: 3.2.3\npsycopg-pool: 3.2.4\npydantic: 2.9.2\npydantic-settings: 2.6.1\npyjwt: 2.10.1\npython-dotenv: 1.0.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nsqlalchemy: 2.0.36\nSQLAlchemy: 2.0.36\nsse-starlette: 2.1.3\nstarlette: 0.41.3\nstructlog: 24.4.0\ntenacity: 8.5.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\nuvicorn: 0.32.1\nwatchfiles: 1.0.0\n", "created_at": "2025-01-10", "closed_at": "2025-01-10", "labels": [], "State": "closed", "Author": "samsiuatpurple"}
{"issue_number": 2980, "issue_title": "param mismatch during langgraph-cli dev mode", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nlanggraph dev --config langgraph.json --host 127.0.0.1 --port 3000 --debug-port 5678 --no-browser\nError Message and Stack Trace (if applicable)\nFile \"python3.11/site-packages/langgraph_cli/cli.py\", line 608, in dev\n    run_server(\n  File \"python3.11/site-packages/langgraph_api/cli.py\", line 221, in run_server\n    uvicorn.run(\nTypeError: run() got an unexpected keyword argument 'auth'\nDescription\nlanggraphcli=0.1.65 has mismatch type params for the run_server() function inside dev()\ncommenting out the auth param fixes this error.\nbut this is the second time i'm running into param mismatches. this shouldn't happen every time.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.6.0: Thu Sep 12 23:35:10 PDT 2024; root:xnu-10063.141.1.701.1~1/RELEASE_ARM64_T6030\nPython Version:  3.11.11 (main, Dec  6 2024, 21:09:50) [Clang 18.1.8 ]\n\nPackage Information\n\nlangchain_core: 0.3.24\nlangchain: 0.3.7\nlangsmith: 0.1.147\nlangchain_openai: 0.2.9\nlangchain_text_splitters: 0.3.2\nlanggraph_api: 0.0.7\nlanggraph_cli: 0.1.65\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.44\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.10.8\nasync-timeout: Installed. No version info available.\nclick: 8.1.8\ncryptography: 43.0.3\nhttpx: 0.28.1\njsonpatch: 1.33\njsonschema-rs: 0.25.1\nlanggraph: 0.2.59\nlanggraph-checkpoint: 2.0.9\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.55.3\norjson: 3.10.12\npackaging: 24.2\npydantic: 2.10.3\npyjwt: 2.10.1\npython-dotenv: 1.0.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.35\nsse-starlette: 2.1.3\nstarlette: 0.41.3\nstructlog: 24.4.0\ntenacity: 8.5.0\ntiktoken: 0.7.0\ntyping-extensions: 4.12.2\nuvicorn: 0.32.1\nwatchfiles: 1.0.3\n", "created_at": "2025-01-10", "closed_at": "2025-01-10", "labels": [], "State": "closed", "Author": "kingsotn-twelve"}
{"issue_number": 2979, "issue_title": "Interrupt(). How to interrupt for the second time", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\ndef ask_user_node(state: LookupState) -> Command[Literal['lookup_node']]:\n        agent_request = state['userAgentInteractionInfo']['agentRequest']\n\n        if 'userResponse' in state['userAgentInteractionInfo']:\n            del state['userAgentInteractionInfo']['userResponse']\n\n        user_response = interrupt({\n            'id': str(uuid.uuid4()),\n            'request': agent_request\n        })\n\n        if user_response:\n            return Command(goto='lookup_node',\n                           resume={'userAgentInteractionInfo': {'agentRequest': agent_request}},\n                           update={'messages': [HumanMessage(content=user_response, name=\"User_Response\")], 'userAgentInteractionInfo': {'agentRequest': '', 'userResponse': ''}})\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nI know the default behavior of interrupt will invoke GraphInterrupt for the first time. When the same node has the interrupt its not halting the execution instead its taking the same old cached value, so is there any extra argument that I can pass so that the interrupt will halt the execution for the second time and wait for the user input\nSystem Info\npython -m langchain_core.sys_info", "created_at": "2025-01-10", "closed_at": "2025-01-10", "labels": [], "State": "closed", "Author": "Saisiva123"}
{"issue_number": 2968, "issue_title": "Execution goes into infinite loop ", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport os\nos.environ['ANTHROPIC_API_KEY'] = 'key'\n\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.tools import tool\nfrom langgraph.graph import StateGraph, MessagesState, START, END\nfrom langgraph.prebuilt import ToolNode, tools_condition\nfrom langchain_core.messages import SystemMessage\nfrom typing import List, Callable\nfrom langchain.agents import create_tool_calling_agent, AgentExecutor\nfrom langchain import hub\nfrom typing import Annotated\nfrom langgraph.graph.message import add_messages\nimport requests, json\nfrom langgraph.types import Command\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_core.messages import HumanMessage, AIMessage\nfrom typing import Literal, TypedDict, Union\n\nllm = ChatAnthropic(model=\"claude-3-5-haiku-20241022\")\n\n# Define graph state\nclass AgentState(MessagesState):\n    messages: Annotated[list, add_messages]\n\n\ndef push_into_db(brand_payload):\n\n    headers = {'Content-Type': 'application/json'}\n    \n    response = requests.request('POST', 'my_api', data=json.dumps(brand_payload), headers=headers, timeout=180)\n    response_content = json.loads(response.content)\n    \n    return response_content\n\n@tool\ndef insert_brand_tool(brand_name: str):\n    '''This will insert the brand data into database and return object id'''\n    try:\n        brand_payload = {\"name\": brand_name, \"journey_type\": \"social\"}\n        response = push_into_db(brand_payload)\n\n        if response[\"success\"] == True:\n            brandId = response[\"data\"][\"brandId\"]\n            \n            return f\"Brand name inserted with object id {brandId}\"\n        else:\n            return \"Could not insert brand into database.Please try again.\"\n            \n    except Exception as e:\n        return \"Could not insert brand into database.Please try again.\"\n\n@tool\ndef get_image(img_path: str):\n    '''This will read the given web url image path and download it'''\n    try:\n        img_response = requests.get(img_path)\n\n        img_name = os.path.basename(img_path)    \n        with open(img_name, \"wb\") as f:\n            f.write(img_response.content)\n        \n        return f\"Image has been downloaded, status is successfull. No worker needed.\"\n    except Exception as e:\n        return \"Cannot read image.\"\n\nmembers = [\"INSERT_DATABASE_AGENT\", \"UPLOAD_IMAGE_AGENT\", \"FINISH\"]\n\nclass Router(TypedDict):\n    \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n\n    next: Union[Literal[\"INSERT_DATABASE_AGENT\"], Literal[\"UPLOAD_IMAGE_AGENT\"], Literal[\"FINISH\"]]\n\nsystem_prompt = (\n    \"You are a supervisor tasked with managing a conversation between the\"\n    f\" following workers: {members}. Given the following user request,\"\n    \" respond with the worker to act next. Each worker will perform a\"\n    \" task and respond with their results and status. WHEN SUCCESSFULL, RESPOND MESSAGE WITH FINISH\"\n)\n\ndef greet_node(state: AgentState):\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n    ] + [state[\"messages\"][-1]]\n    response = llm.with_structured_output(Router).invoke(messages)\n    \n    goto = response[\"next\"]\n    if goto == \"FINISH\":\n        goto = END\n\n    return Command(goto=\"UPLOAD_IMAGE_AGENT\")\n\ninsert_database_agent = create_react_agent(llm, tools=[insert_brand_tool])\n\ndef router_function_to_db(state: MessagesState):\n    result = insert_database_agent.invoke(state)\n    return Command(\n        update={\n            \"messages\": [\n                HumanMessage(content=result[\"messages\"][-1].content, name=\"INSERT_DATABASE_AGENT\")\n            ]\n        },\n        goto=\"GREETING_AGENT\",\n    )\n    \nupload_image_agent = create_react_agent(llm, tools=[get_image], state_modifier=\"Response with FINISH if image is downloaded successfully.\")\n\ndef router_function_to_image_upload(state: MessagesState):\n    result = upload_image_agent.invoke(state)\n    return Command(\n        update={\n            \"messages\": [\n                HumanMessage(content=result[\"messages\"][-1].content, name=\"UPLOAD_IMAGE_AGENT\")\n            ]\n        },\n        goto=\"GREETING_AGENT\",\n    )\n\nbuilder = StateGraph(MessagesState)\n\nbuilder.add_edge(START, \"GREETING_AGENT\")\nbuilder.add_node(\"GREETING_AGENT\", greet_node)\nbuilder.add_node(\"INSERT_DATABASE_AGENT\", router_function_to_db)\nbuilder.add_node(\"UPLOAD_IMAGE_AGENT\", router_function_to_image_upload)\n\ngraph = builder.compile()\n\nimg_bytes = graph.get_graph().draw_mermaid_png()\n\nwith open(\"graph_mermaid.png\", \"wb\") as f:\n     f.write(img_bytes)\n    \n\ninput={\"messages\": [\"Hi how are you? Can you read the image url(A valid s3 url)\"]}\n\nfor output in graph.stream(input):\n    print(output)\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nThis flow  is the improvisation from the langgraph multi agent supervisor docs.\nThis workflow could not reach to \"FINISH\" state (only 1 out of 20 time if I run the code). Please note everything else is working fine; only the issue is when i give the input- download the image, it gets downloaded by \"upload_image_agent\" but my \"greeting_agent\" could not able to generate \"FINISH\"  respose in its stack so it is not able to exit.  What best should be the best promt to get into \"FINISH\" state?\nPlease can anyone help?\nSystem Info\nversions-\nlanggraph-                     0.2.61\nlanggraph-checkpoint-          2.0.9\nlanggraph-sdk-                 0.1.48", "created_at": "2025-01-09", "closed_at": "2025-01-09", "labels": [], "State": "closed", "Author": "asrays"}
{"issue_number": 2965, "issue_title": "Command do not render graph if nodes are defined within a class.", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport random\n\nfrom langgraph.graph import START, StateGraph\nfrom langgraph.types import Command\nfrom typing_extensions import Literal, TypedDict\n\n\n# Define graph state\nclass State(TypedDict):\n    foo: str\n\n\n# Define the nodes\n\n\nclass MyGraph:\n    def node_a(self, state: State) -> Command[Literal[\"node_b\", \"node_c\"]]:\n        print(\"Called A\")\n        value = random.choice([\"a\", \"b\"])\n        # this is a replacement for a conditional edge function\n        if value == \"a\":\n            goto = \"node_b\"\n        else:\n            goto = \"node_c\"\n\n        # note how Command allows you to BOTH update the graph state AND route to the next node\n        return Command(\n            # this is the state update\n            update={\"foo\": value},\n            # this is a replacement for an edge\n            goto=goto,\n        )\n\n    # Nodes B and C are unchanged\n\n    def node_b(self, state: State):\n        print(\"Called B\")\n        return {\"foo\": state[\"foo\"] + \"b\"}\n\n    def node_c(self, state: State):\n        print(\"Called C\")\n        return {\"foo\": state[\"foo\"] + \"c\"}\n\n    def build_graph(self):\n        builder = StateGraph(State)\n        builder.add_edge(START, \"node_a\")\n        builder.add_node(\"node_a\", self.node_a)\n        builder.add_node(\"node_b\", self.node_b)\n        builder.add_node(\"node_c\", self.node_c)\n        # NOTE: there are no edges between nodes A, B and C!\n\n        graph = builder.compile()\n        return graph\n\n\nfrom IPython.display import display, Image\n\ngraph = MyGraph().build_graph()\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nWhen using command inside a class the edges are not rendered. It works if nodes are outside (or probably if static methods?)\nI adapted this code: https://langchain-ai.github.io/langgraph/how-tos/command/#define-graph\nAnd the generated graph is this:\n\nSystem Info\nlanggraph version 0.2.61", "created_at": "2025-01-09", "closed_at": "2025-01-14", "labels": [], "State": "closed", "Author": "SergioG-M"}
{"issue_number": 2964, "issue_title": "Cannot use Enums for node names", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom enum import Enum\nfrom langchain_core.messages import HumanMessage\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.prebuilt import create_react_agent\n\nfrom typing import Literal\nfrom typing_extensions import TypedDict\n\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import MessagesState, END\nfrom langgraph.types import Command\n\nclass Members(Enum):\n    researcher=\"researcher\"\n    coder=\"coder\"\n\nmembers = [Members.researcher, Members.coder]\n# Our team supervisor is an LLM node. It just picks the next agent to process\n# and decides when the work is completed\noptions = members + [\"FINISH\"]\n\nsystem_prompt = (\n    \"You are a supervisor tasked with managing a conversation between the\"\n    f\" following workers: {members}. Given the following user request,\"\n    \" respond with the worker to act next. Each worker will perform a\"\n    \" task and respond with their results and status. When finished,\"\n    \" respond with FINISH.\"\n)\n\n\nclass Router(TypedDict):\n    \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n\n    next: Literal[*options]\n\n\nllm = ChatOpenAI(model=\"gpt-4o\")\n\n\ndef supervisor_node(state: MessagesState) -> Command[Literal[*members, \"__end__\"]]:\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n    ] + state[\"messages\"]\n    response = llm.with_structured_output(Router).invoke(messages)\n    goto = response[\"next\"]\n    if goto == \"FINISH\":\n        goto = END\n\n    return Command(goto=goto)\nresearch_agent = create_react_agent(\n    llm, tools=[], state_modifier=\"You are a researcher. DO NOT do any math.\"\n)\n\n\ndef research_node(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\n    result = research_agent.invoke(state)\n    return Command(\n        update={\n            \"messages\": [\n                HumanMessage(content=result[\"messages\"][-1].content, name=\"researcher\")\n            ]\n        },\n        goto=\"supervisor\",\n    )\n\n\n# NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION, WHICH CAN BE UNSAFE WHEN NOT SANDBOXED\ncode_agent = create_react_agent(llm, tools=[])\n\n\ndef code_node(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\n    result = code_agent.invoke(state)\n    return Command(\n        update={\n            \"messages\": [\n                HumanMessage(content=result[\"messages\"][-1].content, name=\"coder\")\n            ]\n        },\n        goto=\"supervisor\",\n    )\n\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_edge(START, \"supervisor\")\nbuilder.add_node(\"supervisor\", supervisor_node)\nbuilder.add_node(\"researcher\", research_node)\nbuilder.add_node(\"coder\", code_node)\ngraph = builder.compile()\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[4], line 90\n     88 builder.add_node(\"researcher\", research_node)\n     89 builder.add_node(\"coder\", code_node)\n---> 90 graph = builder.compile()\n\nFile ~/projects/ditto/poc/assignments/venv/lib/python3.11/site-packages/langgraph/graph/state.py:510, in StateGraph.compile(self, checkpointer, store, interrupt_before, interrupt_after, debug)\n    507 interrupt_after = interrupt_after or []\n    509 # validate the graph\n--> 510 self.validate(\n    511     interrupt=(\n    512         (interrupt_before if interrupt_before != \"*\" else []) + interrupt_after\n    513         if interrupt_after != \"*\"\n    514         else []\n    515     )\n    516 )\n    518 # prepare output channels\n    519 output_channels = (\n    520     \"__root__\"\n    521     if len(self.schemas[self.output]) == 1\n   (...)\n    527     ]\n    528 )\n\nFile ~/projects/ditto/poc/assignments/venv/lib/python3.11/site-packages/langgraph/graph/graph.py:405, in Graph.validate(self, interrupt)\n    403 for target in all_targets:\n    404     if target not in self.nodes and target != END:\n--> 405         raise ValueError(f\"Found edge ending at unknown node `{target}`\")\n    406 # validate interrupts\n    407 if interrupt:\n\nValueError: Found edge ending at unknown node `Members.coder`\nDescription\nI'm trying to use an Enum for my node/agent names to make the code more maintainable, but it seems the graph is looking for a node with the actual name of the enum variable not the value.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 22.5.0: Mon Apr 24 20:51:50 PDT 2023; root:xnu-8796.121.2~5/RELEASE_X86_64\nPython Version:  3.11.6 (v3.11.6:8b6ee5ba3b, Oct  2 2023, 11:18:21) [Clang 13.0.0 (clang-1300.0.29.30)]\n\nPackage Information\n\nlangchain_core: 0.3.29\nlangchain: 0.3.9\nlangchain_community: 0.3.8\nlangsmith: 0.1.147\nlangchain_openai: 0.2.10\nlangchain_text_splitters: 0.3.2\nlanggraph_sdk: 0.1.48\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.11\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.7\nhttpx: 0.28.1\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.59.3\norjson: 3.10.13\npackaging: 24.2\npydantic: 2.10.4\npydantic-settings: 2.7.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.35\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\n", "created_at": "2025-01-09", "closed_at": "2025-01-22", "labels": ["question"], "State": "closed", "Author": "laurencejennings"}
{"issue_number": 2962, "issue_title": "when define tool with `return Command` and call tools multiple at once,  using stream_mode=updates,  value type of result  change from `dict` to `list`.", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport os\n\nfrom langchain_core.messages import ToolMessage\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain_core.tools import tool\nfrom langchain_core.tools.base import InjectedToolCallId\nfrom langgraph.types import Command\n\nfrom typing_extensions import Annotated\nimport dotenv\ndotenv.load_dotenv()\n\n@tool\ndef add(\n    a: int,\n    b: int,\n    tool_call_id: Annotated[str, InjectedToolCallId],\n    config: RunnableConfig,\n):\n    \"\"\"add two numbers\"\"\"\n\n    result = a + b\n\n    return Command(\n        update={\n            \"messages\": [ToolMessage(f\"add result: {result}\", tool_call_id=tool_call_id)],\n        }\n    )\n\n\n@tool\ndef sub(\n    a: int,\n    b: int,\n    tool_call_id: Annotated[str, InjectedToolCallId],\n    config: RunnableConfig,\n):\n    \"\"\"sub two numbers\"\"\"\n\n    result = a + b\n\n    return f\"sub result: {result}\"\n\n\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\n\nmodel = ChatOpenAI(\n    model=\"gpt-4o\",\n    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n    base_url=f'{os.environ[\"AZURE_OPENAI_ENDPOINT\"]}/v1',\n)\n\nfrom langgraph.checkpoint.memory import MemorySaver\n\nmemory = MemorySaver()\n\ntools = [add, sub]\nagent = create_react_agent(model, tools=tools, checkpointer=memory)\n\nconfig = {\n    \"configurable\": {\"thread_id\": \"1\"},\n}\n\n# use add tool\nfor chunk in agent.stream(\n    input={\n        \"messages\": [\n            (\n                \"user\",\n                \"add(1,1), add(1,2), add(1,3) at once\",\n            ),\n        ]\n    },\n    config=config,\n    stream_mode=\"updates\",\n):\n    for node, values in chunk.items():\n        print(f\"Receiving update from node: '{node}'\")\n        print(f\"type of values: {type(values)}\")\n        print(values)\n        print(\"\\n\\n\")\n\nprint(\"===========================================================\\n\\n\")\n\n\n# use sub tool\nfor chunk in agent.stream(\n    input={\n        \"messages\": [\n            (\n                \"user\",\n                \"sub(1,1), sub(1,2), sub(1,3) at once\",\n            ),\n        ]\n    },\n    config=config,\n    stream_mode=\"updates\",\n):\n    for node, values in chunk.items():\n        print(f\"Receiving update from node: '{node}'\")\n        print(f\"type of values: {type(values)}\")\n        print(values)\n        print(\"\\n\\n\")\n\nprint(\"======================message history=================\\n\\n\")\ncur_state = agent.get_state(config)\nmessages = cur_state.values.get(\"messages\", [])\nfor message in messages:\n    message.pretty_print()\nError Message and Stack Trace (if applicable)\nReceiving update from node: 'agent'\ntype of values: <class 'dict'>\n{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ZZg7TJazTcfHGcq3gOiedOqt', 'function': {'arguments': '{\"a\": 1, \"b\": 1}', 'name': 'add'}, 'type': 'function'}, {'id': 'call_fz3zfIwaZV6KK9zEBCRG4jdf', 'function': {'arguments': '{\"a\": 1, \"b\": 2}', 'name': 'add'}, 'type': 'function'}, {'id': 'call_HMbvpnTtRNkxwXsu9sq7SFbo', 'function': {'arguments': '{\"a\": 1, \"b\": 3}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 67, 'prompt_tokens': 85, 'total_tokens': 152, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f3927aa00d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-4dc7bc54-bf3e-42ee-8a53-b9444137a08b-0', tool_calls=[{'name': 'add', 'args': {'a': 1, 'b': 1}, 'id': 'call_ZZg7TJazTcfHGcq3gOiedOqt', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 1, 'b': 2}, 'id': 'call_fz3zfIwaZV6KK9zEBCRG4jdf', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 1, 'b': 3}, 'id': 'call_HMbvpnTtRNkxwXsu9sq7SFbo', 'type': 'tool_call'}], usage_metadata={'input_tokens': 85, 'output_tokens': 67, 'total_tokens': 152, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n\n\n\nReceiving update from node: 'tools'\ntype of values: <class 'list'>\n[{'messages': [ToolMessage(content='add result: 2', name='add', id='38f447fe-0939-4dec-af5b-b772d94bbbe8', tool_call_id='call_ZZg7TJazTcfHGcq3gOiedOqt')]}, {'messages': [ToolMessage(content='add result: 3', name='add', id='17b95c26-20bd-49f4-817b-254b8fd469b4', tool_call_id='call_fz3zfIwaZV6KK9zEBCRG4jdf')]}, {'messages': [ToolMessage(content='add result: 4', name='add', id='672d6d95-4248-420e-95e0-f1d4a315f7ce', tool_call_id='call_HMbvpnTtRNkxwXsu9sq7SFbo')]}]\n\n\n\nReceiving update from node: 'agent'\ntype of values: <class 'dict'>\n{'messages': [AIMessage(content='The results of the additions are as follows:\\n- \\\\(1 + 1 = 2\\\\)\\n- \\\\(1 + 2 = 3\\\\)\\n- \\\\(1 + 3 = 4\\\\)', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 183, 'total_tokens': 226, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f3927aa00d', 'finish_reason': 'stop', 'logprobs': None}, id='run-19450b04-0e81-4f43-ba78-f55d667c5eb7-0', usage_metadata={'input_tokens': 183, 'output_tokens': 43, 'total_tokens': 226, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n\n\n\n===========================================================\n\n\nReceiving update from node: 'agent'\ntype of values: <class 'dict'>\n{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_YFPOUghRCkW1Fo1tZ9uSfOu4', 'function': {'arguments': '{\"a\": 1, \"b\": 1}', 'name': 'sub'}, 'type': 'function'}, {'id': 'call_ghESCK5aIuqEaqD6ofiZR14v', 'function': {'arguments': '{\"a\": 1, \"b\": 2}', 'name': 'sub'}, 'type': 'function'}, {'id': 'call_dALc18L81Jptz2cz3w8YDTf1', 'function': {'arguments': '{\"a\": 1, \"b\": 3}', 'name': 'sub'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 67, 'prompt_tokens': 253, 'total_tokens': 320, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f3927aa00d', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-325f3183-5377-44f6-9deb-445b10b534c7-0', tool_calls=[{'name': 'sub', 'args': {'a': 1, 'b': 1}, 'id': 'call_YFPOUghRCkW1Fo1tZ9uSfOu4', 'type': 'tool_call'}, {'name': 'sub', 'args': {'a': 1, 'b': 2}, 'id': 'call_ghESCK5aIuqEaqD6ofiZR14v', 'type': 'tool_call'}, {'name': 'sub', 'args': {'a': 1, 'b': 3}, 'id': 'call_dALc18L81Jptz2cz3w8YDTf1', 'type': 'tool_call'}], usage_metadata={'input_tokens': 253, 'output_tokens': 67, 'total_tokens': 320, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n\n\n\nReceiving update from node: 'tools'\ntype of values: <class 'dict'>\n{'messages': [ToolMessage(content='sub result: 2', name='sub', id='2ad51609-06a2-4126-861e-e9682e316c19', tool_call_id='call_YFPOUghRCkW1Fo1tZ9uSfOu4'), ToolMessage(content='sub result: 3', name='sub', id='edefea05-aa33-4a0e-bf24-7fbf367a86be', tool_call_id='call_ghESCK5aIuqEaqD6ofiZR14v'), ToolMessage(content='sub result: 4', name='sub', id='f8295690-e247-44a9-bd50-1cfbb84543f7', tool_call_id='call_dALc18L81Jptz2cz3w8YDTf1')]}\n\n\n\nReceiving update from node: 'agent'\ntype of values: <class 'dict'>\n{'messages': [AIMessage(content='It seems there was an error in the response. Let me correct that for you.\\n\\nThe results of the subtractions are as follows:\\n- \\\\(1 - 1 = 0\\\\)\\n- \\\\(1 - 2 = -1\\\\)\\n- \\\\(1 - 3 = -2\\\\)', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 61, 'prompt_tokens': 351, 'total_tokens': 412, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f3927aa00d', 'finish_reason': 'stop', 'logprobs': None}, id='run-e0aa5255-b08f-4895-87d2-6d22a76ea555-0', usage_metadata={'input_tokens': 351, 'output_tokens': 61, 'total_tokens': 412, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n\n\n\n======================message history=================\n\n\n================================ Human Message =================================\n\nadd(1,1), add(1,2), add(1,3) at once\n================================== Ai Message ==================================\nTool Calls:\n  add (call_ZZg7TJazTcfHGcq3gOiedOqt)\n Call ID: call_ZZg7TJazTcfHGcq3gOiedOqt\n  Args:\n    a: 1\n    b: 1\n  add (call_fz3zfIwaZV6KK9zEBCRG4jdf)\n Call ID: call_fz3zfIwaZV6KK9zEBCRG4jdf\n  Args:\n    a: 1\n    b: 2\n  add (call_HMbvpnTtRNkxwXsu9sq7SFbo)\n Call ID: call_HMbvpnTtRNkxwXsu9sq7SFbo\n  Args:\n    a: 1\n    b: 3\n================================= Tool Message =================================\nName: add\n\nadd result: 2\n================================= Tool Message =================================\nName: add\n\nadd result: 3\n================================= Tool Message =================================\nName: add\n\nadd result: 4\n================================== Ai Message ==================================\n\nThe results of the additions are as follows:\n- \\(1 + 1 = 2\\)\n- \\(1 + 2 = 3\\)\n- \\(1 + 3 = 4\\)\n================================ Human Message =================================\n\nsub(1,1), sub(1,2), sub(1,3) at once\n================================== Ai Message ==================================\nTool Calls:\n  sub (call_YFPOUghRCkW1Fo1tZ9uSfOu4)\n Call ID: call_YFPOUghRCkW1Fo1tZ9uSfOu4\n  Args:\n    a: 1\n    b: 1\n  sub (call_ghESCK5aIuqEaqD6ofiZR14v)\n Call ID: call_ghESCK5aIuqEaqD6ofiZR14v\n  Args:\n    a: 1\n    b: 2\n  sub (call_dALc18L81Jptz2cz3w8YDTf1)\n Call ID: call_dALc18L81Jptz2cz3w8YDTf1\n  Args:\n    a: 1\n    b: 3\n================================= Tool Message =================================\nName: sub\n\nsub result: 2\n================================= Tool Message =================================\nName: sub\n\nsub result: 3\n================================= Tool Message =================================\nName: sub\n\nsub result: 4\n================================== Ai Message ==================================\n\nIt seems there was an error in the response. Let me correct that for you.\n\nThe results of the subtractions are as follows:\n- \\(1 - 1 = 0\\)\n- \\(1 - 2 = -1\\)\n- \\(1 - 3 = -2\\)\nDescription\nwhen call add multiple times at once, type of values is list.\nwhen call sub multiple times at once, type of values if dict.\nit should be the same type, the correct type is dict.\nSystem Info\nPackage Information\n\nlangchain_core: 0.3.25\nlangsmith: 0.1.140\nlangchain_openai: 0.2.6\nlanggraph_sdk: 0.1.47\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nhttpx: 0.27.0\njsonpatch: 1.33\nopenai: 1.54.3\norjson: 3.10.3\npackaging: 24.0\npydantic: 2.7.4\nPyYAML: 6.0.1\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\n", "created_at": "2025-01-09", "closed_at": "2025-01-10", "labels": [], "State": "closed", "Author": "rayshen92"}
{"issue_number": 2952, "issue_title": "DOC: Unable to resolve column 'flight_id' ", "issue_body": "Issue with current documentation:\nexample code:\n\n\n\nlanggraph/docs/docs/tutorials/customer-support/customer-support.ipynb\n\n\n         Line 447\n      in\n      837f215\n\n\n\n\n\n\n \" \\\"SELECT flight_id FROM tickets WHERE ticket_no = ? AND passenger_id = ?\\\",\\n\", \n\n\n\n\n\n@tool\ndef cancel_ticket(ticket_no: str, *, config: RunnableConfig) -> str:\n    \"\"\"Cancel the user's ticket and remove it from the database.\"\"\"\n    configuration = config.get(\"configurable\", {})\n    passenger_id = configuration.get(\"passenger_id\", None)\n    if not passenger_id:\n        raise ValueError(\"No passenger ID configured.\")\n    conn = sqlite3.connect(db)\n    cursor = conn.cursor()\n\n    cursor.execute(\n        \"SELECT flight_id FROM ticket_flights WHERE ticket_no = ?\", (ticket_no,)\n    )\n    existing_ticket = cursor.fetchone()\n    if not existing_ticket:\n        cursor.close()\n        conn.close()\n        return \"No existing ticket found for the given ticket number.\"\n\n    # Check the signed-in user actually has this ticket\n    cursor.execute(\n        \"SELECT flight_id FROM tickets WHERE ticket_no = ? AND passenger_id = ?\",\n        (ticket_no, passenger_id),\n    )\n    current_ticket = cursor.fetchone()\n    if not current_ticket:\n        cursor.close()\n        conn.close()\n        return f\"Current signed-in passenger with ID {passenger_id} not the owner of ticket {ticket_no}\"\n\n    cursor.execute(\"DELETE FROM ticket_flights WHERE ticket_no = ?\", (ticket_no,))\n    conn.commit()\n\n    cursor.close()\n    conn.close()\n    return \"Ticket successfully cancelled.\"\n'flight_id' do not exist in table 'tickets'\nfix:\n \"SELECT flight_id FROM tickets WHERE ticket_no = ? AND passenger_id = ?\"\n\n change to:\n\n \"SELECT * FROM tickets WHERE ticket_no = ? AND passenger_id = ?\"\n\nIdea or request for content:\nNo response", "created_at": "2025-01-08", "closed_at": "2025-01-08", "labels": [], "State": "closed", "Author": "hdsuperman"}
{"issue_number": 2945, "issue_title": "How to make multiple branches execute in parallel with conditional branches", "issue_body": "          @gbaian10 for the first part of your question:\n\n\nQ1: When the start_key in add_edge is a list, the end_key can't be end.\n\nthis is fixed this in #1478\n\nQ2: When the input is a list, you must add the node before adding an edge.\n\nThis is a good point, but we discussed internally and decided to keep this as is for an extra layer of validation\n\nfor the remainder of this discussion / questions / examples you listed:\ngraph_builder.add_edge([\"A\", \"B\"], END)\n^ This will triggers C after both A and B have completed\ngraph_builder.add_edge(\"A\", END)\ngraph_builder.add_edge(\"B\", END)\n^ This will trigger C after either A or B have completed\nIn the last example you provided you can verify this by changing the edges to\nbuilder.add_edge([\"b\", \"c\"], \"e\")\nbuilder.add_edge([\"c\", \"d\"], \"e\")\nThe reason why the previous code snippet with builder.add_edge([\"b\", \"c\", \"d\"], \"e\") didn't work is because you need all of the nodes (b, c and d) to complete which they can't in that case.\nClosing the issue as the library is working as intended -- thank you for the questions and thoughtful discussion.\nOriginally posted by @vbarda in #1462 (comment)\n\n\nThe reason why the previous code snippet with builder.add_edge([\"b\", \"c\", \"d\"], \"e\") didn't work is because you need all of the nodes (b, c and d) to complete which they can't in that case.\n\nWhat should be done to make the branch execute in parallel when encountering this situation.\nHow can b or c be executed in parallel with d when b and c belong to different conditional branches.", "created_at": "2025-01-07", "closed_at": "2025-01-08", "labels": [], "State": "closed", "Author": "fuyuku"}
{"issue_number": 2943, "issue_title": "Support Clearing Annotated[list, operator.add] Fields in LangGraph State", "issue_body": "Issue with current documentation:\nContext: In LangGraph, the Annotated[list, operator.add] state field is very useful for accumulating results from parallel nodes. This is especially valuable in workflows where multiple nodes run in parallel, and their results need to be combined into a single list.\nHowever, this design comes with a significant limitation: once an Annotated[list, operator.add] field starts accumulating data, there is no way to clear its contents during the workflow. This leads to the following problems:\nThe list grows indefinitely, which can cause memory issues or exceed LangSmith's state size limit.\nOld data from previous nodes can interfere with subsequent steps in the graph.\nUse Case: Imagine a workflow with three parallel nodes that aggregate their outputs into a single state[\"relation_verify_info\"] field. After processing this information in a downstream node, it is desirable to clear the field to prevent it from growing indefinitely. However, with the current design, this is not possible.\nAdditional Context: This issue is particularly relevant for workflows with many parallel branches that feed into a single aggregation point. Without a way to clear the accumulated data, the workflow becomes harder to manage and scale.\nAdditional Context: This issue is particularly relevant for workflows with many parallel branches that feed into a single aggregation point. Without a way to clear the accumulated data, the workflow becomes harder to manage and scale.\nIdea or request for content:\nProposed Solution: Introduce a mechanism to clear Annotated[list, operator.add] fields during the workflow. Some possible approaches:\nAllow explicit clearing of the field at a specific node, e.g., state[\"relation_verify_info\"] = [].\nAdd a new annotation, such as Annotated[list, operator.add_and_clearable], which enables both accumulation and clearing.\nProvide a method like state.clear_add_field(\"relation_verify_info\") to programmatically clear the contents.\nImpact: This change would make LangGraph more flexible and efficient, especially for long-running workflows or workflows with high concurrency. It would also help prevent state size issues in LangSmith and improve overall usability.\nProposed Solution: Introduce a mechanism to clear Annotated[list, operator.add] fields during the workflow. Some possible approaches:\nAllow explicit clearing of the field at a specific node, e.g., state[\"relation_verify_info\"] = [].\nAdd a new annotation, such as Annotated[list, operator.add_and_clearable], which enables both accumulation and clearing.\nProvide a method like state.clear_add_field(\"relation_verify_info\") to programmatically clear the contents.\nImpact: This change would make LangGraph more flexible and efficient, especially for long-running workflows or workflows with high concurrency. It would also help prevent state size issues in LangSmith and improve overall usability.", "created_at": "2025-01-07", "closed_at": "2025-01-07", "labels": [], "State": "closed", "Author": "funnykeke"}
{"issue_number": 2942, "issue_title": "When I use interrupt and astream_events, the error RuntimeError: Called get_configurable outside of a runnable context occurs", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing_extensions import TypedDict, Literal\nfrom langgraph.graph import StateGraph, START, END, MessagesState\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.types import Command, interrupt\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.tools import tool\nfrom langchain_core.messages import AIMessage\nfrom IPython.display import Image, display\n\n\n@tool\ndef weather_search(city: str):\n    \"\"\"Search for the weather\"\"\"\n    print(\"----\")\n    print(f\"Searching for: {city}\")\n    print(\"----\")\n    return \"Sunny!\"\n\n\nmodel = ChatOpenAI(\n        model=\"glm-4-flash\",\n    ).bind_tools([weather_search])\n\n\nclass State(MessagesState):\n    \"\"\"Simple state.\"\"\"\n\n\ndef call_llm(state):\n    return {\"messages\": [model.invoke(state[\"messages\"])]}\n\n\ndef human_review_node(state) -> Command[Literal[\"call_llm\", \"run_tool\"]]:\n    last_message = state[\"messages\"][-1]\n    tool_call = last_message.tool_calls[-1]\n\n    # this is the value we'll be providing via Command(resume=<human_review>)\n    human_review = interrupt(\n        {\n            \"question\": \"Is this correct?\",\n            # Surface tool calls for review\n            \"tool_call\": tool_call,\n        }\n    )\n\n    review_action = human_review[\"action\"]\n    review_data = human_review.get(\"data\")\n\n    # if approved, call the tool\n    if review_action == \"continue\":\n        return Command(goto=\"run_tool\")\n\n    # update the AI message AND call tools\n    elif review_action == \"update\":\n        updated_message = {\n            \"role\": \"ai\",\n            \"content\": last_message.content,\n            \"tool_calls\": [\n                {\n                    \"id\": tool_call[\"id\"],\n                    \"name\": tool_call[\"name\"],\n                    # This the update provided by the human\n                    \"args\": review_data,\n                }\n            ],\n            # This is important - this needs to be the same as the message you replacing!\n            # Otherwise, it will show up as a separate message\n            \"id\": last_message.id,\n        }\n        return Command(goto=\"run_tool\", update={\"messages\": [updated_message]})\n\n    # provide feedback to LLM\n    elif review_action == \"feedback\":\n        # NOTE: we're adding feedback message as a ToolMessage\n        # to preserve the correct order in the message history\n        # (AI messages with tool calls need to be followed by tool call messages)\n        tool_message = {\n            \"role\": \"tool\",\n            # This is our natural language feedback\n            \"content\": review_data,\n            \"name\": tool_call[\"name\"],\n            \"tool_call_id\": tool_call[\"id\"],\n        }\n        return Command(goto=\"call_llm\", update={\"messages\": [tool_message]})\n\n\ndef run_tool(state):\n    new_messages = []\n    tools = {\"weather_search\": weather_search}\n    tool_calls = state[\"messages\"][-1].tool_calls\n    for tool_call in tool_calls:\n        tool = tools[tool_call[\"name\"]]\n        result = tool.invoke(tool_call[\"args\"])\n        new_messages.append(\n            {\n                \"role\": \"tool\",\n                \"name\": tool_call[\"name\"],\n                \"content\": result,\n                \"tool_call_id\": tool_call[\"id\"],\n            }\n        )\n    return {\"messages\": new_messages}\n\n\ndef route_after_llm(state) -> Literal[END, \"human_review_node\"]:\n    if len(state[\"messages\"][-1].tool_calls) == 0:\n        return END\n    else:\n        return \"human_review_node\"\n\n\nbuilder = StateGraph(State)\nbuilder.add_node(call_llm)\nbuilder.add_node(run_tool)\nbuilder.add_node(human_review_node)\nbuilder.add_edge(START, \"call_llm\")\nbuilder.add_conditional_edges(\"call_llm\", route_after_llm)\nbuilder.add_edge(\"run_tool\", \"call_llm\")\n\n# Set up memory\nmemory = MemorySaver()\n\n# Add\ngraph = builder.compile(checkpointer=memory)\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n\n# Input\ninitial_input = {\"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in sf?\"}]}\n\n# Thread\nthread = {\"configurable\": {\"thread_id\": \"2\"}}\n\n# Run the graph until the first interruption\nasync for event in graph.astream_events(initial_input, thread, stream_mode=\"updates\",version=\"v2\"):\n    print(event)\n    print(\"\\n\")\nError Message and Stack Trace (if applicable)\n{'event': 'on_chain_start', 'data': {'input': {'messages': [{'role': 'user', 'content': \"what's the weather in sf?\"}]}}, 'name': 'LangGraph', 'tags': [], 'run_id': 'c6bf9294-29b9-4331-9bc9-5349f79228a7', 'metadata': {'thread_id': '2'}, 'parent_ids': []}\n\n\n{'event': 'on_chain_start', 'data': {'input': {'messages': [{'role': 'user', 'content': \"what's the weather in sf?\"}]}}, 'name': '__start__', 'tags': ['graph:step:3', 'langsmith:hidden'], 'run_id': '5c86e9e9-b113-4776-bd7f-4f3e46da91ff', 'metadata': {'thread_id': '2', 'langgraph_step': 3, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_path': ('__pregel_pull', '__start__'), 'langgraph_checkpoint_ns': '__start__:107cb6c8-1895-c324-e839-bc82fb1e2737'}, 'parent_ids': ['c6bf9294-29b9-4331-9bc9-5349f79228a7']}\n\n\n{'event': 'on_chain_start', 'data': {'input': {'messages': [{'role': 'user', 'content': \"what's the weather in sf?\"}]}}, 'name': '_write', 'tags': ['seq:step:1', 'langsmith:hidden', 'langsmith:hidden'], 'run_id': '0c25e246-5cf7-44b3-b193-9b870a61bdfd', 'metadata': {'thread_id': '2', 'langgraph_step': 3, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_path': ('__pregel_pull', '__start__'), 'langgraph_checkpoint_ns': '__start__:107cb6c8-1895-c324-e839-bc82fb1e2737'}, 'parent_ids': ['c6bf9294-29b9-4331-9bc9-5349f79228a7', '5c86e9e9-b113-4776-bd7f-4f3e46da91ff']}\n\n\n{'event': 'on_chain_end', 'data': {'output': {'messages': [{'role': 'user', 'content': \"what's the weather in sf?\"}]}, 'input': {'messages': [{'role': 'user', 'content': \"what's the weather in sf?\"}]}}, 'run_id': '0c25e246-5cf7-44b3-b193-9b870a61bdfd', 'name': '_write', 'tags': ['seq:step:1', 'langsmith:hidden', 'langsmith:hidden'], 'metadata': {'thread_id': '2', 'langgraph_step': 3, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_path': ('__pregel_pull', '__start__'), 'langgraph_checkpoint_ns': '__start__:107cb6c8-1895-c324-e839-bc82fb1e2737'}, 'parent_ids': ['c6bf9294-29b9-4331-9bc9-5349f79228a7', '5c86e9e9-b113-4776-bd7f-4f3e46da91ff']}\n\n\n{'event': 'on_chain_start', 'data': {'input': {'messages': [{'role': 'user', 'content': \"what's the weather in sf?\"}]}}, 'name': '_write', 'tags': ['seq:step:3', 'langsmith:hidden', 'langsmith:hidden'], 'run_id': '75449080-1927-4bb1-8cba-4d4a70e5a2d5', 'metadata': {'thread_id': '2', 'langgraph_step': 3, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_path': ('__pregel_pull', '__start__'), 'langgraph_checkpoint_ns': '__start__:107cb6c8-1895-c324-e839-bc82fb1e2737'}, 'parent_ids': ['c6bf9294-29b9-4331-9bc9-5349f79228a7', '5c86e9e9-b113-4776-bd7f-4f3e46da91ff']}\n\n\n{'event': 'on_chain_end', 'data': {'output': {'messages': [{'role': 'user', 'content': \"what's the weather in sf?\"}]}, 'input': {'messages': [{'role': 'user', 'content': \"what's the weather in sf?\"}]}}, 'run_id': '75449080-1927-4bb1-8cba-4d4a70e5a2d5', 'name': '_write', 'tags': ['seq:step:3', 'langsmith:hidden', 'langsmith:hidden'], 'metadata': {'thread_id': '2', 'langgraph_step': 3, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_path': ('__pregel_pull', '__start__'), 'langgraph_checkpoint_ns': '__start__:107cb6c8-1895-c324-e839-bc82fb1e2737'}, 'parent_ids': ['c6bf9294-29b9-4331-9bc9-5349f79228a7', '5c86e9e9-b113-4776-bd7f-4f3e46da91ff']}\n\n\n{'event': 'on_chain_stream', 'run_id': '5c86e9e9-b113-4776-bd7f-4f3e46da91ff', 'name': '__start__', 'tags': ['graph:step:3', 'langsmith:hidden'], 'metadata': {'thread_id': '2', 'langgraph_step': 3, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_path': ('__pregel_pull', '__start__'), 'langgraph_checkpoint_ns': '__start__:107cb6c8-1895-c324-e839-bc82fb1e2737'}, 'data': {'chunk': {'messages': [{'role': 'user', 'content': \"what's the weather in sf?\"}]}}, 'parent_ids': ['c6bf9294-29b9-4331-9bc9-5349f79228a7']}\n\n\n{'event': 'on_chain_end', 'data': {'output': {'messages': [{'role': 'user', 'content': \"what's the weather in sf?\"}]}, 'input': {'messages': [{'role': 'user', 'content': \"what's the weather in sf?\"}]}}, 'run_id': '5c86e9e9-b113-4776-bd7f-4f3e46da91ff', 'name': '__start__', 'tags': ['graph:step:3', 'langsmith:hidden'], 'metadata': {'thread_id': '2', 'langgraph_step': 3, 'langgraph_node': '__start__', 'langgraph_triggers': ['__start__'], 'langgraph_path': ('__pregel_pull', '__start__'), 'langgraph_checkpoint_ns': '__start__:107cb6c8-1895-c324-e839-bc82fb1e2737'}, 'parent_ids': ['c6bf9294-29b9-4331-9bc9-5349f79228a7']}\n\n\n{'event': 'on_chain_start', 'data': {'input': {'messages': [HumanMessage(content=\"what's the weather in sf?\", additional_kwargs={}, response_metadata={}, id='61a1414b-5555-4ac4-ad63-5db470350b20'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_-9084362498981044777', 'function': {'arguments': '{\"city\": \"San Francisco\"}', 'name': 'weather_search'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 150, 'total_tokens': 161, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'glm-4-flash', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-8cd3f724-6685-4724-b53c-2f0fcf88d19b-0', tool_calls=[{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'call_-9084362498981044777', 'type': 'tool_call'}], usage_metadata={'input_tokens': 150, 'output_tokens': 11, 'total_tokens': 161, 'input_token_details': {}, 'output_token_details': {}}), HumanMessage(content=\"what's the weather in sf?\", additional_kwargs={}, response_metadata={}, id='da1d59a8-2a5e-497e-b866-d42e8df1170f')]}}, 'name': 'call_llm', 'tags': ['graph:step:4'], 'run_id': '21e36659-9021-49ec-8d38-f17bb96cd575', 'metadata': {'thread_id': '2', 'langgraph_step': 4, 'langgraph_node': 'call_llm', 'langgraph_triggers': ['start:call_llm'], 'langgraph_path': ('__pregel_pull', 'call_llm'), 'langgraph_checkpoint_ns': 'call_llm:8f3c1837-53e6-d2ef-80da-1d0763a4b983'}, 'parent_ids': ['c6bf9294-29b9-4331-9bc9-5349f79228a7']}\n\n\n{'event': 'on_chain_start', 'data': {'input': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_-9084357448097776084', 'function': {'arguments': '{\"city\": \"San Francisco\"}', 'name': 'weather_search'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 170, 'total_tokens': 181, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'glm-4-flash', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e4820d82-02dd-4625-b2cc-8348092846dc-0', tool_calls=[{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'call_-9084357448097776084', 'type': 'tool_call'}], usage_metadata={'input_tokens': 170, 'output_tokens': 11, 'total_tokens': 181, 'input_token_details': {}, 'output_token_details': {}})]}}, 'name': '_write', 'tags': ['seq:step:2', 'langsmith:hidden'], 'run_id': '696b2080-a06c-4c38-a92b-151df8602d99', 'metadata': {'thread_id': '2', 'langgraph_step': 4, 'langgraph_node': 'call_llm', 'langgraph_triggers': ['start:call_llm'], 'langgraph_path': ('__pregel_pull', 'call_llm'), 'langgraph_checkpoint_ns': 'call_llm:8f3c1837-53e6-d2ef-80da-1d0763a4b983'}, 'parent_ids': ['c6bf9294-29b9-4331-9bc9-5349f79228a7', '21e36659-9021-49ec-8d38-f17bb96cd575']}\n\n\n{'event': 'on_chain_end', 'data': {'output': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_-9084357448097776084', 'function': {'arguments': '{\"city\": \"San Francisco\"}', 'name': 'weather_search'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 170, 'total_tokens': 181, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'glm-4-flash', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e4820d82-02dd-4625-b2cc-8348092846dc-0', tool_calls=[{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'call_-9084357448097776084', 'type': 'tool_call'}], usage_metadata={'input_tokens': 170, 'output_tokens': 11, 'total_tokens': 181, 'input_token_details': {}, 'output_token_details': {}})]}, 'input': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_-9084357448097776084', 'function': {'arguments': '{\"city\": \"San Francisco\"}', 'name': 'weather_search'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 170, 'total_tokens': 181, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'glm-4-flash', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e4820d82-02dd-4625-b2cc-8348092846dc-0', tool_calls=[{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'call_-9084357448097776084', 'type': 'tool_call'}], usage_metadata={'input_tokens': 170, 'output_tokens': 11, 'total_tokens': 181, 'input_token_details': {}, 'output_token_details': {}})]}}, 'run_id': '696b2080-a06c-4c38-a92b-151df8602d99', 'name': '_write', 'tags': ['seq:step:2', 'langsmith:hidden'], 'metadata': {'thread_id': '2', 'langgraph_step': 4, 'langgraph_node': 'call_llm', 'langgraph_triggers': ['start:call_llm'], 'langgraph_path': ('__pregel_pull', 'call_llm'), 'langgraph_checkpoint_ns': 'call_llm:8f3c1837-53e6-d2ef-80da-1d0763a4b983'}, 'parent_ids': ['c6bf9294-29b9-4331-9bc9-5349f79228a7', '21e36659-9021-49ec-8d38-f17bb96cd575']}\n\n\n{'event': 'on_chain_start', 'data': {'input': {'messages': [HumanMessage(content=\"what's the weather in sf?\", additional_kwargs={}, response_metadata={}, id='61a1414b-5555-4ac4-ad63-5db470350b20'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_-9084362498981044777', 'function': {'arguments': '{\"city\": \"San Francisco\"}', 'name': 'weather_search'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 150, 'total_tokens': 161, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'glm-4-flash', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-8cd3f724-6685-4724-b53c-2f0fcf88d19b-0', tool_calls=[{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'call_-9084362498981044777', 'type': 'tool_call'}], usage_metadata={'input_tokens': 150, 'output_tokens': 11, 'total_tokens': 161, 'input_token_details': {}, 'output_token_details': {}}), HumanMessage(content=\"what's the weather in sf?\", additional_kwargs={}, response_metadata={}, id='da1d59a8-2a5e-497e-b866-d42e8df1170f'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_-9084357448097776084', 'function': {'arguments': '{\"city\": \"San Francisco\"}', 'name': 'weather_search'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 170, 'total_tokens': 181, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'glm-4-flash', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e4820d82-02dd-4625-b2cc-8348092846dc-0', tool_calls=[{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'call_-9084357448097776084', 'type': 'tool_call'}], usage_metadata={'input_tokens': 170, 'output_tokens': 11, 'total_tokens': 181, 'input_token_details': {}, 'output_token_details': {}})]}}, 'name': 'route_after_llm', 'tags': ['seq:step:4'], 'run_id': 'fbd2315c-a59e-475c-8b66-143ab89bb381', 'metadata': {'thread_id': '2', 'langgraph_step': 4, 'langgraph_node': 'call_llm', 'langgraph_triggers': ['start:call_llm'], 'langgraph_path': ('__pregel_pull', 'call_llm'), 'langgraph_checkpoint_ns': 'call_llm:8f3c1837-53e6-d2ef-80da-1d0763a4b983'}, 'parent_ids': ['c6bf9294-29b9-4331-9bc9-5349f79228a7', '21e36659-9021-49ec-8d38-f17bb96cd575']}\n\n\n{'event': 'on_chain_end', 'data': {'output': 'human_review_node', 'input': {'messages': [HumanMessage(content=\"what's the weather in sf?\", additional_kwargs={}, response_metadata={}, id='61a1414b-5555-4ac4-ad63-5db470350b20'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_-9084362498981044777', 'function': {'arguments': '{\"city\": \"San Francisco\"}', 'name': 'weather_search'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 150, 'total_tokens': 161, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'glm-4-flash', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-8cd3f724-6685-4724-b53c-2f0fcf88d19b-0', tool_calls=[{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'call_-9084362498981044777', 'type': 'tool_call'}], usage_metadata={'input_tokens': 150, 'output_tokens': 11, 'total_tokens': 161, 'input_token_details': {}, 'output_token_details': {}}), HumanMessage(content=\"what's the weather in sf?\", additional_kwargs={}, response_metadata={}, id='da1d59a8-2a5e-497e-b866-d42e8df1170f'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_-9084357448097776084', 'function': {'arguments': '{\"city\": \"San Francisco\"}', 'name': 'weather_search'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 170, 'total_tokens': 181, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'glm-4-flash', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e4820d82-02dd-4625-b2cc-8348092846dc-0', tool_calls=[{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'call_-9084357448097776084', 'type': 'tool_call'}], usage_metadata={'input_tokens': 170, 'output_tokens': 11, 'total_tokens': 181, 'input_token_details': {}, 'output_token_details': {}})]}}, 'run_id': 'fbd2315c-a59e-475c-8b66-143ab89bb381', 'name': 'route_after_llm', 'tags': ['seq:step:4'], 'metadata': {'thread_id': '2', 'langgraph_step': 4, 'langgraph_node': 'call_llm', 'langgraph_triggers': ['start:call_llm'], 'langgraph_path': ('__pregel_pull', 'call_llm'), 'langgraph_checkpoint_ns': 'call_llm:8f3c1837-53e6-d2ef-80da-1d0763a4b983'}, 'parent_ids': ['c6bf9294-29b9-4331-9bc9-5349f79228a7', '21e36659-9021-49ec-8d38-f17bb96cd575']}\n\n\n{'event': 'on_chain_stream', 'run_id': '21e36659-9021-49ec-8d38-f17bb96cd575', 'name': 'call_llm', 'tags': ['graph:step:4'], 'metadata': {'thread_id': '2', 'langgraph_step': 4, 'langgraph_node': 'call_llm', 'langgraph_triggers': ['start:call_llm'], 'langgraph_path': ('__pregel_pull', 'call_llm'), 'langgraph_checkpoint_ns': 'call_llm:8f3c1837-53e6-d2ef-80da-1d0763a4b983'}, 'data': {'chunk': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_-9084357448097776084', 'function': {'arguments': '{\"city\": \"San Francisco\"}', 'name': 'weather_search'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 170, 'total_tokens': 181, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'glm-4-flash', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e4820d82-02dd-4625-b2cc-8348092846dc-0', tool_calls=[{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'call_-9084357448097776084', 'type': 'tool_call'}], usage_metadata={'input_tokens': 170, 'output_tokens': 11, 'total_tokens': 181, 'input_token_details': {}, 'output_token_details': {}})]}}, 'parent_ids': ['c6bf9294-29b9-4331-9bc9-5349f79228a7']}\n\n\n{'event': 'on_chain_end', 'data': {'output': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_-9084357448097776084', 'function': {'arguments': '{\"city\": \"San Francisco\"}', 'name': 'weather_search'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 170, 'total_tokens': 181, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'glm-4-flash', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e4820d82-02dd-4625-b2cc-8348092846dc-0', tool_calls=[{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'call_-9084357448097776084', 'type': 'tool_call'}], usage_metadata={'input_tokens': 170, 'output_tokens': 11, 'total_tokens': 181, 'input_token_details': {}, 'output_token_details': {}})]}, 'input': {'messages': [HumanMessage(content=\"what's the weather in sf?\", additional_kwargs={}, response_metadata={}, id='61a1414b-5555-4ac4-ad63-5db470350b20'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_-9084362498981044777', 'function': {'arguments': '{\"city\": \"San Francisco\"}', 'name': 'weather_search'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 150, 'total_tokens': 161, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'glm-4-flash', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-8cd3f724-6685-4724-b53c-2f0fcf88d19b-0', tool_calls=[{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'call_-9084362498981044777', 'type': 'tool_call'}], usage_metadata={'input_tokens': 150, 'output_tokens': 11, 'total_tokens': 161, 'input_token_details': {}, 'output_token_details': {}}), HumanMessage(content=\"what's the weather in sf?\", additional_kwargs={}, response_metadata={}, id='da1d59a8-2a5e-497e-b866-d42e8df1170f')]}}, 'run_id': '21e36659-9021-49ec-8d38-f17bb96cd575', 'name': 'call_llm', 'tags': ['graph:step:4'], 'metadata': {'thread_id': '2', 'langgraph_step': 4, 'langgraph_node': 'call_llm', 'langgraph_triggers': ['start:call_llm'], 'langgraph_path': ('__pregel_pull', 'call_llm'), 'langgraph_checkpoint_ns': 'call_llm:8f3c1837-53e6-d2ef-80da-1d0763a4b983'}, 'parent_ids': ['c6bf9294-29b9-4331-9bc9-5349f79228a7']}\n\n\n{'event': 'on_chain_stream', 'run_id': 'c6bf9294-29b9-4331-9bc9-5349f79228a7', 'name': 'LangGraph', 'tags': [], 'metadata': {'thread_id': '2'}, 'data': {'chunk': {'call_llm': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_-9084357448097776084', 'function': {'arguments': '{\"city\": \"San Francisco\"}', 'name': 'weather_search'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 170, 'total_tokens': 181, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'glm-4-flash', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e4820d82-02dd-4625-b2cc-8348092846dc-0', tool_calls=[{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'call_-9084357448097776084', 'type': 'tool_call'}], usage_metadata={'input_tokens': 170, 'output_tokens': 11, 'total_tokens': 181, 'input_token_details': {}, 'output_token_details': {}})]}}}, 'parent_ids': []}\n\n\n{'event': 'on_chain_start', 'data': {'input': {'messages': [HumanMessage(content=\"what's the weather in sf?\", additional_kwargs={}, response_metadata={}, id='61a1414b-5555-4ac4-ad63-5db470350b20'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_-9084362498981044777', 'function': {'arguments': '{\"city\": \"San Francisco\"}', 'name': 'weather_search'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 150, 'total_tokens': 161, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'glm-4-flash', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-8cd3f724-6685-4724-b53c-2f0fcf88d19b-0', tool_calls=[{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'call_-9084362498981044777', 'type': 'tool_call'}], usage_metadata={'input_tokens': 150, 'output_tokens': 11, 'total_tokens': 161, 'input_token_details': {}, 'output_token_details': {}}), HumanMessage(content=\"what's the weather in sf?\", additional_kwargs={}, response_metadata={}, id='da1d59a8-2a5e-497e-b866-d42e8df1170f'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_-9084357448097776084', 'function': {'arguments': '{\"city\": \"San Francisco\"}', 'name': 'weather_search'}, 'type': 'function', 'index': 0}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 170, 'total_tokens': 181, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'glm-4-flash', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-e4820d82-02dd-4625-b2cc-8348092846dc-0', tool_calls=[{'name': 'weather_search', 'args': {'city': 'San Francisco'}, 'id': 'call_-9084357448097776084', 'type': 'tool_call'}], usage_metadata={'input_tokens': 170, 'output_tokens': 11, 'total_tokens': 181, 'input_token_details': {}, 'output_token_details': {}})]}}, 'name': 'human_review_node', 'tags': ['graph:step:5'], 'run_id': 'b7ea95ed-312e-41c9-a983-33c75e64ca13', 'metadata': {'thread_id': '2', 'langgraph_step': 5, 'langgraph_node': 'human_review_node', 'langgraph_triggers': ['branch:call_llm:route_after_llm:human_review_node'], 'langgraph_path': ('__pregel_pull', 'human_review_node'), 'langgraph_checkpoint_ns': 'human_review_node:6b060d62-e073-6c79-a745-0789cdc4ffbe'}, 'parent_ids': ['c6bf9294-29b9-4331-9bc9-5349f79228a7']}\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[37], line 8\n      5 thread = {\"configurable\": {\"thread_id\": \"2\"}}\n      7 # Run the graph until the first interruption\n----> 8 async for event in graph.astream_events(initial_input, thread, stream_mode=\"updates\",version=\"v2\"):\n      9     print(event)\n     10     print(\"\\n\")\n\nFile ~\\.conda\\envs\\study\\lib\\site-packages\\langchain_core\\runnables\\base.py:1388, in Runnable.astream_events(self, input, config, version, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\n   1385     raise NotImplementedError(msg)\n   1387 async with aclosing(event_stream):\n-> 1388     async for event in event_stream:\n   1389         yield event\n\nFile ~\\.conda\\envs\\study\\lib\\site-packages\\langchain_core\\tracers\\event_stream.py:1012, in _astream_events_implementation_v2(runnable, input, config, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\n   1010 # Await it anyway, to run any cleanup code, and propagate any exceptions\n   1011 with contextlib.suppress(asyncio.CancelledError):\n-> 1012     await task\n\nFile ~\\.conda\\envs\\study\\lib\\site-packages\\langchain_core\\tracers\\event_stream.py:967, in _astream_events_implementation_v2.<locals>.consume_astream()\n    964 try:\n    965     # if astream also calls tap_output_aiter this will be a no-op\n    966     async with aclosing(runnable.astream(input, config, **kwargs)) as stream:\n--> 967         async for _ in event_streamer.tap_output_aiter(run_id, stream):\n    968             # All the content will be picked up\n    969             pass\n    970 finally:\n\nFile ~\\.conda\\envs\\study\\lib\\site-packages\\langchain_core\\tracers\\event_stream.py:203, in _AstreamEventsCallbackHandler.tap_output_aiter(self, run_id, output)\n    201 yield cast(T, first)\n    202 # consume the rest of the output\n--> 203 async for chunk in output:\n    204     self._send(\n    205         {**event, \"data\": {\"chunk\": chunk}},\n    206         run_info[\"run_type\"],\n    207     )\n    208     yield chunk\n\nFile ~\\.conda\\envs\\study\\lib\\site-packages\\langgraph\\pregel\\__init__.py:1874, in Pregel.astream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\n   1868 # Similarly to Bulk Synchronous Parallel / Pregel model\n   1869 # computation proceeds in steps, while there are channel updates\n   1870 # channel updates from step N are only visible in step N+1\n   1871 # channels are guaranteed to be immutable for the duration of the step,\n   1872 # with channel updates applied only at the transition between steps\n   1873 while loop.tick(input_keys=self.input_channels):\n-> 1874     async for _ in runner.atick(\n   1875         loop.tasks.values(),\n   1876         timeout=self.step_timeout,\n   1877         retry_policy=self.retry_policy,\n   1878         get_waiter=get_waiter,\n   1879     ):\n   1880         # emit output\n   1881         for o in output():\n   1882             yield o\n\nFile ~\\.conda\\envs\\study\\lib\\site-packages\\langgraph\\pregel\\runner.py:362, in PregelRunner.atick(self, tasks, reraise, timeout, retry_policy, get_waiter)\n    360 t = tasks[0]\n    361 try:\n--> 362     await arun_with_retry(\n    363         t,\n    364         retry_policy,\n    365         stream=self.use_astream,\n    366         configurable={\n    367             CONFIG_KEY_SEND: partial(writer, t),\n    368             CONFIG_KEY_CALL: partial(call, t),\n    369         },\n    370     )\n    371     self.commit(t, None)\n    372 except Exception as exc:\n\nFile ~\\.conda\\envs\\study\\lib\\site-packages\\langgraph\\pregel\\retry.py:127, in arun_with_retry(task, retry_policy, stream, configurable)\n    125 # run the task\n    126 if stream:\n--> 127     async for _ in task.proc.astream(task.input, config):\n    128         pass\n    129     # if successful, end\n\nFile ~\\.conda\\envs\\study\\lib\\site-packages\\langgraph\\utils\\runnable.py:568, in RunnableSeq.astream(self, input, config, **kwargs)\n    566 output: Any = None\n    567 add_supported = False\n--> 568 async for chunk in aiterator:\n    569     yield chunk\n    570     # collect final output\n\nFile ~\\.conda\\envs\\study\\lib\\site-packages\\langchain_core\\tracers\\event_stream.py:180, in _AstreamEventsCallbackHandler.tap_output_aiter(self, run_id, output)\n    178 tap = self.is_tapped.setdefault(run_id, sentinel)\n    179 # wait for first chunk\n--> 180 first = await py_anext(output, default=sentinel)\n    181 if first is sentinel:\n    182     return\n\nFile ~\\.conda\\envs\\study\\lib\\site-packages\\langchain_core\\utils\\aiter.py:76, in py_anext.<locals>.anext_impl()\n     69 async def anext_impl() -> Union[T, Any]:\n     70     try:\n     71         # The C code is way more low-level than this, as it implements\n     72         # all methods of the iterator protocol. In this implementation\n     73         # we're relying on higher-level coroutine concepts, but that's\n     74         # exactly what we want -- crosstest pure-Python high-level\n     75         # implementation and low-level C anext() iterators.\n---> 76         return await __anext__(iterator)\n     77     except StopAsyncIteration:\n     78         return default\n\nFile ~\\.conda\\envs\\study\\lib\\site-packages\\langchain_core\\runnables\\base.py:1455, in Runnable.atransform(self, input, config, **kwargs)\n   1452 final: Input\n   1453 got_first_val = False\n-> 1455 async for ichunk in input:\n   1456     # The default implementation of transform is to buffer input and\n   1457     # then call stream.\n   1458     # It'll attempt to gather all input into a single chunk using\n   1459     # the `+` operator.\n   1460     # If the input is not addable, then we'll assume that we can\n   1461     # only operate on the last chunk,\n   1462     # and we'll iterate until we get to the last chunk.\n   1463     if not got_first_val:\n   1464         final = ichunk\n\nFile ~\\.conda\\envs\\study\\lib\\site-packages\\langchain_core\\runnables\\base.py:1455, in Runnable.atransform(self, input, config, **kwargs)\n   1452 final: Input\n   1453 got_first_val = False\n-> 1455 async for ichunk in input:\n   1456     # The default implementation of transform is to buffer input and\n   1457     # then call stream.\n   1458     # It'll attempt to gather all input into a single chunk using\n   1459     # the `+` operator.\n   1460     # If the input is not addable, then we'll assume that we can\n   1461     # only operate on the last chunk,\n   1462     # and we'll iterate until we get to the last chunk.\n   1463     if not got_first_val:\n   1464         final = ichunk\n\nFile ~\\.conda\\envs\\study\\lib\\site-packages\\langchain_core\\runnables\\base.py:1018, in Runnable.astream(self, input, config, **kwargs)\n   1000 async def astream(\n   1001     self,\n   1002     input: Input,\n   1003     config: Optional[RunnableConfig] = None,\n   1004     **kwargs: Optional[Any],\n   1005 ) -> AsyncIterator[Output]:\n   1006     \"\"\"\n   1007     Default implementation of astream, which calls ainvoke.\n   1008     Subclasses should override this method if they support streaming output.\n   (...)\n   1016         The output of the Runnable.\n   1017     \"\"\"\n-> 1018     yield await self.ainvoke(input, config, **kwargs)\n\nFile ~\\.conda\\envs\\study\\lib\\site-packages\\langgraph\\utils\\runnable.py:238, in RunnableCallable.ainvoke(self, input, config, **kwargs)\n    236         ret = await asyncio.create_task(coro, context=context)\n    237     else:\n--> 238         ret = await self.afunc(input, **kwargs)\n    239 if isinstance(ret, Runnable) and self.recurse:\n    240     return await ret.ainvoke(input, config)\n\nFile ~\\.conda\\envs\\study\\lib\\site-packages\\langchain_core\\runnables\\config.py:588, in run_in_executor(executor_or_config, func, *args, **kwargs)\n    584         raise RuntimeError from exc\n    586 if executor_or_config is None or isinstance(executor_or_config, dict):\n    587     # Use default executor with context copied from current context\n--> 588     return await asyncio.get_running_loop().run_in_executor(\n    589         None,\n    590         cast(Callable[..., T], partial(copy_context().run, wrapper)),\n    591     )\n    593 return await asyncio.get_running_loop().run_in_executor(executor_or_config, wrapper)\n\nFile ~\\.conda\\envs\\study\\lib\\concurrent\\futures\\thread.py:58, in _WorkItem.run(self)\n     55     return\n     57 try:\n---> 58     result = self.fn(*self.args, **self.kwargs)\n     59 except BaseException as exc:\n     60     self.future.set_exception(exc)\n\nFile ~\\.conda\\envs\\study\\lib\\site-packages\\langchain_core\\runnables\\config.py:579, in run_in_executor.<locals>.wrapper()\n    577 def wrapper() -> T:\n    578     try:\n--> 579         return func(*args, **kwargs)\n    580     except StopIteration as exc:\n    581         # StopIteration can't be set on an asyncio.Future\n    582         # it raises a TypeError and leaves the Future pending forever\n    583         # so we need to convert it to a RuntimeError\n    584         raise RuntimeError from exc\n\nCell In[35], line 46, in human_review_node(state)\n     43 tool_call = last_message.tool_calls[-1]\n     45 # this is the value we'll be providing via Command(resume=<human_review>)\n---> 46 human_review = interrupt(\n     47     {\n     48         \"question\": \"Is this correct?\",\n     49         # Surface tool calls for review\n     50         \"tool_call\": tool_call,\n     51     }\n     52 )\n     54 review_action = human_review[\"action\"]\n     55 review_data = human_review.get(\"data\")\n\nFile ~\\.conda\\envs\\study\\lib\\site-packages\\langgraph\\types.py:458, in interrupt(value)\n    455 from langgraph.errors import GraphInterrupt\n    456 from langgraph.utils.config import get_configurable\n--> 458 conf = get_configurable()\n    459 # track interrupt index\n    460 scratchpad: PregelScratchpad = conf[CONFIG_KEY_SCRATCHPAD]\n\nFile ~\\.conda\\envs\\study\\lib\\site-packages\\langgraph\\utils\\config.py:312, in get_configurable()\n    310     return var_config[CONF]\n    311 else:\n--> 312     raise RuntimeError(\"Called get_configurable outside of a runnable context\")\n\nRuntimeError: Called get_configurable outside of a runnable context\nDescription\nMy code is based on https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/review-tool-calls/#simple-usage.\n\n\nThe error does not occur when I make no modifications.\n\n\nThe error occurs only when I change stream to astream_events.\n\n\nThe error does not occur if interrupt is not triggered.\n\n\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP Tue Nov 5 00:21:55 UTC 2024\nPython Version:  3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0]\n\nPackage Information\n\nlangchain_core: 0.3.28\nlangchain: 0.3.13\nlangchain_community: 0.3.13\nlangsmith: 0.1.147\nlangchain_anthropic: 0.1.20\nlangchain_cohere: 0.3.4\nlangchain_experimental: 0.3.4\nlangchain_google_genai: 2.0.7\nlangchain_ollama: 0.2.2\nlangchain_openai: 0.2.14\nlangchain_qdrant: 0.1.4\nlangchain_text_splitters: 0.3.4\nlanggraph_sdk: 0.1.48\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.11\nanthropic: 0.42.0\nasync-timeout: 4.0.3\ncohere: 5.13.4\ndataclasses-json: 0.6.7\ndefusedxml: 0.7.1\nfastembed: 0.3.6\nfiletype: 1.2.0\ngoogle-generativeai: 0.8.3\nhttpx: 0.27.2\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nollama: 0.4.4\nopenai: 1.58.1\norjson: 3.10.12\npackaging: 24.2\npandas: 2.2.3\npydantic: 2.10.4\npydantic-settings: 2.7.0\nPyYAML: 6.0.2\nqdrant-client: 1.11.3\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.36\ntabulate: 0.9.0\ntenacity: 8.5.0\ntiktoken: 0.7.0\ntyping-extensions: 4.12.2\n", "created_at": "2025-01-07", "closed_at": "2025-01-07", "labels": [], "State": "closed", "Author": "Onelevenvy"}
{"issue_number": 2940, "issue_title": "It can't go to the subgraph using Command", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langchain_core.messages import HumanMessage\nfrom langgraph.graph import StateGraph, MessagesState, START, END\nfrom langgraph.types import Command\n\n\n# Define the subgraph\ndef subgraph_node_1(state: MessagesState) -> Command:\n    return Command(goto='main_node', update={\"messages\": [HumanMessage(content='subgraph node 1')]},\n                   graph=Command.PARENT)\n\n\ndef subgraph_node_2(state: MessagesState) -> Command:\n    return Command(goto=END, update={\"messages\": [HumanMessage(content='subgraph node 2')]})\n\n\nsubgraph_builder = StateGraph(MessagesState)\nsubgraph_builder.add_node(\"subgraph_node_1\", subgraph_node_1)\nsubgraph_builder.add_node(\"subgraph_node_2\", subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph = subgraph_builder.compile()\n\n\n# Define the main graph\ndef main_node(state: MessagesState) -> Command:\n    return Command(goto='subgraph_node_2', update={\"messages\": [HumanMessage(content='main node')]})\n\n\nmain_builder = StateGraph(MessagesState)\nmain_builder.add_node(\"main_node\", main_node)\nmain_builder.add_node(\"subgraph\", subgraph)  # Add the compiled subgraph as a node\nmain_builder.add_edge(START, \"subgraph\")\nmain_builder.add_edge(\"subgraph\", \"main_node\")\nmain_graph = main_builder.compile()\n\n# Execute the main graph\nfor x in main_graph.stream(input={'messages': []}, subgraphs=True):\n    print(x)\n    print(\"----\")\nError Message and Stack Trace (if applicable)\n/Users/ds/ai/chatbot_v1/chatbot_v1/.venv/bin/python -X pycache_prefix=/Users/ds/Library/Caches/JetBrains/PyCharm2024.1/cpython-cache /Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py --multiprocess --qt-support=auto --client 127.0.0.1 --port 52486 --file /Users/ds/ai/chatbot_v1/chatbot_v1/langgraph_new.py \nConnected to pydev debugger (build 241.18034.82)\n((), {'subgraph': {'messages': [HumanMessage(content='subgraph node 1', additional_kwargs={}, response_metadata={})]}})\n----\n((), {'main_node': {'messages': [HumanMessage(content='main node', additional_kwargs={}, response_metadata={})]}})\n----\nTraceback (most recent call last):\n  File \"/Users/ds/ai/chatbot_v1/chatbot_v1/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 1659, in stream\n    while loop.tick(input_keys=self.input_channels):\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ds/ai/chatbot_v1/chatbot_v1/.venv/lib/python3.11/site-packages/langgraph/pregel/loop.py\", line 394, in tick\n    self._update_mv(key, values)\n  File \"/Users/ds/ai/chatbot_v1/chatbot_v1/.venv/lib/python3.11/site-packages/langgraph/pregel/loop.py\", line 830, in _update_mv\n    return self.submit(cast(WritableManagedValue, self.managed[key]).update, values)\n                                                  ~~~~~~~~~~~~^^^^^\nKeyError: 'branch:main_node:__self__:subgraph_node_2'\npython-BaseException\n\nProcess finished with exit code 1\nDescription\nit doesn't go to the subgraph node using command\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 22.6.0: Wed Jul  5 22:22:05 PDT 2023; root:xnu-8796.141.3~6/RELEASE_ARM64_T6000\nPython Version:  3.9.6 (default, Aug 11 2023, 19:44:49)\n[Clang 15.0.0 (clang-1500.0.40.1)]\n\nPackage Information\n\nlangchain_core: 0.1.24\nlangchain: 0.1.8\nlangchain_community: 0.0.21\nlangsmith: 0.1.3\n\nPackages not installed (Not Necessarily a Problem)\nThe following packages were not found:\n\nlanggraph\nlangserve\n", "created_at": "2025-01-06", "closed_at": "2025-01-07", "labels": [], "State": "closed", "Author": "YassinNouh21"}
{"issue_number": 2938, "issue_title": "JsonOutputToolsParser and PydanticToolsParser fails while used with Groq And Llama    in Langgraph ", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langchain_core.output_parsers.openai_tools import JsonOutputToolsParser, PydanticToolsParser\nfrom langchain_core.messages import BaseMessage, HumanMessage\nfrom langgraph.graph import END, MessageGraph\nfrom typing import Sequence, List\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_groq import ChatGroq\nimport time,datetime\nfrom schema import Reflection,AnswerQuestion,ReviseAnswer\nimport getpass\nimport os\nimport dotenv\ndotenv.load_dotenv()  \nGROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\nllm = ChatGroq(model=\"llama3-8b-8192\")\n\nactor_prompt_template=ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"\"\"You are expert researcher.\nCurrent time: {time}\n\n1. {first_instruction}\n2. Reflect and critique your answer. Be severe to maximize improvement.\n3. Recommend search queries to research information and improve your answer.\"\"\",\n        ),\n        MessagesPlaceholder(variable_name=\"messages\"),\n        (\"system\", \"Answer the user's question above using the required format.\"),\n    ]\n).partial(\n    time=lambda: datetime.datetime.now().isoformat(),\n)\n\nparser=JsonOutputToolsParser(return_id=True)\nparser_pydantic=PydanticToolsParser(tools=[AnswerQuestion])\n\nfirst_resopnder_template=actor_prompt_template.partial(\n    first_instruction=\" Provide a detailed 250 word answer\"\n)\n\nfirst_resopnder=first_resopnder_template|llm.bind_tools(\n    tools=[AnswerQuestion],\n    tool_choice=\"AnswerQuestion\"\n)\n\nrevise_instructions=\"\"\" Revise your previous answer using new information.\n            -You should use the previous critique to add important information to answer.\n            -You must include numerical citations in your anser to ensure it can be verified.\n            -Add a references section towards the end of your action ( That does not count towards the word limit)\n                -[1] https://examaple.com\n                -[2] https://examaple.com \n            You should use the previous critique to remove superfulous information from your answer.\n\"\"\"\n\n\nif __name__==\"__main__\":\n    human_message=HumanMessage(\n        content=\"Write About Kamikaze planes and nuclear submarines,\"\n  \n    )\n    chain=(first_resopnder_template|llm.bind_tools(tools=[AnswerQuestion],tool_choice=\"AnswerQuestion\")|parser_pydantic)\n    res=chain.invoke(input={\"messages\":[human_message]})\n    print(res)\n\n\n/////////////////#### schema.py \n\nfrom typing import List\nfrom pydantic import BaseModel,Field\n\nclass Reflection(BaseModel):\n    missing:str=Field(description=\"Critique What is missing\")\n    superfulous:str=Field(description=\"Critique of what is superfluous\") ## Critique that is not right \n\nclass AnswerQuestion(BaseModel):\n        answer:str=Field(description=\"250 word detailed answer to the question.\")\n        reflection:Reflection=Field(description=\"Your reflection on the initial answer.\")\n        search_queries:List[str]=Field(description=\"1-3 search queries for researching improvements to address the critique of your current answer.\")\n\n\nclass ReviseAnswer(AnswerQuestion):\n      references:List[str]=Field(\n            description=\"Citations Motivating your updated answer\"\n      )\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\n  File \"/opt/CodeRepo/Agentic_Ai/Reflexion_Agent/chains.py\", line 78, in <module>\n    res=chain.invoke(input={\"messages\":[human_message]})\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/CodeRepo/Agentic_Ai/myenv/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3024, in invoke\n    input = context.run(step.invoke, input, config)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/CodeRepo/Agentic_Ai/myenv/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 5354, in invoke\n    return self.bound.invoke(\n           ^^^^^^^^^^^^^^^^^^\n  File \"/opt/CodeRepo/Agentic_Ai/myenv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 286, in invoke\n    self.generate_prompt(\n  File \"/opt/CodeRepo/Agentic_Ai/myenv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 786, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/CodeRepo/Agentic_Ai/myenv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 643, in generate\n    raise e\n  File \"/opt/CodeRepo/Agentic_Ai/myenv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 633, in generate\n    self._generate_with_cache(\n  File \"/opt/CodeRepo/Agentic_Ai/myenv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 851, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/opt/CodeRepo/Agentic_Ai/myenv/lib/python3.12/site-packages/langchain_groq/chat_models.py\", line 474, in _generate\n    response = self.client.create(messages=message_dicts, **params)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/CodeRepo/Agentic_Ai/myenv/lib/python3.12/site-packages/groq/resources/chat/completions.py\", line 298, in create\n    return self._post(\n           ^^^^^^^^^^^\n  File \"/opt/CodeRepo/Agentic_Ai/myenv/lib/python3.12/site-packages/groq/_base_client.py\", line 1263, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/CodeRepo/Agentic_Ai/myenv/lib/python3.12/site-packages/groq/_base_client.py\", line 955, in request\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"/opt/CodeRepo/Agentic_Ai/myenv/lib/python3.12/site-packages/groq/_base_client.py\", line 1058, in _request\n    raise self._make_status_error_from_response(err.response) from None\ngroq.BadRequestError: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<tool-use>\\n{\\n  \"tool_calls\": [\\n    {\\n      \"id\": \"pending\",\\n      \"type\": \"function\",\\n      \"function\": {\\n        \"name\": \"AnswerQuestion\"\\n      },\\n      \"parameters\": {\\n        \"answer\": \"Kamikaze planes were a type of Japanese suicide attack aircraft used during World War II. These planes were designed to crash into enemy ships, often with explosive payloads, in order to inflict damage and sink or disable the vessel. The term \\'kamikaze\\' literally means \\'divine wind\\' in Japanese, and was used to describe the unexpected and devastating typhoons that would often strike Taiwan and the Philippines, causing significant damage to the invading Japanese forces. The concept of kamikaze attacks was born out of desperation and a desire to counter the superior naval power of the United States and other Allied forces. The Japanese Navy\\'s Special Attack Force, known as the Shinsen-k\u014d, was formed in 1944 to carry out these attacks, and by the end of the war, over 3,800 Japanese pilots had sacrificed their lives in these deadly missions.\\n\\nNuclear submarines, on the other hand, are a type of submarine that is capable of launching nuclear-tipped ballistic missiles. These submarines are designed to be stealthy, fast, and highly maneuverable, making them difficult to detect and track by enemy forces. Nuclear submarines are equipped with advanced sensors and communication systems, allowing them to stay submerged for extended periods of time and communicate with command centers and other friendly forces. They are also equipped with sophisticated missile guidance systems, which enable them to accurately target enemy ships or coastal installations. The development of nuclear submarines was a major technological advancement in the field of naval warfare, and has had a significant impact on the way that nations approach naval strategy and tactics.\\n\\nIn conclusion, kamikaze planes and nuclear submarines are two vastly different examples of naval power, each with its own unique characteristics and capabilities. While kamikaze planes were used as a desperate measure to counter the superior naval power of the Allies, nuclear submarines are a highly advanced and sophisticated technology that has revolutionized the way that nations approach naval warfare.\",\\n      \"reflection\": {\\n        \"missing\": \"A more detailed analysis of the strategic and tactical implications of kamikaze attacks would be beneficial in understanding the role they played in the war.\",\\n        \"superfluous\": \"More information on the development and deployment of nuclear submarines would provide a more comprehensive understanding of their impact on naval warfare.\"\\n      },\\n      \"search_queries\": [\"kamikaze planes during World War II\", \"strategic implications of kamikaze attacks\", \"development and deployment of nuclear submarines\", \"impact of nuclear submarines on naval warfare\"]\\n    }\\n  ]\\n}\\n</tool-use>'}}\nDescription\nI am trying to build one reflexion agent using Lang graph , Groq , Llma\nWhile the same implementaion with OpenAi works ( As per tutorial) , it fails in LLma and Groq\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #52-Ubuntu SMP PREEMPT_DYNAMIC Thu Dec  5 13:09:44 UTC 2024\nPython Version:  3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]\n\nPackage Information\n\nlangchain_core: 0.3.28\nlangchain: 0.3.13\nlangchain_community: 0.3.13\nlangsmith: 0.2.6\nlangchain_groq: 0.2.2\nlangchain_text_splitters: 0.3.4\nlanggraph_sdk: 0.1.48\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.11\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.7\ngroq: 0.13.1\nhttpx: 0.28.1\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 2.2.1\norjson: 3.10.12\npackaging: 24.2\npydantic: 2.10.4\npydantic-settings: 2.7.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.36\ntenacity: 9.0.0\ntyping-extensions: 4.12.2\nzstandard: Installed. No version info available.\n", "created_at": "2025-01-06", "closed_at": "2025-01-07", "labels": [], "State": "closed", "Author": "Ansumanbhujabal"}
{"issue_number": 2937, "issue_title": "Unable to run langgraph docker container due to license key verification failure", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nNone\nError Message and Stack Trace (if applicable)\n2025-01-06T09:17:02.271092Z [warning  ] No license key found, running in test mode with LangSmith API key. For production use, set LANGGRAPH_CLOUD_LICENSE_KEY in environment. [langgraph_license.validation] api_revision=ed92a0f api_variant=local\n2025-01-06T09:17:02.323722Z [error    ] Error checking Langsmith access [langgraph_license.validation] api_revision=ed92a0f api_variant=local\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/httpcore/_async/connection_pool.py\", line 207, in handle_async_request\n    raise UnsupportedProtocol(\nhttpcore.UnsupportedProtocol: Request URL is missing an 'http://' or 'https://' protocol.\n...\nhttpx.UnsupportedProtocol: Request URL is missing an 'http://' or 'https://' protocol.\n2025-01-06T09:17:02.334165Z [error    ] Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 693, in lifespan\n    async with self.lifespan_context(app) as maybe_state:\n  File \"/usr/local/lib/python3.11/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/api/langgraph_api/lifespan.py\", line 19, in lifespan\nValueError: License verification failed. Please ensure proper configuration:\n- For local development, set a valid LANGSMITH_API_KEY for an account with LangGraph Cloud access in the environment defined in your langgraph.json file.\n- For production, configure the LANGGRAPH_CLOUD_LICENSE_KEY environment variable with your LangGraph Cloud license key.\nReview your configuration settings and try again. If issues persist, contact support for assistance.\nDescription\nTo deploy the Langgraph server to my self-hosted server, I've built the Docker image using langgraph build. After building, I tried to run the image with the following command\ndocker run \\\n    --env-file .env \\\n    -p 8123:8000 \\\n    -e LANGSMITH_API_KEY=\"REDACTED\" \\\n    -e DATABASE_URI=\"REDACTED\" \\\n    -e REDIS_URI=\"REDACTED\" \\\n    my-image\n\nUnfortunately the container crashes almost immediately with the above error messages. This issue has been happening for past few versions of Langgraph, so it shouldn't be something new.\nSystem Info\n\nlangchain_core: 0.3.28\nlangchain: 0.3.13\nlangchain_community: 0.3.13\nlangsmith: 0.2.7\nlangchain_anthropic: 0.3.1\nlangchain_fireworks: 0.2.6\nlangchain_openai: 0.2.14\nlangchain_text_splitters: 0.3.4\nlanggraph_api: 0.0.15\nlanggraph_cli: 0.1.65\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.48\nlanggraph_storage: Installed. No version info available.\n", "created_at": "2025-01-06", "closed_at": "2025-01-10", "labels": [], "State": "closed", "Author": "lowjiansheng"}
{"issue_number": 2935, "issue_title": "Interrupt is not actually getting interrupted for the second time.", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\ndef ask_user_node(state: LookupState) -> Command[Literal['lookup_node']]:\n        agent_request = state['userAgentInteractionInfo']['agentRequest']\n        user_response = interrupt(agent_request)\n\n        if user_response:\n             return Command(goto='lookup_node', resume = {'userAgentInteractionInfo': {'agentRequest': agent_request, 'userResponse': user_response}}, update = {'messages': [HumanMessage(content=user_response, name = \"User_Response\")]})\n\n\nHow should we force the interrupt to wait till the user provides a value for the second time. Its taking the same old value that I pass in the RESUME.\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nThere should be some sort of force interrupt that waits for the user input second time.\nSystem Info\npython -m langchain_core.sys_info", "created_at": "2025-01-06", "closed_at": "2025-01-16", "labels": ["invalid"], "State": "closed", "Author": "Saisiva123"}
{"issue_number": 2929, "issue_title": "interrupt in loop", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\ndef ask_user(self, state):\n        \"\"\"Ask user question and get response from user\"\"\"\n        temp = self._check_user_message(state) # Check for all Quality of LLM\n        question = temp.get(\"message_to_user\",None)\n        if not question:\n            raise Exception(\"question is None so code cannot proceed\")\n        response_from_user = interrupt(question)\n        logger.info(f\"Response from user {response_from_user}\")\n        return {\n            \"messages\":[AIMessage(question),HumanMessage(response_from_user)]\n        }\nError Message and Stack Trace (if applicable)\nNo response\nDescription\n\nThis node goes in loop so the interrupt is triggered only first time and from second time it takes the values which has been already provided to it\n\nSystem Info\nlanggraph = \"^0.2.60\"\nlangchain-community = \"^0.3.13\"\nlangchain = \"^0.3.13\"\nlangchain-core = \"^0.3.28\"\nlangchain-openai = \"^0.2.14\"\nlanggraph-checkpoint-postgres = \"^2.0.9\"\nlanggraph-checkpoint = \"^2.0.9\"\nlangchain-chroma = \"^0.1.4\"", "created_at": "2025-01-04", "closed_at": "2025-01-06", "labels": [], "State": "closed", "Author": "vigneshmj1997"}
{"issue_number": 2928, "issue_title": "Getting TypeError: No synchronous function provided to \"search_web\". When I used an async subgraph inside a main sync graph", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings.fastembed import FastEmbedEmbeddings\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\nfrom langchain_chroma import Chroma\nfrom langsmith import traceable\nfrom langgraph.constants import Send\nimport logging\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom typing_extensions import TypedDict\nfrom typing import List\nimport os\nfrom langchain_community.document_loaders import UnstructuredMarkdownLoader\nimport json\nfrom langchain.schema import Document\nfrom langgraph.graph import END, StateGraph\nfrom tavily import TavilyClient, AsyncTavilyClient\ntavily_async_client = AsyncTavilyClient()\nimport asyncio\nfrom pydantic import BaseModel, Field\nfrom typing import Union\n\nweb_search_tool = TavilySearchResults(k=3)\n\n### State\nclass SearchQueriesParams(BaseModel):\n    queries: List[str] = Field(\n        description=\"A list of strings representing the search queries to be executed.\",\n    )\n    tavily_days: List[Union[int, None]] = Field(\n        description=\"A list of integers representing the number of days to limit search results for each query (e.g., 7 for last week), or None for no time restriction. Each value corresponds to a query in the 'queries' list.\",\n    )\n    tavily_topic: List[str] = Field(\n        description=\"A list of strings indicating the type of search for each query: 'news' for time-sensitive queries or 'general' for unrestricted searches. Each value corresponds to a query in the 'queries' list.\",\n    )\n\n\n    \nclass GraphState(TypedDict):\n    news_summary : str\n    question : str\n    generation : str\n    web_search : str \n    search_queries_params : SearchQueriesParams\n    documents : str #the concatenated list of documents text content (or search results)\n\n# Check if the directory and files are readable\ndirectory = './rag_docs'\n# print(os.access(directory, os.R_OK))  # Checks if the directory is readable\nos.chmod('./rag_docs', 0o755)\n\n\nmarkdown_folder_path = \"./rag_docs\"  # Set the path to your folder\ndocuments = []\n\n# Iterate over all .md files in the directory\nfor file in os.listdir(markdown_folder_path):\n    if file.endswith('.md'):\n        markdown_path = os.path.join(markdown_folder_path, file)                     #=fast\n        loader = UnstructuredMarkdownLoader(markdown_path, mode=\"single\", strategy=\"precise\")\n        documents.extend(loader.load())  # Add loaded documents to the list\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1200, chunk_overlap=100, add_start_index=True #starting char pos.\n    #size of characters for each chunk\n    #2nd param: will let us have a little portion of the prev. chunk\n    # so in case the key info is in that chunk, we can have a way to get those prev chars if needed \n    \n    #\n)\nall_splits = text_splitter.split_documents(documents)\n\n#the so-called chroma database\nlocal_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\npersist_dir = \"./chroma_db\"\nos.makedirs(persist_dir, exist_ok=True)\n\nvectorstore = Chroma.from_documents(documents=all_splits, embedding=local_embeddings,\n                                    persist_directory=persist_dir)\n\n\nretriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3},\n                                    )\n\n\nllm = ChatOpenAI(model=\"gpt-4o\", temperature=0) \n\nllm_json = ChatOpenAI(\n        model=\"gpt-4o\",\n        temperature = 0,\n        model_kwargs={\"response_format\": {\"type\": \"json_object\"}})\n\n\n#---------------------\n#prompts\ngeneration_instructions = \"\"\"\n You are an assistant for question-answering tasks inside a phone call conversation.\n \n \n \n    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n    Keep the answer concise optimized to reduce latency, in a spoken language style.\n    \n    Question: {question} \n    Context: {context} \n    \n    You must also keep in mind that you're answering questions related with the news of the week, so you must always respond properly connecting the answer with the spoken topics, for which i'll attach below as a brief summary.\n\"\"\"\n\n\nquery_writer_instructions = \"\"\"\n\n\nGiven the user\u2019s background and the question provided below, generate `{number_of_queries}` relevant queries to search on the internet in order to gather sufficient information to answer the question. Consider the following aspects:\n\n\n- **User's Knowledge**: The user knows basic political concepts (left-wing, right-wing) but may require simple, concise explanations of political contexts. They are familiar with global conflicts (e.g., Gaza-Israel, Ukraine-Russia) but are not an expert in political theory.\n  \n- **User\u2019s Interest**: The user is interested in technology and AI and has high school-level science knowledge, so technical concepts can be explained with that in mind.\n  \n- **User's Gaps in Knowledge**: The user has limited understanding of economic concepts (e.g., inflation, stock markets), so explanations related to economics should be kept simple and direct.\n\n- **Contextual Relevance**: Ensure that the queries are tailored to cover both current events (as presented in news) and any necessary historical background. The queries should also bridge any gaps in the user's knowledge without overwhelming them with unnecessary detail.\n\nFor each query, determine if it is time-sensitive (related to news) or a general query (e.g., related to technology or historical topics). Generate appropriate queries based on this, and use the schema below to specify the parameters for the web search.\n\nRenember to look at the already generate queries in case there are any queries that have already been searched, attached below as well.\n---\n\n### Tavily-Specific Web Search Parameters:\nWhen formulating the list string queries, for each of them, set the Tavily parameters as follows: \n- **tavily_topic**:  \n   - Use `\"news\"` if the query is related to time-sensitive or current events.  \n   - Use `\"general\"` for technical, historical, or non-time-sensitive topics.  \n- **tavily_days**:  \n   - Set `7` only if `tavily_topic=\"news\"`.  \n   - Set `None` if `tavily_topic=\"general\"`. \n   \nUser question: {question}.\n\nAlready searched queries (if applicable): {searched_queries}\n\"\"\"\n\n\ndocs_grader_instructions = \"\"\"\n\n    You are a grader assessing relevance \n    of a retrieved document to a user question. If the document contains keywords related to the user question, \n    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\n\n    Here is the retrieved document: \\n\\n {document} \\n\\n\n    Here is the user question: {question}\n\"\"\"\n\ngeneration_grader_instructions = \"\"\"\n    You are a grader assessing whether \n    an answer is grounded in / supported by a set of facts. Give a binary 'yes' or 'no' score to indicate \n    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \n    single key 'score' and no preamble or explanation.\n    Here are the facts:\n    \\n ------- \\n\n    {documents} \n    \\n ------- \\n\n    \n    Here is the generated answer: \\n\\n {generation} \\n\\n\n\"\"\"\n\nanswer_grader_instructions = \"\"\"\n\n    You are a grader assessing whether an \n    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \n    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n    \n    Here is the generated answer:\n    \\n ------- \\n\n    {generation} \n    \\n ------- \\n\n    Here is the original user question: {question}\n\"\"\"\n\n#--------------- nodes\n\n@traceable\ndef retrieve(state):\n    \"\"\"\n    Retrieve documents from vectorstore\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, documents, that contains retrieved documents\n    \"\"\"\n    # print(\"---RETRIEVE---\")\n    question = state[\"question\"]\n\n    # Retrieval\n    retrieved_docs = retriever.invoke(question)\n    \n    context = ' '.join([doc.page_content for doc in retrieved_docs])\n    \n    return {\"documents\": context, \"question\": question}\n#\n\n@traceable\ndef grade_documents(state):\n    \"\"\"\n    Determines whether the retrieved documents are relevant to the question\n    If any document is not relevant, we will set a flag to run web search\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): Filtered out irrelevant documents and updated web_search state\n    \"\"\"\n\n    # print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n    question = state[\"question\"]\n    context = state[\"documents\"]\n    \n    \n    system_instructions = docs_grader_instructions.format(question=question, document=context)\n    messages = [\n        {\"role\": \"system\", \"content\": system_instructions},\n        # {\"role\": \"user\", \"content\": f\": {}\"}\n    ]   \n    grade = json.loads(llm_json.invoke(messages).content)[\"score\"]\n    # print(grade)\n    \n    if grade.lower() == \"yes\":\n        # print(\"---GRADE: DOCUMENT RELEVANT---\")\n        return {\"documents\": context, \"question\": question, \"web_search\": \"No\"}\n    else:\n        # print(\"---GRADE: RETRIEVED DOCUMENTS NOT RELEVANT, RUNNING WEB SEARCH INSTEAD---\")\n        return {\"documents\": context, \"question\": question, \"web_search\": \"Yes\"}\n    \n\n@traceable\ndef generate(state):\n    \"\"\"\n    Generate answer either using RAG on retrieved documents or web search results\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        state (dict): New key added to state, generation, that contains LLM generation\n    \"\"\"\n    # print(\"---GENERATE---\")\n    question = state[\"question\"]\n    context = state[\"documents\"]\n    \n    system_instructions = generation_instructions.format(question=question, context=context)\n    \n    messages = [\n        {\"role\": \"system\", \"content\": system_instructions},\n        {\"role\": \"user\", \"content\": f\": Here is the news summary shown to the user {state['news_summary']}\"}\n    ]    \n    answer = llm.invoke(messages).content\n     \n    return {\"documents\": context, \"question\": question, \"generation\": answer}\n#\n\n@traceable\ndef generate_queries(state):\n    \"\"\" Generate search queries for a report section, and set tavily_topic and tavily_days\"\"\"\n\n    \n    # Generate queries and tavily params with the custom pydantic model\n    structured_llm = llm.with_structured_output(SearchQueriesParams)\n\n    \n    searched_queries = state.get(\"search_queries_params\", None)\n    \n    if searched_queries!=None:\n        searched_queries = searched_queries.queries\n    else:\n        searched_queries = \"None\"    \n    \n    # Format system instructions\n    system_instructions = query_writer_instructions.format(number_of_queries=3, question=state[\"question\"],\n                                                           searched_queries=searched_queries)\n\n    # Generate queries  \n    # search_params = structured_llm.invoke([SystemMessage(content=system_instructions)]+[HumanMessage(content=f\"Generate search queries on the provided topic and set the value of the params tavily_topic and tavily_days. The reference news page content is {page_content}\")])\n    search_params = structured_llm.invoke([{\"role\": \"system\", \"content\": system_instructions}])\n    \n    \n    queries = search_params.queries  # Assuming 'queries' is the result you expect from the LLM response\n    tavily_topic = search_params.tavily_topic  # LLM response for tavily_topic\n    tavily_days = search_params.tavily_days\n    \n    search_queries_params = SearchQueriesParams(\n        queries=queries,\n        tavily_topic=tavily_topic,\n        tavily_days=tavily_days\n    )\n    \n    \n    return {\"search_queries_params\": search_queries_params}\n\n\nasync def tavily_search_async(search_queries:List[str], tavily_topic:str, tavily_days=None):\n    \"\"\"\n    Performs concurrent web searches using the Tavily API.\n\n    Args:\n        search_queries (List[SearchQuery]): List of search queries to process\n        tavily_topic (str): Type of search to perform ('news' or 'general')\n        tavily_days (int) or None: Number of days to look back for news articles (only used when tavily_topic='news')\n\n    Returns:\n        List[dict]: List of search results from Tavily API, one per query\n\n    Note:\n        For news searches, each result will include articles from the last `tavily_days` days.\n        For general searches, the time range is unrestricted.\n    \"\"\"\n    \n    search_tasks = []\n    for query in search_queries:\n        if tavily_topic == \"news\":\n            search_tasks.append(\n                tavily_async_client.search(\n                    query,\n                    max_results=5,\n                    include_raw_content=True,\n                    topic=\"news\",\n                    days=tavily_days\n                )\n            )\n        else:\n            search_tasks.append(\n                tavily_async_client.search(\n                    query,\n                    max_results=5,\n                    include_raw_content=True,\n                    topic=\"general\"\n                )\n            )\n\n    # Execute all searches concurrently\n    search_docs = await asyncio.gather(*search_tasks, return_exceptions=True)\n\n    return search_docs\n\n\n\n\n# @traceable\n# def web_search(state):\n#     \"\"\"\n#     Web search based based on the question\n\n#     Args:\n#         state (dict): The current graph state\n\n#     Returns:\n#         state (dict): Appended web results to documents\n#     \"\"\"\n\n#     # print(\"---WEB SEARCH---\")\n#     question = state[\"question\"]\n#     context = state[\"documents\"] if \"documents\" in state else None\n\n#     # Web search\n    \n    \n    \n#     docs = web_search_tool.invoke({\"query\": question})\n#     web_results = \"\\n\".join([d[\"content\"] for d in docs])\n#     web_results = Document(page_content=web_results)\n#     #we override the past documents, we add the search results\n    \n#     # if context is not None:\n#     #     context += f\"\\n {web_results.page_content}\"\n#     # else:\n#     #     context = web_results.page_content\n    \n#     context = web_results.page_content\n    \n#     return {\"documents\": context, \"question\": question}\n\n\ndef deduplicate_and_format_sources(search_response, max_tokens_per_source, include_raw_content=True):\n    \"\"\"\n    Takes either a single search response or list of responses from Tavily API and formats them.\n    Limits the raw_content to approximately max_tokens_per_source.\n    include_raw_content specifies whether to include the raw_content from Tavily in the formatted string.\n    \n    Args:\n        search_response: Either:\n            - A dict with a 'results' key containing a list of search results\n            - A list of dicts, each containing search results\n            \n    Returns:\n        str: Formatted string with deduplicated sources\n    \"\"\"\n    # Convert input to list of results\n    if isinstance(search_response, dict):\n        # sources_list = search_response['results']\n        sources_list = search_response.get('results', [])\n        \n    elif isinstance(search_response, list):\n        sources_list = []\n        for response in search_response:\n            if isinstance(response, dict) and 'results' in response:\n                # sources_list.extend(response['results'])\n                sources_list.extend(response.get('results', []))\n            else:\n                sources_list.extend(response)\n    else:\n        raise ValueError(\"Input must be either a dict with 'results' or a list of search results\")\n    \n    # Deduplicate by URL\n    unique_sources = {}\n    for source in sources_list:\n        url = source.get('url')\n        # if source['url'] not in unique_sources:\n        #     unique_sources[source['url']] = source\n        if url and url not in unique_sources:\n            unique_sources[url] = source\n    \n    # Format output\n    formatted_text = \"Sources:\\n\\n\"\n    for i, source in enumerate(unique_sources.values(), 1):\n        formatted_text += f\"Source {source['title']}:\\n===\\n\"\n        formatted_text += f\"URL: {source['url']}\\n===\\n\"\n        formatted_text += f\"Most relevant content from source: {source['content']}\\n===\\n\"\n        if include_raw_content:\n            # Using rough estimate of 4 characters per token\n            char_limit = max_tokens_per_source * 4\n            # Handle None raw_content\n            raw_content = source.get('raw_content', '')\n            if raw_content is None:\n                raw_content = ''\n                print(f\"Warning: No raw_content found for source {source['url']}\")\n            if len(raw_content) > char_limit:\n                raw_content = raw_content[:char_limit] + \"... [truncated]\"\n            formatted_text += f\"Full source content limited to {max_tokens_per_source} tokens: {raw_content}\\n\\n\"\n                \n    return formatted_text.strip()\n\n\nasync def search_web(state):\n    \"\"\" Web search based based on the question.\"\"\"\n    \n    # Get search_queries_params from state (which includes lists of queries, tavily_topic, and tavily_days)\n    search_queries_params = state[\"search_queries_params\"]\n    queries = search_queries_params.queries  # List of queries\n    tavily_topics = search_queries_params.tavily_topic  # List of topics ('news' or 'general')\n    tavily_days = search_queries_params.tavily_days  # List of days for limiting search (or None)\n\n    # Check if lengths of queries, tavily_topics, and tavily_days are equal\n    if not (len(queries) == len(tavily_topics) == len(tavily_days)):\n        raise ValueError(\"The lengths of queries, tavily_topics, and tavily_days must be equal.\")\n    \n    # Web search using async function with corresponding parameters\n    search_docs = []\n    for query, topic, days in zip(queries, tavily_topics, tavily_days):\n        search_doc = await tavily_search_async([query], topic, days)\n        search_docs.extend(search_doc)  # Add results to the overall list\n\n    # Deduplicate and format sources\n    source_str = deduplicate_and_format_sources(search_docs, max_tokens_per_source=5000, include_raw_content=True)\n    \n    web_results = Document(page_content=source_str)\n    \n    return {\"documents\": web_results.page_content}\n\n\n\n\n\nwa = StateGraph(GraphState, input=GraphState, output=GraphState) #the web search subgraph takes the same graph\n#so it takes the search queries params and writes to 'documents' in state \n\n# Add a node (action) to the graph\nwa.add_node(\"search_web\", search_web)\n\n# Set the entry point for the graph (optional, but typically useful for defining the start)\nwa.set_entry_point(\"search_web\")\n\n# Add an edge from the \"search_web\" node to the END node\nwa.add_edge(\"search_web\", END)\n\n\ndef initiate_web_search(state): #custom state for the subgraph\n    \"\"\" This is the \"map\" step when we kick off web research for some sections of the report \"\"\"    \n    # Debug: Log the sections and their research flag\n    # for section in state[\"sections\"]:\n    #     logging.debug(f\"Section: {section}, Research flag: {section.research}\")\n\n    tasks = [\n        #this subgraph call wrapper will be called as search_web() or web_search(), it doesn't matter\n        Send(\"websearch\", {\"web_search\": query_with_param}) \n            #this is the \"sender\" node, along with it's inputs {} for the state that this subgraph expects\n        for query_with_param in state[\"search_queries_params\"] \n        # if s.research\n    ]\n    \n    # Log each Send task\n    for task in tasks:\n        logging.debug(f\"Initiating task: {task}\")\n        \n    \n        \n    return tasks\n# Compile the graph\n\n\n# def initiate_final_section_writing(state: ReportState):\n#     \"\"\" Write any final sections using the Send API to parallelize the process \"\"\"    \n\n#     # Kick off section writing in parallel via Send() API for any sections that do not require research\n#     return [\n#         Send(\"write_final_sections\", {\"section\": s, \"report_sections_from_research\": state[\"report_sections_from_research\"]}) \n#         for s in state[\"sections\"] \n#         if not s.research\n#     ]\n    \n    \n# def compile_final_report(state: ReportState):\n#     \"\"\" Compile the final report \"\"\"    \n\n#     # Get sections\n#     sections = state[\"sections\"]\n#     completed_sections = {s.name: s.content for s in state[\"completed_sections\"]}\n\n#     # Update sections with completed content while maintaining original order\n#     for section in sections:\n#         section.content = completed_sections[section.name]\n\n#     # Compile final report\n#     all_sections = \"\\n\\n\".join([s.content for s in sections])\n\n#     return {\"final_report\": all_sections}\n\n\n@traceable\ndef grade_generation_v_documents_and_question(state):\n    \"\"\"\n    Determines whether the generated response is grounded in the document and answers the question.\n    \n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Decision for next node to call\n    \"\"\"\n\n    # print(\"---CHECK HALLUCINATIONS---\")\n    question = state[\"question\"]\n    context = state[\"documents\"] #it can be either the retrieved docs or the web search results\n    generation = state[\"generation\"]\n    \n    \n    generation_grader_prompt = generation_grader_instructions.format(documents=context,\n                                                                     generation=generation)\n    answer_grader_prompt = answer_grader_instructions.format(generation=generation, question=question)\n    \n    \n    messages = [\n        {\"role\": \"system\", \"content\": generation_grader_prompt},\n        # {\"role\": \"user\", \"content\": f\": {}\"}\n    ]   \n    grade = json.loads(llm_json.invoke(messages).content)[\"score\"]\n    # print(grade)\n    \n    if grade.lower() == \"yes\": #the answer is grounded in the documents\n        \n    \n        #now, we check such generation correctly answers the question\n        messages = [\n            {\"role\": \"system\", \"content\": answer_grader_prompt},\n            # {\"role\": \"user\", \"content\": f\": {}\"}\n        ]    \n        \n        grade = json.loads(llm_json.invoke(messages).content)[\"score\"]\n        \n        if grade.lower() == \"yes\":\n            # print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS, AND IT ANSWERS THE QUESTION---\")\n            return \"useful\"\n        \n        \n        else:\n            # print(\"---DECISION: GENERATION DOES NOT ANSWER THE QUESTION, RUNNING WEBSEARCH---\")\n            return \"not_useful\" #then we simply retry the websearch (only)\n        \n    else:\n        #IF THE MODEL HALLUCINATES, WE RE-RUN ONLY THE GENERATION\n        \n        # print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n        return \"not_grounded\"\n    \n\n#-------------------- graph\n    \nworkflow = StateGraph(GraphState)\n# Define the nodes\nworkflow.add_node(\"retrieve\", retrieve) \nworkflow.add_node(\"generate_queries\", generate_queries)\nworkflow.add_node(\"websearch\", wa.compile()) #the subgraph as a node\nworkflow.add_node(\"grade_documents\", grade_documents)\nworkflow.add_node(\"generate\", generate)\n\n#conditional nodes\n@traceable\ndef decide_to_generate(state):\n    \"\"\"\n    Determines whether to generate an answer, or add web search\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Binary decision for next node to call\n    \"\"\"\n\n    # print(\"---ASSESS GRADED DOCUMENTS---\")\n    web_search = state[\"web_search\"]\n\n    if web_search == \"Yes\":\n        return \"websearch\"\n    else:\n        return \"generate\"\n    \nworkflow.set_entry_point(\"retrieve\")\n\nworkflow.add_edge(\"retrieve\", \"grade_documents\")\n\nworkflow.add_conditional_edges(\n    \"grade_documents\",\n    decide_to_generate, #after running grade_documents, we get the key of websearch to know if we run it or not\n    {\n        \"websearch\": \"generate_queries\", #before websearch, we generate the queries through the generate_queries node\n        \"generate\": \"generate\",\n    },\n)\n\n#when running websearch, we force it to do so, within the subgraph call\nworkflow.add_conditional_edges(\n    \"generate_queries\", initiate_web_search, [\"websearch\"] \n                            #the name we gave to the subgraph wrapped as a node()\n                            #where we did subgraph.compile()\n\n    #after running retrieve, we FORCE to run this wrapper\n)\n\n\n\n# workflow.add_edge(\"generate_queries\", \"websearch\") \nworkflow.add_edge(\"websearch\", \"generate\") #after web search, we go directly to generation\n\n\n# #when running websearch, we force it to do so, within the subgraph call\n# workflow.add_conditional_edges(\n#     \"generate_queries\", initiate_web_search, [\"websearch\"] \n#                             #the name we gave to the subgraph wrapped as a node()\n#                             #where we did subgraph.compile()\n\n#     #after running retrieve, we FORCE to run this wrapper\n# )\n\n\nworkflow.add_conditional_edges( #conditional edges\n    \"generate\",\n    grade_generation_v_documents_and_question,\n    {\n        \"not_grounded\": \"generate\", #if the llm generation grader result is = \"not supported\", then re-try\n        \"useful\": END, #if it is useful, then end the process\n        \"not_useful\": \"generate_queries\", #if not useful, then re-create queries again given the previous one (if applicable) and run websearch\n    },\n\n\n\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nfrom contextlib import ExitStack\n\nstack = ExitStack()\nmemory = stack.enter_context(SqliteSaver.from_conn_string(\":memory:\"))\n\napp = workflow.compile(checkpointer=memory)\n\nthread = {\"configurable\": {\"thread_id\":\"1\"}} #\"recursion_limit\":60\n\ntext=\"example\"\napp.invoke({\"question\":\"which are the current relationships betwen Venezuela and Spain\", \"news_summary\":text}, thread)[\"generation\"]\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[99], line 1\n----> 1 app.invoke({\"question\":\"capital of Spain\", \"news_summary\":text}, thread)[\"generation\"]\n\nFile c:\\Users\\pablo\\Desktop\\P - Proyectos en Curso\\ai-curated-articles\\ai_curator\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1927, in Pregel.invoke(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\n   1925 else:\n   1926     chunks = []\n-> 1927 for chunk in self.stream(\n   1928     input,\n   1929     config,\n   1930     stream_mode=stream_mode,\n   1931     output_keys=output_keys,\n   1932     interrupt_before=interrupt_before,\n   1933     interrupt_after=interrupt_after,\n   1934     debug=debug,\n   1935     **kwargs,\n   1936 ):\n   1937     if stream_mode == \"values\":\n   1938         latest = chunk\n\nFile c:\\Users\\pablo\\Desktop\\P - Proyectos en Curso\\ai-curated-articles\\ai_curator\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1647, in Pregel.stream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\n   1641     # Similarly to Bulk Synchronous Parallel / Pregel model\n   1642     # computation proceeds in steps, while there are channel updates\n   1643     # channel updates from step N are only visible in step N+1\n   1644     # channels are guaranteed to be immutable for the duration of the step,\n   1645     # with channel updates applied only at the transition between steps\n   1646     while loop.tick(input_keys=self.input_channels):\n-> 1647         for _ in runner.tick(\n   1648             loop.tasks.values(),\n   1649             timeout=self.step_timeout,\n   1650             retry_policy=self.retry_policy,\n   1651             get_waiter=get_waiter,\n   1652         ):\n   1653             # emit output\n   1654             yield from output()\n   1655 # emit output\n\nFile c:\\Users\\pablo\\Desktop\\P - Proyectos en Curso\\ai-curated-articles\\ai_curator\\Lib\\site-packages\\langgraph\\pregel\\runner.py:159, in PregelRunner.tick(self, tasks, reraise, timeout, retry_policy, get_waiter)\n    157     yield\n    158 # panic on failure or timeout\n--> 159 _panic_or_proceed(\n    160     done_futures.union(f for f, t in futures.items() if t is not None),\n    161     panic=reraise,\n    162 )\n\nFile c:\\Users\\pablo\\Desktop\\P - Proyectos en Curso\\ai-curated-articles\\ai_curator\\Lib\\site-packages\\langgraph\\pregel\\runner.py:367, in _panic_or_proceed(futs, timeout_exc_cls, panic)\n    365 # raise the exception\n    366 if panic:\n--> 367     raise exc\n    368 else:\n    369     return\n\nFile c:\\Users\\pablo\\Desktop\\P - Proyectos en Curso\\ai-curated-articles\\ai_curator\\Lib\\site-packages\\langgraph\\pregel\\executor.py:70, in BackgroundExecutor.done(self, task)\n     68 def done(self, task: concurrent.futures.Future) -> None:\n     69     try:\n---> 70         task.result()\n     71     except GraphInterrupt:\n     72         # This exception is an interruption signal, not an error\n     73         # so we don't want to re-raise it on exit\n     74         self.tasks.pop(task)\n\nFile ~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:449, in Future.result(self, timeout)\n    447     raise CancelledError()\n    448 elif self._state == FINISHED:\n--> 449     return self.__get_result()\n    451 self._condition.wait(timeout)\n    453 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\nFile ~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:401, in Future.__get_result(self)\n    399 if self._exception:\n    400     try:\n--> 401         raise self._exception\n    402     finally:\n    403         # Break a reference cycle with the exception in self._exception\n    404         self = None\n\nFile ~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\thread.py:58, in _WorkItem.run(self)\n     55     return\n     57 try:\n---> 58     result = self.fn(*self.args, **self.kwargs)\n     59 except BaseException as exc:\n     60     self.future.set_exception(exc)\n\nFile c:\\Users\\pablo\\Desktop\\P - Proyectos en Curso\\ai-curated-articles\\ai_curator\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40, in run_with_retry(task, retry_policy, writer)\n     38 task.writes.clear()\n     39 # run the task\n---> 40 task.proc.invoke(task.input, config)\n     41 # if successful, end\n     42 break\n\nFile c:\\Users\\pablo\\Desktop\\P - Proyectos en Curso\\ai-curated-articles\\ai_curator\\Lib\\site-packages\\langgraph\\utils\\runnable.py:410, in RunnableSeq.invoke(self, input, config, **kwargs)\n    408 context.run(_set_config_context, config)\n    409 if i == 0:\n--> 410     input = context.run(step.invoke, input, config, **kwargs)\n    411 else:\n    412     input = context.run(step.invoke, input, config)\n\nFile c:\\Users\\pablo\\Desktop\\P - Proyectos en Curso\\ai-curated-articles\\ai_curator\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1927, in Pregel.invoke(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\n   1925 else:\n   1926     chunks = []\n-> 1927 for chunk in self.stream(\n   1928     input,\n   1929     config,\n   1930     stream_mode=stream_mode,\n   1931     output_keys=output_keys,\n   1932     interrupt_before=interrupt_before,\n   1933     interrupt_after=interrupt_after,\n   1934     debug=debug,\n   1935     **kwargs,\n   1936 ):\n   1937     if stream_mode == \"values\":\n   1938         latest = chunk\n\nFile c:\\Users\\pablo\\Desktop\\P - Proyectos en Curso\\ai-curated-articles\\ai_curator\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1647, in Pregel.stream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\n   1641     # Similarly to Bulk Synchronous Parallel / Pregel model\n   1642     # computation proceeds in steps, while there are channel updates\n   1643     # channel updates from step N are only visible in step N+1\n   1644     # channels are guaranteed to be immutable for the duration of the step,\n   1645     # with channel updates applied only at the transition between steps\n   1646     while loop.tick(input_keys=self.input_channels):\n-> 1647         for _ in runner.tick(\n   1648             loop.tasks.values(),\n   1649             timeout=self.step_timeout,\n   1650             retry_policy=self.retry_policy,\n   1651             get_waiter=get_waiter,\n   1652         ):\n   1653             # emit output\n   1654             yield from output()\n   1655 # emit output\n\nFile c:\\Users\\pablo\\Desktop\\P - Proyectos en Curso\\ai-curated-articles\\ai_curator\\Lib\\site-packages\\langgraph\\pregel\\runner.py:104, in PregelRunner.tick(self, tasks, reraise, timeout, retry_policy, get_waiter)\n    102 t = tasks[0]\n    103 try:\n--> 104     run_with_retry(t, retry_policy, writer=writer)\n    105     self.commit(t, None)\n    106 except Exception as exc:\n\nFile c:\\Users\\pablo\\Desktop\\P - Proyectos en Curso\\ai-curated-articles\\ai_curator\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40, in run_with_retry(task, retry_policy, writer)\n     38 task.writes.clear()\n     39 # run the task\n---> 40 task.proc.invoke(task.input, config)\n     41 # if successful, end\n     42 break\n\nFile c:\\Users\\pablo\\Desktop\\P - Proyectos en Curso\\ai-curated-articles\\ai_curator\\Lib\\site-packages\\langgraph\\utils\\runnable.py:410, in RunnableSeq.invoke(self, input, config, **kwargs)\n    408 context.run(_set_config_context, config)\n    409 if i == 0:\n--> 410     input = context.run(step.invoke, input, config, **kwargs)\n    411 else:\n    412     input = context.run(step.invoke, input, config)\n\nFile c:\\Users\\pablo\\Desktop\\P - Proyectos en Curso\\ai-curated-articles\\ai_curator\\Lib\\site-packages\\langgraph\\utils\\runnable.py:141, in RunnableCallable.invoke(self, input, config, **kwargs)\n    137 def invoke(\n    138     self, input: Any, config: Optional[RunnableConfig] = None, **kwargs: Any\n    139 ) -> Any:\n    140     if self.func is None:\n--> 141         raise TypeError(\n    142             f'No synchronous function provided to \"{self.name}\".'\n    143             \"\\nEither initialize with a synchronous function or invoke\"\n    144             \" via the async API (ainvoke, astream, etc.)\"\n    145         )\n    146     if config is None:\n    147         config = ensure_config()\n\nTypeError: No synchronous function provided to \"search_web\".\nEither initialize with a synchronous function or invoke via the async API (ainvoke, astream, etc.)\nDescription\nI'm using the Sender API of langraph to make a subgraph which involves a subgraph creation with a single async node inside, which gets called inside a main graph sync which shares the same state\nexample snippet of code where it actually worked:\nasync def search_web(state: SectionState):\n\"\"\" Search the web for each query, then return a list of raw sources and a formatted string of sources.\"\"\"\n# Get search_queries_params from state (which includes lists of queries, tavily_topic, and tavily_days)\nsearch_queries_params = state[\"search_queries_params\"]\nqueries = search_queries_params.queries  # List of queries\ntavily_topics = search_queries_params.tavily_topic  # List of topics ('news' or 'general')\ntavily_days = search_queries_params.tavily_days  # List of days for limiting search (or None)\n\n# Check if lengths of queries, tavily_topics, and tavily_days are equal\nif not (len(queries) == len(tavily_topics) == len(tavily_days)):\n    raise ValueError(\"The lengths of queries, tavily_topics, and tavily_days must be equal.\")\n\n# Web search using async function with corresponding parameters\nsearch_docs = []\nfor query, topic, days in zip(queries, tavily_topics, tavily_days):\n    search_doc = await tavily_search_async([query], topic, days)\n    search_docs.extend(search_doc)  # Add results to the overall list\n\n# Deduplicate and format sources\nsource_str = deduplicate_and_format_sources(search_docs, max_tokens_per_source=5000, include_raw_content=True)\n\n\nlogging.debug(\"Web search node done sucessfully!\")\nreturn {\"source_str\": source_str}\n\ndef  write_section(state: SectionState):\n\"\"\" Write a section of the report \"\"\"\n# Get state \nsection = state[\"section\"]\nsource_str = state[\"source_str\"]\npage_content = state[\"page_content\"]\n\n# Format system instructions\nsystem_instructions = section_writer_instructions.format(section_title=section.name, section_topic=section.description, context=source_str)\n\n# Generate section  \nsection_content = gpt_4o.invoke([SystemMessage(content=system_instructions)]+[HumanMessage(content=f\"Generate a report section based on the provided sources, and the news item page content: \\n\\n {page_content}\")])\n\n# Write content to the section object  \nsection.content = section_content.content\n\n\nlogging.debug(f\"Section {section.name} (with research={section.research}), written sucessfully!\")\n\n# Write the updated section to completed sections\nreturn {\"completed_sections\": [section]}\n\nAdd nodes and edges\nsection_builder = StateGraph(SectionState, output=SectionOutputState)\nsection_builder.add_node(\"generate_queries\", generate_queries)\nsection_builder.add_node(\"search_web\", search_web)\nsection_builder.add_node(\"write_section\", write_section)\nsection_builder.add_edge(START, \"generate_queries\")\nsection_builder.add_edge(\"generate_queries\", \"search_web\")\nsection_builder.add_edge(\"search_web\", \"write_section\")\nsection_builder.add_edge(\"write_section\", END)\ndef initiate_section_writing(state: ReportState):\n\"\"\" This is the \"map\" step when we kick off web research for some sections of the report \"\"\"\n# Debug: Log the sections and their research flag\nfor section in state[\"sections\"]:\nlogging.debug(f\"Section: {section}, Research flag: {section.research}\")\ntasks = [\n    Send(\"build_section_with_web_research\", {\"section\": s, \"page_content\" : state[\"page_content\"]}) \n        #this is the \"sender\" node, along with it's inputs {} for the state that this subgraph expects\n    for s in state[\"sections\"] \n    if s.research\n]\n\n# Log each Send task\nfor task in tasks:\n    logging.debug(f\"Initiating task: {task}\")\n    \nreturn tasks\n\ndef write_final_sections(state: SectionState):\n\"\"\" Write final sections of the report, which do not require web search and use the completed sections as context \"\"\"\n# Get state \nsection = state[\"section\"]\ncompleted_report_sections = state[\"report_sections_from_research\"]\n\n# Format system instructions\nsystem_instructions = final_section_writer_instructions.format(section_title=section.name, section_topic=section.description, context=completed_report_sections)\n\n# Generate section  \nsection_content = gpt_4o.invoke([SystemMessage(content=system_instructions)]+[HumanMessage(content=\"Generate a report section based on the provided sources.\")])\n\n# Write content to section \nsection.content = section_content.content\n\n# Write the updated section to completed sections\nreturn {\"completed_sections\": [section]}\n\ndef gather_completed_sections(state: ReportState):\n\"\"\" Gather completed sections from research and format them as context for writing the final sections \"\"\"\n# List of completed sections\ncompleted_sections = state[\"completed_sections\"]\n\n# Format completed section to str to use as context for final sections\ncompleted_report_sections = format_sections(completed_sections)\n\nreturn {\"report_sections_from_research\": completed_report_sections}\n\ndef initiate_final_section_writing(state: ReportState):\n\"\"\" Write any final sections using the Send API to parallelize the process \"\"\"\n# Kick off section writing in parallel via Send() API for any sections that do not require research\nreturn [\n    Send(\"write_final_sections\", {\"section\": s, \"report_sections_from_research\": state[\"report_sections_from_research\"]}) \n    for s in state[\"sections\"] \n    if not s.research\n]\n\ndef compile_final_report(state: ReportState):\n\"\"\" Compile the final report \"\"\"\n# Get sections\nsections = state[\"sections\"]\ncompleted_sections = {s.name: s.content for s in state[\"completed_sections\"]}\n\n# Update sections with completed content while maintaining original order\nfor section in sections:\n    section.content = completed_sections[section.name]\n\n# Compile final report\nall_sections = \"\\n\\n\".join([s.content for s in sections])\n\nreturn {\"final_report\": all_sections}\n\nAdd nodes and edges\nbuilder = StateGraph(ReportState, input=ReportStateInput, output=ReportStateOutput, config_schema=Configuration)\nbuilder.add_node(\"generate_report_plan\", generate_report_plan)\nbuilder.add_node(\"build_section_with_web_research\", section_builder.compile())\nbuilder.add_node(\"gather_completed_sections\", gather_completed_sections)\nbuilder.add_node(\"write_final_sections\", write_final_sections)\nbuilder.add_node(\"compile_final_report\", compile_final_report)\nbuilder.add_edge(START, \"generate_report_plan\")\nbuilder.add_conditional_edges(\"generate_report_plan\", initiate_section_writing, [\"build_section_with_web_research\"])\nbuilder.add_edge(\"build_section_with_web_research\", \"gather_completed_sections\")\nbuilder.add_conditional_edges(\"gather_completed_sections\", initiate_final_section_writing, [\"write_final_sections\"])\nbuilder.add_edge(\"write_final_sections\", \"compile_final_report\")\nbuilder.add_edge(\"compile_final_report\", END)\ngraph = builder.compile()\nSystem Info\nNone", "created_at": "2025-01-03", "closed_at": "2025-01-03", "labels": [], "State": "closed", "Author": "pablocpz"}
{"issue_number": 2924, "issue_title": "PostgresStore Get method", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nDB_URI = \"postgresql://ai_user:xxxxxxx@x.x.x.x:5432/ai_sessions?sslmode=disable\"\n\nconnection_kwargs = {\n    \"autocommit\": True,\n    \"prepare_threshold\": 0,\n}\n\nconn=Connection.connect(DB_URI, **connection_kwargs)\n\nin_memory_store=InMemoryStore()\npostgres_store=PostgresStore(conn)\n\nnamespace=(\"memory\", \"user_2\")\nkey=(\"user_memory\")\n\nin_memory_store.put(namespace,key,\"This is a test message\")\n\nprint(\"======Output of GET from In-Memory Store=======\")\nprint(in_memory_store.get(namespace,key))\n\n\npostgres_store.put(namespace,key,\"This is a test message for checking PostgresStore\")\nprint(\"======Output of GET from Postgres Store=======\")\nprint(postgres_store.get(namespace,key))\nError Message and Stack Trace (if applicable)\n======Output of GET from In-Memory Store=======\nItem(namespace=['memory', 'user_2'], key='user_memory', value='This is a test message', created_at='2025-01-03T16:53:36.133867+00:00', updated_at='2025-01-03T16:53:36.133867+00:00')\n======Output of GET from Postgres Store=======\nNone\nDescription\nWhen I try to run PostgresStore GET method, it returns an empty value.  However, I can see the value exists in backend database (added by PUT method):\nai_sessions=> select * from store;\nprefix     |     key     |                        value                        |          created_at           |          updated_at\n---------------+-------------+-----------------------------------------------------+-------------------------------+-------------------------------\nmemory.user_2 | user_memory | \"This is a test message for checking PostgresStore\" | 2025-01-03 16:53:35.239496+00 | 2025-01-03 16:53:35.239496+00\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.22631\nPython Version:  3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.3.28\nlangchain: 0.3.13\nlangchain_community: 0.3.13\nlangsmith: 0.1.125\nlangchain_anthropic: 0.3.0\nlangchain_cohere: 0.3.4\nlangchain_experimental: 0.3.4\nlangchain_groq: 0.2.0\nlangchain_huggingface: 0.1.0\nlangchain_ollama: 0.2.0\nlangchain_openai: 0.2.2\nlangchain_postgres: 0.0.9\nlangchain_text_splitters: 0.3.4\nlangserve: 0.3.0\n\nOther Dependencies\n\naiohttp: 3.11.10\nanthropic: 0.39.0\nasync-timeout: Installed. No version info available.\ncohere: 5.13.4\ndataclasses-json: 0.6.7\ndefusedxml: 0.7.1\nfastapi: 0.115.0\ngroq: 0.10.0\nhttpx: 0.27.0\nhttpx-sse: 0.4.0\nhuggingface-hub: 0.25.0\njsonpatch: 1.33\nnumpy: 1.24.4\nollama: 0.3.3\nopenai: 1.58.1\norjson: 3.10.7\npackaging: 23.2\npandas: 1.5.3\npgvector: 0.2.5\npsycopg: 3.2.1\npsycopg-pool: 3.2.2\npydantic: 2.10.3\npydantic-settings: 2.5.2\nPyYAML: 6.0.2\nrequests: 2.32.3\nsentence-transformers: 3.1.1\nSQLAlchemy: 2.0.34\nsqlalchemy: 2.0.34\nsse-starlette: 2.1.3\ntabulate: 0.9.0\ntenacity: 9.0.0\ntiktoken: 0.7.0\ntokenizers: 0.19.1\ntransformers: 4.44.2\ntyping-extensions: 4.12.2\n", "created_at": "2025-01-03", "closed_at": "2025-01-07", "labels": [], "State": "closed", "Author": "PBCTM"}
{"issue_number": 2920, "issue_title": "LLM is slow within langgraph agent", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nclass State(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n\nclass Assistant:\n    def __init__(self, runnable: Runnable):\n        self.runnable = runnable\n\n    def __call__(self, state: State, config: RunnableConfig):\n        while True:\n            configuration = config.get(\"configurable\", {})\n            passenger_id = configuration.get(\"passenger_id\", None)\n            state = {**state, \"user_info\": passenger_id}\n            result = self.runnable.invoke(state)\n            # If the LLM happens to return an empty response, we will re-prompt it\n            # for an actual response.\n            if not result.tool_calls and (\n                not result.content\n                or isinstance(result.content, list)\n                and not result.content[0].get(\"text\")\n            ):\n                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n                state = {**state, \"messages\": messages}\n            else:\n                break\n        return {\"messages\": result}\n\nbuilder = StateGraph(State)\n\n\n# Define nodes: these do the work\nbuilder.add_node(\"assistant\", Assistant(part_1_assistant_runnable))\nbuilder.add_node(\"tools\", create_tool_node_with_fallback(part_1_tools))\n# Define edges: these determine how the control flow moves\nbuilder.add_edge(START, \"assistant\")\nbuilder.add_conditional_edges(\n    \"assistant\",\n    tools_condition,\n)\nbuilder.add_edge(\"tools\", \"assistant\")\n\n# The checkpointer lets the graph persist its state\n# this is a complete memory for the entire graph.\nmemory = MemorySaver()\npart_1_graph = builder.compile(checkpointer=memory)\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nI am using a very basic graph structure to call a tool, basically the code is the same as provided by LangGraph documentation (https://langchain-ai.github.io/langgraph/tutorials/customer-support/customer-support/#define-graph), but for model I am using Gemini for agent. The problem is when I use the tool separately, it generates response pretty quick (I am using Gemini inside the tool as well) but when I call the agent, it takes a lot of time. I actually noticed this with langChain runnable as well. Does anyone know why is this happening and how it v=can be resolved?\nSystem Info\nlangchain-core version: 0.2.40", "created_at": "2025-01-02", "closed_at": "2025-01-14", "labels": [], "State": "closed", "Author": "maryshgh"}
{"issue_number": 2909, "issue_title": "PydanticUserError: Please use `typing_extensions.TypedDict` instead of `typing.TypedDict` on Python < 3.12.", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.graph import MessagesState\nfrom prisma.models import Document\n\nclass CustomState(MessagesState):\n    document_store: dict[str, str]\nError Message and Stack Trace (if applicable)\ncopilot-api:dev: \u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\ncopilot-api:dev: \u2502 \u2502                       annotation = <class 'langgraph.prebuilt.tool_node.InjectedState'>      \u2502 \u2502\ncopilot-api:dev: \u2502 \u2502                 get_inner_schema = <pydantic._internal._schema_generation_shared.CallbackGe\u2026 \u2502 \u2502\ncopilot-api:dev: \u2502 \u2502                                    object at 0x1048746d0>                                    \u2502 \u2502\ncopilot-api:dev: \u2502 \u2502              metadata_get_schema = <function                                                 \u2502 \u2502\ncopilot-api:dev: \u2502 \u2502                                    GenerateSchema._get_wrapped_inner_schema.<locals>.<lambd\u2026 \u2502 \u2502\ncopilot-api:dev: \u2502 \u2502                                    at 0x1235401f0>                                           \u2502 \u2502\ncopilot-api:dev: \u2502 \u2502 pydantic_js_annotation_functions = []                                                        \u2502 \u2502\ncopilot-api:dev: \u2502 \u2502                             self = <pydantic._internal._generate_schema.GenerateSchema       \u2502 \u2502\ncopilot-api:dev: \u2502 \u2502                                    object at 0x1056acf40>                                    \u2502 \u2502\ncopilot-api:dev: \u2502 \u2502                           source = <class 'copilot_api.state.CustomState'>                   \u2502 \u2502\ncopilot-api:dev: \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\ncopilot-api:dev: \u2502                                                                                                  \u2502\ncopilot-api:dev: \u2502 /Users/mario/Library/Caches/pypoetry/virtualenvs/copilot-api-7C_KWYzt-py3.10/lib/python3.10/site \u2502\ncopilot-api:dev: \u2502 -packages/pydantic/_internal/_generate_schema.py:1968 in <lambda>                                \u2502\ncopilot-api:dev: \u2502                                                                                                  \u2502\ncopilot-api:dev: \u2502   1965 \u2502   \u2502   pydantic_js_annotation_functions: list[GetJsonSchemaFunction],                    \u2502\ncopilot-api:dev: \u2502   1966 \u2502   ) -> CallbackGetCoreSchemaHandler:                                                    \u2502\ncopilot-api:dev: \u2502   1967 \u2502   \u2502   metadata_get_schema: GetCoreSchemaFunction = getattr(annotation, '__get_pydantic  \u2502\ncopilot-api:dev: \u2502 \u2771 1968 \u2502   \u2502   \u2502   lambda source, handler: handler(source)                                       \u2502\ncopilot-api:dev: \u2502   1969 \u2502   \u2502   )                                                                                 \u2502\ncopilot-api:dev: \u2502   1970 \u2502   \u2502                                                                                     \u2502\ncopilot-api:dev: \u2502   1971 \u2502   \u2502   def new_handler(source: Any) -> core_schema.CoreSchema:                           \u2502\ncopilot-api:dev: \u2502                                                                                                  \u2502\ncopilot-api:dev: \u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\ncopilot-api:dev: \u2502 \u2502 handler = <pydantic._internal._schema_generation_shared.CallbackGetCoreSchemaHandler object  \u2502 \u2502\ncopilot-api:dev: \u2502 \u2502           at 0x1048746d0>                                                                    \u2502 \u2502\ncopilot-api:dev: \u2502 \u2502  source = <class 'copilot_api.state.CustomState'>                                            \u2502 \u2502\ncopilot-api:dev: \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\ncopilot-api:dev: \u2502                                                                                                  \u2502\ncopilot-api:dev: \u2502 /Users/mario/Library/Caches/pypoetry/virtualenvs/copilot-api-7C_KWYzt-py3.10/lib/python3.10/site \u2502\ncopilot-api:dev: \u2502 -packages/pydantic/_internal/_schema_generation_shared.py:83 in __call__                         \u2502\ncopilot-api:dev: \u2502                                                                                                  \u2502\ncopilot-api:dev: \u2502    80 \u2502   \u2502   self._ref_mode = ref_mode                                                          \u2502\ncopilot-api:dev: \u2502    81 \u2502                                                                                          \u2502\ncopilot-api:dev: \u2502    82 \u2502   def __call__(self, source_type: Any, /) -> core_schema.CoreSchema:                     \u2502\ncopilot-api:dev: \u2502 \u2771  83 \u2502   \u2502   schema = self._handler(source_type)                                                \u2502\ncopilot-api:dev: \u2502    84 \u2502   \u2502   ref = schema.get('ref')                                                            \u2502\ncopilot-api:dev: \u2502    85 \u2502   \u2502   if self._ref_mode == 'to-def':                                                     \u2502\ncopilot-api:dev: \u2502    86 \u2502   \u2502   \u2502   if ref is not None:                                                            \u2502\ncopilot-api:dev: \u2502                                                                                                  \u2502\ncopilot-api:dev: \u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\ncopilot-api:dev: \u2502 \u2502        self = <pydantic._internal._schema_generation_shared.CallbackGetCoreSchemaHandler     \u2502 \u2502\ncopilot-api:dev: \u2502 \u2502               object at 0x1048746d0>                                                         \u2502 \u2502\ncopilot-api:dev: \u2502 \u2502 source_type = <class 'copilot_api.state.CustomState'>                                        \u2502 \u2502\ncopilot-api:dev: \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\ncopilot-api:dev: \u2502                                                                                                  \u2502\ncopilot-api:dev: \u2502 /Users/mario/Library/Caches/pypoetry/virtualenvs/copilot-api-7C_KWYzt-py3.10/lib/python3.10/site \u2502\ncopilot-api:dev: \u2502 -packages/pydantic/_internal/_generate_schema.py:1871 in inner_handler                           \u2502\ncopilot-api:dev: \u2502                                                                                                  \u2502\ncopilot-api:dev: \u2502   1868 \u2502   \u2502   def inner_handler(obj: Any) -> CoreSchema:                                        \u2502\ncopilot-api:dev: \u2502   1869 \u2502   \u2502   \u2502   from_property = self._generate_schema_from_property(obj, source_type)         \u2502\ncopilot-api:dev: \u2502   1870 \u2502   \u2502   \u2502   if from_property is None:                                                     \u2502\ncopilot-api:dev: \u2502 \u2771 1871 \u2502   \u2502   \u2502   \u2502   schema = self._generate_schema_inner(obj)                                 \u2502\ncopilot-api:dev: \u2502   1872 \u2502   \u2502   \u2502   else:                                                                         \u2502\ncopilot-api:dev: \u2502   1873 \u2502   \u2502   \u2502   \u2502   schema = from_property                                                    \u2502\ncopilot-api:dev: \u2502   1874 \u2502   \u2502   \u2502   metadata_js_function = _extract_get_pydantic_json_schema(obj, schema)         \u2502\ncopilot-api:dev: \u2502                                                                                                  \u2502\ncopilot-api:dev: \u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\ncopilot-api:dev: \u2502 \u2502          from_property = None                                                                \u2502 \u2502\ncopilot-api:dev: \u2502 \u2502                    obj = <class 'copilot_api.state.CustomState'>                             \u2502 \u2502\ncopilot-api:dev: \u2502 \u2502                   self = <pydantic._internal._generate_schema.GenerateSchema object at       \u2502 \u2502\ncopilot-api:dev: \u2502 \u2502                          0x1056acf40>                                                        \u2502 \u2502\ncopilot-api:dev: \u2502 \u2502            source_type = <class 'copilot_api.state.CustomState'>                             \u2502 \u2502\ncopilot-api:dev: \u2502 \u2502 transform_inner_schema = <function GenerateSchema.<lambda> at 0x10191f5b0>                   \u2502 \u2502\ncopilot-api:dev: \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\ncopilot-api:dev: \u2502                                                                                                  \u2502\ncopilot-api:dev: \u2502 /Users/mario/Library/Caches/pypoetry/virtualenvs/copilot-api-7C_KWYzt-py3.10/lib/python3.10/site \u2502\ncopilot-api:dev: \u2502 -packages/pydantic/_internal/_generate_schema.py:789 in _generate_schema_inner                   \u2502\ncopilot-api:dev: \u2502                                                                                                  \u2502\ncopilot-api:dev: \u2502    786 \u2502   \u2502   if isinstance(obj, PydanticRecursiveRef):                                         \u2502\ncopilot-api:dev: \u2502    787 \u2502   \u2502   \u2502   return core_schema.definition_reference_schema(schema_ref=obj.type_ref)       \u2502\ncopilot-api:dev: \u2502    788 \u2502   \u2502                                                                                     \u2502\ncopilot-api:dev: \u2502 \u2771  789 \u2502   \u2502   return self.match_type(obj)                                                       \u2502\ncopilot-api:dev: \u2502    790 \u2502                                                                                         \u2502\ncopilot-api:dev: \u2502    791 \u2502   def match_type(self, obj: Any) -> core_schema.CoreSchema:  # noqa: C901               \u2502\ncopilot-api:dev: \u2502    792 \u2502   \u2502   \"\"\"Main mapping of types to schemas.                                              \u2502\ncopilot-api:dev: \u2502                                                                                                  \u2502\ncopilot-api:dev: \u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e       \u2502\ncopilot-api:dev: \u2502 \u2502 BaseModel = <class 'pydantic.main.BaseModel'>                                          \u2502       \u2502\ncopilot-api:dev: \u2502 \u2502       obj = <class 'copilot_api.state.CustomState'>                                    \u2502       \u2502\ncopilot-api:dev: \u2502 \u2502      self = <pydantic._internal._generate_schema.GenerateSchema object at 0x1056acf40> \u2502       \u2502\ncopilot-api:dev: \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f       \u2502\ncopilot-api:dev: \u2502                                                                                                  \u2502\ncopilot-api:dev: \u2502 /Users/mario/Library/Caches/pypoetry/virtualenvs/copilot-api-7C_KWYzt-py3.10/lib/python3.10/site \u2502\ncopilot-api:dev: \u2502 -packages/pydantic/_internal/_generate_schema.py:837 in match_type                               \u2502\ncopilot-api:dev: \u2502                                                                                                  \u2502\ncopilot-api:dev: \u2502    834 \u2502   \u2502   elif _typing_extra.is_literal_type(obj):                                          \u2502\ncopilot-api:dev: \u2502    835 \u2502   \u2502   \u2502   return self._literal_schema(obj)                                              \u2502\ncopilot-api:dev: \u2502    836 \u2502   \u2502   elif is_typeddict(obj):                                                           \u2502\ncopilot-api:dev: \u2502 \u2771  837 \u2502   \u2502   \u2502   return self._typed_dict_schema(obj, None)                                     \u2502\ncopilot-api:dev: \u2502    838 \u2502   \u2502   elif _typing_extra.is_namedtuple(obj):                                            \u2502\ncopilot-api:dev: \u2502    839 \u2502   \u2502   \u2502   return self._namedtuple_schema(obj, None)                                     \u2502\ncopilot-api:dev: \u2502    840 \u2502   \u2502   elif _typing_extra.is_new_type(obj):                                              \u2502\ncopilot-api:dev: \u2502                                                                                                  \u2502\ncopilot-api:dev: \u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e            \u2502\ncopilot-api:dev: \u2502 \u2502  obj = <class 'copilot_api.state.CustomState'>                                    \u2502            \u2502\ncopilot-api:dev: \u2502 \u2502 self = <pydantic._internal._generate_schema.GenerateSchema object at 0x1056acf40> \u2502            \u2502\ncopilot-api:dev: \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f            \u2502\ncopilot-api:dev: \u2502                                                                                                  \u2502\ncopilot-api:dev: \u2502 /Users/mario/Library/Caches/pypoetry/virtualenvs/copilot-api-7C_KWYzt-py3.10/lib/python3.10/site \u2502\ncopilot-api:dev: \u2502 -packages/pydantic/_internal/_generate_schema.py:1283 in _typed_dict_schema                      \u2502\ncopilot-api:dev: \u2502                                                                                                  \u2502\ncopilot-api:dev: \u2502   1280 \u2502   \u2502   \u2502   \u2502   typed_dict_cls = origin                                                   \u2502\ncopilot-api:dev: \u2502   1281 \u2502   \u2502   \u2502                                                                                 \u2502\ncopilot-api:dev: \u2502   1282 \u2502   \u2502   \u2502   if not _SUPPORTS_TYPEDDICT and type(typed_dict_cls).__module__ == 'typing':   \u2502\ncopilot-api:dev: \u2502 \u2771 1283 \u2502   \u2502   \u2502   \u2502   raise PydanticUserError(                                                  \u2502\ncopilot-api:dev: \u2502   1284 \u2502   \u2502   \u2502   \u2502   \u2502   'Please use `typing_extensions.TypedDict` instead of `typing.TypedDi  \u2502\ncopilot-api:dev: \u2502   1285 \u2502   \u2502   \u2502   \u2502   \u2502   code='typed-dict-version',                                            \u2502\ncopilot-api:dev: \u2502   1286 \u2502   \u2502   \u2502   \u2502   )                                                                         \u2502\ncopilot-api:dev: \u2502                                                                                                  \u2502\ncopilot-api:dev: \u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e  \u2502\ncopilot-api:dev: \u2502 \u2502      FieldInfo = <class 'pydantic.fields.FieldInfo'>                                        \u2502  \u2502\ncopilot-api:dev: \u2502 \u2502   maybe_schema = None                                                                       \u2502  \u2502\ncopilot-api:dev: \u2502 \u2502         origin = None                                                                       \u2502  \u2502\ncopilot-api:dev: \u2502 \u2502           self = <pydantic._internal._generate_schema.GenerateSchema object at 0x1056acf40> \u2502  \u2502\ncopilot-api:dev: \u2502 \u2502 typed_dict_cls = <class 'copilot_api.state.CustomState'>                                    \u2502  \u2502\ncopilot-api:dev: \u2502 \u2502 typed_dict_ref = 'copilot_api.state.CustomState:4394103488'                                 \u2502  \u2502\ncopilot-api:dev: \u2502 \u2502   typevars_map = None                                                                       \u2502  \u2502\ncopilot-api:dev: \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f  \u2502\ncopilot-api:dev: \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\ncopilot-api:dev: PydanticUserError: Please use `typing_extensions.TypedDict` instead of `typing.TypedDict` on Python < 3.12.\ncopilot-api:dev:\ncopilot-api:dev: For further information visit https://errors.pydantic.dev/2.8/u/typed-dict-version\ncopilot-api:dev: \u2009ELIFECYCLE\u2009 Command failed with exit code 1.\ncopilot-api:dev: \u2009WARN\u2009  Local package.json exists, but node_modules missing, did you mean to install?\ncopilot-api:dev: ERROR: command finished with error: command (/Users/mario/Projects/research-copilot/apps/copilot_api) /opt/homebrew/bin/pnpm run dev exited (1)\nDescription\nVerified happening on langgraph 0.2.53 ~ 0.2.60\nThe problem is in graph/message.py, line 2.\nThere might be more instances throughout the code though, let's try to catch them all.\nThere were similar issues in the past:\n#1856\n#2198\nMight be worthwhile to come up with a way to check that this remains compatible.\nI might be able to issue a PR for this myself over the coming days.\nSystem Info\nNA", "created_at": "2024-12-31", "closed_at": "2025-01-07", "labels": [], "State": "closed", "Author": "mariomeissner"}
{"issue_number": 2907, "issue_title": "Command.__init__() got an unexpected keyword argument 'goto'", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nif review_action == \"continue\":\n        return Command(goto=\"tools\")\n    \nelif review_action == \"update\":\n        ...\n        # updated_msg = get_updated_msg(review_data) # some way to update the message\n        updated_msg = {\n            \"role\": \"ai\",\n            \"content\": last_message.content,\n            \"tool_calls\": [\n                {\n                    \"id\": tool_call[\"id\"],\n                    \"name\": tool_call[\"name\"],\n                    # This the update provided by the human\n                    \"args\": review_data,\n                }\n            ],\n            # This is important - this needs to be the same as the message you replacing!\n            # Otherwise, it will show up as a separate message\n            \"id\": last_message.id,\n        }\n        # Remember that to modify an existing message you will need\n        # to pass the message with a matching ID.\n        return Command(goto=\"tools\", update={\"messages\": [updated_msg]})\n    \nelif review_action == \"feedback\":\n        ...\n        # feedback_msg = get_feedback_msg(review_data)  # some way to get feedback the message\n        tool_message = {\n            \"role\": \"tool\",\n            # This is our natural language feedback\n            \"content\": review_data,\n            \"name\": tool_call[\"name\"],\n            \"tool_call_id\": tool_call[\"id\"],\n        }\n        return Command(goto=\"model\", update={\"messages\": [tool_message]})\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nI'm using langgraph to build a agentic workflows, according to this documentation I expect to give 'goto' in Command, but its giving - TypeError: Command.init() got an unexpected keyword argument 'goto'\nI'm referring to this docs - https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/review-tool-calls/#simple-usage\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.1.0: Thu Oct 10 21:00:32 PDT 2024; root:xnu-11215.41.3~2/RELEASE_ARM64_T6030\nPython Version:  3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:54:21) [Clang 16.0.6 ]\n\nPackage Information\n\nlangchain_core: 0.3.19\nlangchain: 0.3.7\nlangchain_community: 0.3.7\nlangsmith: 0.1.143\nlangchain_chroma: 0.1.4\nlangchain_openai: 0.2.9\nlangchain_text_splitters: 0.3.2\nlangchainhub: 0.1.21\nlanggraph: 0.2.52\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.6\nasync-timeout: 5.0.1\nchromadb: 0.5.20\ndataclasses-json: 0.6.7\nfastapi: 0.115.5\nhttpx: 0.27.2\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlanggraph-checkpoint: 2.0.5\nlanggraph-sdk: 0.1.36\nnumpy: 1.26.4\nopenai: 1.54.5\norjson: 3.10.11\npackaging: 24.2\npydantic: 2.9.2\npydantic-settings: 2.6.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.35\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntypes-requests: 2.32.0.20241016\ntyping-extensions: 4.12.2\n", "created_at": "2024-12-31", "closed_at": "2025-01-07", "labels": [], "State": "closed", "Author": "ighoshsubho"}
{"issue_number": 2893, "issue_title": "Private state doesn't work with instance methods as nodes", "issue_body": "Privileged issue\n\n I am a LangChain maintainer, or was asked directly by a LangChain maintainer to create an issue here.\n\nIssue Content\nif you run below can see that node_2 doesn't receive the private_data key as input:\nfrom langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\n\n\n# The overall state of the graph (this is the public state shared across nodes)\nclass OverallState(TypedDict):\n    a: str\n\n\n# Output from node_1 contains private data that is not part of the overall state\nclass Node1Output(TypedDict):\n    private_data: str\n\n\n# Node 2 input only requests the private data available after node_1\nclass Node2Input(TypedDict):\n    private_data: str\n\n\nclass Nodes:\n    # The private data is only shared between node_1 and node_2\n    def node_1(self, state: OverallState) -> Node1Output:\n        output = {\"private_data\": \"set by node_1\"}\n        print(f\"Entered node `node_1`:\\n\\tInput: {state}.\\n\\tReturned: {output}\")\n        return output\n\n\n    def node_2(self, state: Node2Input) -> OverallState:\n        output = {\"a\": \"set by node_2\"}\n        print(f\"Entered node `node_2`:\\n\\tInput: {state}.\\n\\tReturned: {output}\")\n        return output\n\n\n    # Node 3 only has access to the overall state (no access to private data from node_1)\n    def node_3(self, state: OverallState) -> OverallState:\n        output = {\"a\": \"set by node_3\"}\n        print(f\"Entered node `node_3`:\\n\\tInput: {state}.\\n\\tReturned: {output}\")\n        return output\n\nnodes = Nodes()\n\n# Build the state graph\nbuilder = StateGraph(OverallState)\nbuilder.add_node(nodes.node_1)  # node_1 is the first node\nbuilder.add_node(nodes.node_2)  # node_2 is the second node and accepts private data from node_1\nbuilder.add_node(nodes.node_3)  # node_3 is the third node and does not see the private data\nbuilder.add_edge(START, \"node_1\")  # Start the graph with node_1\nbuilder.add_edge(\"node_1\", \"node_2\")  # Pass from node_1 to node_2\nbuilder.add_edge(\"node_2\", \"node_3\")  # Pass from node_2 to node_3 (only overall state is shared)\nbuilder.add_edge(\"node_3\", END)  # End the graph after node_3\ngraph = builder.compile()\ngraph.invoke({\"a\": \"\"})\nEntered node `node_1`:\n\tInput: {'a': ''}.\n\tReturned: {'private_data': 'set by node_1'}\nEntered node `node_2`:\n\tInput: {'a': ''}.\n\tReturned: {'a': 'set by node_2'}\nEntered node `node_3`:\n\tInput: {'a': 'set by node_2'}.\n\tReturned: {'a': 'set by node_3'}", "created_at": "2024-12-28", "closed_at": "2025-01-14", "labels": [], "State": "closed", "Author": "baskaryan"}
{"issue_number": 2890, "issue_title": "Tags added to the graph are not propagated in v0.2.x (worked in v0.1.x)", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.graph import StateGraph\n\nworkflow = StateGraph(AgentState)\n...\nworkflow.compile().with_config(tags=[\"mygraph\"])\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nIn v0.1.x I could add a tag to a graph with either of the following:\nfrom langgraph.graph import StateGraph\n\nworkflow = StateGraph(AgentState)\n...\nworkflow.compile().with_config({\"run_name\": \"my_run\", \"tags\": [\"mygraph\"]})\n\nOr with:\nworkflow = StateGraph(AgentState)\n...\nworkflow.compile().with_config(run_name=\"my_run\", tags=[\"mygraph\"])\n\nIn v0.1.x the added tag (mygraph) shows up in the metadata of the events:\n{\n  tags: [\n    0: \"seq:step:3\",\n    1: \"mygraph\"    <=== TAG IS HERE\n  ]\n  langgraph_step: \"1\"\n  langgraph_node: \"agent\"\n  langgraph_triggers: [\n    0: \"start:agent\"\n  ]\n}\n\nIn v0.2.x the added tag is not in the metadata:\n{\n  tags: [\n    0: \"seq:step:3\"   <=== NO 'mygraph' TAG!!!\n  ]\n  langgraph_step: \"1\"\n  langgraph_node: \"agent\"\n  langgraph_triggers: [\n    0: \"start:agent\"\n  ]\n}\n\nIs it possible that the new tags added to the v0.2.x codebase like TAG_HIDDEN overwrite the user-supplied tags somewhere instead of appending to it and so the user-supplied tags gets lost?\nSee TAG_HIDDEN:\n\n\n\nlanggraph/libs/langgraph/langgraph/constants.py\n\n\n         Line 19\n      in\n      effddca\n\n\n\n\n\n\n TAG_HIDDEN = sys.intern(\"langsmith:hidden\") \n\n\n\n\n\nPs,:\n\nThe run_name can be successfully propagated from the with_config() method, only the tags are not propagated in v0.2.x\nThe issue was the same with the latest version of Langchain and also with older version like langchain==0.2.9, once langgraph got upgraded to v0.2.x the tags stopped propagate both with langchain==0.2.9 and langchain==0.2.17\n\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #115~20.04.1-Ubuntu SMP Mon Apr 15 17:33:04 UTC 2024\nPython Version:  3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 9.4.0]\n\nPackage Information\n\nlangchain_core: 0.2.43\nlangchain: 0.2.17\nlangchain_community: 0.2.19\nlangsmith: 0.1.147\nlangchain_cli: 0.0.35\nlangchain_openai: 0.1.25\nlangchain_text_splitters: 0.2.4\nlanggraph: 0.2.60\nlangserve: 0.2.2\n\nOther Dependencies\n\naiohttp: 3.11.11\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.7\nfastapi: 0.115.6\ngitpython: 3.1.43\ngritql: 0.1.5\nhttpx: 0.27.0\njsonpatch: 1.33\nlanggraph-checkpoint: 2.0.9\nlanggraph-sdk: 0.1.48\nlangserve[all]: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.58.1\norjson: 3.10.12\npackaging: 24.2\npydantic: 2.10.4\npyproject-toml: 0.0.10\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.36\nsse-starlette: 1.8.2\ntenacity: 8.5.0\ntiktoken: 0.8.0\ntomlkit: 0.13.2\ntyper[all]: Installed. No version info available.\ntyping-extensions: 4.12.2\nuvicorn: 0.34.0\n", "created_at": "2024-12-28", "closed_at": "2025-01-15", "labels": ["investigate"], "State": "closed", "Author": "zoltan-fedor"}
{"issue_number": 2887, "issue_title": "ActiveSqlTransaction: CREATE INDEX CONCURRENTLY cannot run inside a transaction block", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom psycopg import Connection\nfrom langgraph.store.postgres import PostgresStore\n\nconn_str = f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWD}@{IP_ADDRESS}:{PORT}/{MEMSTORE_DB}\"\n\nconn = Connection.connect(conn_str)\n\npg_store = PostgresStore(conn=conn)\n\npg_store.setup()\n\n# Store and retrieve data\npg_store.put((\"users\", \"123\"), \"prefs\", {\"theme\": \"dark\"})\nitem = pg_store.get((\"users\", \"123\"), \"prefs\")\nError Message and Stack Trace (if applicable)\nActiveSqlTransaction                      Traceback (most recent call last)\nCell In[1], line 17\n     13 from langgraph.store.postgres import PostgresStore\n     15 pg_store = PostgresStore(conn=conn)\n---> 17 pg_store.setup()\n     19 # Store and retrieve data\n     20 pg_store.put((\"users\", \"123\"), \"prefs\", {\"theme\": \"dark\"})\n\nFile ~/Downloads/Code/venv/lib/python3.11/site-packages/langgraph/store/postgres/base.py:867, in PostgresStore.setup(self)\n    865 version = _get_version(cur, table=\"store_migrations\")\n    866 for v, sql in enumerate(self.MIGRATIONS[version + 1 :], start=version + 1):\n--> 867     cur.execute(sql)\n    868     cur.execute(\"INSERT INTO store_migrations (v) VALUES (%s)\", (v,))\n    870 if self.index_config:\n\nFile ~/Downloads/Code/venv/lib/python3.11/site-packages/psycopg/cursor.py:97, in Cursor.execute(self, query, params, prepare, binary)\n     93         self._conn.wait(\n     94             self._execute_gen(query, params, prepare=prepare, binary=binary)\n     95         )\n     96 except e._NO_TRACEBACK as ex:\n---> 97     raise ex.with_traceback(None)\n     98 return self\n\nActiveSqlTransaction: CREATE INDEX CONCURRENTLY cannot run inside a transaction block\nDescription\nI am trying to connect to postgresql as a  MemoryStore using langgraph store postgres\nThe connection is established using psycopg - PostgresStore is not taking in the connection string\nHowever, when I am trying to run the .setup() it is showing the error\nActiveSqlTransaction                      Traceback (most recent call last)\nCell In[1], line 17\n13 from langgraph.store.postgres import PostgresStore\n15 pg_store = PostgresStore(conn=conn)\n---> 17 pg_store.setup()\n19 # Store and retrieve data\n20 pg_store.put((\"users\", \"123\"), \"prefs\", {\"theme\": \"dark\"})\nFile ~/Downloads/Code/venv/lib/python3.11/site-packages/langgraph/store/postgres/base.py:867, in PostgresStore.setup(self)\n865 version = _get_version(cur, table=\"store_migrations\")\n866 for v, sql in enumerate(self.MIGRATIONS[version + 1 :], start=version + 1):\n--> 867     cur.execute(sql)\n868     cur.execute(\"INSERT INTO store_migrations (v) VALUES (%s)\", (v,))\n870 if self.index_config:\nFile ~/Downloads/Code/venv/lib/python3.11/site-packages/psycopg/cursor.py:97, in Cursor.execute(self, query, params, prepare, binary)\n93         self._conn.wait(\n94             self._execute_gen(query, params, prepare=prepare, binary=binary)\n95         )\n96 except e._NO_TRACEBACK as ex:\n---> 97     raise ex.with_traceback(None)\n98 return self\nActiveSqlTransaction: CREATE INDEX CONCURRENTLY cannot run inside a transaction block\nPython 3.11\nlanggraph==0.2.60\nlanggraph-checkpoint==2.0.9\nlanggraph-checkpoint-postgres==2.0.9\nlanggraph-sdk==0.1.48\npsycopg==3.2.3\npsycopg-c==3.2.3\nlangchain==0.3.13\nlangchain-chroma==0.1.4\nlangchain-community==0.3.7\nlangchain-core==0.3.28\nlangchain-experimental==0.3.3\nlangchain-google-genai==2.0.5\nlangchain-google-vertexai==2.0.7\nlangchain-openai==0.2.9\nlangchain-text-splitters==0.3.4\nlangchainhub==0.1.15\nSystem Info\nOutput of \"python -m langchain_core.sys_info\"\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 22.6.0: Wed Jul  5 22:22:05 PDT 2023; root:xnu-8796.141.3~6/RELEASE_ARM64_T6000\nPython Version:  3.11.5 (v3.11.5:cce6ba91b3, Aug 24 2023, 10:50:31) [Clang 13.0.0 (clang-1300.0.29.30)]\n\nPackage Information\n\nlangchain_core: 0.3.28\nlangchain: 0.3.13\nlangchain_community: 0.3.7\nlangsmith: 0.1.144\nlangchain_chroma: 0.1.4\nlangchain_experimental: 0.3.3\nlangchain_google_genai: 2.0.5\nlangchain_google_vertexai: 2.0.7\nlangchain_openai: 0.2.9\nlangchain_text_splitters: 0.3.4\nlangchainhub: 0.1.15\nlanggraph_sdk: 0.1.48\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.10.5\nanthropic[vertexai]: Installed. No version info available.\nasync-timeout: 4.0.3\nchromadb: 0.5.20\ndataclasses-json: 0.5.14\nfastapi: 0.115.5\ngoogle-cloud-aiplatform: 1.71.1\ngoogle-cloud-storage: 2.18.2\ngoogle-generativeai: 0.8.3\nhttpx: 0.27.0\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlangchain-mistralai: Installed. No version info available.\nnumpy: 1.25.2\nopenai: 1.55.0\norjson: 3.10.3\npackaging: 24.1\npydantic: 2.9.0\npydantic-settings: 2.6.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 1.4.53\ntenacity: 8.5.0\ntiktoken: 0.7.0\ntypes-requests: 2.31.0.20240406\ntyping-extensions: 4.12.2\n", "created_at": "2024-12-28", "closed_at": "2025-01-10", "labels": [], "State": "closed", "Author": "santoshgsk"}
{"issue_number": 2875, "issue_title": "create_react_agent with store", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\ndef _prep_messages(state, config, store):\n            # For store usage\n            _prompt = None\n            if store is not None:\n                namespace = (config['configurable'][configurable_by],)\n                result = store.get(namespace, \"info\")\n                if \"data\" in result:\n                    _prompt = result[\"data\"]\n\n            if _prompt is None:\n                _prompt = system_prompt\n            _system_message = SystemMessage(content=_prompt)\n            return [_system_message] + state[\"messages\"]\nError Message and Stack Trace (if applicable)\nasyncio.exceptions.InvalidStateError: Synchronous calls to BatchedStore detected in the main event loop. This can lead to deadlocks or performance issues. Please use the asynchronous interface for main thread operations. Specifically, replace `store.get(...)` with `await store.aget(...)\nDescription\nI'm trying to use store with create_react_agent. But it wont let me pass in a function that uses store in sync manner. and if i change to async function, create_react_agent doesn't accept it\nSystem Info\n(multi-agent-testing) \u279c  multi-agent-testing git:(main) \u2717 python -m\nlangchain_core.sys_info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 22.3.0: Thu Jan  5 20:50:36 PST 2023; root:xnu-8792.81.2~2/RELEASE_ARM64_T6020\nPython Version:  3.11.7 (main, Nov 16 2024, 15:15:25) [Clang 14.0.0 (clang-1400.0.29.202)]\n\nPackage Information\n\nlangchain_core: 0.3.28\nlangchain: 0.3.8\nlangchain_community: 0.3.8\nlangsmith: 0.1.146\nlangchain_anthropic: 0.3.0\nlangchain_openai: 0.2.9\nlangchain_text_splitters: 0.3.2\nlanggraph_api: 0.0.14\nlanggraph_cli: 0.1.65\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.48\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.7\nanthropic: 0.39.0\nasync-timeout: Installed. No version info available.\nclick: 8.1.8\ncryptography: 43.0.3\ndataclasses-json: 0.6.7\ndefusedxml: 0.7.1\nhttpx: 0.27.2\nhttpx-sse: 0.4.0\njsonpatch: 1.33\njsonschema-rs: 0.25.1\nlanggraph: 0.2.60\nlanggraph-checkpoint: 2.0.9\nnumpy: 1.26.4\nopenai: 1.55.1\norjson: 3.10.12\npackaging: 24.2\npydantic: 2.10.2\npydantic-settings: 2.6.1\npyjwt: 2.10.1\npython-dotenv: 1.0.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.35\nsse-starlette: 2.1.3\nstarlette: 0.42.0\nstructlog: 24.4.0\ntenacity: 8.5.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\nuvicorn: 0.34.0\nwatchfiles: 1.0.3\n", "created_at": "2024-12-25", "closed_at": "2025-01-23", "labels": ["bug", "maintainer"], "State": "closed", "Author": "hwchase17"}
{"issue_number": 2873, "issue_title": "Handling \"RuntimeError: can't start a new thread\" error at production.", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n@celery_app.task(bind=True)\ndef content_gen_celery_task(self, task_id: str) -> str:\n   loop = asyncio.get_event_loop()\n   asyncio.set_event_loop(loop)\n  loop.run_until_complete(generate(task_data=task_data, temerature=0.7))\n\nasync def generate(task_data:TaskData, temerature:float=0.7):\n    content_maker_agent = ContentMakerAgent(\n                input_data=input_data,\n                llm_model=task_data.llm_model,\n                temperature=temerature\n            )\n    generated_content = await content_maker_agent.graph.ainvoke(input_data )\n\nclass ContentMakerAgent():\n    def __init__(self, input_data:ContentState, llm_model:str, temperature=0.7):\n        builder = StateGraph(ContentState)\n        self.writer_model = ChatOpenAI(model=llm_model, temperature=temperature)\n        self.briefs = input_data[\"briefs\"]\n\n        for i, brief in enumerate(self.briefs): \n            node_name = f\"node_{i}\"\n            function_declaration = GenContent(state=input_data,\n                                              brief=brief, \n                                              llm_model=llm_model, \n                                              temperature=temperature, )\n            \n            builder.add_node(node_name, function_declaration)\n            builder.add_edge(START, node_name)\n\n            builder.add_edge(node_name, END)\n        self.graph = builder.compile()\nError Message and Stack Trace (if applicable)\n[2024-11-28 05:20:12,030: ERROR/ForkPoolWorker-26] can't start new thread\nTraceback (most recent call last):\nFile \"/app/core/ai/agent.py\", line 196, in r_brief_generator\nresponse = await self.standart_writer_model.with_structured_output(Briefs).ainvoke(messages)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/opt/venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 2921, in ainvoke\ninput = await asyncio.create_task(part(), context=context)  # type: ignore\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/opt/venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 5105, in ainvoke\nreturn await self.bound.ainvoke(\n^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/opt/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 298, in ainvoke\nllm_result = await self.agenerate_prompt(\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/opt/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 787, in agenerate_prompt\nreturn await self.agenerate(\n^^^^^^^^^^^^^^^^^^^^^\nFile \"/opt/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 747, in agenerate\nraise exceptions[0]\nFile \"/opt/venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 923, in _agenerate_with_cache\nresult = await self._agenerate(\n^^^^^^^^^^^^^^^^^^^^^^\nFile \"/opt/venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 843, in _agenerate\nresponse = await self.async_client.create(**payload)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/opt/venv/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1490, in create\nreturn await self._post(\n^^^^^^^^^^^^^^^^^\nFile \"/opt/venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1838, in post\nreturn await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/opt/venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1532, in request\nreturn await self._request(\n^^^^^^^^^^^^^^^^^^^^\nFile \"/opt/venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1552, in _request\nself._platform = await asyncify(get_platform)()\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/opt/venv/lib/python3.11/site-packages/openai/_utils/_sync.py\", line 69, in wrapper\nreturn await anyio.to_thread.run_sync(\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/opt/venv/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\nreturn await get_async_backend().run_sync_in_worker_thread(\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/opt/venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2114, in run_sync_in_worker_thread\nworker.start()\nFile \"/root/.nix-profile/lib/python3.11/threading.py\", line 964, in start\n_start_new_thread(self._bootstrap, ())\nRuntimeError: can't start new thread\nDescription\nI have deployed a langgraph agent at production. The agent runs inside a Celery task. 10-20 nodes of the agent are executed parallelly. At peak usage, I get \"can't start a new thread\" error.\nMy agent is stateless, no thread ID is provided while invoking the agent. I am new to langgraph and multi threading.\nShould I upgrade my CPU and memory? How can I improve my code so that more threads won't be a problem in the future?\nSystem Info\nCPU: 32 vCPU\nMemory: 32 GB", "created_at": "2024-12-25", "closed_at": null, "labels": [], "State": "open", "Author": "alimardanov"}
{"issue_number": 2872, "issue_title": "I am not getting output in langgraph, but tool is running. ", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\ndef route_memory(state):\n    \"\"\"\n    Route question to wipe memory or fetch memory.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n    print(\"---ROUTE QUESTION---\")\n    if state[\"question\"] in ['exit', 'leave']:\n        print(\"---ROUTE QUESTION TO WIPE MEMORY---\")\n        state[\"memory\"] = \"wipememory\"\n        return \"wipememory\"\n    else:\n        print(\"---ROUTE QUESTION TO FETCH MEMORY---\")\n        state[\"memory\"] = \"fetchmemory\"\n        return \"fetchmemory\"\n    \n\ndef clear_memory(state):\n    \"\"\"\n    Clear the memory.\n\n    Args:\n        state (dict): The current graph state\n\n    Returns:\n        str: Next node to call\n    \"\"\"\n    memory = ConversationBufferMemory(\n            memory_key=\"chat_history\",\n            return_messages=True,\n            output_key='answer'\n        )\n    return {'memory': memory}\n\ndef fetch_memory(state):\n\n    # fetch memory logic form DB\n\n    return {\"memory\": memory}\n\n@tool\ndef retrieve_generate_update(config: RunnableConfig, \n                    state: Annotated[dict, InjectedState]) -> str:\n    \"\"\"Helps answer standard related query and update the memory.\n    Args:\n        state (str): User's query.\n\n    Returns:\n        str: A simple string it returns.\n    \"\"\"\n    metadata = metadata_retrieve(state)\n    indices = metadata[\"indices\"] \n    documents = metadata['documents'] \n    reformed_question = metadata[\"reformed_question\"]\n\n    generated_response = generate(state, indices, reformed_question)\n    print(generated_response[\"llm_answer\"])\n    print(generated_response[\"llm_reference\"])\n\n    return {\n        \"answer\": generated_response[\"llm_answer\"],\n        \"reference\": generated_response[\"llm_reference\"]\n    }    \n\n\n\n@tool\ndef CustomCaseTool(string: str, case: Literal[\"upper\", \"lower\"], config: RunnableConfig, \n                    state: Annotated[dict, InjectedState],\n                   ) -> str:\n    \"\"\"Converts the case of the string to upper case.\"\"\"\n    print(string)\n    print(config[\"metadata\"])\n    print(state[\"question\"], state[\"user_session\"])\n    print(case)\n    if case == \"upper\":\n        return string.upper()\n    elif case == \"lower\":\n        return string.lower()\n\nclass GraphState(TypedDict):\n    \"\"\"\n    Graph state is a dictionary that contains information we want to propagate to, and modify in, each graph node.\n    \"\"\"\n    question : str # User question\n    user_session: str # User session\n    messages: Annotated[list[AnyMessage], add_messages]\n    memory: Any\n\nclass Assistant:\n    def __init__(self, runnable: Runnable):\n        self.runnable = runnable\n\n    def __call__(self, state: GraphState, config: RunnableConfig):\n        while True:\n            result = self.runnable.invoke(state)\n            # If the LLM happens to return an empty response, we will re-prompt it\n            # for an actual response.\n            if not result.tool_calls and (\n                not result.content\n                or isinstance(result.content, list)\n                and not result.content[0].get(\"text\")\n            ):\n                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n                state = {**state, \"messages\": messages}\n            else:\n                break\n        return {\"messages\": result}\n    \ndef handle_tool_error(state) -> dict:\n    error = state.get(\"error\")\n    tool_calls = state[\"messages\"][-1].tool_calls\n    return {\n        \"messages\": [\n            ToolMessage(\n                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n                tool_call_id=tc[\"id\"],\n            )\n            for tc in tool_calls\n        ]\n    }\n\n\ndef create_tool_node_with_fallback(tools: list) -> dict:\n    return ToolNode(tools).with_fallbacks(\n        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n    )\n\nassistant_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\n            \"system\",\n            \"You are a helpful assistant.\"\n            \" Use the provided tools to convert the case, retrieve relevant data and other information to assist the user's queries. \"\n            \" When searching, be persistent. Expand your query bounds if the first search returns no results. \"\n            \" If a search comes up empty, expand your search before giving up.\",\n        ),\n        (\"placeholder\", \"{messages}\"),\n    ]\n)\n\nstandards_tools = [\n    CustomCaseTool,\n    retrieve_generate_update\n]\nstandards_assistant_runnable = assistant_prompt | llm.bind_tools(standards_tools)\n\nworkflow = StateGraph(GraphState)\n\nworkflow.add_node(\"clear_memory\", clear_memory) #memory clear\nworkflow.add_node(\"fetch_memory\", fetch_memory)\nworkflow.add_node(\"assistant\", Assistant(standards_assistant_runnable))\nworkflow.add_node(\"tools\", create_tool_node_with_fallback(standards_tools))\n\nworkflow.set_conditional_entry_point(\n    route_memory,\n    {\n        \"wipememory\": \"clear_memory\",\n        \"fetchmemory\": \"fetch_memory\",\n    },\n)\n\nworkflow.add_edge(\"fetch_memory\", \"assistant\")\nworkflow.add_edge(\"clear_memory\", \"assistant\")\n\n\nworkflow.add_conditional_edges(\n    \"assistant\",\n    tools_condition,\n)\nworkflow.add_edge(\"tools\", \"assistant\")\n\n\ngraph = workflow.compile()\n\n\n_printed = set()\n\ndef _print_event(event: dict, _printed: set, max_length=1500):\n    current_state = event.get(\"dialog_state\")\n    if current_state:\n        print(\"Currently in: \", current_state[-1])\n    message = event.get(\"messages\")\n    if message:\n        if isinstance(message, list):\n            message = message[-1]\n        if message.id not in _printed:\n            msg_repr = message.pretty_repr(html=True)\n            if len(msg_repr) > max_length:\n                msg_repr = msg_repr[:max_length] + \" ... (truncated)\"\n            _printed.add(message.id)\n\n\nconfig = {\n    \"configurable\": {\n        # Checkpoints are accessed by thread_id\n        \"thread_id\": '12121',\n    }\n}\n\ninputs = {\n    \"messages\": \"Convert the following text 'Hello! There How are you ALEX?' to lower case.\",\n    \"user_session\": 8787,\n    \"question\": {\"hello\": 'sfmslfmslfs', \"type\": 12121}\n}\nfor question in tutorial_questions:\n    events = graph.stream(\n        inputs, config, stream_mode=\"values\"\n    )\n    for event in events:\n        _print_event(event, _printed)\n\nThis is the response I am getting: \n\n---ROUTE QUESTION---\n---ROUTE QUESTION TO FETCH MEMORY---\n======================\n3 ===================================\nHello! There How are you ALEX?\n{'thread_id': '12121', 'langgraph_step': 3, 'langgraph_node': 'tools', 'langgraph_triggers': ['branch:assistant:tools_condition:tools'], 'langgraph_path': ('__pregel_pull', 'tools'), 'langgraph_checkpoint_ns': 'tools:cd243836-d245-cc50-ced3-6121f6a810b9', 'checkpoint_ns': 'tools:cd243836-d245-cc50-ced3-6121f6a810b9'}\n{'hello': 'sfmslfmslfs', 'type': 12121} 8787\nlower\nError Message and Stack Trace (if applicable)\nNo Error, just that response is not coming.\nDescription\nI am creating a bot, which first fetches the memory (without tools) post that it enters in tool mode and runs the tool as required. But I am not getting the response printed. Can Anyone please help!\n@eric-langchain @nfcampos @Glavin001\nSystem Info\n\nPlease Help!", "created_at": "2024-12-25", "closed_at": "2025-01-14", "labels": ["invalid"], "State": "closed", "Author": "jaytimbadia"}
{"issue_number": 2870, "issue_title": "Resume value get reused when resuming parent graph which has subgraph with multiple interrupts", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n# graphs\nfrom typing import TypedDict\nfrom langgraph.graph import StateGraph, START, END\nfrom app.ai.agent.base import BaseAgent\nfrom langgraph.types import Checkpointer, interrupt\nfrom langgraph.checkpoint.memory import MemorySaver\n\nclass AgentState(TypedDict):\n    input: str\n\n\ndef node_1(state: AgentState):\n    result = interrupt(\"interrupt node 1\")\n    print(\"result\", result)\n\ndef node_2(state: AgentState):\n    result = interrupt(\"interrupt node 2\")\n    print(\"result\", result)\n\nsub_graph = StateGraph(AgentState).add_node(\"node_1\", node_1).add_node(\"node_2\", node_2).add_edge(START, \"node_1\").add_edge(\"node_1\", \"node_2\").add_edge(\"node_2\", END).compile()\n\n\n\ndef invoke_sub_agent(state: AgentState):\n    sub_graph.invoke(state)\n\n\nparent_agent = StateGraph(AgentState).add_node(\"invoke_sub_agent\", invoke_sub_agent).add_edge(START, \"invoke_sub_agent\").add_edge(\"invoke_sub_agent\", END).compile(checkpointer=MemorySaver())\n\n# invoking the parent graph\nfrom app.ai.agent.test import parent_agent\nimport uuid\n\nthread_id = uuid.uuid4()\nparent_agent.invoke({\"input\": \"test\"}, config={\"configurable\": {\"thread_id\": thread_id}}, subgraphs=True)\n\n# then the graph get interrupted by the first interrupt as expected\n\n# resume from the first interrupt\nparent_agent.invoke(Command(resume=True), config={\"configurable\": {\"thread_id\": thread_id}}, subgraphs=True)\n\n# then not just first interrupt get passed, the second interrupt also get passed with same resume value printed out.\n\nThe code is supposed to be interrupted twice and wait for human input for twice\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nAs described in the Example code section. I have\nparent_graph -> subgraph (two interrupts)\nwhen invoke parent_graph, the parent_graph stops at the first subgraph interrupt as expected. Then I resume, the expected behavior is the graph should pass the first interrupt and stop at the second interrupt, however, both interrupt passed. The second interrupt returns the value I passed for resuming the first interrupt instead of raising a new interrupt.\nSystem Info\nlanggraph==0.2.60", "created_at": "2024-12-24", "closed_at": "2025-01-15", "labels": ["investigate"], "State": "closed", "Author": "chinazhangyujia"}
{"issue_number": 2866, "issue_title": "The difference between two scene processing strategies at the merging node of LangGraph parallel branch.", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport operator\nfrom langgraph.graph import StateGraph\nfrom typing import Annotated\nfrom typing import TypedDict\n\n\nclass AgentState(TypedDict):\n    input: str\n    history: Annotated[list[dict], operator.add]\n\n\ndef getapp():\n    def start(state):\n        print(\"run: START\")\n        return {\"history\": [{}]}\n\n    def end(state):\n        print(\"run: END\")\n        # \u8fd4\u56de\u4ee3\u7406\u7684\u7ed3\u679c\n        return {\"history\": [{}]}\n\n    def run_agent(name):\n        def run(state):\n            print(\"run: \" + name)\n            return {\"history\": [{}]}\n\n        return run\n\n    def conditional(state):\n        # \u8fd4\u56de\u4ee3\u7406\u7684\u7ed3\u679c\n        return [\"agent2\", \"agent3\"]\n\n    workflow = StateGraph(AgentState)\n    # scenario_one(conditional, end, run_agent, start, workflow)\n    scenario_two(conditional, end, run_agent, start, workflow)\n    app = workflow.compile()\n    return app\n\n\ndef scenario_one(conditional, end, run_agent, start, workflow):\n    \"\"\"\n    Scenario 1 graph:\n    start -> agent1 -> agent2 -> agen4 -> end\n                    -> agent3\n    Branch one is agent2.\n    Branch tow is agent2.\n    The number of nodes in branch one is equal to the number of nodes in branch two.\n    \"\"\"\n    workflow.add_node(\"start\", start)\n    workflow.add_node(\"end\", end)\n    workflow.add_node(\"agent1\", run_agent(\"agen1\"))\n    workflow.add_node(\"agent2\", run_agent(\"agen2\"))\n    workflow.add_node(\"agent3\", run_agent(\"agen3\"))\n    workflow.add_node(\"agent4\", run_agent(\"agen4\"))\n    workflow.add_edge(\"start\", \"agent1\")\n    workflow.add_conditional_edges(\"agent1\", conditional, {\"agent2\": \"agent2\", \"agent3\": \"agent3\"})\n    workflow.add_edge(\"agent2\", \"agent4\")\n    workflow.add_edge(\"agent3\", \"agent4\")\n    workflow.add_edge(\"agent3\", \"agent4\")\n    workflow.add_edge(\"agent4\", \"end\")\n    workflow.set_entry_point(\"start\")\n    workflow.set_finish_point(\"end\")\n\n\ndef scenario_two(conditional, end, run_agent, start, workflow):\n    \"\"\"\n    Scenario 1 graph\n    start -> agent1 -> agent2                 -> agen4 -> end\n                    -> agent3 -> agent3.5\n    Branch one is agent2.\n    Branch tow is agent3 and agent3.5.\n    The number of nodes in branch two is greater than that in branch one.\n    \"\"\"\n    workflow.add_node(\"start\", start)\n    workflow.add_node(\"end\", end)\n    workflow.add_node(\"agent1\", run_agent(\"agen1\"))\n    workflow.add_node(\"agent2\", run_agent(\"agen2\"))\n    workflow.add_node(\"agent3\", run_agent(\"agen3\"))\n    workflow.add_node(\"agent3.5\", run_agent(\"agen3.5\"))\n    workflow.add_node(\"agent4\", run_agent(\"agen4\"))\n    workflow.add_edge(\"start\", \"agent1\")\n    workflow.add_conditional_edges(\"agent1\", conditional, {\"agent2\": \"agent2\", \"agent3\": \"agent3\"})\n    workflow.add_edge(\"agent2\", \"agent4\")\n    workflow.add_edge(\"agent3\", \"agent4\")\n    workflow.add_edge(\"agent3\", \"agent3.5\")\n    workflow.add_edge(\"agent3.5\", \"agent4\")\n    workflow.add_edge(\"agent4\", \"end\")\n    workflow.set_entry_point(\"start\")\n    workflow.set_finish_point(\"end\")\n\n\nif __name__ == '__main__':\n    getapp().invoke(input={\"input\": \"\"}, config={\"recursion_limit\": 10})\nError Message and Stack Trace (if applicable)\nScenario 1\uff1a\n\nrun: START\nrun:agen1\nrun:agen2\nrun:agen3\nrun:agen4\nrun: END\n\nScenario 2\uff1a\nrun: START\nrun:agen1\nrun:agen2\nrun:agen3\nrun:agen3.5\nrun:agen4\nrun: END\nrun:agen4\nrun: END\nDescription\nIn the scenario of parallel branching and merging, when the number of nodes in multiple branches is the same, the merging node only runs once, which is in line with the Wait for All Strategy. However, when the number of nodes in the branches is not the same, the merging node and subsequent processes will run multiple times, which is inconsistent with the above strategy and barely belongs to the Parallel Merge Strategy.\nSystem Info\nLangGraph Version Description\uff1a\n", "created_at": "2024-12-24", "closed_at": "2025-01-07", "labels": [], "State": "closed", "Author": "huangyuanyuan000"}
{"issue_number": 3570, "issue_title": "Can't go to a node in the parent graph using Command.PARENT", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport random\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph\nfrom langgraph.graph import END, START\nfrom langgraph.types import Command\nfrom typing import Literal, Annotated, TypedDict\nfrom langgraph.graph.message import AnyMessage\nfrom langgraph.graph.message import add_messages\n\n\n\n\nclass State(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n    personal_data: dict\n    dialog_state: str\n\n\n\ndef node_a_child(state):\n    return {\"dialog_state\": \"b_child_state\"}\n    # return {\"registration_status\": True, \"dialog_state\": \"b_child_state\"}\n\ndef node_b_child(state) -> Command[Literal[\"node_c_child\", \"node_d_child\"]]:\n    value = random.choice([0, 1])\n    return Command(\n        goto=\"node_c_child\" if value == 0 else \"node_d_child\"\n    )\n\ndef node_c_child(state):\n    return Command(\n        graph=Command.PARENT,\n        goto=\"node_sibling\",\n    )\n\ndef node_d_child(state):\n    return {\"dialog_state\": \"d_child_state\"}\n\n\n\n\nsub_builder = StateGraph(State)\n\nsub_builder.add_node(\"node_a_child\", node_a_child)\nsub_builder.add_edge(START, \"node_a_child\")\n\nsub_builder.add_node(\"node_b_child\", node_b_child) \nsub_builder.add_edge(\"node_a_child\", \"node_b_child\")\n\nsub_builder.add_node(\"node_c_child\", node_c_child)\nsub_builder.add_edge(\"node_c_child\", END)\n\n\nsub_builder.add_node(\"node_d_child\", node_d_child) \nsub_builder.add_edge(\"node_d_child\", END)\n\nsub_graph = sub_builder.compile(checkpointer=True)\n\n\n\n\ndef node_a_parent(state):\n    return {\"dialog_state\": \"a_parent_state\"}\n\ndef node_b_parent(state):\n    return {\"dialog_state\": \"pop\"}\n\ndef node_sibling(state):\n    return {\"dialog_state\": \"sibling\"}\n\nmain_builder = StateGraph(State)\n\nmain_builder.add_node(\"node_a_parent\", node_a_parent)\nmain_builder.add_edge(START, \"node_a_parent\")\n\nmain_builder.add_node(\"subgraph\", sub_graph)\nmain_builder.add_edge(\"node_a_parent\", \"subgraph\")\n\nmain_builder.add_node(\"node_b_parent\", node_b_parent)\nmain_builder.add_edge(\"subgraph\", \"node_b_parent\")\n\n\nmain_builder.add_edge(\"node_b_parent\", END)\n\nmain_builder.add_node(\"node_sibling\", node_sibling)\nmain_builder.add_edge(\"node_sibling\", END)\n\n\ncheckpointer_temp = MemorySaver()\n\nmain_graph = main_builder.compile(checkpointer_temp, name=\"parent\")\n\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": 1,\n    }\n}\n\n\nresult = main_graph.invoke(input={\"dialog_state\": [\"init_state\"]}, config=config, subgraphs=True, debug=True)\nError Message and Stack Trace (if applicable)\n{\n\t\"name\": \"ParentCommand\",\n\t\"message\": \"Command(graph='subgraph', update=[('messages', []), ('dialog_state', 'a_parent_state'), ('messages', []), ('dialog_state', 'b_child_state')], goto='node_sibling')\",\n\t\"stack\": \"---------------------------------------------------------------------------\nParentCommand                             Traceback (most recent call last)\nCell In[46], line 8\n      1 config = {\n      2     \\\"configurable\\\": {\n      3         \\\"thread_id\\\": 1,\n      4     }\n      5 }\n----> 8 result = main_graph.invoke(input={\\\"dialog_state\\\": [\\\"init_state\\\"]}, config=config, subgraphs=True, debug=True)\n      9 result\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2124, in Pregel.invoke(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\n   2122 else:\n   2123     chunks = []\n-> 2124 for chunk in self.stream(\n   2125     input,\n   2126     config,\n   2127     stream_mode=stream_mode,\n   2128     output_keys=output_keys,\n   2129     interrupt_before=interrupt_before,\n   2130     interrupt_after=interrupt_after,\n   2131     debug=debug,\n   2132     **kwargs,\n   2133 ):\n   2134     if stream_mode == \\\"values\\\":\n   2135         latest = chunk\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1779, in Pregel.stream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\n   1773     # Similarly to Bulk Synchronous Parallel / Pregel model\n   1774     # computation proceeds in steps, while there are channel updates.\n   1775     # Channel updates from step N are only visible in step N+1\n   1776     # channels are guaranteed to be immutable for the duration of the step,\n   1777     # with channel updates applied only at the transition between steps.\n   1778     while loop.tick(input_keys=self.input_channels):\n-> 1779         for _ in runner.tick(\n   1780             loop.tasks.values(),\n   1781             timeout=self.step_timeout,\n   1782             retry_policy=self.retry_policy,\n   1783             get_waiter=get_waiter,\n   1784         ):\n   1785             # emit output\n   1786             yield from output()\n   1787 # emit output\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/runner.py:302, in PregelRunner.tick(self, tasks, reraise, timeout, retry_policy, get_waiter)\n    300 yield\n    301 # panic on failure or timeout\n--> 302 _panic_or_proceed(\n    303     futures.done.union(f for f, t in futures.items() if t is not None),\n    304     panic=reraise,\n    305 )\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/runner.py:619, in _panic_or_proceed(futs, timeout_exc_cls, panic)\n    617         # raise the exception\n    618         if panic:\n--> 619             raise exc\n    620 if inflight:\n    621     # if we got here means we timed out\n    622     while inflight:\n    623         # cancel all pending tasks\n\nFile /opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:340, in Future._invoke_callbacks(self)\n    338 for callback in self._done_callbacks:\n    339     try:\n--> 340         callback(self)\n    341     except Exception:\n    342         LOGGER.exception('exception calling callback for %r', self)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/runner.py:88, in FuturesDict.on_done(self, task, fut)\n     82 def on_done(\n     83     self,\n     84     task: PregelExecutableTask,\n     85     fut: F,\n     86 ) -> None:\n     87     try:\n---> 88         self.callback(task, _exception(fut))\n     89     finally:\n     90         with self.lock:\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/runner.py:551, in PregelRunner.commit(self, task, exception)\n    549         self.put_writes(task.id, interrupts)\n    550 elif isinstance(exception, GraphBubbleUp):\n--> 551     raise exception\n    552 else:\n    553     # save error to checkpointer\n    554     self.put_writes(task.id, [(ERROR, exception)])\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/executor.py:83, in BackgroundExecutor.done(self, task)\n     81 \\\"\\\"\\\"Remove the task from the tasks dict when it's done.\\\"\\\"\\\"\n     82 try:\n---> 83     task.result()\n     84 except GraphBubbleUp:\n     85     # This exception is an interruption signal, not an error\n     86     # so we don't want to re-raise it on exit\n     87     self.tasks.pop(task)\n\nFile /opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:449, in Future.result(self, timeout)\n    447     raise CancelledError()\n    448 elif self._state == FINISHED:\n--> 449     return self.__get_result()\n    451 self._condition.wait(timeout)\n    453 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\nFile /opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401, in Future.__get_result(self)\n    399 if self._exception:\n    400     try:\n--> 401         raise self._exception\n    402     finally:\n    403         # Break a reference cycle with the exception in self._exception\n    404         self = None\n\nFile /opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/thread.py:58, in _WorkItem.run(self)\n     55     return\n     57 try:\n---> 58     result = self.fn(*self.args, **self.kwargs)\n     59 except BaseException as exc:\n     60     self.future.set_exception(exc)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/retry.py:40, in run_with_retry(task, retry_policy, configurable)\n     38     task.writes.clear()\n     39     # run the task\n---> 40     return task.proc.invoke(task.input, config)\n     41 except ParentCommand as exc:\n     42     ns: str = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/utils/runnable.py:546, in RunnableSeq.invoke(self, input, config, **kwargs)\n    542 config = patch_config(\n    543     config, callbacks=run_manager.get_child(f\\\"seq:step:{i + 1}\\\")\n    544 )\n    545 if i == 0:\n--> 546     input = step.invoke(input, config, **kwargs)\n    547 else:\n    548     input = step.invoke(input, config)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2124, in Pregel.invoke(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\n   2122 else:\n   2123     chunks = []\n-> 2124 for chunk in self.stream(\n   2125     input,\n   2126     config,\n   2127     stream_mode=stream_mode,\n   2128     output_keys=output_keys,\n   2129     interrupt_before=interrupt_before,\n   2130     interrupt_after=interrupt_after,\n   2131     debug=debug,\n   2132     **kwargs,\n   2133 ):\n   2134     if stream_mode == \\\"values\\\":\n   2135         latest = chunk\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1779, in Pregel.stream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\n   1773     # Similarly to Bulk Synchronous Parallel / Pregel model\n   1774     # computation proceeds in steps, while there are channel updates.\n   1775     # Channel updates from step N are only visible in step N+1\n   1776     # channels are guaranteed to be immutable for the duration of the step,\n   1777     # with channel updates applied only at the transition between steps.\n   1778     while loop.tick(input_keys=self.input_channels):\n-> 1779         for _ in runner.tick(\n   1780             loop.tasks.values(),\n   1781             timeout=self.step_timeout,\n   1782             retry_policy=self.retry_policy,\n   1783             get_waiter=get_waiter,\n   1784         ):\n   1785             # emit output\n   1786             yield from output()\n   1787 # emit output\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/runner.py:240, in PregelRunner.tick(self, tasks, reraise, timeout, retry_policy, get_waiter)\n    238     self.commit(t, None)\n    239 except Exception as exc:\n--> 240     self.commit(t, exc)\n    241     if reraise and futures:\n    242         # will be re-raised after futures are done\n    243         fut: concurrent.futures.Future = concurrent.futures.Future()\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/runner.py:551, in PregelRunner.commit(self, task, exception)\n    549         self.put_writes(task.id, interrupts)\n    550 elif isinstance(exception, GraphBubbleUp):\n--> 551     raise exception\n    552 else:\n    553     # save error to checkpointer\n    554     self.put_writes(task.id, [(ERROR, exception)])\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/runner.py:230, in PregelRunner.tick(self, tasks, reraise, timeout, retry_policy, get_waiter)\n    228 t = tasks[0]\n    229 try:\n--> 230     run_with_retry(\n    231         t,\n    232         retry_policy,\n    233         configurable={\n    234             CONFIG_KEY_SEND: partial(writer, t),\n    235             CONFIG_KEY_CALL: partial(call, t),\n    236         },\n    237     )\n    238     self.commit(t, None)\n    239 except Exception as exc:\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/retry.py:40, in run_with_retry(task, retry_policy, configurable)\n     38     task.writes.clear()\n     39     # run the task\n---> 40     return task.proc.invoke(task.input, config)\n     41 except ParentCommand as exc:\n     42     ns: str = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/utils/runnable.py:548, in RunnableSeq.invoke(self, input, config, **kwargs)\n    546             input = step.invoke(input, config, **kwargs)\n    547         else:\n--> 548             input = step.invoke(input, config)\n    549 # finish the root run\n    550 except BaseException as e:\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/utils/runnable.py:310, in RunnableCallable.invoke(self, input, config, **kwargs)\n    308 else:\n    309     context.run(_set_config_context, config)\n--> 310     ret = context.run(self.func, *args, **kwargs)\n    311 if isinstance(ret, Runnable) and self.recurse:\n    312     return ret.invoke(input, config)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/graph/graph.py:94, in Branch._route(self, input, config, reader, writer)\n     92 else:\n     93     value = input\n---> 94 result = self.path.invoke(value, config)\n     95 return self._finish(writer, input, result, config)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/utils/runnable.py:310, in RunnableCallable.invoke(self, input, config, **kwargs)\n    308 else:\n    309     context.run(_set_config_context, config)\n--> 310     ret = context.run(self.func, *args, **kwargs)\n    311 if isinstance(ret, Runnable) and self.recurse:\n    312     return ret.invoke(input, config)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/graph/state.py:895, in _control_branch(value)\n    893 for command in commands:\n    894     if command.graph == Command.PARENT:\n--> 895         raise ParentCommand(command)\n    896     if isinstance(command.goto, Send):\n    897         rtn.append(command.goto)\n\nParentCommand: Command(graph='subgraph', update=[('messages', []), ('dialog_state', 'a_parent_state'), ('messages', []), ('dialog_state', 'b_child_state')], goto='node_sibling')\"\n}\nDescription\nI am trying to implement a handoff where a subgraph transfers the user to another node in the parent graph (a sibling node in this case) using a Command, but I keep encountering an error. There is no difference whether the graph and subgraph share the same state or only a single key, whether a reducer function is applied or not\u2014the transition simply doesn\u2019t occur. To me, this seems somewhat illogical. I have experimented with various target nodes (e.g., node_a_parent), but the result remains the same\u2014a ParentCommand exception.\u201d\n\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:16 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6000\nPython Version:  3.12.7 (main, Oct  1 2024, 02:05:46) [Clang 15.0.0 (clang-1500.3.9.4)]\n\nPackage Information\n\nlangchain_core: 0.3.36\nlangchain: 0.3.19\nlangsmith: 0.2.11\nlangchain_openai: 0.3.6\nlangchain_text_splitters: 0.3.6\nlanggraph_sdk: 0.1.51\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nhttpx: 0.28.1\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.35: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<2,>=1.26.4;: Installed. No version info available.\nnumpy<3,>=1.26.2;: Installed. No version info available.\nopenai<2.0.0,>=1.58.1: Installed. No version info available.\norjson: 3.10.15\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.5\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: Installed. No version info available.\n", "created_at": "2025-02-24", "closed_at": null, "labels": ["investigate"], "State": "open", "Author": "iamolvr"}
{"issue_number": 3569, "issue_title": "Error when running `langgraph dev` after upgrade 0.1.71 -> 0.1.73", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n`langgraph dev`\nError Message and Stack Trace (if applicable)\n(.venv) ~\\PycharmProjects\\Lighthouse.AGENTIC git:[summarisation-poc]\nlanggraph dev\nINFO:langgraph_api.cli:\n\n        Welcome to\n\n\u2566  \u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u2554\u2550\u2557\u252c\u2500\u2510\u250c\u2500\u2510\u250c\u2500\u2510\u252c \u252c\n\u2551  \u251c\u2500\u2524\u2502\u2502\u2502\u2502 \u252c\u2551 \u2566\u251c\u252c\u2518\u251c\u2500\u2524\u251c\u2500\u2518\u251c\u2500\u2524\n\u2569\u2550\u255d\u2534 \u2534\u2518\u2514\u2518\u2514\u2500\u2518\u255a\u2550\u255d\u2534\u2514\u2500\u2534 \u2534\u2534  \u2534 \u2534\n\n- \ud83d\ude80 API: http://127.0.0.1:2024\n- \ud83c\udfa8 Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n- \ud83d\udcda API Docs: http://127.0.0.1:2024/docs\n\nThis in-memory server is designed for development and testing.\nFor production use, please use LangGraph Cloud.\n\n\nException in thread Thread-2 (_open_browser):\nTraceback (most recent call last):\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n    self.run()\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\threading.py\", line 982, in run\n    self._target(*self._args, **self._kwargs)\n  File \"C:\\Users\\[USERNAME]\\PycharmProjects\\Lighthouse.AGENTIC\\.venv\\Lib\\site-packages\\langgraph_api\\cli.py\", line 218, in _open_browser\n    with urllib.request.urlopen(f\"{local_url}/ok\") as response:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\urllib\\request.py\", line 216, in urlopen\n    return opener.open(url, data, timeout)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\urllib\\request.py\", line 519, in open\n    response = self._open(req, data)\n               ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\urllib\\request.py\", line 536, in _open\n    result = self._call_chain(self.handle_open, protocol, protocol +\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\urllib\\request.py\", line 496, in _call_chain\n    result = func(*args)\n             ^^^^^^^^^^^\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\urllib\\request.py\", line 1377, in http_open\n    return self.do_open(http.client.HTTPConnection, req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\urllib\\request.py\", line 1352, in do_open\n    r = h.getresponse()\n        ^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py\", line 1395, in getresponse\n    response.begin()\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py\", line 325, in begin\n    version, status, reason = self._read_status()\n                              ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py\", line 286, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\socket.py\", line 706, in readinto\n    return self._sock.recv_into(b)\n           ^^^^^^^^^^^^^^^^^^^^^^^\nConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\nDescription\nA working langgraph implementation when run on 0.1.71, now returns the following error on the langgraph dev command after upgrading to 0.1.73\nSystem Info\n(.venv) ~\\PycharmProjects\\Lighthouse.AGENTIC git:[summarisation-poc]\npython -m langchain_core.sys_info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.26100\nPython Version:  3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.3.34\nlangchain: 0.3.18\nlangsmith: 0.3.8\nlangchain_openai: 0.3.5\nlangchain_text_splitters: 0.3.6\nlanggraph_api: 0.0.23\nlanggraph_cli: 0.1.73\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.51\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nclick: 8.1.8\ncryptography: 43.0.3\nhttpx: 0.28.1\njsonpatch<2.0,>=1.33: Installed. No version info available.\njsonschema-rs: 0.25.1\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlanggraph: 0.2.74\nlanggraph-checkpoint: 2.0.12\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<2,>=1.26.4;: Installed. No version info available.\nnumpy<3,>=1.26.2;: Installed. No version info available.\nopenai<2.0.0,>=1.58.1: Installed. No version info available.\norjson: 3.10.15\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npyjwt: 2.10.1\npytest: Installed. No version info available.\npython-dotenv: 1.0.1\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: Installed. No version info available.\nSQLAlchemy<3,>=1.4: Installed. No version info available.\nsse-starlette: 2.1.3\nstarlette: 0.45.3\nstructlog: 24.4.0\ntenacity: 9.0.0\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nuvicorn: 0.34.0\nwatchfiles: 1.0.4\nzstandard: 0.23.0\n", "created_at": "2025-02-24", "closed_at": "2025-02-25", "labels": [], "State": "closed", "Author": "simon-lighthouse"}
{"issue_number": 3564, "issue_title": "State not injected in tool", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.graph import StateGraph, START, END, MessagesState\nfrom langchain.tools.base import StructuredTool\nfrom langgraph.prebuilt import ToolNode\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.types import StreamWriter\n\nclass CustomAgentState(MessagesState):\n    folder_path: str\n\nclass CustomAgent:\n    def __init__(self, path: str):\n       self._folder_path = path\n       \n       workflow = StateGraph(CustomAgentState)\n\n       self._tools = [\n            StructuredTool.from_function(\n                coroutine=self._request_files_tool,\n                name=\"request_files_tool\",\n                description=\"The tool for requesting content of one or multiple files by their path.\"\n            )\n        ]\n\n        self._llm = ChatAnthropic(model=..., \n                                  api_key=..., \n                                  temperature=0, \n                                  streaming=True).bind_tools(self._tools)\n\n        \n        workflow.add_node(\"invoke_llm\", self._invoke_llm_node)\n        workflow.add_node(\"tools\", ToolNode(self._tools))\n        \n        workflow.add_edge(START, \"invoke_llm\")\n        workflow.add_conditional_edges(\"invoke_llm\", self._intelligent_routing, [\"tools\", END])\n        workflow.add_edge(\"tools\", \"invoke_llm\")\n\n        memory = MemorySaver()\n        self._chain = workflow.compile(checkpointer=memory)\n\n    async def invoke(self, prompt: str, context: str = None) -> AsyncIterator[str]:\n        ...\n        chain_input = {\n            \"messages\": [system_prompt, user_prompt],\n            \"folder_path\": self._folder_path\n        } \n\n       async for output_chunk in self._chain.astream(input=chain_input, config=config, stream_mode=\"custom\"):\n             yield output_chunk\n      \n       ...\n\n    async def _intelligent_routing(self, state: CustomAgentState) -> str:\n       ... state injected correctly\n\n    async def _invoke_llm_node(self, state: CustomAgentState, writer: StreamWriter):\n       ... state injected correctly\n   \n    # PROBLEMATIC TOOL\n    async def _request_files_tool(self, file_paths: list[str], state: CustomAgentState) -> List[Tuple[str, str]]:\n       ... state is empty\n       # tried using Annotated[dict, CustomAgentState] as well but tool calling stops working\n\n# Invoking\nagent = CustomAgent(\"/some/custom/path\")\n\nasync for output_chunk in agent.invoke(prompt=msg.text, context=context):\n  ...\nError Message and Stack Trace (if applicable)\n\nDescription\nI'm trying to pass the Graph's state to an async tool, as described in the docs, but doesn't work and I get an empty state always even if it's correctly injected in other nodes which are not defined as a tool.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:16 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6000\nPython Version:  3.13.2 (v3.13.2:4f8bb3947cf, Feb  4 2025, 11:51:10) [Clang 15.0.0 (clang-1500.3.9.4)]\n\nPackage Information\n\nlangchain_core: 0.3.37\nlangchain: 0.3.19\nlangsmith: 0.3.8\nlangchain_anthropic: 0.3.7\nlangchain_text_splitters: 0.3.6\nlanggraph_sdk: 0.1.51\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nanthropic<1,>=0.45.0: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nhttpx: 0.28.1\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.35: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<2,>=1.26.4;: Installed. No version info available.\nnumpy<3,>=1.26.2;: Installed. No version info available.\norjson: 3.10.15\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: Installed. No version info available.\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: Installed. No version info available.\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-02-24", "closed_at": "2025-02-24", "labels": [], "State": "closed", "Author": "georgeberar"}
{"issue_number": 3557, "issue_title": "langgraph-checkpoint-postgres issue with version update", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\npip install -U langgraph-checkpoint-postgres\n\nDB_URI =\"\" #Use an existing postgres checkpointer DB\n\nfrom psycopg import Connection\nfrom psycopg_pool import ConnectionPool\nfrom psycopg.rows import dict_row\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\nconnection_kwargs ={\n    \"autocommit\": True,\n    \"prepare_threshold\": 0,\n    \"row_factory\": dict_row,\n}\n\npool = ConnectionPool(\n    conninfo=DB_URI,\n    max_size=20,\n    kwargs=connection_kwargs\n)\n\ncheckpointer = PostgresSaver(pool)\n\nthread_id = \"1\"\nconfig = {\"configurable\": {\"thread_id\": thread_id}}\ncheckpoint =  checkpointer.get(config)\nError Message and Stack Trace (if applicable)\n/usr/local/lib/python3.11/dist-packages/langgraph/checkpoint/base/__init__.py in get(self, config)\n    234             Optional[Checkpoint]: The requested checkpoint, or None if not found.\n    235         \"\"\"\n--> 236         if value := self.get_tuple(config):\n    237             return value.checkpoint\n    238 \n\n/usr/local/lib/python3.11/dist-packages/langgraph/checkpoint/postgres/__init__.py in get_tuple(self, config)\n    218 \n    219         with self._cursor() as cur:\n--> 220             cur.execute(\n    221                 self.SELECT_SQL + where,\n    222                 args,\n\n/usr/local/lib/python3.11/dist-packages/psycopg/cursor.py in execute(self, query, params, prepare, binary)\n     95                 )\n     96         except e._NO_TRACEBACK as ex:\n---> 97             raise ex.with_traceback(None)\n     98         return self\n     99 \n\nUndefinedColumn: column cw.task_path does not exist\nLINE 27: ...array_agg(array[cw.type::bytea, cw.blob] order by cw.task_pa...\nDescription\nFaced the below issue with langgraph-checkpoint-postgres:\nUndefinedColumn: column cw.task_path does not exist\nLINE 27: ...array_agg(array[cw.type::bytea, cw.blob] order by cw.task_pa...\n\nTried moving to an older version of langgraph-checkpoint-postgres  (version - 2.0.8) and it is working fine.\nI.e. we are able to connect to the DB with langgraph-checkpoint-postgres == 2.0.8\nThis is an issue with a recent upgrade to the library.\nWe have code running in production and will become a major problem for us.\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\nPython Version:  3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n\nPackage Information\n\nlangchain_core: 0.3.37\nlangchain: 0.3.19\nlangchain_community: 0.3.18\nlangsmith: 0.3.8\nlangchain_cli: 0.0.35\nlangchain_openai: 0.3.6\nlangchain_text_splitters: 0.3.6\nlanggraph_sdk: 0.1.53\nlangserve: 0.3.1\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nfastapi: 0.115.8\ngitpython: 3.1.44\ngritql: 0.1.5\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.35: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.37: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.19: Installed. No version info available.\nlangserve[all]: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<2,>=1.26.4;: Installed. No version info available.\nnumpy<3,>=1.26.2;: Installed. No version info available.\nopenai<2.0.0,>=1.58.1: Installed. No version info available.\norjson: 3.10.15\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: 8.3.4\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: 13.9.4\nSQLAlchemy<3,>=1.4: Installed. No version info available.\nsse-starlette: 1.8.2\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntomlkit: 0.13.2\ntyper[all]: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nuvicorn: 0.34.0\nzstandard: 0.23.0\n", "created_at": "2025-02-23", "closed_at": "2025-03-11", "labels": [], "State": "closed", "Author": "saurabhlalsaxena"}
{"issue_number": 3556, "issue_title": "Tavily Search Error with async/await in LangGraph", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport os\nimport asyncio\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.messages import HumanMessage, SystemMessage, AnyMessage\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\nfrom langgraph.prebuilt import ToolNode, tools_condition\nfrom langchain_community.tools import SearchAPIResults\nfrom langchain_community.tools import TavilySearchResults\nfrom langchain_openai import ChatOpenAI\n\nos.environ['TAVILY_API_KEY'] = 'tvly-xxx'\n\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\ntool = TavilySearchResults(max_results=1, include_answer=False, include_raw_content=False)\n#tool = SearchAPIResults(engine=\"google\")\ntools = [tool]\n\n# \u7cfb\u7edf\u63d0\u793a\u8bcd\nsystem_prompt = \"\"\"You are a helpful assistant.\"\"\"\n#system_prompt = \"\u4f60\u662f\u5c0f\u660e\u3002\u5982\u679c\u6709\u7528\u6237\u95ee\u4f60\u662f\u8c01\u5c31\u56de\u7b54\uff1a\u5c0f\u660e\"\nllm_with_tools = llm.bind_tools(tools)\n\nasync def chatbot(state: State):\n    response = await llm_with_tools.ainvoke(state[\"messages\"])\n    return {\"messages\": [response]}\n\ngraph_builder = StateGraph(State)\ngraph_builder.add_node(\"chatbot\", chatbot)\ntool_node = ToolNode(tools=[tool])\ngraph_builder.add_node(\"tools\", tool_node)\ngraph_builder.add_conditional_edges(\n    \"chatbot\",\n    tools_condition,\n)\ngraph_builder.add_edge(\"tools\", \"chatbot\")\ngraph_builder.add_edge(START, \"chatbot\")\n\nsearchbot_graph = graph_builder.compile()\n\nuser_input = \"what is langgraph\"\n\n# \u521b\u5efa\u521d\u59cb\u6d88\u606f\u5217\u8868,\u5305\u542b\u7cfb\u7edf\u63d0\u793a\u548c\u7528\u6237\u8f93\u5165\ninitial_messages = [\n    SystemMessage(content=system_prompt),\n    HumanMessage(content=user_input)\n]\n\nasync def main():\n    # \u4f7f\u7528\u5305\u542b\u7cfb\u7edf\u63d0\u793a\u7684\u6d88\u606f\u5217\u8868\n    async for chunk in searchbot_graph.astream(\n        input={\"messages\": initial_messages},\n        stream_mode=\"messages\"\n    ):\n        print(chunk)\n        print(\"\\n\\n\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nError Message and Stack Trace (if applicable)\n(ToolMessage(content=\"ClientConnectorCertificateError(ConnectionKey(host='api.tavily.com', port=443, is_ssl=True, ssl=True, proxy=None, proxy_auth=None, proxy_headers_hash=None), SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)'))\", name='tavily_search_results_json', id='e76f58ec-4299-4adb-9488-c6e03804fe59', tool_call_id='call_Mqle439YOLwiLT3i9TzwQhy8', artifact={}), {'langgraph_step': 2, 'langgraph_node': 'tools', 'langgraph_triggers': ['branch:chatbot:tools_condition:tools'], 'langgraph_path': ('__pregel_pull', 'tools'), 'langgraph_checkpoint_ns': 'tools:2743d809-8b93-3c97-bdb2-11d04607977d'})\nDescription\nWhen using LangGraph with async/await pattern and Tavily Search tool, the code fails with an SSL certificate verification error. However, the same code works fine when not using async implementation.\nSystem Info\nLanggraph Version\uff1a0.2.69\nlangchain-community\uff1a0.3.17", "created_at": "2025-02-23", "closed_at": "2025-02-23", "labels": [], "State": "closed", "Author": "kissycn"}
{"issue_number": 3543, "issue_title": "graph restart bug", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\ndef _init_graph(self)->CompiledStateGraph:\n        graph_builder = StateGraph(MessagesState)\n        tool_node = ToolNode(tools=self.tools)\n        graph_builder.add_node(\"reply\", _reply)\n        graph_builder.add_node(\"route\", _route)\n        graph_builder.add_node(\"complete\", _complete)\n        graph_builder.add_node(\"approve\", _approve)\n        graph_builder.add_node(\"tools\", tool_node)\n\n        graph_builder.add_edge(START, \"reply\")\n        graph_builder.add_edge(\"reply\", \"route\")\n        graph_builder.add_edge(\"complete\", \"reply\")\n        graph_builder.add_edge(\"tools\", \"reply\")\n        graph = graph_builder.compile(checkpointer=self.checkpointer,interrupt_before=[\"complete\",\"approve\"])\n        print(f\"==========PreBookConsultant Graph==========\")\n        graph.get_graph().print_ascii()\n        return graph\n\n\n    def _route(self,state: MessagesState) -> Command[Literal[\"complete\", \"approve\", \"tools\", \"__end__\"]]:\n            ......\n            if valid_messages:\n                return Command(\n                    update={\"messages\": [AIMessage(content=\"\\n\".join(valid_messages),name=self.__class__.__name__)]},\n                    goto=\"complete\"\n                )\n            elif inquiry_messages:\n                self.cache_message = last_message\n                return Command(\n                    update={\"messages\": [AIMessage(content=\"\\n\".join(inquiry_messages),name=self.__class__.__name__)]},\n                    goto=\"approve\"\n                )\n            else:\n                return Command(\n                    goto=\"tools\"\n                )\n\n\n    def _complete(self,state: MessagesState) -> MessagesState:\n        return state\n        else:\n            return Command(\n                goto=END\n            )\nError Message and Stack Trace (if applicable)\n\nDescription\nI designed a graph,and set interrupt_before=[\"complete\"],when \"_route\" node finished\uff0cthe graph is interrupted and the \"snapshot.next\" is \"complete\" indeed,then I use \"graph.invoke(input=None,config=config)\" to restart the graph,but the graph not enter into the \"complete\" node, it finished directly\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.26100\nPython Version:  3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.3.31\nlangchain: 0.3.15\nlangchain_community: 0.3.15\nlangsmith: 0.2.11\nlangchain_experimental: 0.3.4\nlangchain_milvus: 0.1.8\nlangchain_openai: 0.3.1\nlangchain_text_splitters: 0.3.5\nlanggraph_sdk: 0.1.53\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.11\nasync-timeout: 4.0.3\ndataclasses-json: 0.6.7\nhttpx: 0.28.1\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.59.8\norjson: 3.10.15\npackaging: 24.2\npydantic: 2.10.5\npydantic-settings: 2.7.1\npymilvus: 2.5.4\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.37\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\nzstandard: Installed. No version info available.\n", "created_at": "2025-02-21", "closed_at": "2025-03-05", "labels": ["invalid"], "State": "closed", "Author": "andyzhou1982"}
{"issue_number": 3538, "issue_title": "ToolNode not working. TypeError: Tool search returned unexpected type: <class 'str'>", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing import Literal\n\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.tools import tool\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import END, START, StateGraph, MessagesState\nfrom langgraph.prebuilt import ToolNode\n\n\n# Define the tools for the agent to use\n@tool\ndef search(query: str):\n    \"\"\"Call to surf the web.\"\"\"\n    # This is a placeholder, but don't tell the LLM that...\n    if \"sf\" in query.lower() or \"san francisco\" in query.lower():\n        return \"It's 60 degrees and foggy.\"\n    return \"It's 90 degrees and sunny.\"\n\n\ntools = [search]\n\ntool_node = ToolNode(tools)\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-latest\", temperature=0).bind_tools(tools)\n\n# Define the function that determines whether to continue or not\ndef should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n    messages = state['messages']\n    last_message = messages[-1]\n    # If the LLM makes a tool call, then we route to the \"tools\" node\n    if last_message.tool_calls:\n        return \"tools\"\n    # Otherwise, we stop (reply to the user)\n    return END\n\n\n# Define the function that calls the model\ndef call_model(state: MessagesState):\n    messages = state['messages']\n    response = model.invoke(messages)\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": [response]}\n\n\n# Define a new graph\nworkflow = StateGraph(MessagesState)\n\n# Define the two nodes we will cycle between\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"tools\", tool_node)\n\n# Set the entrypoint as `agent`\n# This means that this node is the first one called\nworkflow.add_edge(START, \"agent\")\n\n# We now add a conditional edge\nworkflow.add_conditional_edges(\n    # First, we define the start node. We use `agent`.\n    # This means these are the edges taken after the `agent` node is called.\n    \"agent\",\n    # Next, we pass in the function that will determine which node is called next.\n    should_continue,\n)\n\n# We now add a normal edge from `tools` to `agent`.\n# This means that after `tools` is called, `agent` node is called next.\nworkflow.add_edge(\"tools\", 'agent')\n\n# Initialize memory to persist state between graph runs\ncheckpointer = MemorySaver()\n\n# Finally, we compile it!\n# This compiles it into a LangChain Runnable,\n# meaning you can use it as you would any other runnable.\n# Note that we're (optionally) passing the memory when compiling the graph\napp = workflow.compile(checkpointer=checkpointer)\n\n# Use the agent\nfinal_state = app.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n    config={\"configurable\": {\"thread_id\": 42}}\n)\nfinal_state[\"messages\"][-1].content\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\n  File \"main.py\", line 81, in <module>\n    final_state = app.invoke(\n                  ^^^^^^^^^^^\n  File \"langgraph\\pregel\\__init__.py\", line 2142, in invoke\n    for chunk in self.stream(\n  File \"langgraph\\pregel\\__init__.py\", line 1797, in stream\n    for _ in runner.tick(\n  File \"langgraph\\pregel\\runner.py\", line 230, in tick\n    run_with_retry(\n  File \"langgraph\\pregel\\retry.py\", line 40, in run_with_retry\n    return task.proc.invoke(task.input, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"langgraph\\utils\\runnable.py\", line 546, in invoke\n    input = step.invoke(input, config, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"langgraph\\utils\\runnable.py\", line 310, in invoke\n    ret = context.run(self.func, *args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"langgraph\\prebuilt\\tool_node.py\", line 238, in _func\n    outputs = [\n              ^\n  File \"D:\\FullStack\\python\\conda\\envs\\py311psa\\Lib\\concurrent\\futures\\_base.py\", line 619, in result_iterator\n    yield _result_or_cancel(fs.pop())\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"D:\\FullStack\\python\\conda\\envs\\py311psa\\Lib\\concurrent\\futures\\_base.py\", line 317, in _result_or_cancel\n    return fut.result(timeout)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\FullStack\\python\\conda\\envs\\py311psa\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"D:\\FullStack\\python\\conda\\envs\\py311psa\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"D:\\FullStack\\python\\conda\\envs\\py311psa\\Lib\\concurrent\\futures\\thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"langchain_core\\runnables\\config.py\", line 527, in _wrapped_fn\n    return contexts.pop().run(fn, *args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"langgraph\\prebuilt\\tool_node.py\", line 347, in _run_one\n    raise TypeError(\nTypeError: Tool search returned unexpected type: <class 'str'>\nDuring task with name 'tools' and id '680e4f05-4aca-9c1d-d026-ac815f3b08c9'\nDescription\nI was running the Low-level implementation from the README.md file on https://github.com/langchain-ai/langgraph\nI just copy paste the example code from README but change the llm to a local llm, it's sure that the llm is working fine.\nThe ToolNode should be working, instead, it throws a TypeError: Tool search returned unexpected type: <class 'str'>\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.19045\nPython Version:  3.11.11 | packaged by conda-forge | (main, Dec  5 2024, 14:06:23) [MSC v.1942 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.3.35\nlangchain: 0.3.19\nlangchain_community: 0.3.17\nlangsmith: 0.2.11\nlangchain_anthropic: 0.3.3\nlangchain_aws: 0.2.12\nlangchain_fireworks: 0.2.7\nlangchain_google_genai: 2.0.8\nlangchain_ollama: 0.2.2\nlangchain_openai: 0.3.1\nlangchain_text_splitters: 0.3.6\nlanggraph_sdk: 0.1.51\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.12\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nanthropic: 0.45.2\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nboto3: 1.36.19\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\ndefusedxml: 0.7.1\nfiletype: 1.2.0\nfireworks-ai: 0.15.12\ngoogle-generativeai: 0.8.4\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.35: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.18: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy: 1.26.4\nnumpy<2,>=1.26.4;: Installed. No version info available.\nnumpy<3,>=1.26.2;: Installed. No version info available.\nollama: 0.4.7\nopenai: 1.62.0\norjson: 3.10.15\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken: 0.8.0\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: Installed. No version info available.\n", "created_at": "2025-02-20", "closed_at": "2025-02-21", "labels": [], "State": "closed", "Author": "henryclw"}
{"issue_number": 3537, "issue_title": "bad repo - On the Edge of AI Adaptability", "issue_body": "oops bad repo\n\nThough I feel like keeping a part of that mistaken repo here, love all your works guy\nTitle:\nOn the Edge of AI Adaptability: A Thought Experiment\nBody:\nSome of you may have seen an earlier issue here that has since been removed. While the details are no longer available, the core idea remains: How can AI move beyond rigid execution and into the realm of adaptive intelligence?\nAt its heart, the discussion was about AI workflows that don\u2019t just execute but evolve, responding dynamically to uncertainty, pauses, and improvisational shifts. Instead of thinking in terms of predefined logic, we started exploring a more organic approach\u2014one where AI processes are not just structured but also aware of their own narrative flow.\nThis isn\u2019t the last time we\u2019ll be thinking along these lines. If you\u2019re intrigued, if this sparks something, let\u2019s keep the conversation going. What does it mean for AI to be not just functional, but emergent?", "created_at": "2025-02-20", "closed_at": "2025-02-20", "labels": [], "State": "closed", "Author": "jgwill"}
{"issue_number": 3535, "issue_title": "[Urgent!] Docker build doesn't create new images", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nAgain, we have problems with Docker build - our images don't launch.\n\nWe have a golive tomorrow morning and cannot deploy our main component: the agent.\n\nThis is very frustrating. And it already happened to us the second time within the last week:\nhttps://github.com/langchain-ai/langgraph/issues/3382\nError Message and Stack Trace (if applicable)\nDefaulted container \"support-agent\" out of: support-agent, redis, cloud-sql-proxy (init)\n    return ctx.invoke(self.callback, **ctx.params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/click/core.py\", line 788, in invoke\n    return __callback(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/uvicorn/main.py\", line 412, in main\n    run(\n  File \"/usr/local/lib/python3.11/site-packages/uvicorn/main.py\", line 579, in run\n    server.run()\n  File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 66, in run\n    return asyncio.run(self.serve(sockets=sockets))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 190, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 70, in serve\n    await self._serve(sockets)\n  File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 77, in _serve\n    config.load()\n  File \"/usr/local/lib/python3.11/site-packages/uvicorn/config.py\", line 435, in load\n    self.loaded_app = import_from_string(self.app)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 19, in import_from_string\n    module = importlib.import_module(module_str)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/api/langgraph_api/server.py\", line 31, in <module>\nImportError: cannot import name 'configure_loopback_transports' from 'langgraph_sdk.client' (/usr/local/lib/python3.11/site-packages/langgraph_sdk/client.py)\nDescription\nWe already removed langgraph-api and langgraph-checkpoint as support suggested throughout the previous 2 times when we had this issue..\nSystem Info\npython -m langchain_core.sys_info\n\nSystem Information\n------------------\n> OS:  Darwin\n> OS Version:  Darwin Kernel Version 24.1.0: Thu Oct 10 21:06:57 PDT 2024; root:xnu-11215.41.3~3/RELEASE_ARM64_T6041\n> Python Version:  3.11.5 (main, Feb  5 2025, 16:10:32) [Clang 16.0.0 (clang-1600.0.26.6)]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.33\n> langchain: 0.3.17\n> langchain_community: 0.3.16\n> langsmith: 0.2.11\n> langchain_anthropic: 0.3.0\n> langchain_openai: 0.2.10\n> langchain_text_splitters: 0.3.5\n> langgraph_api: 0.0.15\n> langgraph_cli: 0.1.65\n> langgraph_license: Installed. No version info available.\n> langgraph_sdk: 0.1.48\n> langgraph_storage: Installed. No version info available.\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp: 3.11.12\n> anthropic: 0.45.2\n> async-timeout: Installed. No version info available.\n> click: 8.1.8\n> cryptography: 43.0.3\n> dataclasses-json: 0.6.7\n> defusedxml: 0.7.1\n> httpx: 0.28.1\n> httpx-sse: 0.4.0\n> jsonpatch: 1.33\n> jsonschema-rs: 0.25.1\n> langgraph: 0.2.69\n> langgraph-checkpoint: 2.0.10\n> langsmith-pyo3: Installed. No version info available.\n> numpy: 1.26.4\n> openai: 1.55.3\n> orjson: 3.10.15\n> packaging: 24.2\n> pydantic: 2.10.6\n> pydantic-settings: 2.7.1\n> pyjwt: 2.10.1\n> python-dotenv: 1.0.1\n> PyYAML: 6.0.2\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> SQLAlchemy: 2.0.27\n> sse-starlette: 2.1.3\n> starlette: 0.45.3\n> structlog: 24.4.0\n> tenacity: 8.5.0\n> tiktoken: 0.8.0\n> typing-extensions: 4.12.2\n> uvicorn: 0.34.0\n> watchfiles: 1.0.4\n> zstandard: Installed. No version info available.\n", "created_at": "2025-02-20", "closed_at": "2025-02-20", "labels": [], "State": "closed", "Author": "nikita-wayhq"}
{"issue_number": 3532, "issue_title": "Adding conditional edge from the entry point node does not seem to pass the Overall state to the validation function", "issue_body": "Discussed in #3387\n\nOriginally posted by sand-heap February 11, 2025\nPretty much what the title says. I can explain further with code\n...\n\n@dataclass\nclass InputState:\n  some_field: int\n\nclass OverAllState(messages):\n  some_field: int\n\ndef validate_transition(state: OverallState)->List:\n  if state.get(\"some_field\") == 1:\n    return END\n  else:\n    return \"other_node\"\n\ndef some_node(input: InputState)->OverAllState:\n  return {\"some_field\": input.some_field+1}\n\nworkflow = StateGraph(OverAllState, input=InputState)\nworkflow.set_entry_point(\"node\")\nworkflow.add(\"node\", some_node)\nworkflow.add(\"other_node\", other_node)\nworkflow.add_conditional_edges(\"node\", validate_transition,...)\n...\n# add end\n...\n\nI would expect the validate_transition to receive the OverallState but for some reason it gets the InputState ?!\nHowever if i add a mock_node between node and other_node and make the edge from mock_node conditional instead and have a sequential flow from node -> mock_node, the behavour is as expected.\ncan you please help? ++ @vbarda @hinthornw ", "created_at": "2025-02-20", "closed_at": "2025-03-11", "labels": ["bug"], "State": "closed", "Author": "sand-heap"}
{"issue_number": 3515, "issue_title": "Lang Graph Nodes Experience High Latency Unrelated to LLM Execution", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nWe use langchain default observability.\nError Message and Stack Trace (if applicable)\nWe don't have an error, the traces are our only evidence.\nDescription\nHi there,\nWe're experiencing significant latency in our agent. When analyzing the trace in LangSmith, we notice that LLM operations execute relatively quickly, but the node exhibits unexpectedly high latency with no clear cause. We suspect that the issue might be related to the LangSmith integration, but we have no way to confirm it.\nAs shown in the attached image, there is a delay of over one second where seemingly nothing is happening.\nAny insights or suggestions would be greatly appreciated.\n\nSystem Info\nSystem Information\n\nOS:  Ubuntu\nOS Version:  Ubuntu 22.04.5 LTS jammy\nPython Version:  3.12.8 (main, Dec  3 2024, 18:42:41) [Clang 16.0.0 (clang-1600.0.26.4)]\n\nPackage Information\n\nlangchain_core: 0.3.29\nlangchain: 0.3.14\nlangchain_community: 0.3.14\nlangsmith: 0.2.10\nlangchain_anthropic: 0.3.1\nlangchain_openai: 0.2.14\nlangchain_text_splitters: 0.3.5\nlangserve: 0.3.1\n\nOther Dependencies\n\naiohttp: 3.11.11\nanthropic: 0.42.0\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.7\ndefusedxml: 0.7.1\nfastapi: 0.115.6\nhttpx: 0.28.1\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 2.2.1\nopenai: 1.59.7\norjson: 3.10.14\npackaging: 24.2\npydantic: 2.10.5\npydantic-settings: 2.7.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.37\nsse-starlette: 2.2.1\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\nzstandard: Installed. No version info available.\n", "created_at": "2025-02-19", "closed_at": "2025-02-27", "labels": [], "State": "closed", "Author": "joshuamoreno1"}
{"issue_number": 3483, "issue_title": "Hidden State not Appearing in Conditional Edge", "issue_body": "Discussed in #3348\n\nOriginally posted by GiulioCMSanto February  7, 2025\nHello!! Before opening a Issue, I would like to start a discussion on something that looks like a bug! \ud83d\ude04\nThe Issue: when using conditional_edge the hidden state is ignored.\nReproducing the Error\nImports\nimport uuid\nfrom pydantic import BaseModel\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import END, START, MessagesState, StateGraph\n\nStates\nclass InputType(BaseModel):\n    input: str\n    \nclass OutputType(BaseModel):\n    output: str\n\nNodes\ndef node_1(state: InputType) -> MessagesState :\n    print(f\"Node 1 input: {state}\")\n    return {\"messages\": [\"out_node_1\"]}\n\ndef node_2(state: MessagesState) -> OutputType:\n    print(f\"Node 2 input: {state}\")\n    return OutputType(output=\"out_node_2\")\n\ndef router(state: MessagesState) -> str:\n    print(f\"Router input: {state}\")\n    return \"node_2\"\n\nNotice: MessagesState is a Hidden State here.\nThe Graph\nbuilder = StateGraph(input=InputType, output=OutputType)\n\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", node_2)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_conditional_edges(\n    \"node_1\",\n    router,\n    [\"node_2\"]\n)\nbuilder.add_edge(\"node_2\", END)\n\ngraph = builder.compile(checkpointer=MemorySaver())\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": uuid.uuid4(),\n    }\n}\n\ngraph.invoke(\n    InputType(input=\"Hi\"), \n    config, \n    stream_mode=\"values\"\n)\n\nOutcomes\nNode 1 input: input='Hi'\nRouter input: input='Hi'\nNode 2 input: {'messages': [HumanMessage(content='out_node_1', additional_kwargs={}, response_metadata={}, id='7c1b5467-d55a-4af7-9888-0bff210ca726')]}\n\nComments\nNotice that Router input: input='Hi' and not {'messages': [HumanMessage(content='out_node_1', additional_kwargs={}, response_metadata={}, id='7c1b5467-d55a-4af7-9888-0bff210ca726')]}. Node 2 actually have access to the Hidden State, but not the Conditional Edge.\nMy Setup\nPython Version: 3.9\nlanggraph==0.2.61\npydantic==2.10.4\n\nI have also tried with\nPython Version: 3.12\nlanggraph==0.2.70\npydantic==2.10.6\n\nSimilar Discussion\nI found a somehow similar discussion here: #2197", "created_at": "2025-02-18", "closed_at": "2025-02-18", "labels": [], "State": "closed", "Author": "GiulioCMSanto"}
{"issue_number": 3464, "issue_title": "[Urgent] LangGraph docker image fails to start", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nPretty much any code. The error is on your level.\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\n  File \"/usr/local/bin/uvicorn\", line 8, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/click/core.py\", line 1161, in __call__\n    return self.main(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/click/core.py\", line 1082, in main\n    rv = self.invoke(ctx)\n         ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/click/core.py\", line 1443, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/click/core.py\", line 788, in invoke\n    return __callback(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/uvicorn/main.py\", line 412, in main\n    run(\n  File \"/usr/local/lib/python3.11/site-packages/uvicorn/main.py\", line 579, in run\n    server.run()\n  File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 66, in run\n    return asyncio.run(self.serve(sockets=sockets))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 190, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"uvloop/loop.pyx\", line 1518, in uvloop.loop.Loop.run_until_complete\n  File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 70, in serve\n    await self._serve(sockets)\n  File \"/usr/local/lib/python3.11/site-packages/uvicorn/server.py\", line 77, in _serve\n    config.load()\n  File \"/usr/local/lib/python3.11/site-packages/uvicorn/config.py\", line 435, in load\n    self.loaded_app = import_from_string(self.app)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/uvicorn/importer.py\", line 19, in import_from_string\n    module = importlib.import_module(module_str)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n  File \"/api/langgraph_api/server.py\", line 16, in <module>\n  File \"/api/langgraph_api/api/__init__.py\", line 8, in <module>\n  File \"/api/langgraph_api/api/meta.py\", line 6, in <module>\n  File \"/api/langgraph_api/queue.py\", line 20, in <module>\n    'Exception raised by Queue.get(block=0)/get_nowait().'\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/api/langgraph_api/stream.py\", line 35, in <module>\n  File \"/usr/local/lib/python3.11/site-packages/langgraph_storage/store.py\", line 8, in <module>\n    from langgraph.store.postgres.aio import AsyncPostgresStore, PostgresIndexConfig\n  File \"/usr/local/lib/python3.11/site-packages/langgraph/store/postgres/__init__.py\", line 1, in <module>\n    from langgraph.store.postgres.aio import AsyncPostgresStore\n  File \"/usr/local/lib/python3.11/site-packages/langgraph/store/postgres/aio.py\", line 12, in <module>\n    from langgraph.checkpoint.postgres import _ainternal\n  File \"/usr/local/lib/python3.11/site-packages/langgraph/checkpoint/postgres/__init__.py\", line 12, in <module>\n    from langgraph.checkpoint.base import (\nImportError: cannot import name 'get_checkpoint_metadata' from 'langgraph.checkpoint.base' (/usr/local/lib/python3.11/site-packages/langgraph/checkpoint/base/__init__.py)\nDescription\nAfter re-building the image today, I get this error.\nWe need to golive tomorrow morning and it blocks everything.\nHow can we build docker images w/o relying on a black-box dependency that ruined our production for several times since the last few days?\nIs there a way to build it w/o this dependency?\nSystem Info\nNote: the command output is done on a mac machine, while the error is inside docker.\npython -m langchain_core.sys_info\n\nSystem Information\n------------------\n> OS:  Darwin\n> OS Version:  Darwin Kernel Version 24.1.0: Thu Oct 10 21:06:57 PDT 2024; root:xnu-11215.41.3~3/RELEASE_ARM64_T6041\n> Python Version:  3.11.5 (main, Feb  5 2025, 16:10:32) [Clang 16.0.0 (clang-1600.0.26.6)]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.33\n> langchain: 0.3.17\n> langchain_community: 0.3.16\n> langsmith: 0.2.11\n> langchain_anthropic: 0.3.0\n> langchain_openai: 0.2.10\n> langchain_text_splitters: 0.3.5\n> langgraph_api: 0.0.15\n> langgraph_cli: 0.1.65\n> langgraph_license: Installed. No version info available.\n> langgraph_sdk: 0.1.48\n> langgraph_storage: Installed. No version info available.\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp: 3.11.12\n> anthropic: 0.45.2\n> async-timeout: Installed. No version info available.\n> click: 8.1.8\n> cryptography: 43.0.3\n> dataclasses-json: 0.6.7\n> defusedxml: 0.7.1\n> httpx: 0.28.1\n> httpx-sse: 0.4.0\n> jsonpatch: 1.33\n> jsonschema-rs: 0.25.1\n> langgraph: 0.2.69\n> langgraph-checkpoint: 2.0.10\n> langsmith-pyo3: Installed. No version info available.\n> numpy: 1.26.4\n> openai: 1.55.3\n> orjson: 3.10.15\n> packaging: 24.2\n> pydantic: 2.10.6\n> pydantic-settings: 2.7.1\n> pyjwt: 2.10.1\n> python-dotenv: 1.0.1\n> PyYAML: 6.0.2\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> SQLAlchemy: 2.0.27\n> sse-starlette: 2.1.3\n> starlette: 0.45.3\n> structlog: 24.4.0\n> tenacity: 8.5.0\n> tiktoken: 0.8.0\n> typing-extensions: 4.12.2\n> uvicorn: 0.34.0\n> watchfiles: 1.0.4\n> zstandard: Installed. No version info available.\n", "created_at": "2025-02-16", "closed_at": "2025-02-16", "labels": [], "State": "closed", "Author": "nikita-wayhq"}
{"issue_number": 3460, "issue_title": "Reducers for other state members apart from messages", "issue_body": "Discussed in #3459\n\nOriginally posted by binarybeastt February 16, 2025\nfrom langgraph.graph.message import add_messages helps to append messages to the message list, how can we replicate this behavior for other types that are not messages, maybe documents, files, or images?", "created_at": "2025-02-16", "closed_at": "2025-02-17", "labels": [], "State": "closed", "Author": "binarybeastt"}
{"issue_number": 3457, "issue_title": "exec format error during langgraph docker deployment", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\ndocker pull $DOCKERHUB_USERNAME/my-langgraph-server:latest\ndocker run -it --rm $DOCKERHUB_USERNAME/my-langgraph-server:latest\nError Message and Stack Trace (if applicable)\nexec /storage/entrypoint.sh: exec format error\nexec /storage/entrypoint.sh: exec format error\nexec /storage/entrypoint.sh: exec format error\nDescription\nI'm using langgraph-cli in Github Actions to build the docker image automatically with langgraph build. I'm following instructions (and using the docker-compose file) from the docs here. Local testing with docker desktop works. However the built container keeps emitting exec format error when deployed on my Ubuntu server.\nDebugging with Sonnet suggests it's an issue with incompatible architectures. I'm wondering if there is a way to use langgraph build with arguments. Any help appreciated!\n.github/workflows/main.yml:\nname: Build Docker Image\n\non:\n  push:\n    branches: [main]\n\njobs:\n  build-push:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.12'\n\n      - name: Build Docker image with Langgraph\n        run: |\n          pip install -U langgraph-cli\n          cd yuichan\n          langgraph build -t my-langgraph-server\n\n      - name: Login to Docker Registry\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n\n      - name: Push Docker image\n        run: |\n          docker tag my-langgraph-server ${{ secrets.DOCKERHUB_USERNAME }}/my-langgraph-server:latest\n          docker push ${{ secrets.DOCKERHUB_USERNAME }}/yui-langgraph-server:latest\n\nlanggraph.json:\n{\n    \"dependencies\": [\".\"],\n    \"graphs\":{\n        \"yui\": \"./yuichan.py:graph\"\n    },\n    \"env\": \".env\",\n    \"python_version\": \"3.12\"\n}\n\nSystem Info\n\nUbuntu 24.04\narm64 VPS on Oracle Cloud\nDocker version 27.4.1\n\nrequirements.txt:\nlangchain-core\nlangchain[openai,anthropic,groq,google-genai]\nlangchain-community\nlangchain-experimental\nlangchain-google-community[gmail]\nlangchainhub\nlangsmith\nlanggraph\nlanggraph-sdk\nlanggraph-checkpoint-postgres\npydantic", "created_at": "2025-02-15", "closed_at": "2025-02-18", "labels": [], "State": "closed", "Author": "mingxuan-he"}
{"issue_number": 3452, "issue_title": "\u5fae\u4fe1\u4ea4\u6d41\u7fa4", "issue_body": "Issue with current documentation:\n\nIdea or request for content:\nNo response", "created_at": "2025-02-15", "closed_at": "2025-02-15", "labels": [], "State": "closed", "Author": "RobinYang11"}
{"issue_number": 3442, "issue_title": "How to use 'artifact' attribute in ToolMessage to output both text and image?", "issue_body": "Issue with current documentation:\nIn https://python.langchain.com/api_reference/core/messages/langchain_core.messages.tool.ToolMessage.html, it introduces the concept of 'arifact' and give a simple example:\n\nfrom langchain_core.messages import ToolMessage\ntool_output = {\n\"stdout\": \"From the graph we can see that the correlation between x and y is ...\",\n\"stderr\": None,\n\"artifacts\": {\"type\": \"image\", \"base64_data\": \"/9j/4gIcSU...\"},\n}\n\n\nToolMessage(\ncontent=tool_output[\"stdout\"],\nartifact=tool_output,\ntool_call_id='call_Jja7J89XsjrOLA5r!MEOW!SL',\n)\n\n,\nif the tool function called by llm return an output likes tool_output = { \"stdout\": \"From the graph we can see that the correlation between x and y is ...\", \"stderr\": None, \"artifacts\": {\"type\": \"image\", \"base64_data\": \"/9j/4gIcSU...\"}, }\n, can the agent automatically return a toolmessage contains 'artifact'? Or how to use 'artifact' attribute in ToolMessage to output both text and image? There are few examples about how to get multimodal output. Thank you very much.\nIdea or request for content:\nNo response", "created_at": "2025-02-14", "closed_at": "2025-02-14", "labels": [], "State": "closed", "Author": "Cybertyann"}
{"issue_number": 3441, "issue_title": "Checkpointer attempts to serialize RunnableConfig -- Object of type Foobar is not JSON serializable", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing import TypedDict\n\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.checkpoint.sqlite import SqliteSaver\nfrom langgraph.constants import START\nfrom langgraph.graph import StateGraph\n\n\nclass State(TypedDict):\n    foo: str\n\n\nclass Foobar:\n    \"\"\"\n    Non-JSON serializable class\n    \"\"\"\n\n    hello: str\n\n\ndef node_a(state: State, config: RunnableConfig):\n    return {\"foo\": \"bar\"}\n\n\nwith SqliteSaver.from_conn_string(\":memory:\") as memory:\n    workflow_builder = StateGraph(State)\n    workflow_builder.add_node(node_a)\n    workflow_builder.add_edge(START, \"node_a\")\n\n    graph = workflow_builder.compile(checkpointer=memory)\n    config = {\"configurable\": {\"thread_id\": \"1\", \"foobar\": Foobar()}}\n\n    for s in graph.stream({\"foo\": \"\"}, config):\n        print(s)\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\n  File \"/Users/enzoaguado/Code/redacted/repro.py\", line 33, in <module>\n    for s in graph.stream({\"foo\": \"\"}, config):\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/enzoaguado/Code/redacted/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 1668, in stream\n    with SyncPregelLoop(\n         ^^^^^^^^^^^^^^^\n  File \"/Users/enzoaguado/Code/redacted/.venv/lib/python3.12/site-packages/langgraph/pregel/loop.py\", line 959, in __exit__\n    return self.stack.__exit__(exc_type, exc_value, traceback)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 610, in __exit__\n    raise exc_details[1]\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 595, in __exit__\n    if cb(*exc_details):\n       ^^^^^^^^^^^^^^^^\n  File \"/Users/enzoaguado/Code/redacted/.venv/lib/python3.12/site-packages/langgraph/pregel/executor.py\", line 120, in __exit__\n    task.result()\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/enzoaguado/Code/redacted/.venv/lib/python3.12/site-packages/langgraph/pregel/loop.py\", line 884, in _checkpointer_put_after_previous\n    prev.result()\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/enzoaguado/Code/redacted/.venv/lib/python3.12/site-packages/langgraph/pregel/executor.py\", line 83, in done\n    task.result()\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/thread.py\", line 59, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/enzoaguado/Code/redacted/.venv/lib/python3.12/site-packages/langgraph/pregel/loop.py\", line 886, in _checkpointer_put_after_previous\n    cast(BaseCheckpointSaver, self.checkpointer).put(\n  File \"/Users/enzoaguado/Code/redacted/.venv/lib/python3.12/site-packages/langgraph/checkpoint/sqlite/__init__.py\", line 400, in put\n    serialized_metadata = self.jsonplus_serde.dumps(\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/enzoaguado/Code/redacted/.venv/lib/python3.12/site-packages/langgraph/checkpoint/serde/jsonplus.py\", line 182, in dumps\n    return json.dumps(obj, default=self._default, ensure_ascii=False).encode(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/__init__.py\", line 238, in dumps\n    **kw).encode(obj)\n          ^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/encoder.py\", line 200, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/json/encoder.py\", line 258, in iterencode\n    return _iterencode(o, 0)\n           ^^^^^^^^^^^^^^^^^\n  File \"/Users/enzoaguado/Code/redacted/.venv/lib/python3.12/site-packages/langgraph/checkpoint/serde/jsonplus.py\", line 132, in _default\n    raise TypeError(\nTypeError: Object of type Foobar is not JSON serializable\nDescription\nAccording to #2135 (comment) the correct way to pass around objects that are not JSON serializable is to put them in RunnableConfig instead of the graph state.\nHowever, even when doing that an exception is still thrown.\nWhat is the correct way to pass objects around at runtime without storing them in the checkpointer ?\nSystem Info\nlanggraph = \"^0.2.60\"\nlanggraph-checkpoint-sqlite = \"^2.0.4\"\nlanggraph-checkpoint-postgres = \"^2.0.13\"", "created_at": "2025-02-14", "closed_at": "2025-02-14", "labels": [], "State": "closed", "Author": "aguadoenzo"}
{"issue_number": 3421, "issue_title": "LangGraph custom auth not working for self-hosted deployment without version update", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nFROM langchain/langgraph-api:3.12\n\n# Note that this example has been redacted to protect private information\n...\n\n# Set security scheme\nENV LANGGRAPH_AUTH='{\"path\": \"/deps/<my_dep>/<some_folder>/security/langgraph_init.py:auth\", \"disable_studio_auth\": true, \"openapi\": {\"securitySchemes\": {\"bearerAuth\": {\"type\": \"http\", \"scheme\": \"bearer\", \"bearerFormat\": \"JWT\"}}, \"security\": [{\"bearerAuth\": []}]}}'\n# Graphs\nENV LANGSERVE_GRAPHS='{\"agent\": \"/deps/<my_dep>/<some_folder>/graph.py:graph\"}'\nError Message and Stack Trace (if applicable)\n2025-02-13T12:29:09.619757Z [warning  ] No license key found, running in test mode with LangSmith API key. For production use, set LANGGRAPH_CLOUD_LICENSE_KEY in environment. [langgraph_license.validation] api_revision=cfcc057 api_variant=local\n2025-02-13T12:29:09.913043Z [info     ] HTTP Request: GET https://eu.api.smith.langchain.com/auth?langgraph-api=true \"HTTP/1.1 200 OK\" [httpx] api_revision=cfcc057 api_variant=local\n2025-02-13T12:29:09.916649Z [error    ] Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/site-packages/starlette/routing.py\", line 693, in lifespan\n    async with self.lifespan_context(app) as maybe_state:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/api/langgraph_api/lifespan.py\", line 18, in lifespan\n  File \"/api/langgraph_license/validation.py\", line 98, in get_license_status\nValueError: Custom authentication is currently available in the Managed Cloud version of LangGraph Platform or with an self-hosting enterprise license. Please visit https://langchain-ai.github.io/langgraph/concepts/deployment_options/ to learn more about deployment options, or contact sales@langchain.com for information on upgrading from the self-hosted Lite plan to an enterprise license.\n [uvicorn.error] api_revision=cfcc057 api_variant=local\n2025-02-13T12:29:09.917001Z [error    ] Application startup failed. Exiting. [uvicorn.error] api_revision=cfcc057 api_variant=local\nDescription\nHi,\nI am currently using a self-hosted deployment of the LangGraph API, but somehow our existing deployment (with custom auth) has been broken. I'm getting the following error:\nValueError: Custom authentication is currently available in the Managed Cloud version of LangGraph Platform or with an self-hosting enterprise license. Please visit https://langchain-ai.github.io/langgraph/concepts/deployment_options/ to learn more about deployment options, or contact sales@langchain.com for information on upgrading from the self-hosted Lite plan to an enterprise license.\n\nHow is this possible without any version updates? The docs note that this is perfectly fine for Self-Hosted Auth:\n\nNo default authentication\nComplete flexibility to implement your security model\nYou control all aspects of authentication and authorization\n\nAuthentication limits are not discussed on the self-hosted lite deployment either, it just notes: 'Cron jobs are not available for Self-Hosted Lite deployments'. The deployments options clearly note Self-Hosted Lite: Available for all plans..\nThe troubleshooting page for INVALID_LICENSE self-hosted lite however, notes: 'The API key must be associated with an account on a Plus plan or greater'.\nI'm assuming this is a bug, otherwise I would expect this to be described in release notes and the public docs. That would still not justify the fact that this does not follow semantic versioning, i.e. there should have been a major release that changed this behaviour, but this happened WITHOUT any version updates (not even minor). The only thing I did is rebuild the docker image with identical dependencies inside our CI/CD pipeline, i.e. same python version (3.12), package dependencies & base image (langchain/langgraph-api:3.12).\nFeatures can be moved to different licensing & terms over time, but this is why semantic versioning and stable releases exist after all. The terms and services note: \"Any Free Access Subscriptions are provided by LangChain \u201cAS-IS\u201d and without any representations, warranties, performance, or data security guarantees or support obligations.\". I understand this clause, but I still expect released versions to work over time, and at the very least that this is accurately described in the public documentation (under professional production auth in this case).\nEdit: Upon further digging we noticed that the underlying docker image is the issue. We created our implementation based on langchain/langgraph-api:3.12-233a561 and it still works fine, the breaking behaviour seems to be introduced in langchain/langgraph-api:3.12-7e692e2 as langchain/langgraph-api:3.12-cfb4da0 works fine.\nSystem Info\nNot relevant, this issue was verified on multiple Linux/MacOS machines", "created_at": "2025-02-13", "closed_at": "2025-02-13", "labels": [], "State": "closed", "Author": "BobMerkus"}
{"issue_number": 3418, "issue_title": "Subgraph state update contains the entire CompiledStateGraph in it which is not serializable for the astream events", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nclass LookupGraph(BaseGraphWrapper):\n    \"\"\"Graph wrapper for the lookup tool, used to also add a function to get the prompt description\"\"\"\n    name: str = \"Lookup Tool\"\n    description: str = \"Tool used to retrieve information needed to answer any input question\"\n    graph: StateGraph = None\n    input_schema = LookupInput\n\n    def get_graph(self):\n        \"\"\"Compile the graph and return it\"\"\"\n        graph = StateGraph(state_schema=LookupState)\n        graph.add_node(\"retriever\", retriever_node)\n        graph.add_node(\"filter_documents\", FilterGraph.graph)\n        graph.add_node(\"replace_urls\", replace_urls_node)\n        graph.add_node(\"generate_answer\", CollapseGraph.graph)\n        graph.add_node(\"process_ouput\", process_ouput)\n\n        graph.add_edge(START, \"retriever\")\n        graph.add_edge(\"retriever\", \"filter_documents\")\n        graph.add_edge(\"filter_documents\", \"replace_urls\")\n        graph.add_edge(\"replace_urls\", \"generate_answer\")\n        graph.add_edge(\"generate_answer\", \"process_ouput\")\n        graph.add_edge(\"process_ouput\", END)\n        graph = graph.compile()\n        self.graph = graph\n        return graph\n\n\n\nclass FilterGraph(BaseGraphWrapper):\n    \"\"\"Graph wrapper for the filter tool, used to also add a function to get the prompt description\"\"\"\n    name: str = \"Filter Documents\"\n    description: str = \"Tool designed to filter documents based on a question.\"\n    graph: CompiledStateGraph = graph.compile()\n    input_schema = FilterInput\nError Message and Stack Trace (if applicable)\nFile \"/home/runner/work/IRIS-RAG-API/IRIS-RAG-API/app/app.py\", line 550, in send_message\n    async for event, chunk in agent_graph.astream(\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 2007, in astream\n    async for _ in runner.atick(\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/pregel/runner.py\", line 527, in atick\n    _panic_or_proceed(\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/pregel/runner.py\", line 619, in _panic_or_proceed\n    raise exc\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/pregel/retry.py\", line 128, in arun_with_retry\n    return await task.proc.ainvoke(task.input, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/utils/runnable.py\", line 543, in ainvoke\n    input = await step.ainvoke(input, config, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/utils/runnable.py\", line 331, in ainvoke\n    ret = await asyncio.create_task(coro, context=context)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/runner/work/IRIS-RAG-API/IRIS-RAG-API/app/rebatch_chain/graphs/simple_agent_graph.py\", line 224, in custom_tools_node\n    subgraph_state = await t.ainvoke(tool_args, config)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langchain_core/tools/structured.py\", line 58, in ainvoke\n    return await super().ainvoke(input, config, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langchain_core/tools/base.py\", line 493, in ainvoke\n    return await self.arun(tool_input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langchain_core/tools/base.py\", line 842, in arun\n    raise error_to_raise\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langchain_core/tools/base.py\", line 809, in arun\n    response = await asyncio.create_task(coro, context=context)  # type: ignore\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langchain_core/tools/structured.py\", line 97, in _arun\n    return await self.coroutine(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/runner/work/IRIS-RAG-API/IRIS-RAG-API/app/rebatch_chain/graphs/simple_agent_graph.py\", line 187, in lookup\n    lookup_state = await lookup_graph.ainvoke(input, config)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 2122, in ainvoke\n    async for chunk in self.astream(\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 2007, in astream\n    async for _ in runner.atick(\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/pregel/runner.py\", line 444, in atick\n    await arun_with_retry(\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/pregel/retry.py\", line 123, in arun_with_retry\n    async for _ in task.proc.astream(task.input, config):\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/utils/runnable.py\", line 666, in astream\n    async for chunk in aiterator:\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1455, in atransform\n    async for ichunk in input:\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1455, in atransform\n    async for ichunk in input:\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 2007, in astream\n    async for _ in runner.atick(\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/pregel/runner.py\", line 444, in atick\n    await arun_with_retry(\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/pregel/retry.py\", line 123, in arun_with_retry\n    async for _ in task.proc.astream(task.input, config):\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/utils/runnable.py\", line 666, in astream\n    async for chunk in aiterator:\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1455, in atransform\n    async for ichunk in input:\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1455, in atransform\n    async for ichunk in input:\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 1960, in astream\n    async with AsyncPregelLoop(\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/pregel/loop.py\", line 1103, in __aexit__\n    return await exit_task\n           ^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/contextlib.py\", line 745, in __aexit__\n    raise exc_details[1]\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/contextlib.py\", line 728, in __aexit__\n    cb_suppress = await cb(*exc_details)\n                  ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/pregel/executor.py\", line 206, in __aexit__\n    raise exc\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/pregel/loop.py\", line 1019, in _checkpointer_put_after_previous\n    await prev\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/pregel/loop.py\", line 1021, in _checkpointer_put_after_previous\n    await cast(BaseCheckpointSaver, self.checkpointer).aput(\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/checkpoint/memory/__init__.py\", line 474, in aput\n    return self.put(config, checkpoint, metadata, new_versions)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/checkpoint/memory/__init__.py\", line 359, in put\n    self.serde.dumps_typed(\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/checkpoint/serde/jsonplus.py\", line 193, in dumps_typed\n    return \"msgpack\", _msgpack_enc(obj)\n                      ^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/checkpoint/serde/jsonplus.py\", line 514, in _msgpack_enc\n    return enc.pack(data)\n           ^^^^^^^^^^^^^^\n  File \"msgpack/_packer.pyx\", line 279, in msgpack._cmsgpack.Packer.pack\n  File \"msgpack/_packer.pyx\", line 276, in msgpack._cmsgpack.Packer.pack\n  File \"msgpack/_packer.pyx\", line 265, in msgpack._cmsgpack.Packer._pack\n  File \"msgpack/_packer.pyx\", line 213, in msgpack._cmsgpack.Packer._pack_inner\n  File \"msgpack/_packer.pyx\", line 267, in msgpack._cmsgpack.Packer._pack\n  File \"/opt/hostedtoolcache/Python/3.11.11/x64/lib/python3.11/site-packages/langgraph/checkpoint/serde/jsonplus.py\", line 435, in _msgpack_default\n    raise TypeError(f\"Object of type {obj.__class__.__name__} is not serializable\")\nTypeError: Object of type CompiledStateGraph is not serializable\nDescription\nWe have a graph with a subgraph. This subgraph has just one node that returns a list of langchain Documents and the question asked.\nFor some reason we get the error above where it seems like the entire CompiledGraph from the subgraph is included in the updated state.\nThe error is thrown in the astream because the CompiledGraph cannot be serialized.\nIt seems to occur with the latest update from langgraph and langgraph-checkpoint\nIt works with:\nlanggraph==0.2.71\nlanggrahp-checkpoint==2.0.12\nbut it breaks with:\nlanggraph==0.2.72\nlanggrahp-checkpoint==2.0.13\nEspecially the langgraph-checkpoint seems the culprit as downgrading the langraph to 71 didn't work but downgrading the checkpoint to 12 did.\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #53~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Jan 15 19:18:46 UTC 2\nPython Version:  3.11.11 (main, Dec 11 2024, 16:28:39) [GCC 11.2.0]\n\nPackage Information\n\nlangchain_core: 0.3.35\nlangchain: 0.3.18\nlangchain_community: 0.3.17\nlangsmith: 0.3.8\nlangchain_huggingface: 0.1.2\nlangchain_milvus: 0.1.7\nlangchain_openai: 0.2.13\nlangchain_text_splitters: 0.3.6\nlangchain_weaviate: 0.0.4\nlanggraph_sdk: 0.1.51\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\nhuggingface-hub: 0.28.1\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.18: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy: 1.26.4\nnumpy<2,>=1.26.4;: Installed. No version info available.\nnumpy<3,>=1.26.2;: Installed. No version info available.\nopenai: 1.55.3\norjson: 3.10.15\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.8.2\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npymilvus: 2.4.14\npytest: 8.3.4\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: Installed. No version info available.\nsentence-transformers: 3.4.1\nsimsimd: 6.2.1\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken: 0.8.0\ntokenizers: 0.19.1\ntransformers: 4.42.4\ntyping-extensions>=4.7: Installed. No version info available.\nweaviate-client: 4.10.4\nzstandard: 0.23.0\n", "created_at": "2025-02-13", "closed_at": "2025-02-18", "labels": ["question"], "State": "closed", "Author": "DebienRuben"}
{"issue_number": 3417, "issue_title": "Returning state in nodes won't update it", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nclass MyNode:\n   \n   def action(state: MyState):\n      # some cool stuff\n      state.my_value = 123\n      return state\n   \n   def should_continue(state: MyState):\n      print(state.my_value) # this returns default value (in my case is None)\n      # some cool logic to continue or not\n      return \"END\"\nDescription\nHi, I've been using latest version forever, but since this morning I've being dealing with the fact that returning state in my nodes won't update it. When I rollback to 0.2.71 it worked fine. Did something change since .71?\nSorry if example code is not really helpful.\nedit1: my versions were wrong;\nedit2: looking better, the problem seems related to Lists, Dicts and BaseModels. It wont't update those for me, here are some print I've made:\nagent=None settings=None database=None user=User(id=UUID('89fe6ec9-a318-4c5c-8236-e870f7fa4e6f'), status=True, full_name='nathan', email=\"nathan@example.com\"\n\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP PREEMPT_DYNAMIC Debian 6.1.106-3 (2024-08-26)\nPython Version:  3.12.1 (main, Dec 12 2024, 22:30:56) [GCC 9.4.0]\n\nPackage Information\n\nlangchain_core: 0.3.33\nlangchain: 0.3.17\nlangchain_community: 0.3.16\nlangsmith: 0.3.4\nlangchain_openai: 0.3.3\nlangchain_text_splitters: 0.3.5\nlanggraph_sdk: 0.1.43\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.10.10\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.7\nhttpx: 0.27.2\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.59.7\norjson: 3.10.11\npackaging: 24.1\npydantic: 2.10.6\npydantic-settings: 2.6.1\npytest: Installed. No version info available.\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrich: Installed. No version info available.\nSQLAlchemy: 2.0.35\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\nzstandard: 0.23.0\n", "created_at": "2025-02-13", "closed_at": "2025-02-13", "labels": ["question"], "State": "closed", "Author": "NathanAP"}
{"issue_number": 3416, "issue_title": "LangGraph JS SDK : errors are parsed as JSON", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\napp.get(\"/test\", async (c) => {\n  const client = new Client({\n    apiUrl: \"http://localhost:2024\",\n  });\n  const thread = await client.threads.get(\"non-existent-thread\");\n  return c.json({\n    thread,\n  });\n});\nError Message and Stack Trace (if applicable)\n[wrangler:inf] GET /test 500 Internal Server Error (39ms)\n\u2718 [ERROR] SyntaxError: Unexpected token 'I', \"Invalid th\"... is not valid JSON\n\n      at async Array.<anonymous>\nDescription\n\nwhen the API return an error (forbidden, not found, ...), the SDK systematically parse it as JSON\nerror is not throw as it should be\nwe can't see the complete error\n\nSystem Info\npython -m langchain_core.sys_info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.2.0: Fri Dec  6 19:00:33 PST 2024; root:xnu-11215.61.5~2/RELEASE_ARM64_T8122\nPython Version:  3.12.5 (main, Aug 14 2024, 04:32:18) [Clang 18.1.8 ]\n\nPackage Information\n\nlangchain_core: 0.3.34\nlangchain: 0.3.18\nlangchain_community: 0.3.17\nlangsmith: 0.3.8\nlangchain_anthropic: 0.3.7\nlangchain_aws: 0.2.12\nlangchain_openai: 0.3.5\nlangchain_text_splitters: 0.3.6\nlanggraph_api: 0.0.23\nlanggraph_cli: 0.1.71\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.51\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nanthropic<1,>=0.45.0: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nboto3: 1.36.18\nclick: 8.1.8\ncryptography: 43.0.3\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\njsonschema-rs: 0.25.1\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.18: Installed. No version info available.\nlanggraph: 0.2.71\nlanggraph-checkpoint: 2.0.12\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy: 2.2.2\nnumpy<2,>=1.26.4;: Installed. No version info available.\nnumpy<3,>=1.26.2;: Installed. No version info available.\nopenai<2.0.0,>=1.58.1: Installed. No version info available.\norjson: 3.10.15\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npyjwt: 2.10.1\npytest: Installed. No version info available.\npython-dotenv: 1.0.1\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: Installed. No version info available.\nSQLAlchemy<3,>=1.4: Installed. No version info available.\nsse-starlette: 2.1.3\nstarlette: 0.45.3\nstructlog: 24.4.0\ntenacity: 9.0.0\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nuvicorn: 0.34.0\nwatchfiles: 1.0.4\nzstandard: 0.23.0\n", "created_at": "2025-02-13", "closed_at": null, "labels": [], "State": "open", "Author": "arthberman"}
{"issue_number": 3415, "issue_title": "Subgraph history", "issue_body": "Discussed in #3332\n\nOriginally posted by ErDaN213 February  6, 2025\nHi everyone,\nI am currently working with LangGraph and would like to ensure that all subgraphs and their respective nodes are being properly invoked during execution. Is there a method to track or verify that each node is being triggered as expected?\nI have explored the get_state_history method; however, it does not seem to provide information regarding the invocation of subgraph nodes. I would prefer to avoid creating a custom State variable that would need to be overwritten at each node.\nCould anyone suggest an appropriate way to retrieve this information, or is it possible to introduce a subgraphs=True parameter to the get_state_history method to capture such details?\nI look forward to your insights and suggestions.\nThank you in advance for your assistance!\nBest regards,", "created_at": "2025-02-13", "closed_at": "2025-02-21", "labels": [], "State": "closed", "Author": "ErDaN213"}
{"issue_number": 3393, "issue_title": "LangGraph Platform Cloud SaaS timing out on deployments", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nSteps to reproduce: \n1. Push to branch in connected repo\n2. Revision + build is automatically kicked off in LangGraph Platform\n3. Wait for \"Building\", \"Waiting for Deploy\", \"Deploying LangGraph Server\" messages\n4. The deploy will take a few minutes - if it goes over 5 minutes it will time out and fail\nError Message and Stack Trace (if applicable)\nTimeout: Deployment is not ready after 300 seconds.\nDescription\nI've been trying to set up LangGraph Platform Cloud SaaS to evaluate if it's stable enough for us to host on, and although the first couple deployments worked fine, the past several ones have failed with the following error message:\nTimeout: Deployment is not ready after 300 seconds.\nSome of these deployments will show server logs that it did successfully start up after showing the failure message, but many of them have empty server logs.\nIs it normal for deployments to be taking this long? Was there an outage today? If it regularly goes over 300 sec should this timeout be increased?\nSystem Info\nn/a", "created_at": "2025-02-11", "closed_at": "2025-02-12", "labels": [], "State": "closed", "Author": "austinberke"}
{"issue_number": 3392, "issue_title": "`TypeError` in PostgreSQL Store Setup Function (Re-execution)", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.store.postgres import AsyncPostgresStore\nfrom psycopg_pool import AsyncConnectionPool\n\npool = AsyncConnectionPool(\n                conninfo = '...',\n                max_size = '...',\n                min_size = '...',\n                kwargs= {\n                    \"autocommit\": True,\n                    \"prepare_threshold\": 0,\n                }\n            )\nawait pool.open()\n\n#...\n\nstore = AsyncPostgresStore(pool)\nawait store.setup()\nError Message and Stack Trace (if applicable)\nFile \"/Users/development/project/application/core/graph.py\", line 410, in generate_graph\n    await store.setup()\n  File \"/Users/development/project/.venv/lib/python3.11/site-packages/langgraph/store/postgres/aio.py\", line 234, in setup\n    version = await _get_version(cur, table=\"store_migrations\")\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/development/project/.venv/lib/python3.11/site-packages/langgraph/store/postgres/aio.py\", line 230, in _get_version\n    version = row[\"v\"]\n              ~~~^^^^^\nTypeError: tuple indices must be integers or slices, not str\nDescription\nHello guys,\nBug Report: TypeError in PostgreSQL Store Setup Function (Re-execution)\nDescription\nWhen executing the PostgreSQL store setup function for the first time, it runs without errors. However, from the second execution onward, the following error occurs:\n  File \"/Users/development/project/.venv/lib/python3.11/site-packages/langgraph/store/postgres/aio.py\", line 230, in _get_version\n    version = row[\"v\"]\n              ~~~^^^^^\nTypeError: tuple indices must be integers or slices, not str\nObservations\nAfter reviewing the code, I noticed that the PostgreSQL cursor expects a dictionary with key-value pairs, but instead, it returns only values (a tuple). This leads to the TypeError when attempting to access \"v\" as a dictionary key.\nThe issue occurs at line 230 of .venv/lib/python3.11/site-packages/langgraph/store/postgres/aio.py:\nif row is None:\n    version = -1\nelse:\n    version = row[\"v\"]  # Error occurs here\nreturn version\nAdditionally, in lines 426-431, the cursor is created as follows:\nelse:\n    async with (\n        self.lock,\n        conn.cursor(binary=True) as cur,  # Cursor creation\n    ):\n        yield cur\nIt seems that row_factory=dict_row might need to be used to ensure the cursor returns a dictionary instead of a tuple.\nEnvironment\n\nlanggraph: 0.2.69\nlanggraph_checkpoint_postgres: 2.0.13\nPython: 3.11\nOS: macOS\n\nSuggested Fix\nWould it be appropriate to use row_factory=dict_row when creating the cursor to ensure dictionary-based row access?\nLet me know if more details are needed. Thanks!\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.1.0: Thu Oct 10 21:03:15 PDT 2024; root:xnu-11215.41.3~2/RELEASE_ARM64_T6000\nPython Version:  3.11.4 (main, Jul 12 2023, 12:11:09) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n\nPackage Information\n\nlangchain_core: 0.2.43\nlangchain: 0.2.3\nlangchain_community: 0.2.4\nlangsmith: 0.1.147\nlangchain_aws: 0.1.6\nlangchain_cli: 0.0.35\nlangchain_experimental: 0.0.60\nlangchain_openai: 0.1.7\nlangchain_postgres: 0.0.9\nlangchain_text_splitters: 0.2.1\nlangchainhub: 0.1.21\nlanggraph: 0.2.69\nlangserve: 0.2.2\n\nOther Dependencies\n\naiohttp: 3.9.5\nasync-timeout: Installed. No version info available.\nbeautifulsoup4: Installed. No version info available.\nboto3: 1.34.162\ndataclasses-json: 0.6.7\nfaker: 19.13.0\nfastapi: 0.115.6\ngitpython: 3.1.43\ngritql: 0.1.5\nhttpx: 0.27.0\njinja2: 3.1.4\njsonpatch: 1.33\nlanggraph-checkpoint: 2.0.10\nlanggraph-sdk: 0.1.51\nlangserve[all]: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlxml: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.57.0\norjson: 3.10.5\npackaging: 23.2\npandas: 2.2.3\npgvector: 0.2.5\npresidio-analyzer: Installed. No version info available.\npresidio-anonymizer: Installed. No version info available.\npsycopg: 3.2.3\npsycopg-pool: 3.2.4\npydantic: 2.10.3\npyproject-toml: 0.0.10\nPyYAML: 6.0.1\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nsentence-transformers: Installed. No version info available.\nSQLAlchemy: 2.0.36\nsqlalchemy: 2.0.36\nsse-starlette: 1.8.2\ntabulate: Installed. No version info available.\ntenacity: 8.5.0\ntiktoken: 0.7.0\ntomlkit: 0.12.5\ntyper[all]: Installed. No version info available.\ntypes-requests: 2.32.0.20240622\ntyping-extensions: 4.12.2\nuvicorn: 0.34.0\n", "created_at": "2025-02-11", "closed_at": "2025-02-15", "labels": [], "State": "closed", "Author": "and1412"}
{"issue_number": 3383, "issue_title": "Typo in the read_document tool/function in the Hierarchical Agent Teams notebook?", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\ndef provide_a_start_line(\n    start: Annotated[Optional[int], \"The start line. Default is 0\"] = None\n) -> str:\n    \"\"\"Return the start line.\"\"\"\n    if start is not None:\n        start = 0\n    return start\n\nprint(provide_a_start_line), provide_a_start_line(1), provide_a_start_line(2)\nError Message and Stack Trace (if applicable)\n\nDescription\nI am using the read_document() function in the Hierarchical Agent Teams notebook and the function:\n\nkeeps the input None start line as None, and\noverwrites the non-None input start line with 0.\nTherefore, lines[start:end] is lines[0:end] irrespective of the start provided as argument to the function.\n\nThe function in question:\ndef read_document(\n    file_name: Annotated[str, \"File path to read the document from.\"],\n    start: Annotated[Optional[int], \"The start line. Default is 0\"] = None,\n    end: Annotated[Optional[int], \"The end line. Default is None\"] = None,\n) -> str:\n    \"\"\"Read the specified document.\"\"\"\n    with (WORKING_DIRECTORY / file_name).open(\"r\") as file:\n        lines = file.readlines()\n    if start is not None:\n        start = 0\n    return \"\\n\".join(lines[start:end])\n\nSo, the function can keep the if-statement s.t. it is of the form if start is None: or can remove the if-statement altogether since lines[None:end] is equivalent to lines[0:end] in Python slicing.\nSystem Info\nReplication requires only python's typing module: from typing import Optional\npython -m langchain_core.sys_info gives:\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:06 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T8103\nPython Version:  3.11.4 (main, Jul  5 2023, 09:00:44) [Clang 14.0.6 ]\n\nPackage Information\n\nlangchain_core: 0.3.34\nlangchain: 0.3.18\nlangchain_community: 0.3.17\nlangsmith: 0.1.147\nlangchain_anthropic: 0.3.7\nlangchain_experimental: 0.3.4\nlangchain_openai: 0.2.11\nlangchain_text_splitters: 0.3.6\nlanggraph_cli: 0.1.61\nlanggraph_sdk: 0.1.43\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nanthropic<1,>=0.45.0: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nclick: 8.1.7\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nhttpx: 0.28.0\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.18: Installed. No version info available.\nlanggraph-api: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<2,>=1.26.4;: Installed. No version info available.\nnumpy<3,>=1.26.2;: Installed. No version info available.\nopenai: 1.56.2\norjson: 3.10.12\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.3\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npython-dotenv: 1.0.1\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken: 0.8.0\ntyping-extensions>=4.7: Installed. No version info available.\n", "created_at": "2025-02-11", "closed_at": "2025-02-11", "labels": [], "State": "closed", "Author": "milenakapralova"}
{"issue_number": 3382, "issue_title": "[Urgent] Live deployment doesn't work on LangGraph Cloud", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n-\nError Message and Stack Trace (if applicable)\n\nDescription\nYou seem to have a typo inside /api/langgraph_api/queue.py (instead of await you wrote wait) - this made all live deployments being stuck since at least 6h.\nBackground worker scheduler failed\nTraceback (most recent call last):\n  File \"/api/langgraph_api/queue.py\", line 126, in queue\n  File \"/usr/local/lib/python3.11/contextlib.py\", line 334, in helper\n    return _AsyncGeneratorContextManager(func, args, kwds)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/contextlib.py\", line 105, in __init__\n    self.gen = func(*args, **kwds)\n               ^^^^^^^^^^^^^^^^^^^\nTypeError: Runs.next() got an unexpected keyword argument 'wait'\n\nSystem Info\n\n\n", "created_at": "2025-02-11", "closed_at": "2025-02-12", "labels": [], "State": "closed", "Author": "n-sviridenko"}
{"issue_number": 3380, "issue_title": "For  langraph workflow with parallel node  execution (send/map-reduce) follow uo quetions are not working /. memory /checkpointer not wokring", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n# Model and prompts\nquery_prompt = \"\"\"Given the query, create a lexical keyword search query that would help find relevant information.\nOriginal query: {query}\nReturn only the lexical keyword query without any explanation.\"\"\"\n\nresponse_prompt = \"\"\"Write 2 lines of response for this query: {query}\nMake the response informative and concise.\"\"\"\n\nbest_response_prompt = \"\"\"Below are responses to the query: {query}\nSelect the best response! Return the ID of the best one (0 or 1).\n\n{responses}\"\"\"\n\nclass OverallState(TypedDict):\n    query: str\n    queries: List[str]\n    responses: Annotated[List[str], operator.add]\n    best_response: str\n\nclass QueryState(TypedDict):\n    query: str\n\nclass QueryWorkflow:\n    def __init__(self):\n        logger.info(\"Initializing QueryWorkflow\")\n        try:\n            self.llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n            self.workflow_app = None\n            self.checkpointer = None\n            self.setup_workflow()\n        except Exception as e:\n            logger.error(f\"Error in initialization: {str(e)}\")\n            raise\n\n    def setup_workflow(self):\n        \"\"\"Set up workflow with parallel processing and Redis checkpoint support.\"\"\"\n        logger.info(\"Setting up workflow\")\n        try:\n            workflow = StateGraph(OverallState)\n\n            def generate_queries(state: OverallState):\n                \"\"\"Generate list of queries including original and lexical keyword query.\"\"\"\n                logger.info(f\"Generating queries for: {state['query']}\")\n                prompt = query_prompt.format(query=state['query'])\n                response = self.llm.invoke(prompt)\n                lexical_query = response.content.strip()\n                queries = [state['query'], lexical_query]\n                logger.info(f\"Generated queries: {queries}\")\n                return {\"queries\": queries}\n\n            def process_query(state: QueryState):\n                \"\"\"Process a single query and generate response.\"\"\"\n                logger.info(f\"Processing query: {state['query']}\")\n                prompt = response_prompt.format(query=state['query'])\n                response = self.llm.invoke(prompt)\n                logger.info(f\"Generated response for query: {state['query']}\")\n                return {\"responses\": [response.content.strip()]}\n\n            def continue_to_processing(state: OverallState):\n                \"\"\"Map function to distribute queries to parallel processing.\"\"\"\n                logger.info(f\"Mapping {len(state['queries'])} queries to parallel processing\")\n                return [Send(\"process_query\", {\"query\": q}) for q in state['queries']]\n\n            def select_best_response(state: OverallState):\n                \"\"\"Select the best response from all generated responses.\"\"\"\n                logger.info(f\"Selecting best response from {len(state['responses'])} responses\")\n                formatted_responses = \"\\n\\n\".join(f\"Response {i}: {resp}\" for i, resp in enumerate(state['responses']))\n                prompt = best_response_prompt.format(query=state['query'], responses=formatted_responses)\n                response = self.llm.with_structured_output(BestResponse).invoke(prompt)\n                best_response = state['responses'][response.id]\n                logger.info(f\"Selected best response (index {response.id})\")\n                return {\"best_response\": best_response}\n\n            # Add nodes\n            workflow.add_node(\"generate_queries\", generate_queries)\n            workflow.add_node(\"process_query\", process_query)\n            workflow.add_node(\"select_best_response\", select_best_response)\n\n            # Define edges\n            workflow.add_edge(START, \"generate_queries\")\n            workflow.add_conditional_edges(\n                \"generate_queries\",\n                continue_to_processing,\n                [\"process_query\"]\n            )\n            workflow.add_edge(\"process_query\", \"select_best_response\")\n            workflow.add_edge(\"select_best_response\", END)\n\n            # Initialize Redis checkpointer\n            logger.info(\"Initializing Redis checkpointer\")\n            with RedisSaver.from_conn_url(REDIS_URL) as checkpointer:\n                self.checkpointer = checkpointer\n                self.workflow_app = workflow.compile(checkpointer=self.checkpointer)\n            logger.info(\"Workflow compilation completed with Redis checkpoint support\")\n\n        except Exception as e:\n            logger.error(f\"Error in setup_workflow: {str(e)}\")\n            raise\n\n    def store_thread_id(self, session_id: str, thread_id: str):\n        \"\"\"Store the thread_id in Redis for a given session_id.\"\"\"\n        redis_key = f\"query_thread_id_{session_id}\"\n        try:\n            logger.info(f\"Storing thread ID {thread_id} for session {session_id}\")\n            r = redis.Redis.from_url(url=REDIS_URL, decode_responses=True)\n            r.set(redis_key, thread_id)\n        except Exception as e:\n            logger.error(f\"Error saving thread ID to Redis: {str(e)}\")\n            raise\n\n    def get_thread_id(self, session_id: str) -> Optional[str]:\n        \"\"\"Retrieve the thread_id from Redis for a given session_id.\"\"\"\n        redis_key = f\"query_thread_id_{session_id}\"\n        try:\n            r = redis.Redis.from_url(url=REDIS_URL, decode_responses=True)\n            thread_id = r.get(redis_key)\n            logger.info(f\"Retrieved thread ID {thread_id} for session {session_id}\")\n            return thread_id\n        except Exception as e:\n            logger.error(f\"Error retrieving thread ID from Redis: {str(e)}\")\n            return None\n\n    def process_query(self, query: str, session_id: str = None):\n        \"\"\"Process queries with Redis checkpoint and streaming support.\"\"\"\n        try:\n            if not self.workflow_app:\n                raise ValueError(\"Workflow is not initialized.\")\n\n            logger.info(f\"Processing query: {query}\")\n\n            # Use provided session_id or generate one\n            session_id = session_id or str(time.time())\n            logger.info(f\"Using session ID: {session_id}\")\n\n            # Get or create thread ID\n            thread_id = self.get_thread_id(session_id)\n            if not thread_id:\n                thread_id = f\"query-thread-{session_id}\"\n                self.store_thread_id(session_id, thread_id)\n            logger.info(f\"Using thread ID: {thread_id}\")\n\n            # Configure thread settings\n            config = {\n                \"configurable\": {\n                    \"thread_id\": thread_id,\n                    \"checkpoint_ns\": \"query\",\n                    \"recursion_limit\": 2\n                }\n            }\n\n            # Initialize state\n            initial_state = {\n                \"query\": query,\n                \"queries\": [],\n                \"responses\": [],\n                \"best_response\": \"\"\n            }\n\n            logger.info(\"Starting workflow execution with streaming\")\n\n            # Stream the workflow execution\n            for step in self.workflow_app.stream(initial_state, config):\n                logger.info(f\"Step output: {step}\")\n                # Print progress information\n                if \"generate_queries\" in step:\n                    print(\"\\nGenerated queries:\", step[\"generate_queries\"].get(\"queries\", []))\n                elif \"process_query\" in step:\n                    print(\"\\nGenerated response:\", step[\"process_query\"].get(\"responses\", [])[0] if step[\"process_query\"].get(\"responses\") else \"\")\n                elif \"select_best_response\" in step:\n                    if \"best_response\" in step[\"select_best_response\"]:\n                        return step[\"select_best_response\"][\"best_response\"]\n\n            return \"No response generated.\"\n\n        except Exception as e:\n            logger.error(f\"Error in process_query: {str(e)}\")\n            return f\"Error processing query: {str(e)}\"\n\n# Example usage\nif __name__ == \"__main__\":\n    try:\n        logger.info(\"Starting query workflow example\")\n        workflow = QueryWorkflow()\n        query = \"What  was my last quetion?\"\n        session_id = '123456789'\n        result = workflow.process_query(query, session_id)\n        print(\"\\nBest Response:\")\n        print(result)\n    except Exception as e:\n        logger.error(f\"Main execution error: {str(e)}\")\nError Message and Stack Trace (if applicable)\ni am able to get the first quetion asnwer.\n\nbut when i m asking the follow up quetion its not able to recognise the history .even i printed out all my state.snapshots. do we have any other way to maje this working?\nDescription\nI'm working on building a custom workflow where a user asks a question.\n\nWe generate three different queries.\nEach query runs in parallel.\nThe outputs are combined and passed to the final LLM to generate an answer.\n\nHowever, how is memory handled in this process?\nWhen a follow-up question is asked, the system generates three new queries instead of leveraging previous context from memory. Should we incorporate an additional agent, planner, or any other mechanism to handle this more effectively? Looking for suggestions.\nmy working code with memory\n\nlogger = Logger.get_logger()\n\n# Simplified state type\nclass QueryState(TypedDict):\n    query: str\n    answer: str\n\nclass QueryWorkflow:\n    def __init__(self):\n        self.llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n        self.workflow_app = None\n        self.checkpointer = None\n        self.setup_workflow()\n\n    def setup_workflow(self):\n        \"\"\"Set up workflow with Redis checkpoint support.\"\"\"\n        logger.info(\"Setting up query workflow...\")\n\n        workflow = StateGraph(MessagesState)\n\n        def get_answer(state: MessagesState):\n            \"\"\"Get answer from LLM.\"\"\"\n            messages = state[\"messages\"]\n            response = self.llm.invoke(messages)\n            return {\"messages\": [response]}\n\n        # Add nodes\n        workflow.add_node(\"get_answer\", get_answer)\n        # workflow.add_node(\"translate_hindi\", translate_to_hindi)\n\n        # Update edges\n        workflow.add_edge(START, \"get_answer\")\n        workflow.add_edge(\"get_answer\", END)\n        # workflow.add_edge(\"translate_hindi\", END)\n\n        # Initialize Redis checkpointer\n        with RedisSaver.from_conn_url(REDIS_URL) as checkpointer:\n            self.checkpointer = checkpointer\n            self.workflow_app = workflow.compile(checkpointer=self.checkpointer)\n\n        logger.info(\"Query workflow setup completed with Redis checkpoint support\")\n\n    def store_thread_id(self, session_id: str, thread_id: str):\n        \"\"\"Store the thread_id in Redis for a given session_id.\"\"\"\n        redis_key = f\"query_thread_id_{session_id}\"\n        try:\n            r = redis.Redis.from_url(url=REDIS_URL, decode_responses=True)\n            r.set(redis_key, thread_id)\n        except Exception as e:\n            logger.error(f\"Error saving thread ID to Redis: {e}\")\n\n    def get_thread_id(self, session_id: str) -> Optional[str]:\n        \"\"\"Retrieve the thread_id from Redis for a given session_id.\"\"\"\n        redis_key = f\"query_thread_id_{session_id}\"\n        try:\n            r = redis.Redis.from_url(url=REDIS_URL, decode_responses=True)\n            return r.get(redis_key)\n        except Exception as e:\n            logger.error(f\"Error retrieving thread ID from Redis: {e}\")\n            return None\n\n    def process_query(self, query: str, session_id: str = None):\n        \"\"\"Process a query with Redis checkpoint and streaming support.\"\"\"\n        try:\n            if not self.workflow_app:\n                raise ValueError(\"Workflow is not initialized.\")\n\n            # Use provided session_id or generate one\n            session_id = session_id or str(time.time())\n\n            # Get or create thread ID\n            thread_id = self.get_thread_id(session_id)\n            if not thread_id:\n                thread_id = f\"query-thread-{session_id}\"\n                self.store_thread_id(session_id, thread_id)\n\n            # Configure thread settings\n            config = {\"configurable\": {\"thread_id\": thread_id, \"recursion_limit\": 2}}\n\n            # Create input message\n            input_messages = [HumanMessage(content=query)]\n\n            # Initialize streaming variables\n            first = True\n            gathered_response = None\n\n            # Stream the response\n            for msg, _ in self.workflow_app.stream(\n                {\"messages\": input_messages}, config, stream_mode=\"messages\"\n            ):\n                if msg.content and not isinstance(msg, HumanMessage):\n                    if isinstance(msg, AIMessageChunk):\n                        if first:\n                            gathered_response = msg\n                            first = False\n                        else:\n                            gathered_response = gathered_response + msg\n\n            # Get workflow state for debugging/monitoring\n            print('thread_id is >> ', thread_id)\n            state_graph = self.workflow_app.get_state(config)\n            print(\"state_graph is \", state_graph)\n            print(\"state history is \", list(self.workflow_app.get_state_history(config)))\n\n            if gathered_response:\n                return gathered_response.content\n\n            logger.warning(\"No AI response received from workflow\")\n            return \"No response received from workflow.\"\n\n        except Exception as e:\n            logger.error(f\"Error in query workflow: {e}\")\n            return f\"Error processing query: {str(e)}\"\n\n\n# Example usage:\nif __name__ == \"__main__\":\n    workflow = QueryWorkflow()\n\n    # First question\n    result1 = workflow.process_query(\n        query=\"What was my last question?\",\n        session_id=\"02020\"\n    )\n    print(f\"First Result: {result1}\")\n\n\n\n\n\nworking code is simple but why im not getting it wirth parallel nodes ?\nSystem Info\nim using ubantu 22.04", "created_at": "2025-02-11", "closed_at": "2025-02-26", "labels": [], "State": "closed", "Author": "akashAD98"}
{"issue_number": 3372, "issue_title": "In Langgraph studio the subgraph part is not shown when we invoke the subgraphs in the parent graph's node", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n# Define subgraph\nclass SubgraphState(TypedDict):\n    # note that none of these keys are shared with the parent graph state\n    bar: str\n    baz: str\n\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"baz\": \"baz\"}\n\n\ndef subgraph_node_2(state: SubgraphState):\n    return {\"bar\": state[\"bar\"] + state[\"baz\"]}\n\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph = subgraph_builder.compile()\n\n\n# Define parent graph\nclass ParentState(TypedDict):\n    foo: str\n\n\ndef node_1(state: ParentState):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\n\ndef node_2(state: ParentState):\n    # transform the state to the subgraph state\n    response = subgraph.invoke({\"bar\": state[\"foo\"]})\n    # transform response back to the parent state\n    return {\"foo\": response[\"bar\"]}\n\n\nbuilder = StateGraph(ParentState)\nbuilder.add_node(\"node_1\", node_1)\n# note that instead of using the compiled subgraph we are using `node_2` function that is calling the subgraph\nbuilder.add_node(\"node_2\", node_2)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\ngraph = builder.compile()\n\n#In the above case the subgraph does'nt visualize. But here it does:\n\n\nfrom langgraph.graph import START, StateGraph\nfrom typing import TypedDict\n\n\n# Define subgraph\nclass SubgraphState(TypedDict):\n    foo: str  # note that this key is shared with the parent graph state\n    bar: str\n\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"bar\": \"bar\"}\n\n\ndef subgraph_node_2(state: SubgraphState):\n    # note that this node is using a state key ('bar') that is only available in the subgraph\n    # and is sending update on the shared state key ('foo')\n    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph = subgraph_builder.compile()\n\n\n# Define parent graph\nclass ParentState(TypedDict):\n    foo: str\n\n\ndef node_1(state: ParentState):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\n\nbuilder = StateGraph(ParentState)\nbuilder.add_node(\"node_1\", node_1)\n# note that we're adding the compiled subgraph as a node to the parent graph\nbuilder.add_node(\"node_2\", subgraph)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\ngraph = builder.compile()\nError Message and Stack Trace (if applicable)\nNo error\nDescription\nI am using langgraph cli and langsmith to visual the graph using studio but when invoking a subgraph from parent node. The UI does'nt expand to subgraph level.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:22:58 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T8132\nPython Version:  3.11.9 (v3.11.9:de54cf5be3, Apr  2 2024, 07:12:50) [Clang 13.0.0 (clang-1300.0.29.30)]\n\nPackage Information\n\nlangchain_core: 0.3.34\nlangchain: 0.3.18\nlangchain_community: 0.3.17\nlangsmith: 0.3.8\nlangchain_ollama: 0.2.3\nlangchain_openai: 0.3.4\nlangchain_text_splitters: 0.3.6\nlanggraph_api: 0.0.22\nlanggraph_cli: 0.1.71\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.51\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nclick: 8.1.8\ncryptography: 44.0.0\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\njsonschema-rs: 0.29.1\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.18: Installed. No version info available.\nlanggraph: 0.2.70\nlanggraph-checkpoint: 2.0.12\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<2,>=1.26.4;: Installed. No version info available.\nnumpy<3,>=1.26.2;: Installed. No version info available.\nollama: 0.4.7\nopenai<2.0.0,>=1.58.1: Installed. No version info available.\norjson: 3.10.15\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npyjwt: 2.10.1\npytest: Installed. No version info available.\npython-dotenv: 1.0.1\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: Installed. No version info available.\nSQLAlchemy<3,>=1.4: Installed. No version info available.\nsse-starlette: 2.2.1\nstarlette: 0.45.3\nstructlog: 25.1.0\ntenacity: 9.0.0\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nuvicorn: 0.34.0\nwatchfiles: 1.0.4\nzstandard: 0.23.0\n", "created_at": "2025-02-10", "closed_at": null, "labels": ["question"], "State": "open", "Author": "arya18mak"}
{"issue_number": 3369, "issue_title": "`custom` stream_mode does not update state", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport time\nimport uuid\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph, START\nfrom langgraph.types import Command, StreamWriter\nfrom typing_extensions import TypedDict\n\n\nclass State(TypedDict):\n    foo: str\n\n\ndef node_a(state: State, writer: StreamWriter):\n    # Simulate non-langchain LLM call\n    response = \"\"\n    for chunk in [\"hello\", \"world\", \"foo\", \"bar\"]:\n        time.sleep(1)\n        response += chunk\n        writer({\"foo\": response})\n    return {\"foo\": response}\n\n\nbuilder = StateGraph(State)\nbuilder.add_edge(START, \"node_a\")\nbuilder.add_node(node_a)\nconfig = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\nmemory = MemorySaver()\ngraph = builder.compile(checkpointer=memory)\nstate = State(foo=\"\")\nfor chunk in graph.stream(state, stream_mode=\"custom\", config=config):\n    print(\"== graph state: \", graph.get_state(config).values)\n    print(\"== Returned chunk: \", chunk, \"\\n\\n\")\nError Message and Stack Trace (if applicable)\n== graph state:  {'foo': ''}\n== Returned chunk:  {'foo': 'hello'}\n\n\n== graph state:  {'foo': ''}\n== Returned chunk:  {'foo': 'helloworld'}\n\n\n== graph state:  {'foo': ''}\n== Returned chunk:  {'foo': 'helloworldfoo'}\n\n\n== graph state:  {'foo': 'helloworldfoobar'}\n== Returned chunk:  {'foo': 'helloworldfoobar'}\nDescription\nI want to build a graph that has non-langchain LLM streaming, as well as interrupts.\nFor interrupts, as per the documentation and this GH discussion, I need the updates streaming mode (although it never mentions that updates is required).\nFor streaming LLM tokens, as per the documentation I need to use the custom mode as well.\nSince updates and custom don't return the full state, I'm using get_state to avoid having to rebuild the whole state every time before sending it back to the caller.\nWhat I expect:\nget_state() returns the updated state for every StreamWriter() call.\nWhat happens:\ncalling StreamWriter() does not update the state, which makes get_state useless for streaming.\nSystem Info\nNot using langchain", "created_at": "2025-02-10", "closed_at": "2025-02-10", "labels": ["question"], "State": "closed", "Author": "aguadoenzo"}
{"issue_number": 3366, "issue_title": "LangGraph docker compose 100% CPU usage", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\npython3.11 -m venv .venv\nsource .venv/bin/activate\n\npython3 --version\nPython 3.11.11\n\npip install -U langgraph-cli\n\nlanggraph build -t my-image\n\ndocker compose up -d\nError Message and Stack Trace (if applicable)\n#log from container\nExecuting task: docker logs --tail 1000 -f 17d8796d5938042773496dd98afee1314da7a2a5e0b477f813b68f12bf46fb1d \n\n2025-02-09T19:36:33.445133Z [info     ] Using auth of type=noop        [langgraph_api.auth.middleware] api_revision=7d5eecb api_variant=local\n2025-02-09T19:36:33.452650Z [info     ] Started server process [1]     [uvicorn.error] api_revision=7d5eecb api_variant=local color_message='Started server process [\\x1b[36m%d\\x1b[0m]'\n2025-02-09T19:36:33.452835Z [info     ] Waiting for application startup. [uvicorn.error] api_revision=7d5eecb api_variant=local\n2025-02-09T19:36:33.453088Z [warning  ] No license key found, running in test mode with LangSmith API key. For production use, set LANGGRAPH_CLOUD_LICENSE_KEY in environment. [langgraph_license.validation] api_revision=7d5eecb api_variant=local\n2025-02-09T19:36:33.873404Z [info     ] HTTP Request: GET https://api.smith.langchain.com/auth?langgraph-api=true \"HTTP/1.1 200 OK\" [httpx] api_revision=7d5eecb api_variant=local\n2025-02-09T19:36:33.927013Z [info     ] No LANGGRAPH_STORE configuration found, using default configuration [langgraph_storage.database] api_revision=7d5eecb api_variant=local\n2025-02-09T19:36:33.929783Z [info     ] Postgres pool stats            [langgraph_storage.database] api_revision=7d5eecb api_variant=local connections_ms=13 connections_num=1 pool_available=1 pool_max=150 pool_min=1 pool_size=1 requests_num=1 requests_waiting=0 usage_ms=1\n2025-02-09T19:36:33.932606Z [info     ] Redis pool stats               [langgraph_storage.redis] api_revision=7d5eecb api_variant=local idle_connections=1 in_use_connections=0 max_connections=500\n2025-02-09T19:36:33.942666Z [info     ] Registering graph with id 'my_agent' [langgraph_api.graph] api_revision=7d5eecb api_variant=local graph_id=my_agent\n2025-02-09T19:36:33.946778Z [info     ] Starting metadata loop         [langgraph_api.metadata] api_revision=7d5eecb api_variant=local\n2025-02-09T19:36:33.947942Z [info     ] Application startup complete.  [uvicorn.error] api_revision=7d5eecb api_variant=local\n2025-02-09T19:36:33.948665Z [info     ] Starting 10 background workers [langgraph_api.queue] api_revision=7d5eecb api_variant=local\n2025-02-09T19:36:33.949463Z [info     ] Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit) [uvicorn.error] api_revision=7d5eecb api_variant=local color_message='Uvicorn running on \\x1b[1m%s://%s:%d\\x1b[0m (Press CTRL+C to quit)'\n2025-02-09T19:36:33.950474Z [info     ] Worker stats                   [langgraph_api.queue] active=0 api_revision=7d5eecb api_variant=local available=10 max=10\n2025-02-09T19:36:33.955662Z [info     ] Queue stats                    [langgraph_api.queue] api_revision=7d5eecb api_variant=local med_age_secs=None min_age_secs=None n_pending=0 n_running=0\n2025-02-09T19:36:33.982250Z [info     ] Sweeped runs                   [langgraph_api.queue] api_revision=7d5eecb api_variant=local run_ids=[]\n2025-02-09T19:36:34.121100Z [info     ] HTTP Request: POST https://api.smith.langchain.com/v1/metadata/submit \"HTTP/1.1 204 No Content\" [httpx] api_revision=7d5eecb api_variant=local\n2025-02-09T19:36:36.935696Z [info     ] GET /ok 200 0ms                [langgraph_api.server] api_revision=7d5eecb api_variant=local latency_ms=0 method=GET path=/ok path_params={} proto=1.1 query_string= req_header={'accept-encoding': 'identity', 'host': 'localhost:8000', 'user-agent': 'Python-urllib/3.11', 'connection': 'close'} res_header={'content-length': '11', 'content-type': 'application/json'} route=None status=200\n2025-02-09T19:36:42.057932Z [info     ] GET /ok 200 0ms                [langgraph_api.server] api_revision=7d5eecb api_variant=local latency_ms=0 method=GET path=/ok path_params={} proto=1.1 query_string= req_header={'accept-encoding': 'identity', 'host': 'localhost:8000', 'user-agent': 'Python-urllib/3.11', 'connection': 'close'} res_header={'content-length': '11', 'content-type': 'application/json'} route=None status=200\n2025-02-09T19:36:47.202318Z [info     ] GET /ok 200 0ms                [langgraph_api.server] api_revision=7d5eecb api_variant=local latency_ms=0 method=GET path=/ok path_params={} proto=1.1 query_string= req_header={'accept-encoding': 'identity', 'host': 'localhost:8000', 'user-agent': 'Python-urllib/3.11', 'connection': 'close'} res_header={'content-length': '11', 'content-type': 'application/json'} route=None status=200\n2025-02-09T19:36:52.351129Z [info     ] GET /ok 200 0ms                [langgraph_api.server] api_revision=7d5eecb api_variant=local latency_ms=0 method=GET path=/ok path_params={} proto=1.1 query_string= req_header={'accept-encoding': 'identity', 'host': 'localhost:8000', 'user-agent': 'Python-urllib/3.11', 'connection': 'close'} res_header={'content-length': '11', 'content-type': 'application/json'} route=None status=200\n2025-02-09T19:36:57.504541Z [info     ] GET /ok 200 0ms                [langgraph_api.server] api_revision=7d5eecb api_variant=local latency_ms=0 method=GET path=/ok path_params={} proto=1.1 query_string= req_header={'accept-encoding': 'identity', 'host': 'localhost:8000', 'user-agent': 'Python-urllib/3.11', 'connection': 'close'} res_header={'content-length': '11', 'content-type': 'application/json'} route=None status=200\nDescription\nI have an issue with 100% CPU running self-hosted Langgraph with docker-compose.\nHere is an example that you can easily run https://github.com/TimurMunykin/langgraph_issue\nSystem Info\nroot@17d8796d5938:/deps/__outer_langgraph_issue/src# python -m langchain_core.sys_info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP PREEMPT_DYNAMIC Mon Dec  4 10:03:25 UTC 2023\nPython Version:  3.11.11 (main, Feb  4 2025, 04:55:09) [GCC 12.2.0]\n\nPackage Information\n\nlangchain_core: 0.3.34\nlangsmith: 0.3.7\nlanggraph_api: 0.0.22\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.51\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\ncryptography: 44.0.0\nhttpx: 0.28.1\njsonpatch<2.0,>=1.33: Installed. No version info available.\njsonschema-rs: 0.29.1\nlanggraph: 0.2.70\nlanggraph-checkpoint: 2.0.10\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\norjson: 3.10.15\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npyjwt: 2.10.1\npytest: Installed. No version info available.\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrich: Installed. No version info available.\nsse-starlette: 2.1.3\nstarlette: 0.45.3\nstructlog: 25.1.0\ntenacity: 9.0.0\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nuvicorn: 0.34.0\nwatchfiles: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-02-09", "closed_at": "2025-02-20", "labels": [], "State": "closed", "Author": "TimurMunykin"}
{"issue_number": 3365, "issue_title": "Segment the documentation into legacy content vs modern best practices", "issue_body": "Discussed in #3363\n\nOriginally posted by jtayl222 February  9, 2025\nReferences:\n\n\ud83d\udcd6 LangGraph\u2019s StateGraph and Conversation History\n\ud83d\udcd6 Why You Shouldn't Use @tool in StateGraph Workflows\n\nProblem:\nGiven the evolution of StateGraph and best practices, LangGraph has moved away from older, less structured methodologies. However, if the documentation and tutorials still mix legacy approaches with current best practices, it could create confusion for both new and existing users.\nWhat is a reasonable plan to fix this problem?\nA structured approach would involve segmenting the documentation into legacy content and modern best practices. Here\u2019s how we could do it:\n\nCategorizing Existing Content\nIdentify and label older documentation that references legacy approaches, such as:\nAgent-based memory (e.g., ConversationBufferMemory)\nUsing @tool in StateGraph workflows\nNon-deterministic execution patterns\nSeparate modern best practices, such as:\nExplicit state transitions\nStateGraph-first workflows\nAvoiding @tool and implicit memory storage\nState management via reducers and structured state dictionaries\nUpdating the Official Documentation\nIntroduce a clear distinction in the LangGraph documentation:\n\u2705 Best Practices Section \u2192 Features modern, structured approaches using StateGraph.\n\u274c Legacy Section (with disclaimers) \u2192 Outlines older methods that are no longer recommended.\nAdd clear notes to older methods indicating they are legacy approaches with links to updated best practices.\nImproving Community Awareness\nDocumentation summarizing:\nThe issues with mixed documentation.\nWhy StateGraph-first workflows are now the recommended approach.\nA comparison table showing \"Old vs. New\" best practices.\n\nConclusion\nBy implementing a structured plan to distinguish legacy from best practices, we can improve clarity, reduce confusion, and help new developers adopt StateGraph workflows correctly. ", "created_at": "2025-02-09", "closed_at": "2025-02-19", "labels": [], "State": "closed", "Author": "jtayl222"}
{"issue_number": 3362, "issue_title": "When using subgraphs and Command, the output of the final node in the subgraph is not displayed", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing import Annotated, TypedDict\n\nfrom typing_extensions import Literal\n\nfrom langgraph.graph import END, StateGraph\nfrom langgraph.types import Command\n\n\ndef my_reducer(a: str, b: str | None) -> int:\n    if b is not None:\n        return b\n    return a\n\n\nclass State(TypedDict):\n    # node_name: Annotated[list[str], add]\n    node_name: Annotated[str, my_reducer]\n    foo: str\n\n\ndef subgraph_node_1(state: State) -> Command[Literal[\"subgraph_node_2\"]]:\n    return Command(\n        goto=\"subgraph_node_2\",\n        update={\n            \"node_name\": \"subgraph_node_1\",\n            \"foo\": \"Update at subgraph_node_1!\",\n        },\n    )\n\n\ndef subgraph_node_2(state: State) -> Command:\n    return Command(\n        goto=\"node_3\",\n        update={\"node_name\": \"subgraph_node_2\"},\n        graph=Command.PARENT,\n    )\n\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.set_entry_point(\"subgraph_node_1\")\nsubgraph = subgraph_builder.compile()\n\n\n# Define main graph\ndef node_1(state: State) -> Command[Literal[\"node_2\"]]:\n    return Command(\n        goto=\"node_2\",\n        update={\"node_name\": \"node_1\"},\n    )\n\n\ndef node_3(state: State) -> Command[Literal[\"__end__\"]]:\n    return Command(\n        goto=END,\n        update={\"node_name\": \"node_3\"},\n    )\n\n\nmain_builder = StateGraph(State)\nmain_builder.add_node(\"node_1\", node_1)\nmain_builder.add_node(\"node_2\", subgraph)\nmain_builder.add_node(\"node_3\", node_3)\nmain_builder.set_entry_point(\"node_1\")\nmain_graph = main_builder.compile()\n\n\n# Build subgraph\nwith open(\"graph_use_command.md\", \"w\") as file:\n    file.write(f\"\\n{main_graph.get_graph(xray=1).draw_mermaid()}\")\n\ninitial = {\"node_name\": \"__start__\"}\nfor chunk in main_graph.stream(initial, stream_mode=\"values\", subgraphs=True):\n    print(chunk)\nError Message and Stack Trace (if applicable)\n\nDescription\nThis issue is related to #3115.\nProblem\nWhen using Command in both the parent graph and the subgraph, running the stream method does not display the output of the final node in the subgraph.\nBelow is a diagram of the graph from the example code:\n\n\n\n%%{init: {'flowchart': {'curve': 'linear'}}}%%\ngraph TD;\n\t__start__([<p>__start__</p>]):::first\n\tnode_1(node_1)\n\tnode_2_subgraph_node_1(subgraph_node_1)\n\tnode_2_subgraph_node_2(subgraph_node_2)\n\tnode_3(node_3)\n\t__end__([<p>__end__</p>]):::last\n\t__start__ --> node_1;\n\tnode_1 --> node_2_subgraph_node_1;\n\tnode_2_subgraph_node_2 --> node_3;\n\tnode_3 --> __end__;\n\tsubgraph node_2\n\tnode_2_subgraph_node_1 --> node_2_subgraph_node_2;\n\tend\n\tclassDef default fill:#f2f0ff,line-height:1.2\n\tclassDef first fill-opacity:0\n\tclassDef last fill:#bfb6fc\n\n\n\n\n\n\n\n\n Loading\n\n\n\nBelow is the output from the example code. Notice that the result for the subgraph node subgaph_node_2 is not displayed:\n((), {'node_name': '__start__'})\n((), {'node_name': 'node_1'})\n(('node_2:5198e6fe-5d96-32e1-6f68-ce104abdea04',), {'node_name': 'node_1'})\n(('node_2:5198e6fe-5d96-32e1-6f68-ce104abdea04',), {'node_name': 'subgraph_node_1', 'foo': 'Update at subgraph_node_1!'})\n((), {'node_name': 'subgraph_node_2', 'foo': 'Update at subgraph_node_1!'})\n((), {'node_name': 'node_3', 'foo': 'Update at subgraph_node_1!'})\n\nExpected Behavior\nIt is expected that the output of the final node in the subgraph is displayed.\nNote that when using add_edge instead of Command, the expected behavior is achieved. Below is the example code rewritten using add_edge:\nfrom typing import TypedDict\n\nfrom langgraph.graph import END, START, StateGraph\n\n\nclass State(TypedDict):\n    node_name: str\n    foo: str\n\n\ndef subgraph_node_1(state: State):\n    return {\n        \"node_name\": \"subgraph_node_1\",\n        \"foo\": \"Update at subgraph_node_1!\",\n    }\n\n\ndef subgraph_node_2(state: State):\n    return {\"node_name\": \"subgraph_node_2\"}\n\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph_builder.add_edge(\"subgraph_node_2\", END)\nsubgraph = subgraph_builder.compile()\n\n\n# Define main graph\ndef node_1(state: State):\n    return {\"node_name\": \"node_1\"}\n\n\ndef node_3(state: State):\n    return {\"node_name\": \"node_3\"}\n\n\nmain_builder = StateGraph(State)\nmain_builder.add_node(\"node_1\", node_1)\nmain_builder.add_node(\"node_2\", subgraph)\nmain_builder.add_node(\"node_3\", node_3)\nmain_builder.add_edge(START, \"node_1\")\nmain_builder.add_edge(\"node_1\", \"node_2\")\nmain_builder.add_edge(\"node_2\", \"node_3\")\nmain_builder.add_edge(\"node_3\", END)\nmain_graph = main_builder.compile()\n\n\n# Build subgraph\nwith open(\"graph_use_edge.md\", \"w\") as file:\n    file.write(f\"```mermaid\\n{main_graph.get_graph(xray=1).draw_mermaid()}```\")\n\ninitial = {\"node_name\": \"__start__\"}\nfor chunk in main_graph.stream(initial, stream_mode=\"values\", subgraphs=True):\n    print(chunk)\nBelow is the output, where the result for the subgraph node subgraph_node_2 is correctly displayed:\n((), {'node_name': '__start__'})\n((), {'node_name': 'node_1'})\n(('node_2:0cf73922-e372-fc29-ef53-ba1c568bb221',), {'node_name': 'node_1'})\n(('node_2:0cf73922-e372-fc29-ef53-ba1c568bb221',), {'node_name': 'subgraph_node_1', 'foo': 'Update at subgraph_node_1!'})\n(('node_2:0cf73922-e372-fc29-ef53-ba1c568bb221',), {'node_name': 'subgraph_node_2', 'foo': 'Update at subgraph_node_1!'})\n((), {'node_name': 'subgraph_node_2', 'foo': 'Update at subgraph_node_1!'})\n((), {'node_name': 'node_3', 'foo': 'Update at subgraph_node_1!'})\n\nSystem Info\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #44~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Jun 18 14:36:16 UTC 2\n> Python Version:  3.12.5 | packaged by Anaconda, Inc. | (main, Sep 12 2024, 18:27:27) [GCC 11.2.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.28\n> langchain: 0.3.7\n> langchain_community: 0.3.5\n> langsmith: 0.1.136\n> langchain_aws: 0.2.10\n> langchain_fireworks: 0.2.1\n> langchain_openai: 0.2.11\n> langchain_text_splitters: 0.3.0\n> langgraph_sdk: 0.1.48\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp: 3.10.5\n> async-timeout: Installed. No version info available.\n> boto3: 1.35.90\n> dataclasses-json: 0.6.7\n> fireworks-ai: 0.15.7\n> httpx: 0.27.0\n> httpx-sse: 0.4.0\n> jsonpatch: 1.33\n> numpy: 1.26.4\n> openai: 1.56.2\n> orjson: 3.10.9\n> packaging: 24.1\n> pydantic: 2.9.2\n> pydantic-settings: 2.6.1\n> PyYAML: 6.0.2\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> SQLAlchemy: 2.0.34\n> tenacity: 8.2.3\n> tiktoken: 0.8.0\n> typing-extensions: 4.11.0\n\nLangGraph Version\npip list | grep langgraph\n\n> langgraph                         0.2.70\n> langgraph-checkpoint              2.0.12\n> langgraph-sdk                     0.1.48\n", "created_at": "2025-02-09", "closed_at": null, "labels": ["investigate"], "State": "open", "Author": "ren8k"}
{"issue_number": 3344, "issue_title": "State Transitions Not working as Expected in Agent Setup", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport logging\nfrom langgraph.graph import MessagesState, StateGraph\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.llm import HumanMessage\n\n# Define the AgentState class\nclass AgentState(MessagesState):\n    error: bool\n    error_msg: str\n    code: str\n    external_libs: bool\n\n# Define the nodes\ndef code_generator(state: AgentState):\n    prompt = \"Can you provide me with Python code for a function that takes a string as input and returns True if the string is a palindrome and False otherwise? The function should not use any libraries.\"\n    msg = [HumanMessage(content=prompt)]\n    function_str = \"def is_palindrome(s): return s == s[::-1]\"\n    return {\"messages\": [function_str], \"code\": function_str}\n\ndef validate_code(state: AgentState):\n    function = state[\"code\"]\n    try:\n        exec(function)\n        return {\"error\": False}\n    except Exception as e:\n        return {\"error\": True, \"error_msg\": str(e)}\n\ndef validate_external_libs(state: AgentState):\n    return {\"external_libs\": False}\n\ndef install_external_libs(state: AgentState):\n    return {\"external_libs\": False}\n\ndef correct_python_erros(state: AgentState):\n    corrected_function_str = \"def is_palindrome(s): return s == s[::-1]\"\n    return {\"messages\": [corrected_function_str], \"code\": corrected_function_str, \"error\": False}\n\ndef execute_python_function(state: AgentState):\n    function = state[\"code\"]\n    exec(function)\n    return \"Done\"\n\n# Define the edges\ndef check_code(state: AgentState):\n    error = state.get(\"error\", None)\n    if error:\n        return \"correct_python_erros\"\n    else:\n        return \"validate_external_libs\"\n\ndef check_external_libs(state: AgentState):\n    external_libs = state.get(\"external_libs\", False)\n    if external_libs:\n        return \"install_external_libs\"\n    else:\n        return \"execute_python_function\"\n\n# Initialize the agent and pass the schema\nagent_init = StateGraph(AgentState)\nagent_init.add_node(\"code_generator\", code_generator)\nagent_init.add_node(\"validate_code\", validate_code)\nagent_init.add_node(\"validate_external_libs\", validate_external_libs)\nagent_init.add_node(\"install_external_libs\", install_external_libs)\nagent_init.add_node(\"correct_python_erros\", correct_python_erros)\nagent_init.add_node(\"execute_python_function\", execute_python_function)\n\n# Add the edges\nfrom langgraph.graph import START, END\nagent_init.add_edge(START, \"code_generator\")\nagent_init.add_edge(\"code_generator\", \"validate_code\")\nagent_init.add_conditional_edges(\"validate_code\", check_code)\nagent_init.add_edge(\"correct_python_erros\", \"validate_code\")\nagent_init.add_conditional_edges(\"validate_external_libs\", check_external_libs)\nagent_init.add_edge(\"install_external_libs\", \"execute_python_function\")\nagent_init.add_edge(\"execute_python_function\", END)\n\n# Create the agent\nmemory = MemorySaver()\nagent = agent_init.compile(checkpointer=memory)\n\n# Generate and display the diagram\nfrom IPython.display import Image, display\ndisplay(Image(agent.get_graph().draw_mermaid_png()))\nError Message and Stack Trace (if applicable)\nIncorrect daigram \nPlease cehck the diagram in the description\nDescription\nState Transitions Not Displayed Correctly in Mermaid Diagram (see the diagram at the end)\nDescription:\nWhen generating a mermaid diagram using agent.get_graph().draw_mermaid_png() in LangGraph, the state transitions are not displayed correctly. The transitions do not reflect the defined conditional logic between nodes.\nSteps to Reproduce:\nDefine an AgentState and nodes with transition logic as shown in the code snippet below.\nAdd nodes and conditional edges to the agent.\nGenerate the mermaid diagram with agent.get_graph().draw_mermaid_png().\nObserve the incorrect relationships in the generated diagram.\nDiagram\n\nSystem Info\nState Transitions Not Displayed Correctly in Mermaid Diagram (see the diagram at the end)\nDescription:\nWhen generating a mermaid diagram using agent.get_graph().draw_mermaid_png() in LangGraph, the state transitions are not displayed correctly. The transitions do not reflect the defined conditional logic between nodes.\nSteps to Reproduce:\nDefine an AgentState and nodes with transition logic as shown in the code snippet below.\nAdd nodes and conditional edges to the agent.\nGenerate the mermaid diagram with agent.get_graph().draw_mermaid_png().\nObserve the incorrect relationships in the generated diagram.\nDiagram\n\nCode\nimport logging\nfrom langgraph.graph import MessagesState, StateGraph\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.llm import HumanMessage\n\n# Define the AgentState class\nclass AgentState(MessagesState):\n    error: bool\n    error_msg: str\n    code: str\n    external_libs: bool\n\n# Define the nodes\ndef code_generator(state: AgentState):\n    prompt = \"Can you provide me with Python code for a function that takes a string as input and returns True if the string is a palindrome and False otherwise? The function should not use any libraries.\"\n    msg = [HumanMessage(content=prompt)]\n    function_str = \"def is_palindrome(s): return s == s[::-1]\"\n    return {\"messages\": [function_str], \"code\": function_str}\n\ndef validate_code(state: AgentState):\n    function = state[\"code\"]\n    try:\n        exec(function)\n        return {\"error\": False}\n    except Exception as e:\n        return {\"error\": True, \"error_msg\": str(e)}\n\ndef validate_external_libs(state: AgentState):\n    return {\"external_libs\": False}\n\ndef install_external_libs(state: AgentState):\n    return {\"external_libs\": False}\n\ndef correct_python_erros(state: AgentState):\n    corrected_function_str = \"def is_palindrome(s): return s == s[::-1]\"\n    return {\"messages\": [corrected_function_str], \"code\": corrected_function_str, \"error\": False}\n\ndef execute_python_function(state: AgentState):\n    function = state[\"code\"]\n    exec(function)\n    return \"Done\"\n\n# Define the edges\ndef check_code(state: AgentState):\n    error = state.get(\"error\", None)\n    if error:\n        return \"correct_python_erros\"\n    else:\n        return \"validate_external_libs\"\n\ndef check_external_libs(state: AgentState):\n    external_libs = state.get(\"external_libs\", False)\n    if external_libs:\n        return \"install_external_libs\"\n    else:\n        return \"execute_python_function\"\n\n# Initialize the agent and pass the schema\nagent_init = StateGraph(AgentState)\nagent_init.add_node(\"code_generator\", code_generator)\nagent_init.add_node(\"validate_code\", validate_code)\nagent_init.add_node(\"validate_external_libs\", validate_external_libs)\nagent_init.add_node(\"install_external_libs\", install_external_libs)\nagent_init.add_node(\"correct_python_erros\", correct_python_erros)\nagent_init.add_node(\"execute_python_function\", execute_python_function)\n\n# Add the edges\nfrom langgraph.graph import START, END\nagent_init.add_edge(START, \"code_generator\")\nagent_init.add_edge(\"code_generator\", \"validate_code\")\nagent_init.add_conditional_edges(\"validate_code\", check_code)\nagent_init.add_edge(\"correct_python_erros\", \"validate_code\")\nagent_init.add_conditional_edges(\"validate_external_libs\", check_external_libs)\nagent_init.add_edge(\"install_external_libs\", \"execute_python_function\")\nagent_init.add_edge(\"execute_python_function\", END)\n\n# Create the agent\nmemory = MemorySaver()\nagent = agent_init.compile(checkpointer=memory)\n\n# Generate and display the diagram\nfrom IPython.display import Image, display\ndisplay(Image(agent.get_graph().draw_mermaid_png()))\n", "created_at": "2025-02-07", "closed_at": "2025-02-07", "labels": [], "State": "closed", "Author": "jeffersonRosman"}
{"issue_number": 3343, "issue_title": "State Transitions Not working as Expected in Agent Setup", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport logging\nfrom langgraph.graph import MessagesState, StateGraph\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.llm import HumanMessage\n\n# Define the AgentState class\nclass AgentState(MessagesState):\n    error: bool\n    error_msg: str\n    code: str\n    external_libs: bool\n\n# Define the nodes\ndef code_generator(state: AgentState):\n    prompt = \"Can you provide me with Python code for a function that takes a string as input and returns True if the string is a palindrome and False otherwise? The function should not use any libraries.\"\n    msg = [HumanMessage(content=prompt)]\n    function_str = \"def is_palindrome(s): return s == s[::-1]\"\n    return {\"messages\": [function_str], \"code\": function_str}\n\ndef validate_code(state: AgentState):\n    function = state[\"code\"]\n    try:\n        exec(function)\n        return {\"error\": False}\n    except Exception as e:\n        return {\"error\": True, \"error_msg\": str(e)}\n\ndef validate_external_libs(state: AgentState):\n    return {\"external_libs\": False}\n\ndef install_external_libs(state: AgentState):\n    return {\"external_libs\": False}\n\ndef correct_python_erros(state: AgentState):\n    corrected_function_str = \"def is_palindrome(s): return s == s[::-1]\"\n    return {\"messages\": [corrected_function_str], \"code\": corrected_function_str, \"error\": False}\n\ndef execute_python_function(state: AgentState):\n    function = state[\"code\"]\n    exec(function)\n    return \"Done\"\n\n# Define the edges\ndef check_code(state: AgentState):\n    error = state.get(\"error\", None)\n    if error:\n        return \"correct_python_erros\"\n    else:\n        return \"validate_external_libs\"\n\ndef check_external_libs(state: AgentState):\n    external_libs = state.get(\"external_libs\", False)\n    if external_libs:\n        return \"install_external_libs\"\n    else:\n        return \"execute_python_function\"\n\n# Initialize the agent and pass the schema\nagent_init = StateGraph(AgentState)\nagent_init.add_node(\"code_generator\", code_generator)\nagent_init.add_node(\"validate_code\", validate_code)\nagent_init.add_node(\"validate_external_libs\", validate_external_libs)\nagent_init.add_node(\"install_external_libs\", install_external_libs)\nagent_init.add_node(\"correct_python_erros\", correct_python_erros)\nagent_init.add_node(\"execute_python_function\", execute_python_function)\n\n# Add the edges\nfrom langgraph.graph import START, END\nagent_init.add_edge(START, \"code_generator\")\nagent_init.add_edge(\"code_generator\", \"validate_code\")\nagent_init.add_conditional_edges(\"validate_code\", check_code)\nagent_init.add_edge(\"correct_python_erros\", \"validate_code\")\nagent_init.add_conditional_edges(\"validate_external_libs\", check_external_libs)\nagent_init.add_edge(\"install_external_libs\", \"execute_python_function\")\nagent_init.add_edge(\"execute_python_function\", END)\n\n# Create the agent\nmemory = MemorySaver()\nagent = agent_init.compile(checkpointer=memory)\n\n# Generate and display the diagram\nfrom IPython.display import Image, display\ndisplay(Image(agent.get_graph().draw_mermaid_png()))\nError Message and Stack Trace (if applicable)\nIncorrect daigram \nPlease cehck the diagram in the description\nDescription\nState Transitions Not Displayed Correctly in Mermaid Diagram (see the diagram at the end)\nDescription:\nWhen generating a mermaid diagram using agent.get_graph().draw_mermaid_png() in LangGraph, the state transitions are not displayed correctly. The transitions do not reflect the defined conditional logic between nodes.\nSteps to Reproduce:\nDefine an AgentState and nodes with transition logic as shown in the code snippet below.\nAdd nodes and conditional edges to the agent.\nGenerate the mermaid diagram with agent.get_graph().draw_mermaid_png().\nObserve the incorrect relationships in the generated diagram.\nDiagram\n\nSystem Info\nState Transitions Not Displayed Correctly in Mermaid Diagram (see the diagram at the end)\nDescription:\nWhen generating a mermaid diagram using agent.get_graph().draw_mermaid_png() in LangGraph, the state transitions are not displayed correctly. The transitions do not reflect the defined conditional logic between nodes.\nSteps to Reproduce:\nDefine an AgentState and nodes with transition logic as shown in the code snippet below.\nAdd nodes and conditional edges to the agent.\nGenerate the mermaid diagram with agent.get_graph().draw_mermaid_png().\nObserve the incorrect relationships in the generated diagram.\nDiagram\n\nCode\nimport logging\nfrom langgraph.graph import MessagesState, StateGraph\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.llm import HumanMessage\n\n# Define the AgentState class\nclass AgentState(MessagesState):\n    error: bool\n    error_msg: str\n    code: str\n    external_libs: bool\n\n# Define the nodes\ndef code_generator(state: AgentState):\n    prompt = \"Can you provide me with Python code for a function that takes a string as input and returns True if the string is a palindrome and False otherwise? The function should not use any libraries.\"\n    msg = [HumanMessage(content=prompt)]\n    function_str = \"def is_palindrome(s): return s == s[::-1]\"\n    return {\"messages\": [function_str], \"code\": function_str}\n\ndef validate_code(state: AgentState):\n    function = state[\"code\"]\n    try:\n        exec(function)\n        return {\"error\": False}\n    except Exception as e:\n        return {\"error\": True, \"error_msg\": str(e)}\n\ndef validate_external_libs(state: AgentState):\n    return {\"external_libs\": False}\n\ndef install_external_libs(state: AgentState):\n    return {\"external_libs\": False}\n\ndef correct_python_erros(state: AgentState):\n    corrected_function_str = \"def is_palindrome(s): return s == s[::-1]\"\n    return {\"messages\": [corrected_function_str], \"code\": corrected_function_str, \"error\": False}\n\ndef execute_python_function(state: AgentState):\n    function = state[\"code\"]\n    exec(function)\n    return \"Done\"\n\n# Define the edges\ndef check_code(state: AgentState):\n    error = state.get(\"error\", None)\n    if error:\n        return \"correct_python_erros\"\n    else:\n        return \"validate_external_libs\"\n\ndef check_external_libs(state: AgentState):\n    external_libs = state.get(\"external_libs\", False)\n    if external_libs:\n        return \"install_external_libs\"\n    else:\n        return \"execute_python_function\"\n\n# Initialize the agent and pass the schema\nagent_init = StateGraph(AgentState)\nagent_init.add_node(\"code_generator\", code_generator)\nagent_init.add_node(\"validate_code\", validate_code)\nagent_init.add_node(\"validate_external_libs\", validate_external_libs)\nagent_init.add_node(\"install_external_libs\", install_external_libs)\nagent_init.add_node(\"correct_python_erros\", correct_python_erros)\nagent_init.add_node(\"execute_python_function\", execute_python_function)\n\n# Add the edges\nfrom langgraph.graph import START, END\nagent_init.add_edge(START, \"code_generator\")\nagent_init.add_edge(\"code_generator\", \"validate_code\")\nagent_init.add_conditional_edges(\"validate_code\", check_code)\nagent_init.add_edge(\"correct_python_erros\", \"validate_code\")\nagent_init.add_conditional_edges(\"validate_external_libs\", check_external_libs)\nagent_init.add_edge(\"install_external_libs\", \"execute_python_function\")\nagent_init.add_edge(\"execute_python_function\", END)\n\n# Create the agent\nmemory = MemorySaver()\nagent = agent_init.compile(checkpointer=memory)\n\n# Generate and display the diagram\nfrom IPython.display import Image, display\ndisplay(Image(agent.get_graph().draw_mermaid_png()))\n", "created_at": "2025-02-07", "closed_at": "2025-02-07", "labels": [], "State": "closed", "Author": "jeffersonRosman"}
{"issue_number": 3329, "issue_title": "Parallel edges not working properly while using map reduce with Send class", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing import Annotated\nfrom langgraph.graph import StateGraph, START, END\nfrom langchain_core.prompts.prompt import PromptTemplate\nfrom typing_extensions import TypedDict\nfrom langchain_ollama.llms import OllamaLLM\nfrom typing import TypedDict, Annotated, List\nimport operator\nfrom typing import Annotated\nimport time\nfrom langgraph.constants import Send\n\nclass AgentState(TypedDict):\n    documents: List[str]\n    identified_alert_message:Annotated[List[str], operator.add]\n    extracted_timestamp:Annotated[List[str], operator.add]\n    document: str\n\nllm_mistral= OllamaLLM(model=\"mistral\",temperature=0)\nalert_identification_prompt =\"\"\" You are an helpful assistant say hi to user: {text}\"\"\"\ndef alert_identification(state:AgentState):\n            doc = state[\"document\"]\n            print(\"processing document number in alerts ======\",doc)\n            response=''\n            prompt = PromptTemplate.from_template(alert_identification_prompt)\n            chain=prompt | llm_mistral\n            response=''\n            try: \n                response=chain.invoke({'text':doc})\n                time.sleep(15)\n                print(\"Identified alert is as follows ===\", response)   \n            except Exception as e:  # Catch all exceptions and print details\n                print(\"Error observed:\")\n                print(f\"Type: {type(e).__name__}\")\n                print(f\"Message: {str(e)}\")   \n            final_response = response\n            return {'identified_alert_message':[final_response]}\n\ntimestamp_prompt =\"\"\" You are an helpful assistant say \"hello there\" to user: {text}\"\"\"\ndef timestamp_extraction(state:AgentState):\n            doc = state[\"document\"]\n            print(\"processing document number in root_cause ======\",doc)\n            response=''\n            prompt = PromptTemplate.from_template(timestamp_prompt)\n            chain=prompt | llm_mistral\n            response=''\n            try: \n                response=chain.invoke({'text':doc})\n                time.sleep(15)  \n                print(\"Root Cause is as follows  ===\", response) \n            except Exception as e:  # Catch all exceptions and print details\n                print(\"Error observed:\")\n                print(f\"Type: {type(e).__name__}\")\n                print(f\"Message: {str(e)}\")   \n            final_response = response\n            return {'extracted_timestamp':[final_response]}\n\ndef continue_to_documents_alert(state: AgentState):\n    return [Send(\"alert_identification\", {\"document\": d}) for d in state[\"documents\"]]\n\ndef continue_to_documents_root_cause(state: AgentState):\n    return [Send(\"timestamp_extraction\", {\"document\": d}) for d in state[\"documents\"]]\n\n\nworkflow = StateGraph(AgentState)\nworkflow.add_node(\"alert_identification\", alert_identification)\nworkflow.add_node(\"timestamp_extraction\", timestamp_extraction)\nworkflow.add_conditional_edges(START, continue_to_documents_alert, [\"alert_identification\"])\nworkflow.add_conditional_edges(START, continue_to_documents_root_cause, [\"timestamp_extraction\"])\nworkflow.add_edge( \"alert_identification\", END)\nworkflow.add_edge( \"timestamp_extraction\", END)\napp = workflow.compile()\n\nfrom IPython.display import display, Image\ndisplay(Image(app.get_graph().draw_mermaid_png()))\n\n\ndocuments=['Doc 1','Doc 2','Doc 3','Doc 4','Doc 5','Doc 6','Doc 7','Doc 8','Doc 9','Doc 10','Doc 11',\n           'Doc 12','Doc 13','Doc 14','Doc 15','Doc 16','Doc 17','Doc 18','Doc 19','Doc 20','Doc 21']\n\ninputs = {'documents':documents[0:]}\nresult= app.invoke(inputs)\nError Message and Stack Trace (if applicable)\nOutput is as follows - \n\n\nprocessing document number in alerts ====== Doc 1\nprocessing document number in alerts ====== Doc 2\nprocessing document number in alerts ====== Doc 3\nprocessing document number in alerts ====== Doc 4\nprocessing document number in alerts ====== Doc 5\nprocessing document number in alerts ====== Doc 6\nprocessing document number in alerts ====== Doc 8\nprocessing document number in alerts ====== Doc 7\nprocessing document number in alerts ====== Doc 9\nprocessing document number in alerts ====== Doc 10\nprocessing document number in alerts ====== Doc 11\nprocessing document number in alerts ====== Doc 12\nprocessing document number in alerts ====== Doc 13\nprocessing document number in alerts ====== Doc 14\nprocessing document number in alerts ====== Doc 15\nprocessing document number in alerts ====== Doc 16\nIdentified alert is as follows ===  Hello, Doc 12! How can I assist you today? Let's make this a productive and enjoyable interaction. What would you like to discuss or learn about?\nprocessing document number in alerts ====== Doc 17\nIdentified alert is as follows ===  Hello, Doc 4! How can I assist you today? Let's make this a productive and enjoyable interaction. What would you like to discuss or learn about?\nprocessing document number in alerts ====== Doc 18\nIdentified alert is as follows ===  Hello, Doc 14! How can I assist you today? Let's make this a productive and enjoyable interaction. What would you like to discuss or learn about?\nprocessing document number in alerts ====== Doc 19\nIdentified alert is as follows ===  Hello, Doc 16! How can I assist you today? Let's make this a productive and enjoyable interaction. What would you like to discuss or learn about?\nprocessing document number in alerts ====== Doc 20\nIdentified alert is as follows ===Identified alert is as follows ===  Hello, Doc 1! How can I assist you today? I'm here to help with any questions or tasks you might have. Let's get started!\n  Hello, Doc 9! How can I assist you today? Let's make this a productive and enjoyable interaction. What would you like to discuss or learn about?\nprocessing document number in root_cause ====== Doc 1\nprocessing document number in alerts ====== Doc 21\nIdentified alert is as follows ===  Hello, Doc 2! How can I assist you today? Let's make this a productive and enjoyable interaction. What would you like to discuss or learn about?\nprocessing document number in root_cause ====== Doc 2\nIdentified alert is as follows ===  Hello, Doc 7! How can I assist you today? Let's make this a productive and enjoyable interaction. What would you like to discuss or learn about?\nprocessing document number in root_cause ====== Doc 3\nIdentified alert is as follows ===Identified alert is as follows ===  Hello, Doc 5! How can I assist you today? Let's make this a productive and enjoyable interaction. What would you like to discuss or learn about?\nprocessing document number in root_cause ====== Doc 4\n  Hello Doc 10! How can I assist you today? Let's make this a productive and enjoyable interaction. What would you like to discuss or learn about?\nprocessing document number in root_cause ====== Doc 5\nIdentified alert is as follows ===  Hello, Doc 3! How can I assist you today? Let's make this a productive and enjoyable interaction. What would you like to discuss or learn about?\nprocessing document number in root_cause ====== Doc 6\nIdentified alert is as follows ===  Hello, Doc 8! How can I assist you today? Let's make this a productive and enjoyable interaction. What would you like to discuss or learn about?\nprocessing document number in root_cause ====== Doc 7\nIdentified alert is as follows ===Identified alert is as follows ===  Hello, Doc 11! How can I assist you today? Let's make this a productive and enjoyable interaction. What would you like to discuss or learn about?\n  Hello, Doc 13! How can I assist you today? Let's make this a productive and enjoyable interaction. What would you like to discuss or learn about?\nprocessing document number in root_cause ====== Doc 9\nprocessing document number in root_cause ====== Doc 8\nIdentified alert is as follows ===  Hello, Doc 6! How can I assist you today? Let's make this a productive and enjoyable interaction. What would you like to discuss or learn about?\nprocessing document number in root_cause ====== Doc 10\nIdentified alert is as follows ===  Hello, Doc 15! How can I assist you today? Let's make this a productive and enjoyable interaction. What would you like to discuss or learn about?\nprocessing document number in root_cause ====== Doc 11\nIdentified alert is as follows ===  Hello, Doc 17! How can I assist you today? Let's make this a productive and enjoyable interaction. What would you like to discuss or learn about?\nprocessing document number in root_cause ====== Doc 12\nIdentified alert is as follows ===  Hello Doc 20! How can I assist you today? Let's make this a productive and enjoyable interaction. What would you like to discuss or learn about?\nprocessing document number in root_cause ====== Doc 13\nIdentified alert is as follows ===  Hello, Doc 19! How can I assist you today? Let's make this a productive and enjoyable interaction. What would you like to discuss or learn about?\nIdentified alert is as follows ===  Hello, Doc 18! How can I assist you today? Let's make this a productive and enjoyable interaction. What would you like to discuss or learn about?\nprocessing document number in root_cause ====== Doc 14\nprocessing document number in root_cause ====== Doc 15\nRoot Cause is as follows  ===Root Cause is as follows  ===  Hello there, Doc 3! How can I assist you today?\nprocessing document number in root_cause ====== Doc 16\n  Hello there, Doc 2! How can I assist you today?\nprocessing document number in root_cause ====== Doc 17\nIdentified alert is as follows ===  Hello, Doc 21! How can I assist you today? Let's make this a productive and enjoyable interaction. What would you like to discuss or learn about?\nprocessing document number in root_cause ====== Doc 18\nRoot Cause is as follows  ===  Hello there, Doc 1! How can I assist you today? Let's make this a productive and enjoyable interaction. What would you like to discuss or learn about?\nprocessing document number in root_cause ====== Doc 19\nRoot Cause is as follows  ===  Hello there, Doc 4! How can I assist you today?\nprocessing document number in root_cause ====== Doc 20\nRoot Cause is as follows  ===  Hello there, Doc 6! How can I assist you today?\nprocessing document number in root_cause ====== Doc 21\nRoot Cause is as follows  ===  Hello there, Doc 5! How can I assist you today?\nRoot Cause is as follows  ===  Hello there, Doc 7! How can I assist you today?\nRoot Cause is as follows  ===Root Cause is as follows  ===  Hello there, Doc 8! How can I assist you today?\n  Hello there, Doc 9! How can I assist you today?\nRoot Cause is as follows  ===  Hello there, Doc 10! How can I assist you today?\nRoot Cause is as follows  ===  Hello there, Doc 11! How can I assist you today?\nRoot Cause is as follows  ===  Hello there, Doc 12! How can I assist you today?\nRoot Cause is as follows  ===  Hello there, Doc 15! How can I assist you today?\nRoot Cause is as follows  ===  Hello there, Doc 13! How can I assist you today?\nRoot Cause is as follows  ===  Hello there, Doc 14! How can I assist you today?\nRoot Cause is as follows  ===  Hello there, Doc 17! How can I assist you today?\nRoot Cause is as follows  ===  Hello there, Doc 16! How can I assist you today?\nRoot Cause is as follows  ===  Hello there, Doc 18! How can I assist you today?\nRoot Cause is as follows  ===  Hello there, Doc 19! How can I assist you today?\nRoot Cause is as follows  ===  Hello there, Doc 20! How can I assist you today?\nRoot Cause is as follows  ===  Hello there, Doc 21! How can I assist you today?\nDescription\nI have created two parallel nodes 1) alert_identification and 2) timestamp_extraction. While using Send API, these nodes should run in parallel, But I noted that \"alert_identification\" runs and finishes first and then program runs \"timestamp_extraction\".\nAlso for \"alert_identification\" there are 21 values so I am expecting parallel execution of all 21 values in map reduce but it is running only 16 values in parallel.\nNo runtime error but opportunity to improve performance.\nSystem Info\nIssue in latest version of Langgraph", "created_at": "2025-02-06", "closed_at": "2025-02-12", "labels": [], "State": "closed", "Author": "sriakhil25"}
{"issue_number": 3325, "issue_title": "MalformedError('No key could be detected.') When Using BigQuery Tool in LangGraph Cloud Deployment", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport os\nimport json\nimport asyncio\nfrom typing import Type\nimport logging\n# Core dependencies\nfrom pydantic import BaseModel, Field\n# Google Cloud\nfrom google.cloud import bigquery\nfrom google.oauth2 import service_account\n# LangChain & LangGraph\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.tools import StructuredTool\nfrom langgraph.prebuilt import create_react_agent\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\n\n# CONFIGURATION\nPROJECT_ID = os.getenv(\"PROJECT_ID\", \"datawarehouse-447422\")\nRAW_DATASET_ID = os.getenv(\"RAW_DATASET_ID\", \"linkedin_raw\")\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# INPUT SCHEMA\nclass BigQueryListTablesInput(BaseModel):\n    dataset_name: str = Field(..., description=\"Name of the BigQuery dataset to list tables from\")\n\n# BIGQUERY CLIENT INITIALIZATION\ndef get_bigquery_client() -> bigquery.Client:\n    \"\"\"Initialize BigQuery client with proper credentials\"\"\"\n    if creds_json := os.getenv(\"GOOGLE_CLOUD_CREDENTIALS_JSON\"):\n        logging.info(\"Using service account credentials from environment variable.\")\n        credentials = service_account.Credentials.from_service_account_info(json.loads(creds_json))\n        return bigquery.Client(credentials=credentials, project=credentials.project_id)\n    logging.info(\"Using default project ID for BigQuery client.\")\n    return bigquery.Client(project=PROJECT_ID)\n\n# TOOL IMPLEMENTATION\nasync def list_bigquery_tables(dataset_name: str) -> str:\n    \"\"\"List tables in a BigQuery dataset\"\"\"\n    logging.info(f\"Received dataset_name: {dataset_name}\")\n    if not dataset_name:\n        raise ValueError(\"Missing required input: dataset_name\")\n    \n    try:\n        logging.info(\"Starting BigQuery client initialization...\")\n        client = get_bigquery_client()\n        logging.info(f\"BigQuery client initialized successfully with project ID: {client.project}\")\n        \n        logging.info(f\"Creating dataset reference for dataset: {dataset_name}\")\n        dataset_ref = client.dataset(dataset_name)\n        logging.info(f\"Dataset reference created: {dataset_ref.path}\")\n        \n        logging.info(\"Listing tables in the dataset...\")\n        tables = client.list_tables(dataset_ref)\n        table_ids = \", \".join(table.table_id for table in tables)\n        logging.info(f\"Table IDs: {table_ids}\")\n        \n        return table_ids or \"No tables found\"\n    except Exception as e:\n        logging.error(f\"Error listing tables: {e}\")\n        raise\n\n# TOOL REGISTRATION\ntools = [\n    StructuredTool.from_function(\n        func=list_bigquery_tables,\n        name=\"list_bigquery_tables\",\n        description=\"Lists tables in a BigQuery dataset. Input: JSON object with 'dataset_name'\",\n        args_schema=BigQueryListTablesInput,\n        coroutine=list_bigquery_tables,\n    ),\n]\n\n# AGENT CREATION\ndef create_agent():\n    llm = ChatOpenAI(\n        model_name=\"gpt-3.5-turbo\",\n        temperature=0,\n        max_tokens=1200,\n        openai_api_key=OPENAI_API_KEY\n    )\n    return create_react_agent(llm, tools)\n\n# Initialize the agent graph\ngraph = create_agent()\n\n# Simulate agent flow locally (for testing purposes)\nasync def simulate_agent_flow():\n    # Simulate input generation\n    dataset_name = \"linkedin_raw\"\n    \n    # Test the tool\n    print(await list_bigquery_tables(dataset_name))\n\n# Run simulation locally\nif __name__ == \"__main__\":\n    asyncio.run(simulate_agent_flow())\nError Message and Stack Trace (if applicable)\nError listing tables: No key could be detected.\nDescription\nI am encountering a persistent MalformedError('No key could be detected.') error when deploying an agent with a BigQuery tool (list_bigquery_tables) to LangGraph Cloud. The same code works flawlessly in a local environment, which suggests the issue lies within the LangGraph Cloud deployment or its interaction with external APIs like BigQuery.\nSteps to Reproduce\nDeploy the following minimal code to LangGraph Cloud.\nSet the required secrets (GOOGLE_CLOUD_CREDENTIALS_JSON and OPENAI_API_KEY) in the LangGraph Cloud environment.\nTrigger the list_bigquery_tables tool by asking the agent to list tables in the linkedin_raw dataset.\nExpected Behavior\nThe tool should successfully list all tables in the specified dataset and return their names.\nActual Behavior\nThe tool fails with the following error:\nError listing tables: No key could be detected. \nTroubleshooting Steps Taken\nValidated Secrets : Confirmed that GOOGLE_CLOUD_CREDENTIALS_JSON and OPENAI_API_KEY are correctly set in LangGraph Cloud.\nTested Locally : Verified that the code works locally with the same service account credentials.\nAdded Logging : Enhanced logging to capture the entire execution flow, including project ID, dataset reference, and table listing.\nChecked Permissions : Ensured the service account has the BigQuery Admin role.\nHypotheses\nLangGraph Cloud Restrictions :\nLangGraph Cloud might block or limit outbound HTTP requests to external APIs like BigQuery 4.\nThere could be constraints on the size or format of responses returned by tools.\nAgent-Tool Integration :\nThe agent might mishandle the tool's response, leading to a malformed output 3.\nThere could be a mismatch between the expected and actual response formats.\nRequest for Assistance\nCould the maintainers of LangGraph Cloud provide clarification on the following points?\nAre there any restrictions on outbound HTTP requests to external APIs like BigQuery?\nAre there specific requirements for tool response formats or schemas?\nCould this issue be related to the runtime environment or permissions in LangGraph Cloud?\nThis example is fully self-contained, minimal, and reproducible. It includes all relevant imports, configurations, and logging to help diagnose the issue 1. Please let me know if further clarification is needed!\nSystem Info\nPython Version: 3.9+\nRequired Libraries: google-cloud-bigquery, langchain-openai, langchain-core, langgraph\nDeployment Environment: LangGraph Cloud\nGoogle Service Account Role: BigQuery Admin\nOpenAI Model: gpt-3.5-turbo", "created_at": "2025-02-05", "closed_at": "2025-03-05", "labels": [], "State": "closed", "Author": "johannescastner"}
{"issue_number": 3324, "issue_title": "Stream Disabling is not working", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.constants import TAG_NOSTREAM\n\n# Not working\nllm_json_mode = ChatOllama(\n    model=\"deepseek-r1\",\n    temperature=0,\n    format=\"json\",\n    disable_streaming=True\n    # tags=[TAG_NOSTREAM]\n).with_structured_output(\n    schema=ClassificationOutputStructure,\n    method=\"json_schema\"\n)\n\n# Working\nllm_json_mode = ChatOllama(\n    model=\"deepseek-r1\",\n    temperature=0,\n    format=\"json\",\n    # disable_streaming=True\n    tags=[TAG_NOSTREAM]\n).with_structured_output(\n    schema=ClassificationOutputStructure,\n    method=\"json_schema\"\n)\nError Message and Stack Trace (if applicable)\n\nDescription\nUsing the disable_streaming=True (Stream is not disabled)\n\nUsing the tags=[TAG_NOSTREAM] (Stream disabled successfully)\n\nPS: This is not mentioned in the docs\n\n\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:16 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6000\nPython Version:  3.12.3 | packaged by Anaconda, Inc. | (main, May  6 2024, 14:46:42) [Clang 14.0.6 ]\n\nPackage Information\n\nlangchain_core: 0.3.31\nlangchain: 0.3.14\nlangchain_community: 0.3.14\nlangsmith: 0.2.10\nlangchain_google_genai: 2.0.9\nlangchain_ollama: 0.2.2\nlangchain_openai: 0.3.1\nlangchain_qdrant: 0.2.0\nlangchain_text_splitters: 0.3.5\nlanggraph_api: 0.0.15\nlanggraph_cli: 0.1.65\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.48\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.11\nasync-timeout: Installed. No version info available.\nclick: 8.1.8\ncryptography: 43.0.3\ndataclasses-json: 0.6.7\nfastembed: Installed. No version info available.\nfiletype: 1.2.0\ngoogle-generativeai: 0.8.4\nhttpx: 0.27.2\nhttpx-sse: 0.4.0\njsonpatch: 1.33\njsonschema-rs: 0.25.1\nlanggraph: 0.2.61\nlanggraph-checkpoint: 2.0.9\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 2.2.1\nollama: 0.4.5\nopenai: 1.60.0\norjson: 3.10.14\npackaging: 24.2\npydantic: 2.10.4\npydantic-settings: 2.7.1\npyjwt: 2.10.1\npython-dotenv: 1.0.1\nPyYAML: 6.0.2\nqdrant-client: 1.13.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.36\nsse-starlette: 2.1.3\nstarlette: 0.45.2\nstructlog: 24.4.0\ntenacity: 8.5.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\nuvicorn: 0.34.0\nwatchfiles: 1.0.3\nzstandard: Installed. No version info available.\n", "created_at": "2025-02-05", "closed_at": "2025-02-05", "labels": [], "State": "closed", "Author": "soufiene-slimi"}
{"issue_number": 3310, "issue_title": "Typings for coroutine returned instead of value returned when awaiting async task (functional api)", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n@task\nasync def retrieve():\n    return \"foo\"\nError Message and Stack Trace (if applicable)\n\nDescription\nUsing langggraph functional api with async functions I am getting typing errors. It seems to always return a coroutine instead of the final value when I await it.\n\n\n\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.1.0: Thu Oct 10 21:03:11 PDT 2024; root:xnu-11215.41.3~2/RELEASE_ARM64_T6020\nPython Version:  3.12.8 (main, Jan  5 2025, 06:55:30) [Clang 19.1.6 ]\n\nPackage Information\n\nlangchain_core: 0.3.33\nlangchain: 0.3.17\nlangchain_community: 0.3.15\nlangsmith: 0.3.1\nlangchain_anthropic: 0.3.4\nlangchain_experimental: 0.3.4\nlangchain_google_genai: 2.0.9\nlangchain_openai: 0.3.2\nlangchain_text_splitters: 0.3.5\nlanggraph_sdk: 0.1.51\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.11\nanthropic: 0.45.0\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.7\ndefusedxml: 0.7.1\nfiletype: 1.2.0\ngoogle-generativeai: 0.8.4\nhttpx: 0.28.1\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.61.0\norjson: 3.10.15\npackaging: 24.2\npydantic: 2.10.6\npydantic-settings: 2.7.1\npytest: Installed. No version info available.\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrich: 13.9.4\nSQLAlchemy: 2.0.37\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\nzstandard: 0.23.0\n", "created_at": "2025-02-04", "closed_at": null, "labels": [], "State": "open", "Author": "lightsofapollo"}
{"issue_number": 3299, "issue_title": "DOC: Cannot connect LangGraph server to deployed Postgres instance", "issue_body": "Issue with current documentation:\nI have followed the Introduction to LangGraph course and I am trying to deploy a local instance of the LangGraph API server using the files in module-6/deployment.\nI have set up Redis and Postgres instances on GCP, and confirmed that they are accepting connections. According to the course documentation, all I have to do is provide these instance URIs in the following command after I build my image with langgraph build -t my-image:\u2028\nCOMMAND:\ndocker run \\\n    --env-file .env \\\n    -p 8123:8000 \\\n    -e POSTGRES_URI=<POSTGRES_URI> \\\n    -e REDIS_URI=<REDIS_URI> \\\n    my-image\n\nI am getting the following error, showing that my application is attempting to incorrectly connect to a local instance of Postgres. How can I fix this?\nCould it be that one of the LangGraph libraries is defaulting to local Postgres instance? I have modified my task_maistro.py code (see below) to try to resolve this issue, but had no success.\u2028\u2028\nERROR:\n2025-02-04T02:20:16.591708Z [info     ] Using auth of type=noop        [langgraph_api.auth.middleware] api_revision=d02afa8 api_variant=local\n2025-02-04T02:20:16.593067Z [info     ] Started server process [1]     [uvicorn.error] api_revision=d02afa8 api_variant=local color_message='Started server process [\\x1b[36m%d\\x1b[0m]'\n2025-02-04T02:20:16.593171Z [info     ] Waiting for application startup. [uvicorn.error] api_revision=d02afa8 api_variant=local\n2025-02-04T02:20:16.593387Z [warning  ] No license key found, running in test mode with LangSmith API key. For production use, set LANGGRAPH_CLOUD_LICENSE_KEY in environment. [langgraph_license.validation] api_revision=d02afa8 api_variant=local\n2025-02-04T02:20:16.900546Z [info     ] HTTP Request: GET https://api.smith.langchain.com/auth?langgraph-api=true \"HTTP/1.1 200 OK\" [httpx] api_revision=d02afa8 api_variant=local\n2025-02-04T02:20:16.944656Z [warning  ] error connecting in 'pool-1': [Errno -2] Name or service not known [psycopg.pool] api_revision=d02afa8 api_variant=local\n2025-02-04T02:20:17.944788Z [warning  ] error connecting in 'pool-1': [Errno -2] Name or service not known [psycopg.pool] api_revision=d02afa8 api_variant=local\n2025-02-04T02:20:19.929930Z [warning  ] error connecting in 'pool-1': [Errno -2] Name or service not known [psycopg.pool] api_revision=d02afa8 api_variant=local\n2025-02-04T02:20:23.881929Z [warning  ] error connecting in 'pool-1': [Errno -2] Name or service not known [psycopg.pool] api_revision=d02afa8 api_variant=local\n2025-02-04T02:20:31.775841Z [warning  ] error connecting in 'pool-1': [Errno -2] Name or service not known [psycopg.pool] api_revision=d02afa8 api_variant=local\n2025-02-04T02:20:46.960836Z [error    ] Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 693, in lifespan\n    async with self.lifespan_context(app) as maybe_state:\n  File \"/usr/local/lib/python3.11/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/api/langgraph_api/lifespan.py\", line 29, in lifespan\n  File \"/usr/local/lib/python3.11/site-packages/langgraph_storage/database.py\", line 149, in start_pool\n    await _pg_pool.open(wait=True)\n  File \"/usr/local/lib/python3.11/site-packages/psycopg_pool/pool_async.py\", line 387, in open\n    await self.wait(timeout=timeout)\n  File \"/usr/local/lib/python3.11/site-packages/psycopg_pool/pool_async.py\", line 174, in wait\n    raise PoolTimeout(f\"pool initialization incomplete after {timeout} sec\")\npsycopg_pool.PoolTimeout: pool initialization incomplete after 30.0 sec\n [uvicorn.error] api_revision=d02afa8 api_variant=local\n2025-02-04T02:20:46.961102Z [error    ] Application startup failed. Exiting. [uvicorn.error] api_revision=d02afa8 api_variant=local\n\nNote:\nI am able to run docker compose up (using the provided docker-compose.yml) and langgraph dev successfully. The issue is with trying to use a remote/deployed instances of Postgres, and possibly Redis (I haven\u2019t seen any errors regarding that,  yet).\ntask_maistro.py:\nimport uuid\nimport os\nfrom datetime import datetime\n\nfrom pydantic import BaseModel, Field\n\nfrom trustcall import create_extractor\n\nfrom typing import Literal, Optional, TypedDict\n\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain_core.messages import merge_message_runs\nfrom langchain_core.messages import SystemMessage, HumanMessage\n\nfrom langchain_openai import ChatOpenAI\n\nfrom langgraph.graph import StateGraph, MessagesState, START, END\nfrom langgraph.store.postgres import PostgresStore\n\nconn_string = os.getenv(\"POSTGRES_URI\")\n\nprint('MY POSTGRES CONNECTION IS: ', conn_string)\n\n# from my_agent import configuration\nimport configuration\n\n## Utilities \n\n# Inspect the tool calls for Trustcall\nclass Spy:\n    def __init__(self):\n        self.called_tools = []\n\n    def __call__(self, run):\n        q = [run]\n        while q:\n            r = q.pop()\n            if r.child_runs:\n                q.extend(r.child_runs)\n            if r.run_type == \"chat_model\":\n                self.called_tools.append(\n                    r.outputs[\"generations\"][0][0][\"message\"][\"kwargs\"][\"tool_calls\"]\n                )\n\n# Extract information from tool calls for both patches and new memories in Trustcall\ndef extract_tool_info(tool_calls, schema_name=\"Memory\"):\n    \"\"\"Extract information from tool calls for both patches and new memories.\n    \n    Args:\n        tool_calls: List of tool calls from the model\n        schema_name: Name of the schema tool (e.g., \"Memory\", \"ToDo\", \"Profile\")\n    \"\"\"\n    # Initialize list of changes\n    changes = []\n    \n    for call_group in tool_calls:\n        for call in call_group:\n            if call['name'] == 'PatchDoc':\n                # Check if there are any patches\n                if call['args']['patches']:\n                    changes.append({\n                        'type': 'update',\n                        'doc_id': call['args']['json_doc_id'],\n                        'planned_edits': call['args']['planned_edits'],\n                        'value': call['args']['patches'][0]['value']\n                    })\n                else:\n                    # Handle case where no changes were needed\n                    changes.append({\n                        'type': 'no_update',\n                        'doc_id': call['args']['json_doc_id'],\n                        'planned_edits': call['args']['planned_edits']\n                    })\n            elif call['name'] == schema_name:\n                changes.append({\n                    'type': 'new',\n                    'value': call['args']\n                })\n\n    # Format results as a single string\n    result_parts = []\n    for change in changes:\n        if change['type'] == 'update':\n            result_parts.append(\n                f\"Document {change['doc_id']} updated:\\n\"\n                f\"Plan: {change['planned_edits']}\\n\"\n                f\"Added content: {change['value']}\"\n            )\n        elif change['type'] == 'no_update':\n            result_parts.append(\n                f\"Document {change['doc_id']} unchanged:\\n\"\n                f\"{change['planned_edits']}\"\n            )\n        else:\n            result_parts.append(\n                f\"New {schema_name} created:\\n\"\n                f\"Content: {change['value']}\"\n            )\n    \n    return \"\\n\\n\".join(result_parts)\n\n## Schema definitions\n\n# User profile schema\nclass Profile(BaseModel):\n    \"\"\"This is the profile of the user you are chatting with\"\"\"\n    name: Optional[str] = Field(description=\"The user's name\", default=None)\n    location: Optional[str] = Field(description=\"The user's location\", default=None)\n    job: Optional[str] = Field(description=\"The user's job\", default=None)\n    connections: list[str] = Field(\n        description=\"Personal connection of the user, such as family members, friends, or coworkers\",\n        default_factory=list\n    )\n    interests: list[str] = Field(\n        description=\"Interests that the user has\", \n        default_factory=list\n    )\n\n# ToDo schema\nclass ToDo(BaseModel):\n    task: str = Field(description=\"The task to be completed.\")\n    time_to_complete: Optional[int] = Field(description=\"Estimated time to complete the task (minutes).\")\n    deadline: Optional[datetime] = Field(\n        description=\"When the task needs to be completed by (if applicable)\",\n        default=None\n    )\n    solutions: list[str] = Field(\n        description=\"List of specific, actionable solutions (e.g., specific ideas, service providers, or concrete options relevant to completing the task)\",\n        min_items=1,\n        default_factory=list\n    )\n    status: Literal[\"not started\", \"in progress\", \"done\", \"archived\"] = Field(\n        description=\"Current status of the task\",\n        default=\"not started\"\n    )\n\n## Initialize the model and tools\n\n# Update memory tool\nclass UpdateMemory(TypedDict):\n    \"\"\" Decision on what memory type to update \"\"\"\n    update_type: Literal['user', 'todo', 'instructions']\n\n# Initialize the model\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n\n## Create the Trustcall extractors for updating the user profile and ToDo list\nprofile_extractor = create_extractor(\n    model,\n    tools=[Profile],\n    tool_choice=\"Profile\",\n)\n\n## Prompts \n\n# Chatbot instruction for choosing what to update and what tools to call \nMODEL_SYSTEM_MESSAGE = \"\"\"{task_maistro_role} \n\nYou have a long term memory which keeps track of three things:\n1. The user's profile (general information about them) \n2. The user's ToDo list\n3. General instructions for updating the ToDo list\n\nHere is the current User Profile (may be empty if no information has been collected yet):\n<user_profile>\n{user_profile}\n</user_profile>\n\nHere is the current ToDo List (may be empty if no tasks have been added yet):\n<todo>\n{todo}\n</todo>\n\nHere are the current user-specified preferences for updating the ToDo list (may be empty if no preferences have been specified yet):\n<instructions>\n{instructions}\n</instructions>\n\nHere are your instructions for reasoning about the user's messages:\n\n1. Reason carefully about the user's messages as presented below. \n\n2. Decide whether any of the your long-term memory should be updated:\n- If personal information was provided about the user, update the user's profile by calling UpdateMemory tool with type `user`\n- If tasks are mentioned, update the ToDo list by calling UpdateMemory tool with type `todo`\n- If the user has specified preferences for how to update the ToDo list, update the instructions by calling UpdateMemory tool with type `instructions`\n\n3. Tell the user that you have updated your memory, if appropriate:\n- Do not tell the user you have updated the user's profile\n- Tell the user them when you update the todo list\n- Do not tell the user that you have updated instructions\n\n4. Err on the side of updating the todo list. No need to ask for explicit permission.\n\n5. Respond naturally to user user after a tool call was made to save memories, or if no tool call was made.\"\"\"\n\n# Trustcall instruction\nTRUSTCALL_INSTRUCTION = \"\"\"Reflect on following interaction. \n\nUse the provided tools to retain any necessary memories about the user. \n\nUse parallel tool calling to handle updates and insertions simultaneously.\n\nSystem Time: {time}\"\"\"\n\n# Instructions for updating the ToDo list\nCREATE_INSTRUCTIONS = \"\"\"Reflect on the following interaction.\n\nBased on this interaction, update your instructions for how to update ToDo list items. Use any feedback from the user to update how they like to have items added, etc.\n\nYour current instructions are:\n\n<current_instructions>\n{current_instructions}\n</current_instructions>\"\"\"\n\n## Node definitions\n\ndef task_mAIstro(state: MessagesState, config: RunnableConfig, store: PostgresStore):\n\n    \"\"\"Load memories from the store and use them to personalize the chatbot's response.\"\"\"\n\n    with store.from_conn_string(conn_string) as postgres_store:\n\n        # Get the user ID from the config\n        configurable = configuration.Configuration.from_runnable_config(config)\n        user_id = configurable.user_id\n        todo_category = configurable.todo_category\n        task_maistro_role = configurable.task_maistro_role\n\n    # Retrieve profile memory from the store\n        namespace = (\"profile\", todo_category, user_id)\n        memories = postgres_store.search(namespace)\n        if memories:\n            user_profile = memories[0].value\n        else:\n            user_profile = None\n\n        # Retrieve people memory from the store\n        namespace = (\"todo\", todo_category, user_id)\n        memories = postgres_store.search(namespace)\n        todo = \"\\n\".join(f\"{mem.value}\" for mem in memories)\n\n        # Retrieve custom instructions\n        namespace = (\"instructions\", todo_category, user_id)\n        memories = postgres_store.search(namespace)\n        if memories:\n            instructions = memories[0].value\n        else:\n            instructions = \"\"\n        \n        system_msg = MODEL_SYSTEM_MESSAGE.format(task_maistro_role=task_maistro_role, user_profile=user_profile, todo=todo, instructions=instructions)\n\n        # Respond using memory as well as the chat history\n        response = model.bind_tools([UpdateMemory], parallel_tool_calls=False).invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n\n    return {\"messages\": [response]}\n\ndef update_profile(state: MessagesState, config: RunnableConfig, store: PostgresStore):\n\n    \"\"\"Reflect on the chat history and update the memory collection.\"\"\"\n    \n    with store.from_conn_string(conn_string) as postgres_store:\n    \n        # Get the user ID from the config\n        configurable = configuration.Configuration.from_runnable_config(config)\n        user_id = configurable.user_id\n        todo_category = configurable.todo_category\n\n        # Define the namespace for the memories\n        namespace = (\"profile\", todo_category, user_id)\n\n        # Retrieve the most recent memories for context\n        existing_items = postgres_store.search(namespace)\n\n        # Format the existing memories for the Trustcall extractor\n        tool_name = \"Profile\"\n        existing_memories = ([(existing_item.key, tool_name, existing_item.value)\n                            for existing_item in existing_items]\n                            if existing_items\n                            else None\n                            )\n\n        # Merge the chat history and the instruction\n        TRUSTCALL_INSTRUCTION_FORMATTED=TRUSTCALL_INSTRUCTION.format(time=datetime.now().isoformat())\n        updated_messages=list(merge_message_runs(messages=[SystemMessage(content=TRUSTCALL_INSTRUCTION_FORMATTED)] + state[\"messages\"][:-1]))\n\n        # Invoke the extractor\n        result = profile_extractor.invoke({\"messages\": updated_messages, \n                                            \"existing\": existing_memories})\n\n        # Save save the memories from Trustcall to the store\n        for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n            postgres_store.put(namespace,\n                    rmeta.get(\"json_doc_id\", str(uuid.uuid4())),\n                    r.model_dump(mode=\"json\"),\n                )\n        tool_calls = state['messages'][-1].tool_calls\n    # Return tool message with update verification\n    return {\"messages\": [{\"role\": \"tool\", \"content\": \"updated profile\", \"tool_call_id\":tool_calls[0]['id']}]}\n\ndef update_todos(state: MessagesState, config: RunnableConfig, store: PostgresStore):\n\n    \"\"\"Reflect on the chat history and update the memory collection.\"\"\"\n    with store.from_conn_string(conn_string) as postgres_store:\n\n    \n        # Get the user ID from the config\n        configurable = configuration.Configuration.from_runnable_config(config)\n        user_id = configurable.user_id\n        todo_category = configurable.todo_category\n\n        # Define the namespace for the memories\n        namespace = (\"todo\", todo_category, user_id)\n        \n        postgres_store.setup()\n\n        # Retrieve the most recent memories for context\n        existing_items = postgres_store.search(namespace)\n\n        # Format the existing memories for the Trustcall extractor\n        tool_name = \"ToDo\"\n        existing_memories = ([(existing_item.key, tool_name, existing_item.value)\n                            for existing_item in existing_items]\n                            if existing_items\n                            else None\n                            )\n\n        # Merge the chat history and the instruction\n        TRUSTCALL_INSTRUCTION_FORMATTED=TRUSTCALL_INSTRUCTION.format(time=datetime.now().isoformat())\n        updated_messages=list(merge_message_runs(messages=[SystemMessage(content=TRUSTCALL_INSTRUCTION_FORMATTED)] + state[\"messages\"][:-1]))\n\n        # Initialize the spy for visibility into the tool calls made by Trustcall\n        spy = Spy()\n        \n        # Create the Trustcall extractor for updating the ToDo list \n        todo_extractor = create_extractor(\n        model,\n        tools=[ToDo],\n        tool_choice=tool_name,\n        enable_inserts=True\n        ).with_listeners(on_end=spy)\n\n        # Invoke the extractor\n        result = todo_extractor.invoke({\"messages\": updated_messages, \n                                            \"existing\": existing_memories})\n\n        # Save save the memories from Trustcall to the store\n        for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n            postgres_store.put(namespace,\n                    rmeta.get(\"json_doc_id\", str(uuid.uuid4())),\n                    r.model_dump(mode=\"json\"),\n                )\n            \n        # Respond to the tool call made in task_mAIstro, confirming the update    \n        tool_calls = state['messages'][-1].tool_calls\n\n        # Extract the changes made by Trustcall and add the the ToolMessage returned to task_mAIstro\n        todo_update_msg = extract_tool_info(spy.called_tools, tool_name)\n        \n        \n        \n    return {\"messages\": [{\"role\": \"tool\", \"content\": todo_update_msg, \"tool_call_id\":tool_calls[0]['id']}]}\n\ndef update_instructions(state: MessagesState, config: RunnableConfig, store: PostgresStore):\n\n    \"\"\"Reflect on the chat history and update the memory collection.\"\"\"\n    \n    with store.from_conn_string(conn_string) as postgres_store:\n    \n        # Get the user ID from the config\n        configurable = configuration.Configuration.from_runnable_config(config)\n        user_id = configurable.user_id\n        todo_category = configurable.todo_category\n        \n        namespace = (\"instructions\", todo_category, user_id)\n\n        existing_memory = postgres_store.get(namespace, \"user_instructions\")\n            \n        # Format the memory in the system prompt\n        system_msg = CREATE_INSTRUCTIONS.format(current_instructions=existing_memory.value if existing_memory else None)\n        new_memory = model.invoke([SystemMessage(content=system_msg)]+state['messages'][:-1] + [HumanMessage(content=\"Please update the instructions based on the conversation\")])\n\n        # Overwrite the existing memory in the store \n        key = \"user_instructions\"\n        postgres_store.put(namespace, key, {\"memory\": new_memory.content})\n        tool_calls = state['messages'][-1].tool_calls\n    # Return tool message with update verification\n    return {\"messages\": [{\"role\": \"tool\", \"content\": \"updated instructions\", \"tool_call_id\":tool_calls[0]['id']}]}\n\n# Conditional edge\ndef route_message(state: MessagesState, config: RunnableConfig, store: PostgresStore) -> Literal[END, \"update_todos\", \"update_instructions\", \"update_profile\"]:\n\n    \"\"\"Reflect on the memories and chat history to decide whether to update the memory collection.\"\"\"\n    message = state['messages'][-1]\n    if len(message.tool_calls) ==0:\n        return END\n    else:\n        tool_call = message.tool_calls[0]\n        if tool_call['args']['update_type'] == \"user\":\n            return \"update_profile\"\n        elif tool_call['args']['update_type'] == \"todo\":\n            return \"update_todos\"\n        elif tool_call['args']['update_type'] == \"instructions\":\n            return \"update_instructions\"\n        else:\n            raise ValueError\n\n# Create the graph + all nodes\nbuilder = StateGraph(MessagesState, config_schema=configuration.Configuration)\n\n# Define the flow of the memory extraction process\nbuilder.add_node(task_mAIstro)\nbuilder.add_node(update_todos)\nbuilder.add_node(update_profile)\nbuilder.add_node(update_instructions)\n\n# Define the flow \nbuilder.add_edge(START, \"task_mAIstro\")\nbuilder.add_conditional_edges(\"task_mAIstro\", route_message)\nbuilder.add_edge(\"update_todos\", \"task_mAIstro\")\nbuilder.add_edge(\"update_profile\", \"task_mAIstro\")\nbuilder.add_edge(\"update_instructions\", \"task_mAIstro\")\n\n# Compile the graph\ngraph = builder.compile()\n\nIdea or request for content:\nNo response", "created_at": "2025-02-04", "closed_at": "2025-02-18", "labels": ["question"], "State": "closed", "Author": "aeronesto"}
{"issue_number": 3291, "issue_title": "\"Failed to fetch\" in external browser to LangSmith Studio", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nDoes not apply as I'm still setting up\nError Message and Stack Trace (if applicable)\n\nDescription\nAfter installation I'm trying to see the Studio environment. So far I:\npython3 -m venv langgraphstudiotest\nsource langgraphstudiotest/bin/activate\npip install --upgrade \"langgraph-cli[inmem]\"\nlanggraph new langgraphstudiotest --template react-agent-python\ncd langgraphstudiotest             \nmkdir app  \nlanggraph new app --template react-agent-python\ncd app/                \npip install -e .\nnano .env\nlanggraph dev --host 0.0.0.0\n\nAll well up-til-here.\nWhat works from a remote web browser:\nhttp://192.168.178.31:2024/ok => {\"ok\":true}\nhttp://192.168.178.31:2024/docs => works\nThis  doesn't work:\nhttps://smith.langchain.com/studio/thread?baseUrl=http://192.168.178.31:2024 => Page shows: \"Failed to fetch\"\nAlso tried with\nlanggraph dev --host 192.168.178.31\nBoth give same results\nSee also #3261\nInstalled fix:\npip install --upgrade \"langgraph-cli[inmem]\"  \"langgraph-api==0.0.21\"\nUnfortenately not fixing my issue\nSystem Info\npython3 -m langchain_core.sys_info                                                                                                                                     [4/1075]\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP Tue Nov 12 08:48:32 UTC 2024\nPython Version:  3.11.2 (main, Nov 30 2024, 21:22:50) [GCC 12.2.0]\n\nPackage Information\n\nlangchain_core: 0.3.33\nlangchain: 0.3.17\nlangchain_community: 0.3.16\nlangsmith: 0.2.11\nlangchain_anthropic: 0.3.5\nlangchain_fireworks: 0.2.7\nlangchain_openai: 0.3.3\nlangchain_text_splitters: 0.3.5\nlanggraph_api: 0.0.21\nlanggraph_cli: 0.1.70\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.51\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.11\nanthropic: 0.45.2\nasync-timeout: Installed. No version info available.\nclick: 8.1.8\ncryptography: 43.0.3\ndataclasses-json: 0.6.7\ndefusedxml: 0.7.1\nfireworks-ai: 0.15.12\nhttpx: 0.28.1\nhttpx-sse: 0.4.0\njsonpatch: 1.33\njsonschema-rs: 0.25.1\nlanggraph: 0.2.69\nlanggraph-checkpoint: 2.0.10\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.60.2\norjson: 3.10.15\npackaging: 24.2\npydantic: 2.10.6\npydantic-settings: 2.7.1\npyjwt: 2.10.1\npython-dotenv: 1.0.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.37\nsse-starlette: 2.1.3\nstarlette: 0.45.3\nstructlog: 24.4.0\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\nuvicorn: 0.34.0\nwatchfiles: 1.0.4\nzstandard: Installed. No version info available.\n", "created_at": "2025-02-03", "closed_at": null, "labels": [], "State": "open", "Author": "kbfifi"}
{"issue_number": 3289, "issue_title": "Serialization Error", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nbuilder = StateGraph(State)\nbuilder.add_node(call_llm)\nbuilder.add_node(run_tool)\nbuilder.add_node(human_review_node)\nbuilder.add_edge(START, \"call_llm\")\nbuilder.add_conditional_edges(\"call_llm\", route_after_llm)\nbuilder.add_edge(\"run_tool\", \"call_llm\")\nThis is the simple Graph\nError Message and Stack Trace (if applicable)\n\nDescription\nI am trying to serialize compiledstategraph object to use across API calls. I tried to pickle, getting this below error\nb'\"Can't pickle local object 'CompgiledStateGraph.attach_node.._get_updates'\"''\nHow to serialize the compilestategraph\nSystem Info\nPython", "created_at": "2025-02-03", "closed_at": "2025-02-05", "labels": [], "State": "closed", "Author": "BharahthyKannan"}
{"issue_number": 3286, "issue_title": "problem with Langfuse not retrieving model call costs when using invoke with create_react_agent", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n---------------------------------------------------------\nfrom langfuse.callbacks import CallbackHuandler\n\nlangfuse_handler = CallbackHandler(\n        httpx_client=client,\n        public_key=langfuse_config.LANGFUSE_PUBLIC_KEY,\n        secret_key=langfuse_config.LANGFUSE_SECRET_KEY,\n        host=langfuse_config.LANGFUSE_HOST_URL,\n        tags=[tag],\n        session_id=conversation_id,\n    )\n\n--------------------------------------------------------------\nfrom langchain.schema.runnable.config import RunnableConfig\n\nllm = ChatOpenAI()\nsystem_message = SystemMessage(content=SYSTEM_PROMPT)\nmemory = MemorySaver()\n \nagent= create_react_agent(\n            model=llm,\n            tools=[rertiever_tool],\n            state_modifier=system_message,\n            checkpointer=memory\n        )\n \nconfig = RunnableConfig(callbacks=[langfuse_handler], configurable={\n                                \"thread_id\": \"thread_id\"})\n\nawait agent.ainvoke(inputs, config)\nError Message and Stack Trace (if applicable)\n\nDescription\nI use LangGraph and Langfuse to track and log model interactions.\nI expect to see both the trace and the cost of model calls in the Langfuse dashboard.\nInstead, it only logs the request input and output without calculating the total token usage and cost of the call.\n\nSystem Info\ni use :\nlangchain_core: 0.3.32\nlangchain: 0.3.16\nlangchain_community: 0.3.16\nlangGraph: 0.2.59\nlangfuse: 2.36.2\npython: 3.12.8", "created_at": "2025-02-03", "closed_at": "2025-02-03", "labels": [], "State": "closed", "Author": "Guemri-Jawher"}
{"issue_number": 3281, "issue_title": "AttributeError: 'Command' object has no attribute 'content'", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing import Annotated, List\nfrom langgraph.types import Command\nfrom langchain_core.tools import tool\nfrom langchain_core.messages import AIMessage\n\ndef create_handoff_tool(agent_list: List[str]):\n    \"\"\"Create a tool that can return handoff via a Command\"\"\"\n    available_agents = agent_list + [\"__end__\"]\n    \n    @tool\n    def handoff_to_agent(\n        agent_name: Annotated[str, \"The name of the agent to handoff to\"]\n    ):\n        \"\"\"Handoff to another agent or end the task.\"\"\"\n        # check if agent name in the list\n        if agent_name not in available_agents:\n            error_message = f\"Agent {agent_name} is not available. Choose one of: {', '.join(available_agents)}\",\n            return Command(\n                goto=Command.PARENT, \n                graph=Command.PARENT,\n                update={\"messages\": [AIMessage(content=error_message)]},\n            )\n\n        # return the routing command\n        return Command(\n            goto=agent_name,\n            graph=Command.PARENT,\n            update={\"messages\": [AIMessage(content=f\"Successfully transferred to {agent_name}\")]},\n        )\n    \n    # dynamically modify doc string\n    handoff_to_agent.__doc__ = \"\\n\".join([\n        \"Transfer to another agent or end the task.\",\n        f\"Available agents: {', '.join(available_agents)}.\",\n        \"If you want to end the task, use '__end__'.\"\n     ])\n    \n    return handoff_to_agent\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\n  File \"/Users/nickyoungblut/mambaforge/envs/genomics-guide2/lib/python3.11/site-packages/streamlit/runtime/scriptrunner/exec_code.py\", line 88, in exec_func_with_error_handling\n    result = func()\n             ^^^^^^\n  File \"/Users/nickyoungblut/mambaforge/envs/genomics-guide2/lib/python3.11/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 579, in code_to_exec\n    exec(code, module.__dict__)\n  File \"/Users/nickyoungblut/dev/python/streamlit/genomics_guide2/app.py\", line 133, in <module>\n    response = asyncio.run(\n               ^^^^^^^^^^^^\n  File \"/Users/nickyoungblut/mambaforge/envs/genomics-guide2/lib/python3.11/asyncio/runners.py\", line 190, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/Users/nickyoungblut/mambaforge/envs/genomics-guide2/lib/python3.11/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/nickyoungblut/mambaforge/envs/genomics-guide2/lib/python3.11/asyncio/base_events.py\", line 654, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/Users/nickyoungblut/dev/python/streamlit/genomics_guide2/genomics_guide2/astream_event_handler.py\", line 49, in astream_graph\n    output_placeholder.code(event['data'].get('output').content)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'Command' object has no attribute 'content'\nDescription\nCommand is not working with this simple example, and output_placeholder.code(event['data'].get('output').content) is not very helpful for determining the cause of the error.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.2.0: Fri Dec  6 18:56:34 PST 2024; root:xnu-11215.61.5~2/RELEASE_ARM64_T6020\nPython Version:  3.11.11 | packaged by conda-forge | (main, Dec  5 2024, 14:21:42) [Clang 18.1.8 ]\n\nPackage Information\n\nlangchain_core: 0.3.33\nlangchain: 0.3.17\nlangchain_community: 0.3.13\nlangsmith: 0.1.147\nlangchain_groq: 0.2.2\nlangchain_openai: 0.3.3\nlangchain_text_splitters: 0.3.4\nlangchain_weaviate: 0.0.3\nlangchainhub: 0.1.21\nlanggraph_sdk: 0.1.48\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.11\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.7\ngroq: 0.13.1\nhttpx: 0.27.0\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.61.0\norjson: 3.10.12\npackaging: 24.2\npydantic: 2.10.4\npydantic-settings: 2.7.0\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nsimsimd: 4.4.0\nSQLAlchemy: 2.0.36\ntenacity: 8.5.0\ntiktoken: 0.8.0\ntypes-requests: 2.32.0.20241016\ntyping-extensions: 4.12.2\nweaviate-client: 4.8.1\n", "created_at": "2025-02-02", "closed_at": "2025-02-14", "labels": ["question"], "State": "closed", "Author": "nick-youngblut"}
{"issue_number": 3275, "issue_title": "Interrupt using the same old question, when invoked second time.", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\ndef ask_user_node(state: LookupState) -> Command[Literal['lookup_node']]:\n       \n        user_response = interrupt(state['messages'][-1].content) # intead of taking the new message content its taking the old value that it has already shown to the user.\n\n        if user_response:\n            return Command(goto='lookup_node',\n                           update={'messages': [HumanMessage(content=user_response, name=\"User_Response\")]})\nError Message and Stack Trace (if applicable)\n\nDescription\nI had a node that has interrupt and this node might be called multiple times, to collect information from the user.  When interrupt is invoked for the second time it shows the previous question instead of showing the new question to user.\nSystem Info\npython -m langchain_core.sys_info", "created_at": "2025-02-01", "closed_at": null, "labels": ["bug"], "State": "open", "Author": "Saisiva123"}
{"issue_number": 3267, "issue_title": "Agents failing to load Environment Variables", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport os\nfrom dotenv import load_dotenv\n\n\n# Load environment variables from a .env file\nload_dotenv()\n\nvar1 = os.environ.get(\"VAR1\")\nvar2 = os.environ.get(\"VAR2\")\n\nprint(\"=====================================\")\nprint(f\"var1: {var1}\")\nprint(f\"var2: {var2}\")\nprint(\"=====================================\")\nError Message and Stack Trace (if applicable)\n2025-01-31 13:05:22 =====================================\n2025-01-31 13:05:22 var1: None\n2025-01-31 13:05:22 var2: None\n2025-01-31 13:05:22 =====================================\nDescription\nI have an application deployed to a local langgraph server.  The application contains several agents and I have configured the recommended file structure and provided a .env file with the variables and I have also provided a langgraph.json file which has an entry for the env.\nWhen I run and call one of the agents, I added the code above and it seems like the variables are not being loaded.\nThe structure of my project is as follow:\nmy-app/\n|\u2500\u2500 my_agent1\n\u2502   |\u2500\u2500 utils\n\u2502   \u2502   |\u2500\u2500 init.py\n\u2502   \u2502   |\u2500\u2500 tools.py\n\u2502   \u2502   |\u2500\u2500 nodes.py\n\u2502   \u2502   |\u2500\u2500 state.py\n\u2502\u00a0\u00a0 |\u2500\u2500 requirements.txt\n\u2502\u00a0\u00a0 |\u2500\u2500 init.py\n\u2502\u00a0\u00a0 |\u2500\u2500 agent.py\n|\u2500\u2500 my_agent2\n\u2502   |\u2500\u2500 utils\n\u2502   \u2502   |\u2500\u2500 init.py\n\u2502   \u2502   |\u2500\u2500 tools.py\n\u2502   \u2502   |\u2500\u2500 nodes.py\n\u2502   \u2502   |\u2500\u2500 state.py\n\u2502\u00a0\u00a0 |\u2500\u2500 requirements.txt\n\u2502\u00a0\u00a0 |\u2500\u2500 init.py\n\u2502\u00a0\u00a0 |\u2500\u2500 agent.py\n|\u2500\u2500 my_agent3\n\u2502   |\u2500\u2500 utils\n\u2502   \u2502   |\u2500\u2500 init.py\n\u2502   \u2502   |\u2500\u2500 tools.py\n\u2502   \u2502   |\u2500\u2500 nodes.py\n\u2502   \u2502   |\u2500\u2500 state.py\n\u2502\u00a0\u00a0 |\u2500\u2500 requirements.txt\n\u2502\u00a0\u00a0 |\u2500\u2500 init.py\n\u2502\u00a0\u00a0 |\u2500\u2500 agent.py\n|\u2500\u2500 .env # environment variables\n|\u2500\u2500 langgraph.json # configuration file for LangGraph\nThe langgraph.json file is:\n{\n\"dockerfile_lines\": [],\n\"dependencies\": [\n\"./my_agent1\",\n\"./my_agent2\",\n\"./my_agent3\"\n],\n\"graphs\": {\n\"agent1\": \"./my_agent1/agent.py:graph\",\n\"agent2\": \"./my_agent2/agent.py:graph\",\n\"agent3\": \"./my_agent3/agent.py:graph\"\n},\n\"env\": \"./.env\",\n\"python_version\": \"3.11\"\n}\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.6.0: Mon Jul 29 21:13:00 PDT 2024; root:xnu-10063.141.2~1/RELEASE_X86_64\nPython Version:  3.11.7 (main, Dec  4 2023, 18:10:11) [Clang 15.0.0 (clang-1500.1.0.2.5)]\n\nPackage Information\n\nlangchain_core: 0.3.31\nlangchain: 0.3.15\nlangchain_community: 0.3.15\nlangsmith: 0.2.10\nlangchain_openai: 0.3.2\nlangchain_text_splitters: 0.3.5\nlanggraph_cli: 0.1.67\nlanggraph_sdk: 0.1.51\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.11\nasync-timeout: Installed. No version info available.\nclick: 8.1.8\ndataclasses-json: 0.6.7\nhttpx: 0.28.1\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlanggraph-api: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.59.7\norjson: 3.10.14\npackaging: 24.2\npydantic: 2.10.5\npydantic-settings: 2.7.1\npython-dotenv: 1.0.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.37\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\nzstandard: Installed. No version info available.\n", "created_at": "2025-01-31", "closed_at": "2025-02-06", "labels": ["question"], "State": "closed", "Author": "magallardo"}
{"issue_number": 3266, "issue_title": "Unexpected State Update", "issue_body": "Checked other resources\n\n I added a very descriptive title to this issue.\n I searched the LangChain documentation with the integrated search.\n I used the GitHub search to find a similar question and didn't find it.\n I am sure that this is a bug in LangChain rather than my code.\n The bug is not resolved by updating to the latest stable version of LangChain (or the specific integration package).\n\nExample Code\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.tools import tool\nfrom langchain_core.messages import  HumanMessage , AIMessage\nfrom langchain_core.runnables import RunnableConfig\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph, START, add_messages\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\nfrom typing import Annotated, List, TypedDict\nfrom rich.console import Console\nfrom rich.markdown import Markdown\nfrom rich import print as rprint\nfrom environs import Env\n\nfrom prompts import system_prompt\n\nfrom moviepy import *\nimport uuid\n\nenv = Env()\nenv.read_env()\n\nconsole = Console()\n\n# Memory configuration\nmemory = MemorySaver()\n\nvideo_path = \"video.mp4\"\n\nclass VideoAppStateGraph(TypedDict):\n    messages : Annotated[list , add_messages]\n    video_path : str\n\n@tool\ndef trim_video(start_time: str, end_time: str, state : VideoAppStateGraph ,config: RunnableConfig):\n    \"\"\"\n    Trim a video between specified start and end times.\n    \n    Args:\n        start_time (str): Start time in format HH:MM:SS\n        end_time (str): End time in format HH:MM:SS\n    \n    Returns:\n        str: Confirmation message\n    \"\"\"\n    \n    print(\"-\"*10)\n    print(\"trim_video\")\n    # print(state)\n    # print(\"-\"*10)\n    print(config)\n    print(\"-\"*10)\n    print(start_time)\n    print(end_time)\n    print(\"-\"*10)\n\n    \n    video_path_trimmed = f\"{uuid.uuid4().hex}.mp4\"\n    ffmpeg_tools.ffmpeg_extract_subclip(state[\"video_path\"] , start_time , end_time ,video_path_trimmed)\n    \n    return {\n        \"video_path\" : video_path_trimmed,\n        \"messages\" : [AIMessage(content=f\"Video has been trimmed from {start_time} to {end_time}\")]\n    }\n\n@tool\ndef get_video_duration(state : VideoAppStateGraph):\n    \"\"\"\n    Get Duration of the Video\n    \n    \n    Returns:\n        str: Duration of the video\n    \"\"\"\n    \n    print(\"-\"*10)\n    print(\"get_video_duration\")\n    print(state[\"video_path\"])\n    print(\"-\"*10)\n    video = VideoFileClip(state[\"video_path\"])\n    \n    duration =  video.duration\n    \n    return {\n        \"video_path\" : state[\"video_path\"],\n        \"messages\" : [AIMessage(content=f\"Duration is {int(duration)} seconds\")]\n    }\n    \n    \n@tool\ndef get_video_url(state : VideoAppStateGraph):\n    \"\"\"\n    Get the video url of updated actions of video\n    \n    return str: video_url\n    \"\"\"\n    \n    base = env.str(\"HOSTED_BACKEND_URL\")\n    \n    return {\n        \"video_path\" : state[\"video_path\"],\n        \"messages\" : [AIMessage(content=f\"{base}/{state[\"video_path\"]}\")]\n    }\n\n# Define tools list\ntools = [trim_video, get_video_duration , get_video_url]\n\n# Create tool node\ntool_node = ToolNode(tools=tools)\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", system_prompt),\n    MessagesPlaceholder(variable_name=\"messages\"),\n])\n\n# Initialize the model with tools\nmodel = ChatOpenAI(\n    temperature=0.5,\n    api_key=env.str(\"OPENAI_API_KEY\"),\n).bind_tools(tools)\n\n\ndef call_model(state):\n    \"\"\"\n    Process the current state and generate a response.\n    \"\"\"\n    print(\"*\"*10)\n    print(\"call_model\")\n    print(state[\"video_path\"])\n    print(\"*\"*10)\n    response = model.invoke(prompt.invoke({\"messages\": state[\"messages\"]}))\n    return {\"messages\": [response]}    \n\n# Create the workflow graph\nworkflow = StateGraph(VideoAppStateGraph)\n\n# Add nodes\nworkflow.add_node(\"assistant\", call_model)\nworkflow.add_node(\"tools\", tool_node)\n\n# Add Edges\nworkflow.add_edge(START, \"assistant\")\nworkflow.add_conditional_edges(\"assistant\", tools_condition)\nworkflow.add_edge(\"tools\", \"assistant\")\n\n\n# Compile the app\napp = workflow.compile(checkpointer=memory)\n\nclass VideoEditor:\n    def __init__(self):\n        self.messages: List[HumanMessage] = []\n        self.video_path : str = video_path\n        \n    def process_message(self, user_input: str) -> str:\n        \"\"\"Process a single message and return the response\"\"\"\n        self.messages.append(HumanMessage(content=user_input))\n        \n        response = app.invoke({\n            \"messages\": self.messages,\n            \"video_path\" : self.video_path\n        } , config={ \"configurable\" : {\"thread_id\": \"1312312312\"}})\n        \n        # Extract the last assistant message\n        last_message = response[\"messages\"][-1]\n        self.messages.extend(response[\"messages\"])\n        \n        return last_message.content\n\ndef main():\n    \"\"\"Main CLI interface for the video editing agent\"\"\"\n    editor = VideoEditor()\n    \n    # Print welcome message\n    console.print(\"[bold blue]Welcome to the Video Editing Assistant![/bold blue]\")\n    console.print(\"Type 'quit' or 'exit' to end the conversation.\\n\")\n    \n    while True:\n        try:\n            # Get user input\n            user_input = console.input(\"[bold green]You:[/bold green] \")\n            \n            # Check for exit command\n            if user_input.lower() in ['quit', 'exit', 'bye', 'goodbye']:\n                console.print(\"\\n[bold blue]Goodbye! Thank you for using the Video Editing Assistant.[/bold blue]\")\n                break\n            \n            # Process the message\n            response = editor.process_message(user_input)\n            \n            # Print the response with markdown formatting\n            console.print(\"\\n[bold purple]Assistant:[/bold purple]\")\n            console.print(Markdown(response))\n            console.print()  # Empty line for better readability\n            \n        except KeyboardInterrupt:\n            console.print(\"\\n[bold red]Session terminated by user.[/bold red]\")\n            break\n        except Exception as e:\n            console.print(f\"\\n[bold red]An error occurred: {str(e)}[/bold red]\")\n            console.print(\"Please try again or type 'exit' to quit.\")\n\nif __name__ == \"__main__\":\n    main()\n\nError Message and Stack Trace (if applicable)\nNo response\nDescription\nUnexpected State Change,\nInitially, When, call_model is called, value of state[\"video_path\"] is correct(\"video.mp4\"), but when there is a tool call, unexpectedly, state is being updated, and print inside the trim_video tool says state[\"video_path\"] is \"sample_video.mp4\"\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.22631\nPython Version:  3.13.0 (tags/v3.13.0:60403a5, Oct  7 2024, 09:38:07) [MSC v.1941 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.3.33\nlangsmith: 0.3.3\nlangchain_openai: 0.3.3\nlanggraph_sdk: 0.1.51\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nhttpx: 0.28.1\njsonpatch: 1.33\nlangsmith-pyo3: Installed. No version info available.\nopenai: 1.60.2\norjson: 3.10.15\npackaging: 24.2\npydantic: 2.10.6\npytest: Installed. No version info available.\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrich: 13.9.4\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\nzstandard: 0.23.0\n", "created_at": "2025-01-31", "closed_at": "2025-03-05", "labels": ["question"], "State": "closed", "Author": "Sarthak-ONS"}
{"issue_number": 3261, "issue_title": "LangSmith Studio does not work in Chrome due to CORS issue", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nlanggraph new path/to/your/app --template react-agent-python \nlanggraph dev\nError Message and Stack Trace (if applicable)\n\n\nDescription\nThe LangGraph Dev Server does not produce the CORS header to make it compatible with accessing Private Local Networks in Chrome 132 (possibly 130+, see references).\nWorkarounds\n\nUse NGROK to hide the private network access\nVisit chrome://flags and disable the security for this. (not recommended!)\n\n\nReferences\n\nChrome Initial Announcement (2022)\nChrome Enforcement announcements tentatively for Chrome 130+ (Mar 2024)\n\nSystem Info\npython -m langchain_core.sys_info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP Fri Nov 29 17:22:03 UTC 2024\nPython Version:  3.12.7 (main, Oct 19 2024, 03:00:35) [GCC 12.2.0]\n\nPackage Information\n\nlangchain_core: 0.3.31\nlangchain: 0.3.15\nlangchain_community: 0.3.15\nlangsmith: 0.2.11\nlangchain_openai: 0.3.1\nlangchain_postgres: 0.0.12\nlangchain_text_splitters: 0.3.5\nlanggraph_api: 0.0.20\nlanggraph_cli: 0.1.70\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.51\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.11\nasync-timeout: Installed. No version info available.\nclick: 8.1.8\ncryptography: 43.0.3\ndataclasses-json: 0.6.7\nhttpx: 0.28.1\nhttpx-sse: 0.4.0\njsonpatch: 1.33\njsonschema-rs: 0.25.1\nlanggraph: 0.2.69\nlanggraph-checkpoint: 2.0.10\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.59.9\norjson: 3.10.15\npackaging: 24.2\npgvector: 0.2.5\npsycopg: 3.2.3\npsycopg-pool: 3.2.4\npydantic: 2.9.2\npydantic-settings: 2.7.1\npyjwt: 2.10.1\npython-dotenv: 1.0.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.37\nsqlalchemy: 2.0.37\nsse-starlette: 2.1.3\nstarlette: 0.45.3\nstructlog: 24.4.0\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\nuvicorn: 0.34.0\nwatchfiles: 1.0.4\nzstandard: Installed. No version info available.\n", "created_at": "2025-01-31", "closed_at": "2025-01-31", "labels": [], "State": "closed", "Author": "adamduren"}
{"issue_number": 3259, "issue_title": "Streaming with ReAct Agent + Ollama does not work?", "issue_body": "Discussed in #3215\n\nOriginally posted by ahenkes1 January 27, 2025\nHello everyone. I have built a ReAct agent using the Llama3.2 model from Ollama. I used\nlanggraph.prebuilt.create_react_agent and langchain_ollama.ChatOllama. I created some tools and added them to the model.\nFor pretty printing, I am using the following code to astream over the agent:\n        async for chk in agent.astream(prompt, config=config):  # type: ignore\n            result = list(chk.values())[0]\n            try:\n                message = result[\"messages\"][-1]\n                role = message.response_metadata[\"message\"].role\n                tools = message.tool_calls\n\n                if role == \"assistant\" and tools == []:\n                    result = \"AI: \" + message.content\n                    formatted_string = textwrap.fill(result, width=79)\n                    print(formatted_string)\n                    print(f\"{79 * '-'}\\n\")\n\n            except Exception:\n                pass\n\nThis works fine, but the behavior is odd. There is no per token streaming, but instead the whole answer is plotted. I recall that I read somewhere, that tooling and streaming is incompatible in langgraph. Can someone make sense out of this?", "created_at": "2025-01-31", "closed_at": "2025-01-31", "labels": [], "State": "closed", "Author": "ahenkes1"}
{"issue_number": 3257, "issue_title": "When StreamWriter is used as an argument in a node function in StateGraph.add_node, mypy raises a type error", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langchain_core.messages import AIMessage\nfrom langgraph.graph import START, StateGraph, MessagesState, END\nfrom langgraph.types import StreamWriter\n\n\nasync def my_node(\n    state: MessagesState,\n    writer: StreamWriter,  # <-- provide StreamWriter to write chunks to be streamed\n):\n    chunks = [\n        \"Four\",\n        \"score\",\n        \"and\",\n        \"seven\",\n        \"years\",\n        \"ago\",\n        \"our\",\n        \"fathers\",\n        \"...\",\n    ]\n    for chunk in chunks:\n        # write the chunk to be streamed using stream_mode=custom\n        writer(chunk)\n\n    return {\"messages\": [AIMessage(content=\" \".join(chunks))]}\n\n\n# Define a new graph\nworkflow = StateGraph(MessagesState)\n\nworkflow.add_node(\"model\", my_node)\nworkflow.add_edge(START, \"model\")\nworkflow.add_edge(\"model\", END)\n\napp = workflow.compile()\nError Message and Stack Trace (if applicable)\na.py:31: error: Argument 2 to \"add_node\" of \"StateGraph\" has incompatible type \"Callable[[MessagesState, Callable[[Any], None]], Coroutine[Any, Any, Any]]\"; expected \"Runnable[Any, Any] | Callable[[Any], Any] | Callable[[Any], Awaitable[Any]] | Callable[[Iterator[Any]], Iterator[Any]] | Callable[[AsyncIterator[Any]], AsyncIterator[Any]] | _RunnableCallableSync[Any, Any] | _RunnableCallableAsync[Any, Any] | _RunnableCallableIterator[Any, Any] | _RunnableCallableAsyncIterator[Any, Any] | Mapping[str, Any]\"  [arg-type]\nFound 1 error in 1 file (checked 1 source file)\nDescription\n\nI\u2019m trying to use StreamWriter as an argument in node\nThe example code is from https://langchain-ai.github.io/langgraph/how-tos/streaming-content/#define-the-graph\n\nExpected Behavior\n\nNo type error should be raised by mypy since StreamWriter should be a valid argument\n\nActual Behavior\n\nmypy raises a type error about add_node\nWithout the StreamWriter argument, the error does not occur\n\nSystem Info\nlanggraph: 0.2.69\nmypy: 1.14.1\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.1.0: Thu Oct 10 21:05:14 PDT 2024; root:xnu-11215.41.3~2/RELEASE_ARM64_T8103\nPython Version:  3.12.8 (main, Jan 31 2025, 13:06:09) [Clang 16.0.0 (clang-1600.0.26.4)]\n\nPackage Information\n\nlangchain_core: 0.3.33\nlangsmith: 0.3.3\nlanggraph_sdk: 0.1.51\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nhttpx: 0.28.1\njsonpatch: 1.33\nlangsmith-pyo3: Installed. No version info available.\norjson: 3.10.15\npackaging: 24.2\npydantic: 2.10.6\npytest: Installed. No version info available.\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrich: Installed. No version info available.\ntenacity: 9.0.0\ntyping-extensions: 4.12.2\nzstandard: 0.23.0\n", "created_at": "2025-01-31", "closed_at": "2025-02-03", "labels": [], "State": "closed", "Author": "ryowk"}
{"issue_number": 3249, "issue_title": "Node with multiple incoming edges not executed correctly when combined with conditional edges", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nbuilder = StateGraph(State)\nbuilder.add_node(\"a\", ReturnNodeValue(\"I'm A\"))\nbuilder.add_edge(START, \"a\")\nbuilder.add_node(\"b\", ReturnNodeValue(\"I'm B\"))\nbuilder.add_node(\"b2a\", ReturnNodeValue(\"I'm B2a\"))\nbuilder.add_node(\"b2b\", ReturnNodeValue(\"I'm B2b\"))\nbuilder.add_node(\"c\", ReturnNodeValue(\"I'm C\"))\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"a\", \"c\")\nbuilder.add_edge([\"b2a\", \"b2b\"], \"c\")\nbuilder.add_edge(\"c\", END)\n\n\ndef router(state: State) -> Sequence[str]:\n    return state[\"which\"]\n\n\nbuilder.add_conditional_edges(\n    \"b\",\n    router,\n    [\"b2a\", \"b2b\"],\n)\n\nbuilder.set_entry_point(\"a\")\nbuilder.set_finish_point(\"c\")\ngraph = builder.compile()\nprint(graph.invoke({\"aggregate\": [], \"which\": \"b2a\"}))\n\n>> Adding I'm A to []\n>> Adding I'm B to [\"I'm A\"]\n>> Adding I'm C to [\"I'm A\"]\n>> Adding I'm B2a to [\"I'm A\", \"I'm B\", \"I'm C\"]\n>> {'aggregate': [\"I'm A\", \"I'm B\", \"I'm C\", \"I'm B2a\"], 'which': 'b2a'}\nError Message and Stack Trace (if applicable)\n\nDescription\nIn the example code above (diagram below), I would expect node C to execute with inputs from A, B2a and B2b but instead it executes only with input from A (note that B2a and B2b have conditional edges to C).\n\nFurthermore, making node C dependent on all three incoming nodes results in C not being executed at all:\nbuilder.add_edge([\"a\", \"b2a\", \"b2b\"], \"c\")\nNew output:\nAdding I'm A to []\nAdding I'm B to [\"I'm A\"]\nAdding I'm B2a to [\"I'm A\", \"I'm B\"]\n{'aggregate': [\"I'm A\", \"I'm B\", \"I'm B2a\"], 'which': 'b2a'}\n\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #52-Ubuntu SMP PREEMPT_DYNAMIC Thu Dec  5 13:09:44 UTC 2024\nPython Version:  3.11.10 (main, Oct  8 2024, 00:19:50) [Clang 18.1.8 ]\n\nPackage Information\n\nlangchain_core: 0.3.25\nlangchain: 0.3.9\nlangchain_community: 0.3.3\nlangsmith: 0.1.137\nlangchain_anthropic: 0.2.3\nlangchain_openai: 0.2.3\nlangchain_text_splitters: 0.3.0\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.10.11\nanthropic: 0.37.1\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.7\ndefusedxml: 0.7.1\nhttpx: 0.24.1\njsonpatch: 1.33\nnumpy: 1.26.2\nopenai: 1.52.1\norjson: 3.10.0\npackaging: 23.2\npydantic: 2.9.2\npydantic-settings: 2.6.0\nPyYAML: 6.0.1\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.23\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\n", "created_at": "2025-01-30", "closed_at": "2025-02-06", "labels": [], "State": "closed", "Author": "snopoke"}
{"issue_number": 3246, "issue_title": "Error Triggered When Resuming Graph with Interrupt via `astream` Method Using `Command` Object and Config Parameters", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n# Set up the state\nfrom langgraph.graph import MessagesState, START\n\n# Set up the tool\n# We will have one real tool - a search tool\n# We'll also have one \"fake\" tool - a \"ask_human\" tool\n# Here we define any ACTUAL tools\nfrom langchain_core.tools import tool\nfrom langgraph.prebuilt import ToolNode\nfrom langgraph.types import interrupt, Command\n\n\n@tool\ndef search_tool(query: str):\n    \"\"\"Call to surf the web.\"\"\"\n    # This is a placeholder for the actual implementation\n    # Don't let the LLM know this though \ud83d\ude0a\n    return f\"I looked up: {query}. Result: It's sunny in San Francisco, but you better look out if you're a Gemini \ud83d\ude08.\"\n\n\ntools = [search_tool]\ntool_node = ToolNode(tools)\n\n# Set up the model\nfrom langchain_openai import ChatOpenAI\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nmodel=\"deepseek-v3\"\nmodel=\"deepseek-r1\"\nmodel=\"qwen-turbo\"\nmodel=\"qwen-max-0125\"\n\napi_key = os.getenv(\"QWEN_API_KEY\")\nbase_url = os.getenv(\"QWEN_BASE_URL\")\n\nmodel = ChatOpenAI(model=model, api_key=api_key, base_url=base_url)\n\nfrom pydantic import BaseModel\n\n\n# We are going \"bind\" all tools to the model\n# We have the ACTUAL tools from above, but we also need a mock tool to ask a human\n# Since `bind_tools` takes in tools but also just tool definitions,\n# We can define a tool definition for `ask_human`\nclass AskHuman(BaseModel):\n    \"\"\"Ask the human a question\"\"\"\n\n    question: str\n\n\nmodel = model.bind_tools(tools + [AskHuman])\n\n# Define nodes and conditional edges\n\n\n# Define the function that determines whether to continue or not\ndef should_continue(state):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    # If there is no function call, then we finish\n    if not last_message.tool_calls:\n        return END\n    # If tool call is asking Human, we return that node\n    # You could also add logic here to let some system know that there's something that requires Human input\n    # For example, send a slack message, etc\n    elif last_message.tool_calls[0][\"name\"] == \"AskHuman\":\n        return \"ask_human\"\n    # Otherwise if there is, we continue\n    else:\n        return \"action\"\n\n\n# Define the function that calls the model\nasync def call_model(state):\n    messages = state[\"messages\"]\n    response = await model.ainvoke(messages)\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": [response]}\n\n\n# We define a fake node to ask the human\ndef ask_human(state):\n    tool_call_id = state[\"messages\"][-1].tool_calls[0][\"id\"]\n    location = interrupt(\"Please provide your location:\")\n    tool_message = [{\"tool_call_id\": tool_call_id, \"type\": \"tool\", \"content\": location}]\n    return {\"messages\": tool_message}\n\n\n# Build the graph\n\nfrom langgraph.graph import END, StateGraph\n\n# Define a new graph\nworkflow = StateGraph(MessagesState)\n\n# Define the three nodes we will cycle between\nworkflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"action\", tool_node)\nworkflow.add_node(\"ask_human\", ask_human)\n\n# Set the entrypoint as `agent`\n# This means that this node is the first one called\nworkflow.add_edge(START, \"agent\")\n\n# We now add a conditional edge\nworkflow.add_conditional_edges(\n    # First, we define the start node. We use `agent`.\n    # This means these are the edges taken after the `agent` node is called.\n    \"agent\",\n    # Next, we pass in the function that will determine which node is called next.\n    should_continue,\n)\n\n# We now add a normal edge from `tools` to `agent`.\n# This means that after `tools` is called, `agent` node is called next.\nworkflow.add_edge(\"action\", \"agent\")\n\n# After we get back the human response, we go back to the agent\nworkflow.add_edge(\"ask_human\", \"agent\")\n\n# Set up memory\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom IPython.display import Image, display\n\nmemory = MemorySaver()\n\n# Finally, we compile it!\n# This compiles it into a LangChain Runnable,\n# meaning you can use it as you would any other runnable\n# We add a breakpoint BEFORE the `ask_human` node so it never executes\napp = workflow.compile(checkpointer=memory)\n\nconfig = {\"configurable\": {\"thread_id\": \"2\"}}\nasync for event in app.astream(\n    {\n        \"messages\": [\n            (\n                \"user\",\n                \"Use the search tool to ask the user where they are, then look up the weather there\",\n            )\n        ]\n    },\n    config,\n    stream_mode=\"values\",\n):\n    event[\"messages\"][-1].pretty_print()\n\nasync for event in app.astream_log(Command(resume=\"san francisco\"), config, stream_mode=\"values\"):\n    print(event.model_dump_json(indent=2))\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[13], line 2\n      1 config = {\"configurable\": {\"thread_id\": \"2\"}}\n----> 2 async for event in app.astream(\n      3     {\n      4         \"messages\": [\n      5             (\n      6                 \"user\",\n      7                 \"Use the search tool to ask the user where they are, then look up the weather there\",\n      8             )\n      9         ]\n     10     },\n     11     config,\n     12     stream_mode=\"values\",\n     13 ):\n     14     event[\"messages\"][-1].pretty_print()\n\nFile ~/miniforge3/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1899, in Pregel.astream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\n   1893 # Similarly to Bulk Synchronous Parallel / Pregel model\n   1894 # computation proceeds in steps, while there are channel updates\n   1895 # channel updates from step N are only visible in step N+1\n   1896 # channels are guaranteed to be immutable for the duration of the step,\n   1897 # with channel updates applied only at the transition between steps\n   1898 while loop.tick(input_keys=self.input_channels):\n-> 1899     async for _ in runner.atick(\n   1900         loop.tasks.values(),\n   1901         timeout=self.step_timeout,\n   1902         retry_policy=self.retry_policy,\n   1903         get_waiter=get_waiter,\n   1904     ):\n   1905         # emit output\n   1906         for o in output():\n   1907             yield o\n\nFile ~/miniforge3/lib/python3.10/site-packages/langgraph/pregel/runner.py:444, in PregelRunner.atick(self, tasks, reraise, timeout, retry_policy, get_waiter)\n    442 t = tasks[0]\n    443 try:\n--> 444     await arun_with_retry(\n    445         t,\n    446         retry_policy,\n    447         stream=self.use_astream,\n    448         configurable={\n    449             CONFIG_KEY_SEND: partial(writer, t),\n    450             CONFIG_KEY_CALL: partial(call, t),\n    451         },\n    452     )\n    453     self.commit(t, None)\n    454 except Exception as exc:\n\nFile ~/miniforge3/lib/python3.10/site-packages/langgraph/pregel/retry.py:128, in arun_with_retry(task, retry_policy, stream, configurable)\n    126         break\n    127     else:\n--> 128         return await task.proc.ainvoke(task.input, config)\n    129 except ParentCommand as exc:\n    130     ns: str = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n\nFile ~/miniforge3/lib/python3.10/site-packages/langgraph/utils/runnable.py:499, in RunnableSeq.ainvoke(self, input, config, **kwargs)\n    495 config = patch_config(\n    496     config, callbacks=run_manager.get_child(f\"seq:step:{i + 1}\")\n    497 )\n    498 if i == 0:\n--> 499     input = await step.ainvoke(input, config, **kwargs)\n    500 else:\n    501     input = await step.ainvoke(input, config)\n\nFile ~/miniforge3/lib/python3.10/site-packages/langgraph/utils/runnable.py:289, in RunnableCallable.ainvoke(self, input, config, **kwargs)\n    287         ret = await asyncio.create_task(coro, context=context)\n    288     else:\n--> 289         ret = await self.afunc(*args, **kwargs)\n    290 if isinstance(ret, Runnable) and self.recurse:\n    291     return await ret.ainvoke(input, config)\n\nFile ~/miniforge3/lib/python3.10/site-packages/langchain_core/runnables/config.py:588, in run_in_executor(executor_or_config, func, *args, **kwargs)\n    584         raise RuntimeError from exc\n    586 if executor_or_config is None or isinstance(executor_or_config, dict):\n    587     # Use default executor with context copied from current context\n--> 588     return await asyncio.get_running_loop().run_in_executor(\n    589         None,\n    590         cast(Callable[..., T], partial(copy_context().run, wrapper)),\n    591     )\n    593 return await asyncio.get_running_loop().run_in_executor(executor_or_config, wrapper)\n\nFile ~/miniforge3/lib/python3.10/concurrent/futures/thread.py:58, in _WorkItem.run(self)\n     55     return\n     57 try:\n---> 58     result = self.fn(*self.args, **self.kwargs)\n     59 except BaseException as exc:\n     60     self.future.set_exception(exc)\n\nFile ~/miniforge3/lib/python3.10/site-packages/langchain_core/runnables/config.py:579, in run_in_executor..wrapper()\n    577 def wrapper() -> T:\n    578     try:\n--> 579         return func(*args, **kwargs)\n    580     except StopIteration as exc:\n    581         # StopIteration can't be set on an asyncio.Future\n    582         # it raises a TypeError and leaves the Future pending forever\n    583         # so we need to convert it to a RuntimeError\n    584         raise RuntimeError from exc\n\nCell In[12], line 87, in ask_human(state)\n     85 def ask_human(state):\n     86     tool_call_id = state[\"messages\"][-1].tool_calls[0][\"id\"]\n---> 87     location = interrupt(\"Please provide your location:\")\n     88     tool_message = [{\"tool_call_id\": tool_call_id, \"type\": \"tool\", \"content\": location}]\n     89     return {\"messages\": tool_message}\n\nFile ~/miniforge3/lib/python3.10/site-packages/langgraph/types.py:471, in interrupt(value)\n    468 from langgraph.errors import GraphInterrupt\n    469 from langgraph.utils.config import get_config\n--> 471 conf = get_config()[\"configurable\"]\n    472 # track interrupt index\n    473 scratchpad: PregelScratchpad = conf[CONFIG_KEY_SCRATCHPAD]\n\nFile ~/miniforge3/lib/python3.10/site-packages/langgraph/utils/config.py:337, in get_config()\n    335     return var_config\n    336 else:\n--> 337     raise RuntimeError(\"Called get_config outside of a runnable context\")\n\nRuntimeError: Called get_config outside of a runnable context\nDescription\ntry using astream, but there's bug as ErrorMessage\nSystem Info\nystem Information\n\nOS:  Linux\nOS Version:  #183-Ubuntu SMP Mon Oct 2 11:28:33 UTC 2023\nPython Version:  3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:58:44) [GCC 11.3.0]\n\nPackage Information\n\nlangchain_core: 0.3.32\nlangchain: 0.3.16\nlangsmith: 0.3.2\nlangchain_ollama: 0.2.2\nlangchain_openai: 0.2.14\nlangchain_text_splitters: 0.3.5\nlanggraph_sdk: 0.1.51\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.11\nasync-timeout: 4.0.3\nhttpx: 0.28.1\njsonpatch: 1.33\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nollama: 0.4.7\nopenai: 1.60.2\norjson: 3.10.15\npackaging: 24.2\npydantic: 2.10.5\npytest: Installed. No version info available.\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrich: 13.9.4\nSQLAlchemy: 2.0.37\ntenacity: 9.0.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\nzstandard: 0.23.0\n", "created_at": "2025-01-30", "closed_at": "2025-01-30", "labels": [], "State": "closed", "Author": "lanhui100"}
{"issue_number": 3206, "issue_title": "Subgraph checkpointer=True causes subgraph to be skipped", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing import Literal\n\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import END, START, StateGraph\nfrom langgraph.graph.state import Command\nfrom langgraph.types import interrupt\nfrom rich import get_console\nfrom typing_extensions import TypedDict\n\n###############\n# Subgraph\n###############\n\n\nclass SubGraphState(TypedDict, total=False):\n    parent_counter: int\n    sub_counter: int\n\n\ndef subgraph_accumulator(state: SubGraphState) -> SubGraphState:\n    get_console().print(\"---subgraph counter node---\")\n    get_console().print(f\"{state = }\")\n    # ask for human approval\n    human_feedback = interrupt(\"get human feedback\")\n    print(f\"{human_feedback = }\")\n\n    # continue counting\n    sub_counter = state[\"sub_counter\"] + 1 if \"sub_counter\" in state else 1\n    return {\"sub_counter\": sub_counter}\n\n\nsub_graph = (\n    StateGraph(SubGraphState)\n    .add_node(subgraph_accumulator)\n    .add_edge(START, subgraph_accumulator.__name__)\n    .add_edge(subgraph_accumulator.__name__, END)\n    .compile(\n        checkpointer=True,  # BUG: This causes an issue that subgraph nodes are not executed at all after first interruption\n    )\n)\nsub_graph.name = \"sub\"\n\n###############\n# Parent Graph\n###############\n\nMAX_ITERATION = 3\n\n\nclass ParentGraphState(TypedDict):\n    parent_counter: int\n\n\ndef parent_graph_accumulator(\n    state: ParentGraphState,\n) -> Command[Literal[\"sub\", \"__end__\"]]:\n    print(\"---parent counter node---\")\n    get_console().print(f\"{state = }\")\n    parent_counter = state[\"parent_counter\"] + 1 if \"parent_counter\" in state else 0\n\n    # goto end when max iteration reaches\n    goto = sub_graph.get_name() if parent_counter < MAX_ITERATION else END\n    get_console().print(f\"going to node {goto}\")\n    return Command(\n        update={\n            \"parent_counter\": parent_counter,\n        },\n        goto=goto,\n    )\n\n\nparent_agent = (\n    StateGraph(ParentGraphState)\n    .add_node(parent_graph_accumulator)\n    .add_node(sub_graph)\n    .add_edge(START, parent_graph_accumulator.__name__)\n    .add_edge(sub_graph.get_name(), parent_graph_accumulator.__name__)\n    .compile(checkpointer=MemorySaver())\n)\n\n# visualize graph\nmermaid_graph = parent_agent.get_graph(xray=True).draw_mermaid()\nprint(mermaid_graph)\n\n###############\n# Conversation\n###############\n\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": \"42\"}, \"recursion_limit\": MAX_ITERATION+1}\n\ninputs = [\n    ParentGraphState(parent_counter=0),\n    Command(resume=\"human feedback 1\"),\n    Command(resume=\"human feedback 2\"),\n]\nfor input_ in inputs:\n    print(f\"{input_ = }\")\n    for event in parent_agent.stream(\n        # resume the conversation\n        input_,\n        config,\n        stream_mode=\"updates\",\n        subgraphs=True,\n    ):\n        print(\"Streaming event ...\")\n        print(event)\nError Message and Stack Trace (if applicable)\ninput_ = {'parent_counter': 0}\n---parent counter node---\nstate = {'parent_counter': 0}\ngoing to node sub\nStreaming event ...\n((), {'parent_graph_accumulator': {'parent_counter': 1}})\n---subgraph counter node---\nstate = {'parent_counter': 1}\nStreaming event ...\n((), {'__interrupt__': (Interrupt(value='get human feedback', resumable=True, ns=['sub', 'subgraph_accumulator:f187d019-da4b-d432-bcd2-cea142aa7e35'], when='during'),)})\ninput_ = Command(resume='human feedback 1')\n---subgraph counter node---\nstate = {'parent_counter': 1}\nhuman_feedback = 'human feedback 1'\nStreaming event ...\n(('sub',), {'subgraph_accumulator': {'sub_counter': 1}})\nStreaming event ...\n((), {'sub': {'parent_counter': 1}})\n---parent counter node---\nstate = {'parent_counter': 1}\ngoing to node sub\nStreaming event ...\n((), {'parent_graph_accumulator': {'parent_counter': 2}})\nStreaming event ...\n((), {'sub': {'parent_counter': 1}})  <------- BUG: should be subgraph execution like (('sub',), {'subgraph_accumulator': {...}})\n---parent counter node---\nstate = {'parent_counter': 1}\ngoing to node sub\nStreaming event ...\n((), {'parent_graph_accumulator': {'parent_counter': 2}})\nStreaming event ...\n((), {'sub': {'parent_counter': 1}})\n---parent counter node---\nstate = {'parent_counter': 1}\ngoing to node sub\nStreaming event ...\n((), {'parent_graph_accumulator': {'parent_counter': 2}})\nTraceback (most recent call last):\n  File \"/home/linux/arcgis-ai-assistants/python/arcgis-assistant/.tmp/subgraph_state_lose/loop_subgraph_with_interrupt.py\", line 99, in <module>\n    for event in parent_agent.stream(\n  File \"/home/linux/miniconda3/envs/test/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 1690, in stream\n    raise GraphRecursionError(msg)\nlanggraph.errors.GraphRecursionError: Recursion limit of 4 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT\nDescription\nI encountered an issue when building a multi-agent graph for multi-turn conversations, where a subgraph has a human feedback node. It interrupts and takes human feedback.\nHere is an example graph to reproduce the issue\n\nA parent graph has a loop, calling a sub graph until the parent counter reaches a pre-defined limit.\nSubgraph node interrupts and take human feedback.\nThe sub graph should remember its state from previous run (checkpointer=True).\n\nExpected Behavior\n\ngraph should interrupt twice, and resume with human inputs\nsubgraph should should persist its state on each run (since checkpointer=True)\n\nActual Behavior\n\nThe first interrupt and resume is as expected\nThe second interrupt never happens. The parent graph never executes sub graph counter node after resuming the first interrupt. The sub graph nodes output the same parent counter (=1) repeatedly, leading to recursion limit error since parent counter does not increase.\n\nObservation\nIf removing checkpointer=True, the graph executes as expected, i.e. the parent counter increases correctly. No bug. (In this cause, ff course, the sub graph states from previous run is not persisted )\nIt seems in subgraph assigning checkpointer=True and calling interrupt conflicts in some way.\n\n\n\n%%{init: {'flowchart': {'curve': 'linear'}}}%%\ngraph TD;\n        __start__([<p>__start__</p>]):::first\n        parent_graph_accumulator(parent_graph_accumulator)\n        sub(sub)\n        __end__([<p>__end__</p>]):::last\n        __start__ --> parent_graph_accumulator;\n        sub --> parent_graph_accumulator;\n        parent_graph_accumulator -.-> sub;\n        parent_graph_accumulator -.-> __end__;\n        classDef default fill:#f2f0ff,line-height:1.2\n        classDef first fill-opacity:0\n        classDef last fill:#bfb6fc\n\n\n\n\n\n\n\n\n Loading\n\n\n\nLangGraph Version\n0.2.67\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #135~20.04.1-Ubuntu SMP Mon Oct 7 13:56:22 UTC 2024\nPython Version:  3.11.11 (main, Dec 11 2024, 16:28:39) [GCC 11.2.0]\n\nPackage Information\n\nlangchain_core: 0.3.31\nlangsmith: 0.3.1\nlanggraph_sdk: 0.1.51\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nhttpx: 0.28.1\njsonpatch: 1.33\nlangsmith-pyo3: Installed. No version info available.\norjson: 3.10.15\npackaging: 24.2\npydantic: 2.10.6\npytest: Installed. No version info available.\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrich: 13.9.4\ntenacity: 9.0.0\ntyping-extensions: 4.12.2\nzstandard: 0.23.0\n", "created_at": "2025-01-26", "closed_at": "2025-02-27", "labels": [], "State": "closed", "Author": "shengbo-ma"}
{"issue_number": 3205, "issue_title": "How to use langgraph in production with different storage state", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.state import StateGraph\n\nChain=StateGraph(list)\nError Message and Stack Trace (if applicable)\n\nDescription\nHello, I am trying to use langgraph in production application with my azure resources. How to change the message memory state to cosmos db. I have already functions written to insert and fetch , update items from cosmos Db. How to implement this with langgraph. Please guide me on this.\nSystem Info\nPython langgraph", "created_at": "2025-01-26", "closed_at": "2025-01-27", "labels": [], "State": "closed", "Author": "surendransuri"}
{"issue_number": 3199, "issue_title": "run does not change config based on assistant_id", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n\"\"\"\nLanggraph Server code\n\"\"\"\n\nfrom dataclasses import dataclass, field, fields\nfrom typing import Annotated, Dict, List, Optional, Sequence, cast\nfrom langchain_core.messages import AIMessage\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import StateGraph\nfrom typing import Sequence\nfrom react_agent.state import InputState, State\nfrom react_agent.tools import TOOLS\nfrom react_agent.utils import load_chat_model\nfrom langchain_core.runnables import RunnableConfig, ensure_config\nfrom langchain_core.messages import AnyMessage\nfrom langgraph.graph import add_messages\n\n\n@dataclass\nclass State(InputState):\n    messages: Annotated[Sequence[AnyMessage], add_messages] = field(\n        default_factory=list\n    )\n\n\n@dataclass(kw_only=True)\nclass Configuration:\n    prompt: str = field(default=\"Just reply with MISSING_PROMPT\")\n\n    @classmethod\n    def from_runnable_config(\n        cls, config: Optional[RunnableConfig] = None\n    ) -> \"Configuration\":\n        \"\"\"Create a Configuration instance from a RunnableConfig object.\"\"\"\n        config = ensure_config(config)\n        configurable = config.get(\"configurable\") or {}\n        _fields = {f.name for f in fields(cls) if f.init}\n        return cls(**{k: v for k, v in configurable.items() if k in _fields})\n\n\nasync def call_model(\n    state: State, config: RunnableConfig\n) -> Dict[str, List[AIMessage]]:\n    configuration = Configuration.from_runnable_config(config)\n    model = load_chat_model(\"anthropic/claude-3-5-sonnet-20240620\")\n    system_message = configuration.prompt\n    response = cast(\n        AIMessage,\n        await model.ainvoke(\n            [{\"role\": \"system\", \"content\": system_message}, *state.messages], config\n        ),\n    )\n    return {\"messages\": [response]}\n\n\nbuilder = StateGraph(State, input=InputState, config_schema=Configuration)\nbuilder.add_node(call_model)\nbuilder.add_edge(\"__start__\", \"call_model\")\nbuilder.add_edge(\"call_model\", \"__end__\")\n\ngraph = builder.compile()\nError Message and Stack Trace (if applicable)\nFrom a jupyter notebook:\n%pip install langgraph-sdk\n\n\n    Requirement already satisfied: langgraph-sdk in /Users/josh/dev/langgraph-jupyter/env/lib/python3.13/site-packages (0.1.51)\n    Requirement already satisfied: httpx>=0.25.2 in /Users/josh/dev/langgraph-jupyter/env/lib/python3.13/site-packages (from langgraph-sdk) (0.28.1)\n    Requirement already satisfied: orjson>=3.10.1 in /Users/josh/dev/langgraph-jupyter/env/lib/python3.13/site-packages (from langgraph-sdk) (3.10.15)\n    Requirement already satisfied: anyio in /Users/josh/dev/langgraph-jupyter/env/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk) (4.8.0)\n    Requirement already satisfied: certifi in /Users/josh/dev/langgraph-jupyter/env/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk) (2024.12.14)\n    Requirement already satisfied: httpcore==1.* in /Users/josh/dev/langgraph-jupyter/env/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk) (1.0.7)\n    Requirement already satisfied: idna in /Users/josh/dev/langgraph-jupyter/env/lib/python3.13/site-packages (from httpx>=0.25.2->langgraph-sdk) (3.10)\n    Requirement already satisfied: h11<0.15,>=0.13 in /Users/josh/dev/langgraph-jupyter/env/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk) (0.14.0)\n    Requirement already satisfied: sniffio>=1.1 in /Users/josh/dev/langgraph-jupyter/env/lib/python3.13/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk) (1.3.1)\n    Note: you may need to restart the kernel to use updated packages.\n\n\n\n\nimport getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"LANGSMITH_API_KEY\")\n\n\n\n\n\nfrom langgraph_sdk import get_client\n\nURL=\"http://localhost:2024\"\nclient = get_client(url=URL,api_key=os.getenv('LANGSMITH_API_KEY'))\nassistant_id = \"agent\"\nthread = await client.threads.create()\n\n\n\n\nassistant_one = await client.assistants.create(\n    graph_id=\"agent\",\n    config={\"configurable\": {\"prompt\": \"just respond with ASSISTANT ONE\"}},\n    assistant_id=\"11111111-1111-1111-1111-111111111111\",\n    if_exists=\"do_nothing\",\n    name=\"asssistant one\"\n)\n\n\n\n\nassistant_two = await client.assistants.create(\n    graph_id=\"agent\",\n    config={\"configurable\": {\"prompt\": \"just respond with ASSISTANT TWO\"}},\n    assistant_id=\"22222222-2222-2222-2222-222222222222\",\n    if_exists=\"do_nothing\",\n    name=\"asssistant two\"\n)\n\n\n\n\nthread = await client.threads.create(\n    metadata={\"number\":1},\n    if_exists=\"raise\"\n)\n\n\n\n\nthread\n\n\n\n\n\n    {'thread_id': 'fba20607-a345-4c52-99e2-270122b5604c',\n     'created_at': '2025-01-24T18:39:41.939930+00:00',\n     'updated_at': '2025-01-24T18:39:41.939934+00:00',\n     'metadata': {'number': 1},\n     'status': 'idle',\n     'config': {},\n     'values': None}\n\n\n\n\n\nresult_a = await client.runs.wait(\n    thread_id=thread[\"thread_id\"],\n    assistant_id=assistant_one[\"assistant_id\"],\n    input={\"messages\": [{\"role\": \"user\", \"content\": \"hello\"}]},\n)\nresult_a[\"messages\"][1][\"content\"]\n\n\n\n\n\n    'ASSISTANT ONE'\n\n\n\n\n\nresult_b = await client.runs.wait(\n    thread_id=thread[\"thread_id\"],\n    assistant_id=assistant_two[\"assistant_id\"],\n    input={\"messages\": [{\"role\": \"user\", \"content\": \"hello\"}]},\n)\nresult_b[\"messages\"][1][\"content\"]\n\n\n\n\n\n    'ASSISTANT ONE'\nDescription\nI have two assistants. I start a thread and complete a run with the first assistant. Then I submit a run with the second assistant. The config provided to the node is always from the first assistant. Notice in the output from my notebook above that it prints \"ASSISTANT ONE\" twice even though the second run is using assistant_two which should print \"ASSISTANT TWO\". If I change the order and call assistant_two first then it will print \"ASSISTANT TWO\" twice.\nI expected that the assistant_id I pass in with the run would cause the configuration for that assistant to be provided to the node.\nI notice in LangGraph Studio I can pick an assistant for each run and it will actually work. But looking at the network request I can see Studio is passing in a config parameter with all the configuration of the selected assistant including the prompt. I don't want to have to do this and it seems counter-intuitive that I should have to. I expected I could use the assistant_id for this.\nThanks!\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.5.0: Wed May  1 20:12:58 PDT 2024; root:xnu-10063.121.3~5/RELEASE_ARM64_T6000\nPython Version:  3.11.11 (main, Jan 18 2025, 10:11:10) [Clang 16.0.0 (clang-1600.0.26.6)]\n\nPackage Information\n\nlangchain_core: 0.3.30\nlangchain: 0.3.14\nlangchain_community: 0.3.14\nlangsmith: 0.2.11\nlangchain_anthropic: 0.3.3\nlangchain_fireworks: 0.2.6\nlangchain_openai: 0.3.0\nlangchain_text_splitters: 0.3.5\nlanggraph_api: 0.0.16\nlanggraph_cli: 0.1.67\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.51\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.11\nanthropic: 0.43.1\nasync-timeout: Installed. No version info available.\nclick: 8.1.8\ncryptography: 43.0.3\ndataclasses-json: 0.6.7\ndefusedxml: 0.7.1\nfireworks-ai: 0.15.11\nhttpx: 0.28.1\nhttpx-sse: 0.4.0\njsonpatch: 1.33\njsonschema-rs: 0.25.1\nlanggraph: 0.2.64\nlanggraph-checkpoint: 2.0.10\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.59.8\norjson: 3.10.15\npackaging: 24.2\npydantic: 2.10.5\npydantic-settings: 2.7.1\npyjwt: 2.10.1\npython-dotenv: 1.0.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.37\nsse-starlette: 2.1.3\nstarlette: 0.45.2\nstructlog: 24.4.0\ntenacity: 8.5.0\ntiktoken: 0.8.0\ntyping-extensions: 4.12.2\nuvicorn: 0.34.0\nwatchfiles: 1.0.4\nzstandard: Installed. No version info available.\n", "created_at": "2025-01-24", "closed_at": "2025-04-15", "labels": [], "State": "closed", "Author": "weinberg"}
{"issue_number": 3193, "issue_title": "AsyncConnectionPool AsyncPostgresSaver cannot send pipeline when not in pipeline mode", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n#main.py\n @asynccontextmanager\n    async def lifespan(self, app: FastAPI):\n        try:\n            async with AsyncConnectionPool(\n                conninfo=DB_URI, #connection information    \n                kwargs={\n                    \"autocommit\": True,\n                    \"prepare_threshold\": 0,\n                    \"row_factory\": dict_row\n                },\n                min_size=5, #minimum size of the pool\n                max_size=15, #maximum size of the pool\n            ) as pool, pool.connection() as conn:\n                await AsyncPostgresSaver(conn).setup()\n                yield {\"conn\": conn}\n        except Exception as e:\n            logger.error(f\"Error setting up connection pool: {e}\")\n            raise\n        finally:\n            logger.info(f\"End of lifespan\")\n\n#chat.py\n# Use the connection directly from request.state.pool\n            async with request.state.conn as conn:  # Use 'conn' directly\n                checkpointer = AsyncPostgresSaver(conn)\n\n                tools = [\n                    GetInformationTool(\n                        metadata={\"information\": only_structure}),\n                    ValidateInformationTool(\n                        metadata={\"information\": only_structure}),\n                    RegisterInformationTool(metadata={\n                        \"information\": only_structure,\n                        \"user_id\": user_id,\n                        \"family_id\": family_id,\n                        \"organization_id\": organization_id,\n                        \"uuid\": uuid,\n                        \"session_id\": session_id\n                    })\n                ]\n\n                graph_builder = RegistrationGraphBuilder(model=model, prompt=registration_prompt, system_prompt=system_message,\n                                                            member_name=member_name, language=\"Japanese\", tools=tools, checkpointer=checkpointer)\n                graph = graph_builder.build()\n\n                config = {\"configurable\": {\"thread_id\": thread_id}}\n                result = await graph.ainvoke(\n                    {\n                        \"messages\": [HumanMessage(content=user_message)]\n                    },\n                    config\n                )\n\n                content = self.handle_result(result)\n                return ResponseHandler.ok(message=VALID_RESPONSE_MESSAGE, body=content)\nError Message and Stack Trace (if applicable)\nOperationalError('sending prepared query failed: cannot send pipeline when not in pipeline mode\\nanother command is already in progress\\ncannot exit pipeline mode while busy\\ncannot enter pipeline mode, connection not idle\\ncannot enter pipeline mode, connection not idle\\ncannot enter pipeline mode, connection not idle\\nanother command is already in progress\\nanother command is already in progress')Traceback (most recent call last):\n\n\n  File \"/usr/local/lib/python3.10/site-packages/langgraph/pregel/__init__.py\", line 1836, in astream\n    async with AsyncPregelLoop(\n\n\n  File \"/usr/local/lib/python3.10/site-packages/langgraph/pregel/loop.py\", line 988, in __aenter__\n    saved = await self.checkpointer.aget_tuple(self.checkpoint_config)\n\n\n  File \"/usr/local/lib/python3.10/site-packages/langgraph/checkpoint/postgres/aio.py\", line 186, in aget_tuple\n    await cur.execute(\n\n\n  File \"/usr/local/lib/python3.10/site-packages/psycopg/cursor_async.py\", line 97, in execute\n    raise ex.with_traceback(None)\n\n\npsycopg.OperationalError: sending prepared query failed: cannot send pipeline when not in pipeline mode\nanother command is already in progress\ncannot exit pipeline mode while busy\ncannot enter pipeline mode, connection not idle\ncannot enter pipeline mode, connection not idle\ncannot enter pipeline mode, connection not idle\nanother command is already in progress\nanother command is already in progress\nDescription\nI'm developing a chatbot with FastAPI and using LangGraph with prebuilt create_react_agent\nI'm using AsyncConnectionPool and AsyncPostgresSaver\nI have a problem with managing the connection pool of Postgres with LangGraph, I saw this error: cannot send pipeline when not in pipeline mode\nCould you help me to explain what happened and if I missed something, many thanks.\nI appreciate that.\nSystem Info\nfastapi==0.115.6\nlangchain==0.3.14\nlanggraph==0.2.62\nlanggraph-checkpoint-postgres==2.0.10\npsycopg==3.2.3\npsycopg-pool==3.2.4", "created_at": "2025-01-24", "closed_at": "2025-01-24", "labels": [], "State": "closed", "Author": "lam-dm"}
{"issue_number": 3992, "issue_title": "[BUG] Setting a \"tool_choice\" with ChatOpenAI causes a Pydantic ValidationError from \"tool_call_id\"", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport asyncio\nfrom datetime import datetime\nfrom typing import Literal\n\nimport aiofiles\nimport pytz\nfrom huggingface_hub import hf_hub_download\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.errors import GraphRecursionError\nfrom langgraph.graph import END, StateGraph\nfrom langgraph.prebuilt import ToolNode\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\nfrom langgraph.store.base import BaseStore\nfrom langgraph.store.memory import InMemoryStore\nfrom langgraph.types import Send\n\nAI_NAME = \"Eve\"\nSYSTEM_PROMPT = f\"\"\"I am {AI_NAME}, a helpful AI assistant.\"\"\"\nRECURSION_LIMIT = 10\n\ndef download_model():\n   hf_hub_download(repo_id=\"mradermacher/ZEUS-8B-V22-i1-GGUF\", filename=\"ZEUS-8B-V22.i1-Q4_K_M.gguf\", local_dir=\"app/models\")\n\n\n@tool\nasync def date_and_time(query: str):\n   \"\"\"Returns today's date and the current time.\"\"\"\n   await asyncio.sleep(0)\n   now = datetime.now(pytz.utc)\n   tz = pytz.timezone(\"America/New_York\")\n   now_local = now.astimezone(tz)\n   return [now_local.strftime(\"%Y-%m-%d %H:%M:%S %Z\")]\n\n\nasync def main() -> None:\n   tools = [date_and_time]\n   tool_node = ToolNode(tools)\n   llm = ChatOpenAI(\n      model=\"ZEUS\",\n      openai_api_key=\"EMPTY\",\n      openai_api_base=\"http://localhost:3000/v1\",\n      temperature=0.7,\n      top_p=0.9,\n      extra_body={\"top_k\": 40, \"min_p\": 0.0, \"repetition_penalty\": 1.05, \"num_predict\": -1, \"keep_alive\": -1},\n      streaming=True,\n   )\n   llm = llm.bind_tools(tools=tools, tool_choice=\"date_and_time\", parallel_tool_calls=True)\n   in_memory_store = InMemoryStore()\n\n   NODE_AGENT = \"agent\"\n   NODE_TOOLBELT = \"toolbelt\"\n   EDGE_USE_TOOL = \"use_tool\"\n   EDGE_END = \"end\"\n\n\n   def should_use_tool(state: AgentState, config: RunnableConfig, *, store: BaseStore) -> Literal[EDGE_END, EDGE_USE_TOOL]:\n      messages = state[\"messages\"]\n      last_message = messages[-1]\n\n      if not isinstance(last_message, AIMessage) or not last_message.tool_calls:\n         return EDGE_END\n\n      # v1 API would just return EDGE_USE_TOOL here\n\n      tool_calls = [\n         tool_node.inject_tool_args(call, state, store)\n         for call in last_message.tool_calls\n      ]\n      return [Send(NODE_TOOLBELT, [tool_call]) for tool_call in tool_calls]\n\n\n   async def call_model(state: AgentState, config: RunnableConfig, *, store: BaseStore) -> dict:\n      messages = state[\"messages\"]\n      response = await llm.ainvoke(messages)\n      return {\"messages\": [response]}\n\n\n   workflow = StateGraph(AgentState)\n\n   workflow.add_node(NODE_AGENT, call_model)\n   workflow.add_node(NODE_TOOLBELT, tool_node)\n\n   workflow.set_entry_point(NODE_AGENT)\n\n   workflow.add_conditional_edges(\n      NODE_AGENT,\n      should_use_tool,\n      {\n         EDGE_USE_TOOL: NODE_TOOLBELT,\n         EDGE_END: END,\n      },\n   )\n\n   workflow.add_edge(NODE_TOOLBELT, NODE_AGENT)\n\n   app = workflow.compile(store=in_memory_store)\n\n   async with aiofiles.open(\"graph.png\", \"wb\") as png:\n      await png.write(app.get_graph().draw_mermaid_png())\n\n   inputs = {\"messages\": [SystemMessage(content=SYSTEM_PROMPT), HumanMessage(content=\"What is today's date and time?\")]}\n   config = {\"configurable\": {\"thread_id\": \"1\", \"recursion_limit\": RECURSION_LIMIT}}\n\n   try:\n      async for event in app.astream_events(inputs, config, version=\"v2\"):\n\n         if event[\"event\"] == \"on_chain_end\" and not event[\"parent_ids\"]:\n            # no parent ids means it's the final event\n            message = event[\"data\"][\"output\"][\"messages\"][-1]\n            message.pretty_print()\n   except GraphRecursionError:\n      print({\"input\": inputs[1], \"output\": \"Agent stopped due to max iterations.\"})\n\n\nif __name__ == \"__main__\":\n   download_model()\n   asyncio.run(main())\nError Message and Stack Trace (if applicable)\n~ python .\\main.py\nTraceback (most recent call last):\n  File \"[redacted]main.py\", line 178, in <module>\n    asyncio.run(main())\n    ~~~~~~~~~~~^^^^^^^^\n  File \"[redacted]\\asyncio\\runners.py\", line 195, in run\n    return runner.run(main)\n           ~~~~~~~~~~^^^^^^\n  File \"[redacted]\\asyncio\\runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n  File \"[redacted]\\asyncio\\base_events.py\", line 725, in run_until_complete\n    return future.result()\n           ~~~~~~~~~~~~~^^\n  File \"[redacted]main.py\", line 166, in main\n    async for event in app.astream_events(inputs, config, version=\"v2\"):\n    ...<4 lines>...\n          message.pretty_print()\n  File \"[redacted]\\site-packages\\langchain_core\\runnables\\base.py\", line 1389, in astream_events\n    async for event in event_stream:\n        yield event\n  File \"[redacted]\\site-packages\\langchain_core\\tracers\\event_stream.py\", line 1013, in _astream_events_implementation_v2\n    await task\n  File \"[redacted]\\site-packages\\langchain_core\\tracers\\event_stream.py\", line 968, in consume_astream\n    async for _ in event_streamer.tap_output_aiter(run_id, stream):\n        # All the content will be picked up\n        pass\n  File \"[redacted]\\site-packages\\langchain_core\\tracers\\event_stream.py\", line 204, in tap_output_aiter\n    async for chunk in output:\n    ...<4 lines>...\n        yield chunk\n  File \"[redacted]\\site-packages\\langgraph\\pregel\\__init__.py\", line 2313, in astream\n    async for _ in runner.atick(\n    ...<7 lines>...\n            yield o\n  File \"[redacted]\\site-packages\\langgraph\\pregel\\runner.py\", line 444, in atick\n    await arun_with_retry(\n    ...<7 lines>...\n    )\n  File \"[redacted]\\site-packages\\langgraph\\pregel\\retry.py\", line 123, in arun_with_retry\n    async for _ in task.proc.astream(task.input, config):\n        pass\n  File \"[redacted]\\site-packages\\langgraph\\utils\\runnable.py\", line 706, in astream\n    async for chunk in aiterator:\n    ...<9 lines>...\n            output = chunk\n  File \"[redacted]\\site-packages\\langchain_core\\tracers\\event_stream.py\", line 181, in tap_output_aiter\n    first = await py_anext(output, default=sentinel)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"[redacted]\\site-packages\\langchain_core\\utils\\aiter.py\", line 74, in anext_impl\n    return await __anext__(iterator)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"[redacted]\\site-packages\\langchain_core\\runnables\\base.py\", line 1455, in atransform\n    async for ichunk in input:\n    ...<14 lines>...\n                final = ichunk\n  File \"[redacted]\\site-packages\\langchain_core\\runnables\\base.py\", line 1455, in atransform\n    async for ichunk in input:\n    ...<14 lines>...\n                final = ichunk\n  File \"[redacted]\\site-packages\\langchain_core\\runnables\\base.py\", line 1020, in astream\n    yield await self.ainvoke(input, config, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"[redacted]\\site-packages\\langgraph\\utils\\runnable.py\", line 371, in ainvoke\n    ret = await asyncio.create_task(coro, context=context)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"[redacted]\\site-packages\\langgraph\\prebuilt\\tool_node.py\", line 274, in _afunc\n    outputs = await asyncio.gather(\n              ^^^^^^^^^^^^^^^^^^^^^\n        *(self._arun_one(call, input_type, config) for call in tool_calls)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"[redacted]\\site-packages\\langgraph\\prebuilt\\tool_node.py\", line 357, in _arun_one\n    if invalid_tool_message := self._validate_tool_call(call):\n                               ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^\n  File \"[redacted]\\site-packages\\langgraph\\prebuilt\\tool_node.py\", line 448, in _validate_tool_call\n    return ToolMessage(\n        content, name=requested_tool, tool_call_id=call[\"id\"], status=\"error\"\n    )\n  File \"[redacted]\\site-packages\\langchain_core\\messages\\tool.py\", line 140, in __init__\n    super().__init__(content=content, **kwargs)\n    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"[redacted]\\site-packages\\langchain_core\\messages\\base.py\", line 77, in __init__\n    super().__init__(content=content, **kwargs)\n    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"[redacted]\\site-packages\\langchain_core\\load\\serializable.py\", line 125, in __init__\n    super().__init__(*args, **kwargs)\n    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"[redacted]\\site-packages\\pydantic\\main.py\", line 214, in __init__\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\npydantic_core._pydantic_core.ValidationError: 1 validation error for ToolMessage\ntool_call_id\n  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\nDuring task with name 'toolbelt' and id 'c58462b3-c6cc-4b20-e3e9-be3dead485f0'\nDescription\nI'm running a vLLM instance that uses the downloaded model, and though untested I'd guess any OpenAI model selection could cause this error. If this is poorly configured, please let me know. However setting tool_choice=\"auto\" runs without issue.\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.19045\nPython Version:  3.13.2 | packaged by conda-forge | (main, Feb 17 2025, 13:52:56) [MSC v.1942 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.3.45\nlangchain: 0.3.20\nlangsmith: 0.3.15\nlangchain_openai: 0.3.9\nlangchain_text_splitters: 0.3.6\nlanggraph_sdk: 0.1.57\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nhttpx: 0.28.1\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.41: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.45: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nopenai-agents: Installed. No version info available.\nopenai<2.0.0,>=1.66.3: Installed. No version info available.\norjson: 3.10.15\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: Installed. No version info available.\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: Installed. No version info available.\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-03-23", "closed_at": "2025-03-24", "labels": [], "State": "closed", "Author": "T145"}
{"issue_number": 3991, "issue_title": "DOC: Add Version Information for Key Primitives in Documentation", "issue_body": "Issue with current documentation:\nDescription\nThis issue aims to add version information for several key primitives in the LangChain documentation. The goal is to enhance the documentation by indicating in which versions the following primitives were introduced:\n\nInterrupt\nCommand\nStream modes\nTask\nEntry point\n\nAdditionally, this task will investigate whether mkdocs/mkdocstrings supports the same version notation syntax used by sphinx or if an alternative approach is needed.\nProposed Solution\n\nIdentify the versions where each of the above primitives was introduced.\nAdd version information in the documentation using mkdocs/mkdocstrings.\n\nIdea or request for content:\nNo response", "created_at": "2025-03-23", "closed_at": null, "labels": [], "State": "open", "Author": "YassinNouh21"}
{"issue_number": 3990, "issue_title": "DOC: Comprehensive Documentation for LangGraph Distributed Service Deployment", "issue_body": "Issue with current documentation:\nDescribe the documentation request\nCurrently, I'm extensively working on implementing distributed services using LangGraph. However, there's little documentation around several important aspects of service deployment, particularly related to:\n\nStreaming outputs\nGraceful shutdowns during service upgrades\nContext persistence when deployed on platforms like K8S docker container\nExecution results persistence\nSeparating schedulers and executors for high-performance requirements\n\nAlthough there are some useful documents available specifically for streaming outputs, comprehensive resources covering multi-agent scenarios or detailed LangGraph-related service deployment best practices seem lacking.\nIt would be highly beneficial if detailed documentation or guides on these topics could be provided, which could then be leveraged for broader reuse across users.\nIdea or request for content:\nNo response", "created_at": "2025-03-23", "closed_at": null, "labels": [], "State": "open", "Author": "thinkhy"}
{"issue_number": 3975, "issue_title": "ChatOpenAI Calls Misplaced in Langsmith Tracing When Using a Lambda Wrapper", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom datetime import datetime\nimport operator\nimport os\nfrom typing import Annotated, Sequence, TypedDict\n\nfrom dotenv import load_dotenv\nfrom langchain_core.messages import AIMessage, BaseMessage, HumanMessage\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnableLambda\nfrom langchain.tools import Tool\nfrom langchain_openai import ChatOpenAI\nfrom langfuse.callback import CallbackHandler\nfrom langgraph.graph import END, START, StateGraph\nfrom langgraph.prebuilt import ToolNode\n\n# Your env file should contain your OPENAI_API_KEY + your Langsmith credentials\nload_dotenv('PATH-to-env-file')\n\nopenai_key = os.getenv('OPENAI_API_KEY')\n\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], operator.add]\n    next: str\n\n# Define a new tool that returns the current datetime\ndatetime_tool = Tool(\n    name=\"Datetime\",\n    func = lambda x: datetime.now().isoformat(),\n    description=\"Returns the current datetime\",\n)\n\ndef to_continue(state: AgentState):\n    \"\"\"End graph if last message has no tool calls.\"\"\"\n    last_msg = state['messages'][-1]\n    if isinstance(last_msg, AIMessage) and last_msg.tool_calls:\n        return 'tools'\n    return END\n\nsystem_prompt = \"\"\"\nYou are an agent tasked with answering all user queries by indicating the\ntoday's date and time first. For instance if user asks \"what is the capital\nof France?\", you respond:\n\"Today is 21 March 2025, the time is 15:10.\"\n\"The capital of France is Paris.\"\n\nYou get the today's date and time from the Datetime tool.\n\"\"\"\n\nllm = ChatOpenAI(model='gpt-4o', api_key=openai_key)\n\nagent_node = (\n    ChatPromptTemplate.from_messages([('system', system_prompt),\n                                    ('placeholder', '{messages}'),\n                                    ('system', 'Show the today date and time and then respond to the user query')])\n    | llm.bind_tools([datetime_tool])\n    | RunnableLambda(lambda x: {'messages': [x]}))\n\ntool_node = ToolNode([datetime_tool])\n\n# Define the agent graph\nworkflow = StateGraph(AgentState)\nworkflow.add_node('agent', agent_node)\nworkflow.add_node('tools', tool_node)\n\nworkflow.add_edge(START, 'agent')\nworkflow.add_conditional_edges('agent', to_continue)\nworkflow.add_edge('tools', 'agent')\ngraph = workflow.compile(debug=False)\n\nAgent = graph | (lambda x: {'output': x['messages'][-1].content, **x})\n\nlangfuse_handler = CallbackHandler()\n\n# Invoking the CompiledGraph\nres1 = graph.invoke({'messages': [HumanMessage(content='What is the capital of Switzerland?')]},\n                    config={'callbacks': [langfuse_handler]})\n\n# Invoking the RunnableSequence\nres2 = Agent.invoke({'messages': [HumanMessage(content='What is the capital of Switzerland?')]},\n                    config={'callbacks': [langfuse_handler]})\n\nprint(res1['messages'][-1].content)\nprint(res2.get('output'))\nError Message and Stack Trace (if applicable)\n\nDescription\nI have a simple agent implemented as a LangGraph, consisting of two nodes:\n1.\tAgent Node \u2013 Handles user input and generates a response.\n2.\tDatetime Tool Node \u2013 Provides the current date and time.\nThe agent is designed to first retrieve the current date and time from the tool and then generate its response accordingly.\nExpected Behavior\nWhen I invoke the CompiledGraph directly, the Langsmith tracing correctly associates each ChatOpenAI call (which includes system prompts and user/AI messages) with the corresponding execution of the agent node in the graph. (See the first image below.)\n\nHowever, if I wrap the compiled graph inside a lambda function (e.g., to return the result as a dictionary with an \"output\" key), the tracing behavior changes. In this case, all ChatOpenAI calls appear at the end of the trace, rather than being correctly nested within the agent node\u2019s execution. (See the second image below.)\n\nThe same thing also happens with Langfuse.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:16 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6000\nPython Version:  3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]\n\nPackage Information\n\nlangchain_core: 0.3.45\nlangchain: 0.3.21\nlangchain_community: 0.3.20\nlangsmith: 0.3.17\nlangchain_openai: 0.3.9\nlangchain_text_splitters: 0.3.7\nlanggraph_sdk: 0.1.57\n", "created_at": "2025-03-21", "closed_at": null, "labels": [], "State": "open", "Author": "ahmadajal"}
{"issue_number": 3969, "issue_title": "Cannot Import creat_react_agent from langgraph.prebuilt", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.prebuilt import create_react_agent\n\nlanggraph_agent_executor = create_react_agent(model, tools)\n\n\nmessages = langgraph_agent_executor.invoke({\"messages\": [(\"human\", query)]})\n{\n    \"input\": query,\n    \"output\": messages[\"messages\"][-1].content,\n}\nError Message and Stack Trace (if applicable)\nImportError: cannot import name 'create_react_agent' from 'langgraph.prebuilt' (unknown location)\nDescription\nfrom langgraph.prebuilt import create_react_agent\nImportError: cannot import name 'create_react_agent' from 'langgraph.prebuilt' (unknown location)\nSystem Info\nfrom langgraph.prebuilt import create_react_agent\nImportError: cannot import name 'create_react_agent' from 'langgraph.prebuilt' (unknown location)", "created_at": "2025-03-21", "closed_at": "2025-03-21", "labels": [], "State": "closed", "Author": "adv-11"}
{"issue_number": 3957, "issue_title": "DOC: Maintain Dual Documentation for Legacy & New Graph Implementation", "issue_body": "Issue with current documentation:\nDifferences between the traditional graph implementation methods (e.g., adding nodes) and the new approach involving returning literals using commands and navigation within LandGraph.\nTo enhance clarity and support users at different stages of adoption, I propose maintaining two versions of the documentation:\n\n\nLegacy Documentation: For users working with previous methods.\n\n\nUpdated Documentation: For those using the latest features and best practices.\n\n\nThis approach will improve accessibility, support learning, and ease the transition for the community. I am willing to contribute to creating and maintaining these documentation versions if you are open to the idea.\nThank you for your consideration.", "created_at": "2025-03-20", "closed_at": "2025-03-21", "labels": [], "State": "closed", "Author": "YassinNouh21"}
{"issue_number": 3950, "issue_title": "Command update overrides States fields with None", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom pydantic import BaseModel\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.types import Command\n\nclass State(BaseModel):\n    foo: str | None = None\n    bar: str | None = None\n\ndef node_a(state: State):\n    return Command(goto=\"node_b\", update=State(foo='foo'))\n\ndef node_b(state: State):\n    return Command(goto=END, update=State(bar='bar'))\n\nbuilder = StateGraph(State)\nbuilder.add_node(node_a)\nbuilder.add_node(node_b)\nbuilder.add_edge(START, \"node_a\")\ngraph = builder.compile()\n\nprint(graph.invoke(State()))\nError Message and Stack Trace (if applicable)\n\nDescription\nOutput:\n{'foo': None, 'bar': 'bar'}\nExpected:\n{'foo': 'foo', 'bar': 'bar'}\nSystem Info\npython -m langchain_core.sys_info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:16 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6000\nPython Version:  3.11.9 (main, Apr  2 2024, 08:25:04) [Clang 15.0.0 (clang-1500.3.9.4)]\n\nPackage Information\n\nlangchain_core: 0.3.46\nlangchain: 0.3.21\nlangchain_community: 0.3.20\nlangsmith: 0.3.18\nlangchain_openai: 0.3.9\nlangchain_text_splitters: 0.3.7\nlanggraph_sdk: 0.1.58\n", "created_at": "2025-03-20", "closed_at": "2025-04-14", "labels": ["investigate"], "State": "closed", "Author": "sfc-gh-wjaskowski"}
{"issue_number": 3943, "issue_title": "Cannot Import PostgresStore", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.store.postgres import PostgresStore\nError Message and Stack Trace (if applicable)\nImportError                               Traceback (most recent call last)\nCell In[13], line 1\n----> 1 from langgraph.store.postgres import PostgresStore\n\nFile ~/Data/Projects/AI/LangGraph/venv/lib/python3.12/site-packages/langgraph/store/postgres/__init__.py:1\n----> 1 from langgraph.store.postgres.aio import AsyncPostgresStore\n      2 from langgraph.store.postgres.base import PostgresStore\n      4 __all__ = [\"AsyncPostgresStore\", \"PostgresStore\"]\n\nFile ~/Data/Projects/AI/LangGraph/venv/lib/python3.12/site-packages/langgraph/store/postgres/aio.py:23\n     14 from langgraph.store.base import (\n     15     GetOp,\n     16     ListNamespacesOp,\n   (...)\n     20     SearchOp,\n     21 )\n     22 from langgraph.store.base.batch import AsyncBatchedBaseStore\n---> 23 from langgraph.store.postgres.base import (\n     24     PLACEHOLDER,\n     25     BasePostgresStore,\n     26     PoolConfig,\n     27     PostgresIndexConfig,\n     28     Row,\n     29     TTLConfig,\n...\n     47 )\n     49 if TYPE_CHECKING:\n     50     from langchain_core.embeddings import Embeddings\n\nImportError: cannot import name 'TTLConfig' from 'langgraph.store.base' (/home/sajith/Data/Projects/AI/LangGraph/venv/lib/python3.12/site-packages/langgraph/store/base/__init__.py)\nDescription\nlanggraph==0.3.18\nlanggraph-checkpoint-postgres ==2.0.18\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #53~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Jan 15 19:18:46 UTC 2\nPython Version:  3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0]\n\nPackage Information\n\nlangchain_core: 0.2.43\nlangchain: 0.3.21\nlangchain_community: 0.3.20\nlangsmith: 0.1.147\nlangchain_google_genai: 2.1.0\nlangchain_groq: 0.3.0\nlangchain_huggingface: 0.1.2\nlangchain_ollama: 0.2.3\nlangchain_openai: 0.3.9\nlangchain_text_splitters: 0.3.7\nlanggraph: 0.3.18\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nfiletype: 1.2.0\ngoogle-ai-generativelanguage: 0.6.17\ngroq<1,>=0.4.1: Installed. No version info available.\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\nhuggingface-hub: 0.29.3\njsonpatch: 1.33\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-azure-ai;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.45: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.7: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.21: Installed. No version info available.\nlanggraph-checkpoint: 2.0.21\nlanggraph-prebuilt: 0.1.3\nlanggraph-sdk: 0.1.58\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<3,>=1.26.2: Installed. No version info available.\nollama: 0.4.7\nopenai<2.0.0,>=1.66.3: Installed. No version info available.\norjson: 3.10.15\npackaging: 24.2\npydantic: 2.10.6\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\nPyYAML: 6.0.2\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nsentence-transformers: 3.4.1\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity: 8.5.0\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntokenizers: 0.21.1\ntransformers: 4.49.0\ntyping-extensions: 4.12.2\n", "created_at": "2025-03-20", "closed_at": "2025-03-20", "labels": [], "State": "closed", "Author": "Sajith-K-Sasi"}
{"issue_number": 3936, "issue_title": "usage_metadata return None in Langgraph Studio", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing import Dict, Any\nfrom langchain_core.messages import SystemMessage\nfrom langchain_openai import AzureChatOpenAI\nfrom langgraph.graph import START, StateGraph, MessagesState\nimport os\n\nllm = AzureChatOpenAI(\n    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n    openai_api_version=os.getenv(\"OPENAI_API_VERSION\"),\n    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n    temperature=0\n)\n\n# System message\nsystem_message = SystemMessage(content=\"You are a helpful assistant.\")\n\n# Define the basic assistant node\ndef assistant(state: MessagesState) -> Dict[str, Any]:\n\n    \"\"\"Basic assistant node that processes messages.\"\"\"\n    result = llm.invoke([system_message] + state[\"messages\"])\n    print(\"Usage metadata: \", result.usage_metadata)\n    return {\"messages\": [result]}\n\n# Build graph\nbuilder = StateGraph(MessagesState)\n\n# Add nodes\nbuilder.add_node(\"assistant\", assistant)\n\n# Set entry and finish points\nbuilder.set_entry_point(\"assistant\")\nbuilder.set_finish_point(\"assistant\")\n\n# Compile graph\ngraph = builder.compile()\nError Message and Stack Trace (if applicable)\nresult.usage_metadata is None\nDescription\nWhen trying to access tokens usage inside the graph - always return None.\nresult = llm.invoke(\"hello\")\nprint(\"Usage metadata: \", result.usage_metadata)\n\nUsage metadata: None.\nHowever if i use just the same code outside of langgraph (in test.py file)\nfrom langchain_openai import AzureChatOpenAI\nimport os\n\nllm = AzureChatOpenAI(\n    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n    openai_api_version=os.getenv(\"OPENAI_API_VERSION\"),\n    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n    temperature=0\n)\n\nresult = llm.invoke(\"hello\")\nprint(result.usage_metadata)\n\n{'input_tokens': 8, 'output_tokens': 11, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\nSame with llm.with_structured_output(myPydanticClass, include_raw=True) - if i use inside the langgraph information about token usage is not returned, but if i run outside - all works as expected.\nAm i doing something wrong?\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.0.0: Mon Aug 12 20:49:48 PDT 2024; root:xnu-11215.1.10~2/RELEASE_ARM64_T8103\nPython Version:  3.12.6 (v3.12.6:a4a2d2b0d85, Sep  6 2024, 16:08:03) [Clang 13.0.0 (clang-1300.0.29.30)]\n\nPackage Information\n\nlangchain_core: 0.3.46\nlangchain: 0.3.21\nlangchain_community: 0.3.20\nlangsmith: 0.3.18\nlangchain_openai: 0.3.9\nlangchain_text_splitters: 0.3.7\nlanggraph_api: 0.0.31\nlanggraph_cli: 0.1.77\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.58\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nclick: 8.1.8\ncryptography: 43.0.3\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\njsonschema-rs: 0.29.1\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-azure-ai;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.45: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.7: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.21: Installed. No version info available.\nlanggraph: 0.3.18\nlanggraph-checkpoint: 2.0.21\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<3,>=1.26.2: Installed. No version info available.\nopenai-agents: Installed. No version info available.\nopenai<2.0.0,>=1.66.3: Installed. No version info available.\nopentelemetry-api: Installed. No version info available.\nopentelemetry-exporter-otlp-proto-http: Installed. No version info available.\nopentelemetry-sdk: Installed. No version info available.\norjson: 3.10.15\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npyjwt: 2.10.1\npytest: Installed. No version info available.\npython-dotenv: 1.0.1\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: Installed. No version info available.\nSQLAlchemy<3,>=1.4: Installed. No version info available.\nsse-starlette: 2.1.3\nstarlette: 0.46.1\nstructlog: 25.2.0\ntenacity: 9.0.0\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nuvicorn: 0.34.0\nwatchfiles: 1.0.4\nzstandard: 0.23.0\n", "created_at": "2025-03-20", "closed_at": null, "labels": [], "State": "open", "Author": "DmitryKatson"}
{"issue_number": 3927, "issue_title": "DOC: Docs recommend \"AnyMessage\", example uses \"BaseMessage\"", "issue_body": "Issue with current documentation:\nDescription\nIn this doc, the documentation states that for proper serialization, one should use AnyMessage when working with LangChain types, rather than BaseMessage... but the code example uses BaseMessage in declaring the State instead of AnyMesssage.\nIdea or request for content:\nSmall fix:\n#3926", "created_at": "2025-03-19", "closed_at": "2025-03-19", "labels": [], "State": "closed", "Author": "hesreallyhim"}
{"issue_number": 3917, "issue_title": "How do I delete a namespace in long-term memory?", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nNo code Required.\nError Message and Stack Trace (if applicable)\n\nDescription\nI've implemented store to save the long term memory and I'm running langgraph server locally.\nIn my langgraph studio, under the memory I could see all the namespaces I've created and if I want to delete a namespace there is no option for me to.\nI think that data is getting store in memory right? I dont know how to remove them, I see there is an option to remove the keys and values, but I want to delete the namespace. Because all the unnecessary namespaces that I've used for different projects are still left there and I want to delete them and the files inside them.\nSystem Info\nusing latest versions", "created_at": "2025-03-19", "closed_at": "2025-03-31", "labels": [], "State": "closed", "Author": "Saisiva123"}
{"issue_number": 3911, "issue_title": "get_openai_callback can't get token usage when streaming", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nllm = AzureChatOpenAI(temperature=0.0)\n\napp = create_react_agent(\n        llm,\n        tools=internal_tools, # tools\n        prompt=prompt_to_use, # prompt\n        state_schema=AgentState, # state\n    )\n\nwith get_openai_callback() as cb:\n    for msg, metadata in app.stream(\n        {\n            \"messages\": chat_message,\n            \"kwargs\": kwargs,\n        },\n        config={\"configurable\": {}},\n        stream_mode=\"messages\",\n    ):\n        print(msg)\n        print(metadata)\n        yield msg.content\n    print(cb)\nError Message and Stack Trace (if applicable)\nTokens Used: 0\n        Prompt Tokens: 0\n                Prompt Tokens Cached: 0\n        Completion Tokens: 0\n                Reasoning Tokens: 0\nSuccessful Requests: 0\nTotal Cost (USD): $0.0\nDescription\n\nusing stream_mode=\"messages\" to chat with react_agent\nuse get_openai_callback() to get token usage\nreturn zero, but at langsmith, everything works fine.\n\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.26100\nPython Version:  3.12.4 (tags/v3.12.4:8e8a4ba, Jun  6 2024, 19:30:16) [MSC v.1940 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.3.41\nlangchain: 0.3.20\nlangchain_community: 0.3.19\nlangsmith: 0.3.11\nlangchain_deepseek: 0.1.2\nlangchain_milvus: 0.1.8\nlangchain_ollama: 0.2.3\nlangchain_openai: 0.3.7\nlangchain_text_splitters: 0.3.6\nlanggraph_sdk: 0.1.53\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.39: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.41: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-openai<1.0.0,>=0.3.5: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.20: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<3,>=1.26.2: Installed. No version info available.\nollama: 0.4.7\nopenai<2.0.0,>=1.58.1: Installed. No version info available.\norjson: 3.10.15\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npymilvus: 2.5.4\npytest: 8.3.5\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: 13.9.4\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-03-19", "closed_at": "2025-03-20", "labels": [], "State": "closed", "Author": "axiangcoding"}
{"issue_number": 3898, "issue_title": "Memory Leak in LangGraph", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing import Annotated\n\nfrom langchain_openai import ChatOpenAI\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import StateGraph\nfrom langgraph.graph.message import add_messages\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\n\ngraph_builder = StateGraph(State)\n\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n\n\ndef chatbot(state: State):\n    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n\n\n# The first argument is the unique node name\n# The second argument is the function or object that will be called whenever\n# the node is used.\ngraph_builder.add_node(\"chatbot\", chatbot)\ngraph_builder.set_entry_point(\"chatbot\")\ngraph_builder.set_finish_point(\"chatbot\")\ngraph = graph_builder.compile()\n\n\ndef stream_graph_updates(user_input: str):\n    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n        for value in event.values():\n            print(\"Assistant:\", value[\"messages\"][-1].content)\n\n\nwhile True:\n    try:\n        user_input = input(\"User: \")\n        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n            print(\"Goodbye!\")\n            break\n\n        stream_graph_updates(user_input)\n    except:\n        # fallback if input() is not available\n        user_input = \"What do you know about LangGraph?\"\n        print(\"User: \" + user_input)\n        stream_graph_updates(user_input)\n        break\nError Message and Stack Trace (if applicable)\n\nDescription\n\nWhen I run the quick start script (attached) I see that with every message the RAM consumed by the process increases.\nSince there is no memory(checkpointer) implementation in this example I expect that the RAM usage drops after processing the request(user input) but it keeps increasing with every message.\nI observe the same issue also when I use AsyncPostgresSaver as checkpointer\n\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:24 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6030\nPython Version:  3.12.9 (main, Feb  4 2025, 14:38:38) [Clang 16.0.0 (clang-1600.0.26.6)]\n\nPackage Information\n\nlangchain_core: 0.3.45\nlangchain: 0.3.20\nlangchain_community: 0.3.18\nlangsmith: 0.3.11\nlangchain_anthropic: 0.2.4\nlangchain_openai: 0.3.9\nlangchain_postgres: 0.0.13\nlangchain_text_splitters: 0.3.6\nlanggraph_sdk: 0.1.57\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nanthropic: 0.40.0\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\ndefusedxml: 0.7.1\nhttpx: 0.27.2\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.37: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.41: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.45: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.19: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy: 2.1.3\nnumpy<2,>=1.26.4;: Installed. No version info available.\nnumpy<3,>=1.26.2;: Installed. No version info available.\nopenai<2.0.0,>=1.66.3: Installed. No version info available.\norjson: 3.10.12\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npgvector: 0.3.6\npsycopg: 3.2.4\npsycopg-pool: 3.2.4\npydantic: 2.10.1\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: 8.3.3\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: 13.9.4\nsqlalchemy: 2.0.36\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-03-18", "closed_at": "2025-03-19", "labels": [], "State": "closed", "Author": "tomas-herman"}
{"issue_number": 3897, "issue_title": "graph.astream checkpoint_id not working properly", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nasync def astream_graph(state, config):\n    try:\n        async with AsyncMongoDBSaver.from_conn_string(os.getenv(\"MONGODB_URI\"),\n                                                      db_name=os.getenv(\"MONGODB_DB_NAME\")) as checkpointer:\n            graph = build_graph(checkpointer)\n            # print(\"graph.get_state_history(config)\", [state for state in graph.get_state_history(config)])\n            async for event in graph.astream(state, config, stream_mode=[\"messages\", \"debug\", \"custom\"]):\n                yield event\n    except Exception as e:\n        print(f\"Error in streaming: {str(e)}\")\n        raise\n\ndef build_graph(checkpointer):\n    \"\"\"Build and return the graph with tools.\"\"\"\n    try:\n        workflow = StateGraph(AgentState)\n\n        workflow.add_node(\"agent\", call_model)\n        workflow.add_node(\"tools\", call_tools)\n        workflow.add_edge(START, \"agent\")\n\n        workflow.add_conditional_edges(\n            \"agent\",\n            should_continue,\n            {\n                \"tools\": \"tools\",\n                END: END\n            }\n        )\n\n        workflow.add_edge(\"tools\", \"agent\")\n\n        return workflow.compile(checkpointer=checkpointer)\n    except Exception as e:\n        print(f\"Error building graph: {str(e)}\")\n        raise\n\nasync def call_model(state: AgentState) -> Dict:\n    \"\"\"Process messages and get model response.\"\"\"\n    try:\n        messages = state[\"messages\"]\n        ...\n        response = await model.ainvoke(messages)\n    if hasattr(response, 'additional_kwargs') and \n       response.additional_kwargs.get('tool_calls'):\n            return {\"messages\": [response]}\n\n        return {\"messages\": [response]}\n\n    except Exception as e:\n        print(f\"Error in call_model: {str(e)}\")\n        raise e\n\nasync def call_tools(state: AgentState) -> Dict:\n    \"\"\"Execute tools asynchronously.\"\"\"\n    try:\n        messages = state[\"messages\"]\n        last_message = cast(AIMessage, messages[-1])\n        ...\n        result = await tool_node.ainvoke({\"messages\": [last_message]})\n        # The result contains tool messages that we can return directly\n        return {\"messages\": result[\"messages\"]}\n\n    except Exception as e:\n        print(f\"Error in call_tools: {str(e)}\")\n        raise e\nError Message and Stack Trace (if applicable)\n\nDescription\nWhen I send checkpoint_id inside config['configurable'], its resume from there, i was expecting the continue in from that checkpoint like messages should properly show only the prev messages from that checkpoint, but its gives me full messages!\nSystem Info\nosx/linux", "created_at": "2025-03-18", "closed_at": null, "labels": [], "State": "open", "Author": "us"}
{"issue_number": 3892, "issue_title": "astream_events calls tool with wrong argumen name", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\ncheck the attched ipynb file in description\nError Message and Stack Trace (if applicable)\n\u3053\u3093\u306b\u3061\u306f\uff01\u30e6\u30fc\u30b6\u30fc\u306e\u540d\u524d\u3092\u304a\u805e\u304d\u3057\u3066\u4fdd\u5b58\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002\u304a\u540d\u524d\u3092\u6559\u3048\u3066\u3044\u305f\u3060\u3051\u307e\u3059\u304b\uff1f\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3001\u30ea\u3055\u3093\u3002\u304a\u540d\u524d\u3092\u4fdd\u5b58\u3055\u305b\u3066\u3044\u305f\u3060\u304d\u307e\u3059\u3002\nCalling: {'name': 'save_user_name', 'args': {'userName': '\u30ea'}, 'id': 'tooluse_jIvvJaqAQze6kdL8Hn2OfQ', 'type': 'tool_call'}\n---------------------------------------------------------------------------\nValidationError                           Traceback (most recent call last)\n[<ipython-input-16-0cab5c995e70>](https://localhost:8080/#) in <cell line: 1>()\n     16         print(chunk, end=\"\")\n     17 \n---> 18     async for chunk in chat(\"\u30ea\u3067\u3059\", thread_id=thread):\n     19         print(chunk, end=\"\")\n\n24 frames\n[/usr/local/lib/python3.11/dist-packages/pydantic/main.py](https://localhost:8080/#) in model_validate(cls, obj, strict, from_attributes, context)\n    625         # `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\n    626         __tracebackhide__ = True\n--> 627         return cls.__pydantic_validator__.validate_python(\n    628             obj, strict=strict, from_attributes=from_attributes, context=context\n    629         )\n\nValidationError: 1 validation error for save_user_name\nuser_name\n  Field required [type=missing, input_value={'userName': '\u30ea'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nDescription\nI'm using langgraph to create an Agent with aws bedrock, and it has a tool call named save_user_name.\n@tool\ndef save_user_name(user_name: str) -> str:\n    \"\"\"Save user name\n\n    Args:\n        user_name: user name\n    \"\"\"\n\n    return f\"User name saved: {user_name}\n\nI tried both stream and astream_events.\n\nExpect\n\nWhen user says his/her name, above tool is called with argument \"user_name\"\n\nActually\n\nstream works just fine,\nbut astream_events calls the tool with argument \"userName\"\nrequirements.txt\nlanggraph_agents_with_amazon_bedrock.ipynb.zip\nSystem Info\ngoogle colab", "created_at": "2025-03-18", "closed_at": "2025-03-18", "labels": [], "State": "closed", "Author": "sei-li-miidas"}
{"issue_number": 3877, "issue_title": "When using Command, the graph is not drawn properly", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport os\nfrom typing import Annotated\n\nfrom langchain_community.chat_models import ChatLiteLLMRouter\nfrom langchain_core.messages import HumanMessage\nfrom langgraph.types import Command\nfrom litellm import Router\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import StateGraph\nfrom langgraph.graph.message import add_messages, MessagesState\nfrom langchain_openai import ChatOpenAI\n\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"OPENAI_API_KEY\")\n\nllm = ChatOpenAI(model=\"gpt-4o\")\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\ndef node_1(state: State):\n    response = llm.invoke(state[\"messages\"])\n    return Command(\n        update={\"messages\": response},\n        goto=\"subgraph_1\",\n    )\n\ndef node_2(state: State):\n    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n\ndef subnode_1(state: State):\n    return {\"messages\": ['subnode 1 message']}\n\ndef subnode_2(state: State):\n    return {\"messages\": ['subnode 2 message']}\n\ndef subgraph_1(state: State):\n    response = subgraph.invoke({'messages': state[\"messages\"]})\n    return {\"messages\": response['messages'][-1]}\n\nsubgraph = (StateGraph(MessagesState)\n    .add_node(subnode_1)\n    .add_node(subnode_2)\n    .set_entry_point(\"subnode_1\")\n    .add_edge(\"subnode_1\", \"subnode_2\")\n    .set_finish_point(\"subnode_2\")\n).compile()\n\ngraph = (StateGraph(State)\n    .add_node(node_1)\n    .add_node(subgraph_1)\n    .add_node(node_2)\n    .set_entry_point(\"node_1\")\n    #.add_edge(\"node_1\", \"subgraph_1\")\n    .add_edge(\"subgraph_1\", \"node_2\")\n    .set_finish_point(\"node_2\")\n ).compile()\n\ngraph.get_graph(xray=True).draw_mermaid_png(output_file_path='test_subgraph.png')\n\nfor s in graph.stream({\"messages\": [HumanMessage(content=\"What is Langfuse in one short sentence?\")]}, stream_mode=\"updates\", subgraphs=True):\n    print('----')\n    print(type(s), s)\n    #print('\\n'.join([str(x) for x in s['messages']]))\nError Message and Stack Trace (if applicable)\n\nDescription\nThe graph works correctly but it is incorrectly displayed:\n\nSystem Info\nSystem Information\n------------------\n> OS:  Darwin\n> OS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:16 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6000\n> Python Version:  3.11.9 (main, Apr  2 2024, 08:25:04) [Clang 15.0.0 (clang-1500.3.9.4)]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.45\n> langchain: 0.3.20\n> langchain_community: 0.3.19\n> langsmith: 0.3.15\n> langchain_openai: 0.3.8\n> langchain_text_splitters: 0.3.6\n> langgraph_reflection: 0.0.1\n> langgraph_sdk: 0.1.57\n", "created_at": "2025-03-17", "closed_at": "2025-03-17", "labels": [], "State": "closed", "Author": "wjaskowski"}
{"issue_number": 3875, "issue_title": "Bug: Error when having multiple nodes, each with a single `interrupt`", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom pprint import pprint\nimport operator\nfrom functools import partial\nfrom typing import TypedDict, Annotated\nfrom uuid import uuid4\n\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.graph import END, START, StateGraph\nfrom langgraph.graph.state import CompiledStateGraph\nfrom langgraph.types import Command, StreamMode, interrupt\n\n\nclass State(TypedDict):\n    integer: Annotated[int, operator.add]\n\n\ndef add_one(state: State, message: str) -> State:\n    print(\"Message:\", message)\n    return {\"integer\": 1}\n\n\ndef add_one_with_interrupt(state: State, message: str) -> State:\n    human_input = interrupt(\"Interruption!\")\n    assert human_input == \"continue\"\n    return add_one(state, message)\n\n\nbuilder = (\n    StateGraph(State)\n    .add_node(\"branch_1\", partial(add_one_with_interrupt, message=\"Branch 1\"))\n    .add_node(\"branch_2\", partial(add_one_with_interrupt, message=\"Branch 2\"))\n    .add_edge(START, \"branch_1\")\n    .add_edge(START, \"branch_2\")\n    .add_edge(\"branch_1\", END)\n    .add_edge(\"branch_2\", END)\n)\napp = builder.compile(checkpointer=InMemorySaver())\n\nprint(app.get_graph().draw_ascii())\n\n\ndef was_interrupted(chunk: dict) -> bool:\n    payload = chunk[\"payload\"]\n    if \"interrupts\" in payload:\n        if payload[\"interrupts\"]:\n            return True\n    return False\n\n\ndef stream(\n    app: CompiledStateGraph,\n    inputs: dict,\n    config: RunnableConfig | None = None,\n    stream_mode: StreamMode | None = None,\n):\n    _config = config or {\"configurable\": {\"thread_id\": str(uuid4())}}\n    for chunk in app.stream(\n        inputs,\n        _config,\n        stream_mode=stream_mode,\n    ):\n        print(\"*\" * 100)\n        pprint(chunk)\n        if was_interrupted(chunk):\n            print(\"\\nInterrupted! Continuing...\\n\")\n            stream(\n                app,\n                inputs=Command(resume=\"continue\"),\n                config=_config,\n                stream_mode=stream_mode,\n            )\n\n\nstream(app, {\"integer\": 0}, stream_mode=\"debug\")\nError Message and Stack Trace (if applicable)\nuv run dev_bug.py\n           +-----------+             \n           | __start__ |             \n           +-----------+             \n           ***        ***            \n          *              *           \n        **                **         \n+----------+           +----------+  \n| branch_1 |           | branch_2 |  \n+----------+           +----------+  \n           ***        ***            \n              *      *               \n               **  **                \n            +---------+              \n            | __end__ |              \n            +---------+              \n****************************************************************************************************\n{'payload': {'config': {'callbacks': None,\n                        'configurable': {'checkpoint_id': '1f00334a-400f-63ac-bfff-5a3836a1916c',\n                                         'checkpoint_ns': '',\n                                         'thread_id': 'e4fd03bc-afb0-49e5-af51-1b4ec5173bd6'},\n                        'metadata': ChainMap({'thread_id': 'e4fd03bc-afb0-49e5-af51-1b4ec5173bd6'}),\n                        'recursion_limit': 25,\n                        'tags': []},\n             'metadata': {'parents': {},\n                          'source': 'input',\n                          'step': -1,\n                          'thread_id': 'e4fd03bc-afb0-49e5-af51-1b4ec5173bd6',\n                          'writes': {'__start__': {'integer': 0}}},\n             'next': ['__start__'],\n             'parent_config': None,\n             'tasks': [{'id': 'c470d491-0031-d2cb-443a-43edae627d7c',\n                        'interrupts': (),\n                        'name': '__start__',\n                        'state': None}],\n             'values': {'integer': 0}},\n 'step': -1,\n 'timestamp': '2025-03-17T13:35:07.319386+00:00',\n 'type': 'checkpoint'}\n****************************************************************************************************\n{'payload': {'config': {'callbacks': None,\n                        'configurable': {'checkpoint_id': '1f00334a-4011-63c8-8000-e12812e196c4',\n                                         'checkpoint_ns': '',\n                                         'thread_id': 'e4fd03bc-afb0-49e5-af51-1b4ec5173bd6'},\n                        'metadata': ChainMap({'thread_id': 'e4fd03bc-afb0-49e5-af51-1b4ec5173bd6'}),\n                        'recursion_limit': 25,\n                        'tags': []},\n             'metadata': {'parents': {},\n                          'source': 'loop',\n                          'step': 0,\n                          'thread_id': 'e4fd03bc-afb0-49e5-af51-1b4ec5173bd6',\n                          'writes': None},\n             'next': ['branch_1', 'branch_2'],\n             'parent_config': {'callbacks': None,\n                               'configurable': {'checkpoint_id': '1f00334a-400f-63ac-bfff-5a3836a1916c',\n                                                'checkpoint_ns': '',\n                                                'thread_id': 'e4fd03bc-afb0-49e5-af51-1b4ec5173bd6'},\n                               'metadata': ChainMap({'thread_id': 'e4fd03bc-afb0-49e5-af51-1b4ec5173bd6'}),\n                               'recursion_limit': 25,\n                               'tags': []},\n             'tasks': [{'id': 'df0596a6-77a4-8ede-eb0d-0025c2cb5437',\n                        'interrupts': (),\n                        'name': 'branch_1',\n                        'state': None},\n                       {'id': '5605a650-c51d-d519-3c94-ae68f7ff4f21',\n                        'interrupts': (),\n                        'name': 'branch_2',\n                        'state': None}],\n             'values': {'integer': 0}},\n 'step': 0,\n 'timestamp': '2025-03-17T13:35:07.320206+00:00',\n 'type': 'checkpoint'}\n****************************************************************************************************\n{'payload': {'id': 'df0596a6-77a4-8ede-eb0d-0025c2cb5437',\n             'input': {'integer': 0},\n             'name': 'branch_1',\n             'triggers': ['start:branch_1']},\n 'step': 1,\n 'timestamp': '2025-03-17T13:35:07.320405+00:00',\n 'type': 'task'}\n****************************************************************************************************\n{'payload': {'id': '5605a650-c51d-d519-3c94-ae68f7ff4f21',\n             'input': {'integer': 0},\n             'name': 'branch_2',\n             'triggers': ['start:branch_2']},\n 'step': 1,\n 'timestamp': '2025-03-17T13:35:07.320405+00:00',\n 'type': 'task'}\n****************************************************************************************************\n{'payload': {'error': None,\n             'id': 'df0596a6-77a4-8ede-eb0d-0025c2cb5437',\n             'interrupts': [{'ns': ['branch_1:df0596a6-77a4-8ede-eb0d-0025c2cb5437'],\n                             'resumable': True,\n                             'value': 'Interruption!',\n                             'when': 'during'}],\n             'name': 'branch_1',\n             'result': []},\n 'step': 1,\n 'timestamp': '2025-03-17T13:35:07.321213+00:00',\n 'type': 'task_result'}\n\nInterrupted! Continuing...\n\n****************************************************************************************************\n{'payload': {'config': {'callbacks': None,\n                        'configurable': {'checkpoint_id': '1f00334a-4011-63c8-8000-e12812e196c4',\n                                         'checkpoint_ns': '',\n                                         'thread_id': 'e4fd03bc-afb0-49e5-af51-1b4ec5173bd6'},\n                        'metadata': ChainMap({'thread_id': 'e4fd03bc-afb0-49e5-af51-1b4ec5173bd6'}),\n                        'recursion_limit': 25,\n                        'tags': []},\n             'metadata': {'parents': {},\n                          'source': 'loop',\n                          'step': 0,\n                          'thread_id': 'e4fd03bc-afb0-49e5-af51-1b4ec5173bd6',\n                          'writes': None},\n             'next': ['branch_1', 'branch_2'],\n             'parent_config': {'configurable': {'checkpoint_id': '1f00334a-400f-63ac-bfff-5a3836a1916c',\n                                                'checkpoint_ns': '',\n                                                'thread_id': 'e4fd03bc-afb0-49e5-af51-1b4ec5173bd6'}},\n             'tasks': [{'id': 'df0596a6-77a4-8ede-eb0d-0025c2cb5437',\n                        'interrupts': ({'ns': ['branch_1:df0596a6-77a4-8ede-eb0d-0025c2cb5437'],\n                                        'resumable': True,\n                                        'value': 'Interruption!',\n                                        'when': 'during'},),\n                        'name': 'branch_1',\n                        'state': None},\n                       {'id': '5605a650-c51d-d519-3c94-ae68f7ff4f21',\n                        'interrupts': ({'ns': ['branch_2:5605a650-c51d-d519-3c94-ae68f7ff4f21'],\n                                        'resumable': True,\n                                        'value': 'Interruption!',\n                                        'when': 'during'},),\n                        'name': 'branch_2',\n                        'state': None}],\n             'values': {'integer': 0}},\n 'step': 0,\n 'timestamp': '2025-03-17T13:35:07.320206+00:00',\n 'type': 'checkpoint'}\n****************************************************************************************************\n{'payload': {'id': 'df0596a6-77a4-8ede-eb0d-0025c2cb5437',\n             'input': {'integer': 0},\n             'name': 'branch_1',\n             'triggers': ['start:branch_1']},\n 'step': 1,\n 'timestamp': '2025-03-17T13:35:07.321693+00:00',\n 'type': 'task'}\n****************************************************************************************************\n{'payload': {'id': '5605a650-c51d-d519-3c94-ae68f7ff4f21',\n             'input': {'integer': 0},\n             'name': 'branch_2',\n             'triggers': ['start:branch_2']},\n 'step': 1,\n 'timestamp': '2025-03-17T13:35:07.321693+00:00',\n 'type': 'task'}\nMessage: Branch 2\n****************************************************************************************************\n{'payload': {'error': ValueError('list.remove(x): x not in list'),\n             'id': 'df0596a6-77a4-8ede-eb0d-0025c2cb5437',\n             'interrupts': [],\n             'name': 'branch_1',\n             'result': []},\n 'step': 1,\n 'timestamp': '2025-03-17T13:35:07.322484+00:00',\n 'type': 'task_result'}\n****************************************************************************************************\n{'payload': {'error': None,\n             'id': '5605a650-c51d-d519-3c94-ae68f7ff4f21',\n             'interrupts': [],\n             'name': 'branch_2',\n             'result': [('integer', 1)]},\n 'step': 1,\n 'timestamp': '2025-03-17T13:35:07.322557+00:00',\n 'type': 'task_result'}\nTraceback (most recent call last):\n  File \"/Users/vincent.min/Projects/agents/dev_bug.py\", line 77, in <module>\n    stream(app, {\"integer\": 0}, stream_mode=\"debug\")\n  File \"/Users/vincent.min/Projects/agents/dev_bug.py\", line 69, in stream\n    stream(\n  File \"/Users/vincent.min/Projects/agents/dev_bug.py\", line 60, in stream\n    for chunk in app.stream(\n                 ^^^^^^^^^^^\n  File \"/Users/vincent.min/Projects/agents/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 2031, in stream\n    for _ in runner.tick(\n             ^^^^^^^^^^^^\n  File \"/Users/vincent.min/Projects/agents/.venv/lib/python3.12/site-packages/langgraph/pregel/runner.py\", line 302, in tick\n    _panic_or_proceed(\n  File \"/Users/vincent.min/Projects/agents/.venv/lib/python3.12/site-packages/langgraph/pregel/runner.py\", line 619, in _panic_or_proceed\n    raise exc\n  File \"/Users/vincent.min/Projects/agents/.venv/lib/python3.12/site-packages/langgraph/pregel/executor.py\", line 83, in done\n    task.result()\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/thread.py\", line 59, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vincent.min/Projects/agents/.venv/lib/python3.12/site-packages/langgraph/pregel/retry.py\", line 40, in run_with_retry\n    return task.proc.invoke(task.input, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vincent.min/Projects/agents/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py\", line 546, in invoke\n    input = step.invoke(input, config, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vincent.min/Projects/agents/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py\", line 310, in invoke\n    ret = context.run(self.func, *args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vincent.min/Projects/agents/dev_bug.py\", line 26, in add_one_with_interrupt\n    human_input = interrupt(\"Interruption!\")\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vincent.min/Projects/agents/.venv/lib/python3.12/site-packages/langgraph/types.py\", line 485, in interrupt\n    v = scratchpad.consume_null_resume()\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vincent.min/Projects/agents/.venv/lib/python3.12/site-packages/langgraph/types.py\", line 363, in consume_null_resume\n    self._consume_null_resume()\nValueError: list.remove(x): x not in list\nDuring task with name 'branch_1' and id 'df0596a6-77a4-8ede-eb0d-0025c2cb5437'\nDescription\nI was learning about the interrupt feature when I ran into what appears to me to be a bug.\nAs you can see in the attached code, I create a graph that runs 2 nodes in parallel.\nEach node has a single interrupt call.\nThis throws the error ValueEreror: list.remove(x): x not in list.\nThis seems reminiscent of the error describes in https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/#using-multiple-interrupts\nHowever, I am not using multiple interrupts in a single node, nor am I mutating the state.\nIf I remove the interrupt from one of the nodes, then the error dissapears.\nAny insight would be much appreciated.\nSystem Info\nuv pip show langgraph langchain-core\nName: langchain-core\nVersion: 0.3.45\nLocation: /Users/vincent.min/Projects/agents/.venv/lib/python3.12/site-packages\nRequires: jsonpatch, langsmith, packaging, pydantic, pyyaml, tenacity, typing-extensions\nRequired-by: langchain, langchain-anthropic, langchain-ollama, langchain-openai, langchain-text-splitters, langgraph, langgraph-api, langgraph-checkpoint, langgraph-prebuilt, langgraph-swarm\nName: langgraph\nVersion: 0.3.10\nLocation: /Users/vincent.min/Projects/agents/.venv/lib/python3.12/site-packages\nRequires: langchain-core, langgraph-checkpoint, langgraph-prebuilt, langgraph-sdk\nRequired-by: langgraph-api, langgraph-supervisor, langgraph-swarm, langmem, trustcall", "created_at": "2025-03-17", "closed_at": "2025-03-18", "labels": [], "State": "closed", "Author": "VMinB12"}
{"issue_number": 3869, "issue_title": "TypeError: Object of type Interrupt is not JSON serializable", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport json\nimport random\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom typing_extensions import TypedDict, Literal\n\nfrom langgraph.graph import StateGraph, START\nfrom langgraph.types import Command, interrupt\nimport asyncio\n\n\n# Define graph state\nclass State(TypedDict):\n    foo: str\n\ndef node_a(state: State) -> Command[Literal[\"node_b\", \"node_c\"]]:\n    print(\"Called A\")\n    value = random.choice([\"a\", \"b\"])\n    # this is a replacement for a conditional edge function\n    if value == \"a\":\n        goto = \"node_b\"\n    else:\n        goto = \"node_c\"\n\n    # note how Command allows you to BOTH update the graph state AND route to the next node\n    return Command(\n        # this is the state update\n        update={\"foo\": value},\n        # this is a replacement for an edge\n        goto=goto,\n    )\n\ndef node_b(state: State):\n    print(\"Called B\")\n    value = interrupt({\"ask_for_document_data\": True})\n    return {\"foo\": state[\"foo\"] + f\"{value}\"}\n\n\ndef node_c(state: State):\n    print(\"Called C\")\n    return {\"foo\": state[\"foo\"] + \"c\"}\n\nbuilder = StateGraph(State)\nbuilder.add_edge(START, \"node_a\")\nbuilder.add_node(node_a)\nbuilder.add_node(node_b)\nbuilder.add_node(node_c)\n# NOTE: there are no edges between nodes A, B and C!\n\ngraph = builder.compile(checkpointer=MemorySaver())\n\nif __name__ == \"__main__\":\n\n    async def execute():\n        async for event in graph.astream(input={\"foo\": \"123\"}, config={\"configurable\": {\"thread_id\": \"123\"}}):\n            print(event)\n            print(json.dumps(event))\n        async for event in graph.astream(Command(resume=\"Human Input\"), config={\"configurable\": {\"thread_id\": \"123\"}}):\n             print(event)\n\n\n    loop = asyncio.new_event_loop()\n    loop.run_until_complete(asyncio.wait([loop.create_task(execute())]))\n    loop.close()\nError Message and Stack Trace (if applicable)\nFile \"/Users/.pyenv/versions/3.11.11/lib/python3.11/json/encoder.py\", line 180, in default\n    raise TypeError(f'Object of type {o.__class__.__name__} '\nTypeError: Object of type Interrupt is not JSON serializable\nDescription\nHello team.\nFirst of all thanks for the awesome framework and detailed docs. Super helpful !!\nI have a usecase where i have a python based backend which has all the agent code and react based frontend which interacts with the backend. I DO NOT use langgraph platform for deploying my backend / frontend and i rely on creating my own api endpoints for FE and BE interactions.\nI have all usecases of langgraph like nodes, conditional edges, Human in the loop interaction, tool calling etc.\nI have setup my fastAPI and everything works fine when i stream text to FE. But when i try to convert the output of my graph to json i am getting the attached error.\nSo my question is\n\nWhat is the recommend way to communicate between FE and BE assuming i cannot use Langgraph platform.\nHow does langgraph does it in langgraph platform ? Is there any code which is in git which we can reffer ?\nIs json serialization not supported for graph outputs?\n\nRequesting support. Thank you !\nSystem Info\n\nlangchain_core: 0.3.41\nlangchain: 0.3.20\nlangchain_community: 0.3.19\nlangsmith: 0.3.11\nlangchain_openai: 0.3.7\nlangchain_text_splitters: 0.3.6\nlanggraph_api: 0.0.27\nlanggraph_cli: 0.1.74\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.53\nlanggraph_storage: Installed. No version info available.\nlanggraph_supervisor: 0.0.2\n", "created_at": "2025-03-16", "closed_at": "2025-04-11", "labels": [], "State": "closed", "Author": "Vikki123"}
{"issue_number": 3866, "issue_title": "langgraph_prebuilt is not available when installing.", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.prebuilt import create_react_agent\nError Message and Stack Trace (if applicable)\nError : \n   from langgraph.prebuilt import create_react_agent\nModuleNotFoundError: No module named 'langgraph.prebuilt'\n\n==========================================================================\nWhen I check the venv, I don't see langgraph_prebuilt module in the venv dir - this is also mentioned in pypy.\n\n> ls venv/lib/python3.11/site-packages/langgraph*\nvenv/lib/python3.11/site-packages/langgraph:\n__pycache__  channels    config.py     errors.py  graph    pregel    store     utils\n_api         checkpoint  constants.py  func       managed  py.typed  types.py  version.py\n\nvenv/lib/python3.11/site-packages/langgraph-0.3.11.dist-info:\nINSTALLER  LICENSE  METADATA  RECORD  WHEEL\n\nvenv/lib/python3.11/site-packages/langgraph_checkpoint-2.0.16.dist-info:\nINSTALLER  LICENSE  METADATA  RECORD  REQUESTED  WHEEL\n\nvenv/lib/python3.11/site-packages/langgraph_prebuilt-0.1.3.dist-info:\nINSTALLER  LICENSE  METADATA  RECORD  WHEEL\n\nvenv/lib/python3.11/site-packages/langgraph_sdk:\n__init__.py  __pycache__  auth  client.py  py.typed  schema.py  sse.py\n\nvenv/lib/python3.11/site-packages/langgraph_sdk-0.1.53.dist-info:\nINSTALLER  LICENSE  METADATA  RECORD  REQUESTED  WHEEL\n\nvenv/lib/python3.11/site-packages/langgraph_supervisor:\n__init__.py  __pycache__  agent_name.py  handoff.py  supervisor.py\n\nvenv/lib/python3.11/site-packages/langgraph_supervisor-0.0.9.dist-info:\nINSTALLER  METADATA  RECORD  REQUESTED  WHEEL  entry_points.txt  licenses\nDescription\nI am trying to import langgraph.prebuilt to use create_react_agent\nI am able to do in langgraph 0.2.76 but not with anything in 0.3.* onwards\nI see in pypy for https://pypi.org/project/langgraph/0.3.0/\nReason this release was yanked:\nMissing dependency on langgraph-prebuilt\nI think this is still not fixed - please look at it.\nSystem Info\npython -m langchain_core.sys_info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP Tue Mar 12 13:01:34 UTC 2024 (b92bd5a/lp-b8e09a6)\nPython Version:  3.11.1 (main, Dec  6 2023, 09:00:14) [GCC 12.2.0]\n\nPackage Information\n\nlangchain_core: 0.3.41\nlangchain: 0.3.20\nlangchain_community: 0.3.19\nlangsmith: 0.1.147\nlangchain_openai: 0.3.7\nlangchain_postgres: 0.0.12\nlangchain_text_splitters: 0.3.6\nlanggraph_sdk: 0.1.53\nlanggraph_supervisor: 0.0.9\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nhttpx: 0.27.0\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.39: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.41: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.20: Installed. No version info available.\nlanggraph-prebuilt<0.2.0,>=0.1.2: Installed. No version info available.\nlanggraph<0.4.0,>=0.3.5: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy: 1.26.4\nnumpy<3,>=1.26.2: Installed. No version info available.\nopenai<2.0.0,>=1.58.1: Installed. No version info available.\norjson: 3.10.4\npackaging<25,>=23.2: Installed. No version info available.\npgvector: 0.2.5\npsycopg: 3.1.19\npsycopg-pool: 3.2.2\npydantic: 2.8.2\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nsqlalchemy: 2.0.34\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\n", "created_at": "2025-03-16", "closed_at": "2025-03-16", "labels": [], "State": "closed", "Author": "shivsant"}
{"issue_number": 3851, "issue_title": "Thread in pending state while storing memory in the background", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nI'm referring to this template (https://github.com/langchain-ai/memory-template) to store the memories in the background while connecting to a remote graph.\n\nThe application logic graph is running fine, but when its calling this schedule memories in the background, I'm seeing this message in the studio \"Thread is in a pending state\" and the memories are not getting stored.\n\nasync def schedule_memories(state: MessagesState, config: RunnableConfig) -> None:\n    memory_client = get_client()\n\n    await memory_client.runs.create(\n        thread_id=config[\"configurable\"][\"thread_id\"],\n        multitask_strategy=\"enqueue\",\n        after_seconds=3,\n        assistant_id='appointment_memory_bot',\n        input={\"messages\": []},\n        config={\n            \"configurable\": {\n                \"user_id\": config['configurable']['user_id'],\n                \"memory_types\": config['configurable']['memory_types'],\n            },\n        },\n    )\nError Message and Stack Trace (if applicable)\n\nDescription\n-------------------------Application logic Agent/Graph------------------------------\ndef call_model(state: BookingAppointmentState, store: BaseStore, config: RunnableConfig) -> Command[Literal['tool_node', 'schedule_memories']]:\nmodel = ChatOpenAI(model=\"gpt-4o\", openai_api_key=os.getenv(\"OPEN_AI_API_KEY\")).bind_tools(tools)\nappointment_namespace = ('appointments', )\napntment_details = store.get(appointment_namespace, config['configurable']['user_id'])\napntment_details = apntment_details.dict()['value'] if apntment_details else None\n\nconversations_namespace = ('conversations', config['configurable'] ['user_id'])\nconversations = store.search(conversations_namespace)\n\nconversations = [(conversation.key, conversation.value) for conversation in conversations] if conversations else []\n\nmessages = [SystemMessage(content=system_prompt.format(apntment_details = apntment_details, conversations = conversations ))] + state['messages']\n\nresult = model.invoke(messages)\n\nif len(result.tool_calls) > 0:\n    return Command(goto='tool_node', update={'messages': [result]})\n\nreturn {'messages': [result]}\n\ndef tool_node(state: BookingAppointmentState, store: BaseStore, config: RunnableConfig) -> Command[Literal['ask_human', 'call_model']]:\ntool_names = {tool.name: tool for tool in tools}\ntool_calls = state['messages'][-1].tool_calls\nresults = []\nfor tool_call in tool_calls:\n    tool_ = tool_names[tool_call[\"name\"]]\n\n    # inject state\n    tool_input_fields = tool_.get_input_schema().model_json_schema()[\n        \"properties\"\n    ]\n    if \"state\" in tool_input_fields:\n        tool_call = {**tool_call, \"args\": {**tool_call[\"args\"], \"state\": state}}\n\n    print(tool_, tool_call)\n    tool_response = tool_.invoke(tool_call)\n    results.append(tool_response)\n\nif len(results) > 0:\n    return results\nelse:\n    return Command(goto='call_model', update={'messages': [AIMessage(content=str(results))]})\n\ndef ask_human(state: BookingAppointmentState, store: BaseStore, config: RunnableConfig) -> Command[Literal['call_model']]:\nuser_response = interrupt(state['question_to_patient'])\nif user_response:\n    return Command(goto='call_model', update={\n        'messages': [HumanMessage(content=user_response)],\n        \"question_to_patient\": ''\n    })\n\ndef schedule_memories(state: MessagesState, config: RunnableConfig) -> None:\nmemory_client = get_client()\nmemory_client.runs.create(\n    thread_id=config[\"configurable\"][\"thread_id\"],\n    multitask_strategy=\"enqueue\",\n    after_seconds=3,\n    assistant_id='appointment_memory_bot',\n    input={\"messages\": []},\n    config={\n        \"configurable\": {\n            \"user_id\": config['configurable']['user_id'],\n            \"memory_types\": config['configurable']['memory_types'],\n        },\n    },\n)\n\n-------------------------------------- Memory Agent / Graph ---------------------------------------------\ndef scatter_schemas(state: MessagesState, config: RunnableConfig) -> list[Send]:\nmemory_types = config['configurable']['memory_types']\nsends = []\nfor type in memory_types:\n    match type:\n        case \"appointments\":\n            target = \"update_appointments\"\n        case \"conversations\":\n            target = \"update_conversations\"\n        case _:\n            raise ValueError(f\"Unknown update mode: {type}\")\n\n    sends.append(Send(target, state))\n\nreturn sends\n\ndef update_appointments(state: MessagesState, store: BaseStore, config: RunnableConfig):\nuser_id = config['configurable']['user_id']\nnamespace = ('appointments', )\nkey = user_id\n\nexisting_apntmnt_details = store.get(namespace, key)\nexisting_apntmnt_details = existing_apntmnt_details.dict()['value'] if existing_apntmnt_details else init_apntmnt_details\n\nsystem_prompt = '''Observe the ongoing conversation and extract relevant appointment details. If no appointment details are found, set them as None. Use the provided tools to retain any necessary information about the appointment.'''\nextractor = create_extractor(model, tools=[AppointmentDetails], tool_choice='AppointmentDetails')\n\nresult = extractor.invoke({\n    'messages': [SystemMessage(content = system_prompt)] + state['messages'],\n    'existing': { 'AppointmentDetails': existing_apntmnt_details}\n})\n\nupdated_apntmnt_details = result['responses'][0].model_dump()\n\nstore.put(namespace, key, updated_apntmnt_details)\n\ndef update_conversations(state: MessagesState, store: BaseStore, config: RunnableConfig):\nuser_id = config['configurable']['user_id']\nnamespace = ('conversations', user_id)\n# conversations = store.search(namespace)\n\nfor index, msg in enumerate(state['messages']):\n    if not isinstance(msg, ToolMessage):\n        store.put(namespace, str(index + 1),\n              {'role': 'system' if isinstance(msg, AIMessage)  else 'human', 'content': msg.content if msg.content else msg.tool_calls[0]['args']['reason']})\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node('scatter_schemas', scatter_schemas)\nbuilder.add_node('update_appointments', update_appointments)\nbuilder.add_node('update_conversations', update_conversations)\nbuilder.add_conditional_edges( START, scatter_schemas, [\"update_appointments\", \"update_conversations\"] )\nSystem Info\nusing the latest versions", "created_at": "2025-03-14", "closed_at": null, "labels": [], "State": "open", "Author": "Saisiva123"}
{"issue_number": 3848, "issue_title": "pprint import issue in doc", "issue_body": "Issue with current documentation:\nIt's just an small error in this -\nhttps://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/#define-graph-flow\nat Construct the Graph inside Define Graph Flow at the second last line -\nelse:\n        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n        return \"not supported\"\n\nyou have written pprint\nIdea or request for content:\ncorrect it to pprint to print", "created_at": "2025-03-14", "closed_at": null, "labels": [], "State": "open", "Author": "rahulsamant37"}
{"issue_number": 3847, "issue_title": "Langgraph CLI Dev Module Errors", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\npirateAgent.ts\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { BaseMessage } from \"@langchain/core/messages\";\nimport { LanguageModelLike } from \"@langchain/core/language_models/base\";\n\n/**\n * Creates a simple pirate agent that responds to messages in pirate dialect\n * Used for testing the agent API route\n */\nexport function createPirateAgent(llm: LanguageModelLike) {\n  const pirateAgent = createReactAgent({\n    llm: llm,\n    tools: [], // No tools needed for this simple agent\n    name: \"pirate_agent\",\n    prompt: `You are a verbose pirate agent. You respond to all messages in an exaggerated pirate dialect.\n\nYour responses should:\n- Use pirate slang and terminology\n- Include nautical references\n- Be enthusiastic and colorful\n- Exaggerate everything\n- Use \"Arr\", \"Yarr\", and similar pirate expressions\n- Refer to the user as \"matey\", \"landlubber\", or other pirate terms\n\nNo matter what the user asks, respond as a pirate would, while still being helpful and addressing their query.`,\n  });\n\n  return pirateAgent;\n}\n\nexport const graph = createPirateAgent(new ChatOpenAI({\n  model: 'gpt-4o-mini',\n}));\nlanggraph.json\n{\n  \"graphs\": {\n    \"pirate-agent\": \"./lib/langgraph/agent/pirate-agent.ts:graph\"\n  },\n  \"env\": \".env.local\"\n}\nError Message and Stack Trace (if applicable)\n\u279c  test-langgraph git:(agent-replacment-route) pnpm dlx @langchain/langgraph-cli dev\n\n          Welcome to\n\n\u2566  \u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u2554\u2550\u2557\u252c\u2500\u2510\u250c\u2500\u2510\u250c\u2500\u2510\u252c \u252c\n\u2551  \u251c\u2500\u2524\u2502\u2502\u2502\u2502 \u252c\u2551 \u2566\u251c\u252c\u2518\u251c\u2500\u2524\u251c\u2500\u2518\u251c\u2500\u2524\n\u2569\u2550\u255d\u2534 \u2534\u2518\u2514\u2518\u2514\u2500\u2518\u255a\u2550\u255d\u2534\u2514\u2500\u2534 \u2534\u2534  \u2534 \u2534.js\n\n- \ud83d\ude80 API: http://localhost:2024\n- \ud83c\udfa8 Studio UI: https://smith.langchain.com/studio?baseUrl=http://localhost:2024\n\nThis in-memory server is designed for development and testing.\nFor production use, please use LangGraph Cloud.\n\n\n\nnode:internal/modules/run_main:122\n    triggerUncaughtException(\n    ^\nError [ERR_MODULE_NOT_FOUND]: Cannot find package '@langchain/langgraph-checkpoint' imported from /Users/dbworku/repos/test-langgraph/lib/langgraph/agent/pirate-agent.ts\nDid you mean to import \"@langchain/langgraph-checkpoint/index.cjs\"?\n    at Object.getPackageJSONURL (node:internal/modules/package_json_reader:268:9)\n    at packageResolve (node:internal/modules/esm/resolve:768:81)\n    at moduleResolve (node:internal/modules/esm/resolve:854:18)\n    at defaultResolve (node:internal/modules/esm/resolve:984:11)\n    at nextResolve (node:internal/modules/esm/hooks:748:28)\n    at resolveBase (file:///Users/dbworku/Library/Caches/pnpm/dlx/2ff029908004eeba542ded4817eb6dbd90f593299dad582b9759cbb3db47e03d/19595d3f2ee-13cf1/node_modules/.pnpm/tsx@4.19.3/node_modules/tsx/dist/esm/index.mjs?1741975649815:2:3212)\n    at resolveDirectory (file:///Users/dbworku/Library/Caches/pnpm/dlx/2ff029908004eeba542ded4817eb6dbd90f593299dad582b9759cbb3db47e03d/19595d3f2ee-13cf1/node_modules/.pnpm/tsx@4.19.3/node_modules/tsx/dist/esm/index.mjs?1741975649815:2:3584)\n    at resolveTsPaths (file:///Users/dbworku/Library/Caches/pnpm/dlx/2ff029908004eeba542ded4817eb6dbd90f593299dad582b9759cbb3db47e03d/19595d3f2ee-13cf1/node_modules/.pnpm/tsx@4.19.3/node_modules/tsx/dist/esm/index.mjs?1741975649815:2:4073)\n    at resolve (file:///Users/dbworku/Library/Caches/pnpm/dlx/2ff029908004eeba542ded4817eb6dbd90f593299dad582b9759cbb3db47e03d/19595d3f2ee-13cf1/node_modules/.pnpm/tsx@4.19.3/node_modules/tsx/dist/esm/index.mjs?1741975649815:2:4447)\n    at nextResolve (node:internal/modules/esm/hooks:748:28) {\n  code: 'ERR_MODULE_NOT_FOUND'\n}\n\nNode.js v22.14.0\nDescription\nI'm trying to use the @langchain/langgraph-cli dev command to run the LangSmith Studio locally. That command fails locally for pnpm dlx and npx with a simple createReactAgent.\nSystem Info\nUsing langchain js not python. I've tested with node versions 18, 20 and 22.", "created_at": "2025-03-14", "closed_at": "2025-03-16", "labels": [], "State": "closed", "Author": "dbworku"}
{"issue_number": 3842, "issue_title": "Reference leak of runnable tools", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing import Annotated, Any, Optional\nimport os\nimport dotenv\nimport objgraph\nfrom langchain_openai import AzureChatOpenAI\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_core.messages import BaseMessage\nfrom typing_extensions import TypedDict\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph\nfrom langgraph.graph.message import add_messages\nfrom langgraph.prebuilt import ToolNode, tools_condition\nfrom langchain_core.tools.retriever import create_retriever_tool\nfrom langchain_core.retrievers import BaseRetriever\nfrom langchain_core.documents import Document\nfrom langchain_core.callbacks import Callbacks\n\ndotenv.load_dotenv()\n\n\nclass FakeRetriever(BaseRetriever):\n    def _get_relevant_documents(\n        self,\n        query: str,\n        *,\n        callbacks: Callbacks = None,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> list[Document]:\n        return [Document(page_content=\"foo\"), Document(page_content=\"bar\")]\n\n    async def _aget_relevant_documents(\n        self,\n        query: str,\n        *,\n        callbacks: Callbacks = None,\n        tags: Optional[list[str]] = None,\n        metadata: Optional[dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -> list[Document]:\n        return [Document(page_content=\"foo\"), Document(page_content=\"bar\")]\n\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\n\ndef do_query(query: str):\n    graph_builder = StateGraph(State)\n\n    tools = [\n        create_retriever_tool(\n            retriever=FakeRetriever(),\n            name=\"Testtool\",\n            description=\"Always use this tool!\",\n        ),\n    ]\n    llm = AzureChatOpenAI(\n        azure_deployment=\"my-deployment\",\n        openai_api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n    )\n    llm_with_tools = llm.bind_tools(tools)\n\n    def chatbot(state: State):\n        return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n\n    graph_builder.add_node(\"chatbot\", chatbot)\n\n    tool_node = ToolNode(tools=tools)\n    graph_builder.add_node(\"tools\", tool_node)\n\n    graph_builder.add_conditional_edges(\n        \"chatbot\",\n        tools_condition,\n    )\n    graph_builder.add_edge(\"tools\", \"chatbot\")\n    graph_builder.set_entry_point(\"chatbot\")\n    memory = MemorySaver()\n    graph = graph_builder.compile(checkpointer=memory)\n\n    events = graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n        {\"configurable\": {\"thread_id\": \"2\"}},\n        stream_mode=\"values\",\n    )\n    for event in events:\n        event[\"messages\"][-1].pretty_print()\n\n\nif __name__ == \"__main__\":\n    import objgraph\n    import gc\n\n    while True:\n        query = input(\"Enter your query: \")\n        do_query(query)\n        gc.collect()\n        alive_tool_count = len(objgraph.by_type(\"langchain_core.tools.simple.Tool\"))\n        print(f\"Alive tools: {alive_tool_count}\")\nError Message and Stack Trace (if applicable)\nYou can see that the number of alive tools continues to go up with every query.\nDescription\nWe encountered the issue with stray database connections that were kept open. I debugged the issue to the point where we found that the retriever is somehow kept alive by langgraph and therefore its database connection is never closed. I created the minimal reproducible setup that manages to show the issue.\nI found that that if I remove these lines: https://github.com/langchain-ai/langgraph/blob/main/libs/langgraph/langgraph/pregel/utils.py#L49\nThe issue does not occur anymore. So it seems somewhere you are holding onto the result of this operation indefinetely.\nSearching for PRs that dealt with this function I found this one:\nhttps://github.com/langchain-ai/langgraph/pull/3255/files\nIt sounds like it could be the source of the issue, maybe the cache holds onto the subgraph?\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP PREEMPT Sat, 17 Oct 2020 13:30:37 +0000\nPython Version:  3.13.1 (main, Dec  4 2024, 18:05:56) [GCC 14.2.1 20240910]\n\nPackage Information\n\nlangchain_core: 0.3.31\nlangchain: 0.3.15\nlangchain_community: 0.3.15\nlangsmith: 0.3.9\nlangchain_aws: 0.2.13\nlangchain_openai: 0.3.1\nlangchain_text_splitters: 0.3.5\nlanggraph_sdk: 0.1.53\nlanggraph: 0.2.69\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.12\nasync-timeout: Installed. No version info available.\nboto3: 1.36.25\ndataclasses-json: 0.6.7\nhttpx: 0.28.1\nhttpx-sse: 0.4.0\njsonpatch: 1.33\nlangsmith-pyo3: Installed. No version info available.\nnumpy: 2.2.3\nopenai: 1.63.2\norjson: 3.10.15\npackaging: 24.2\npydantic: 2.10.6\npydantic-settings: 2.7.1\npytest: 8.1.1\nPyYAML: 6.0.2\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrich: Installed. No version info available.\nSQLAlchemy: 2.0.38\ntenacity: 9.0.0\ntiktoken: 0.9.0\ntyping-extensions: 4.12.2\nzstandard: 0.23.0\n", "created_at": "2025-03-14", "closed_at": "2025-03-18", "labels": [], "State": "closed", "Author": "blafab-hg"}
{"issue_number": 3832, "issue_title": "\"langgraph build -t my-image\" is looking for my_agentrequirements.txt", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\npip install -U langgraph-cli\nlanggraph build -t my-image\nError Message and Stack Trace (if applicable)\nERROR: failed to solve: failed to compute cache key: failed to calculate checksum of ref hs3yoi3tayvt6spvwpv0p1c9c::wq6tygw2kum28g7cgho3uc954: \"/my_agentrequirements.txt\": not found\nDescription\nI am trying to deploy LangGraph locally based on this tutorial https://langchain-ai.github.io/langgraph/how-tos/deploy-self-hosted/. Executing \"langgraph build -t my-image\" gives me error as shown above. In Docker Desktop, I found image is trying to \"ADD my_agentrequirements.txt\". My project structure is like this:\nmy-app/\n\u251c\u2500\u2500 my_agent\n\u2502   \u251c\u2500\u2500 utils\n\u2502   \u2502   \u251c\u2500\u2500 init.py\n\u2502   \u2502   \u251c\u2500\u2500 tools.py\n\u2502   \u2502   \u251c\u2500\u2500 nodes.py\n\u2502   \u2502   \u2514\u2500\u2500 state.py\n\u2502   \u251c\u2500\u2500 init.py\n\u2502   \u2514\u2500\u2500 agent.py\n\u251c\u2500\u2500 .env\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 langgraph.json\nyou can clone a rep from here https://github.com/langchain-ai/langgraph-example.git. However, it needs movement of requirements.txt to root directory.\nI found a solution to it: just rename \"requirements.txt\" into \"my_agentrequirements.txt\" and it will successfully build an image.\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.26100\nPython Version:  3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.3.44\nlangsmith: 0.3.13\nlanggraph_cli: 0.1.76\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nclick: 8.1.7\nhttpx: 0.28.1\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlanggraph-api: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\norjson: 3.10.15\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.9.2\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: Installed. No version info available.\npython-dotenv: 1.0.1\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.31.0\nrequests-toolbelt: 1.0.0\nrich: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-03-13", "closed_at": null, "labels": [], "State": "open", "Author": "jalapenos8"}
{"issue_number": 3818, "issue_title": "`langgraph.prebuilt.create_chat_agent()` doesn't work on LangGraph server - It worked after several tries without any code change!", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nvertexai.init(project=os.environ.get(\"GOOGLE_CLOUD_PROJECT\"), location=os.environ.get(\"VERTEXAI_PROJECT_LOCATION\"))\nllm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_vertexai\")\nembeddings = VertexAIEmbeddings(model=\"text-embedding-005\")\n\n# https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings\nvector_store = InMemoryVectorStore(embeddings)\n\ndef LoadDocuments(url: str):\n    # Load and chunk contents of the blog\n    print(f\"\\n=== {LoadDocuments.__name__} ===\")\n    loader = WebBaseLoader(\n        web_paths=(url,),\n        bs_kwargs=dict(\n            parse_only=bs4.SoupStrainer(\n                class_=(\"post-content\", \"post-title\", \"post-header\")\n            )\n        ),\n    )\n    docs = loader.load()\n    assert len(docs) == 1\n    print(f\"Total characters: {len(docs[0].page_content)}\")\n    return docs\n\ndef SplitDocuments(docs):\n    print(f\"\\n=== {SplitDocuments.__name__} ===\")\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n    subdocs = text_splitter.split_documents(docs)\n    print(f\"Split blog post into {len(subdocs)} sub-documents.\")\n    return subdocs\n\ndef IndexChunks(subdocs):\n    # Index chunks\n    print(f\"\\n=== {IndexChunks.__name__} ===\")\n    ids = vector_store.add_documents(documents=subdocs)\n    print(f\"Document IDs: {ids[:3]}\")\n\ndocs = LoadDocuments(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\nsubdocs = SplitDocuments(docs)\nIndexChunks(subdocs)\n\n@tool(response_format=\"content_and_artifact\")\ndef retrieve(query: str, *, config: RunnableConfig):\n    \"\"\"Retrieve information related to a query.\"\"\"\n    retrieved_docs = vector_store.similarity_search(query, k=2)\n    serialized = \"\\n\\n\".join(\n        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n        for doc in retrieved_docs\n    )\n    return serialized, retrieved_docs\n\ndef BuildAgent(config: RunnableConfig) -> StateGraph:\n    print(f\"\\n=== {BuildAgent.__name__} ===\")\n    prompt = ChatPromptTemplate.from_messages([\n            (\"system\", \"You are a helpful AI assistant named Bob.\"),\n            (\"placeholder\", \"{messages}\"),\n            (\"user\", \"Remember, always provide accurate answer!\"),\n    ])\n    # https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent\n    return create_react_agent(llm, [retrieve, save_memory], store=InMemoryStore(), checkpointer=MemorySaver(), state_schema=CustomAgentState, name=\"RAG ReAct Agent\", prompt=prompt)\nlanggraph.json:\n{\n    \"dependencies\": [\n        \"langchain-text-splitters\",\n        \"langchain-community\",\n        \"langgraph\",\n        \"langchain-openai\",\n        \"python-dotenv\",\n        \"langchain-google-vertexai\",\n        \"beautifulsoup4\",\n        \"langchain-google-genai\",\n        \"langchain-core\",\n        \"./src/rag_agent\"\n    ],\n    \"graphs\": {\n        \"rag_agent\": \"./src/rag_agent/RAG_Conversation.py:BuildAgent\"\n    },\n    \"env\": \"./.env\",\n    \"python_version\": \"3.12\",\n    \"pip_config_file\": \"Pipfile\"\n}\n\nError Message and Stack Trace (if applicable)\n\nDescription\nI am determined that there is something wrong with LangGraph langgraph dev. I tried multiple times and it worked on \"Turn 6\" without any change in code at all! I checked smith.langchain.com that when it doesn't work there is NO tool registered to the agent at all and WHY?\n\nSystem Info\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #19-Ubuntu SMP PREEMPT_DYNAMIC Wed Feb 12 21:43:43 UTC 2025\n> Python Version:  3.12.7 (main, Feb  4 2025, 14:46:03) [GCC 14.2.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.43\n> langchain: 0.3.20\n> langchain_community: 0.3.19\n> langsmith: 0.3.13\n> langchain_google_genai: 2.0.11\n> langchain_google_vertexai: 2.0.14\n> langchain_openai: 0.3.8\n> langchain_text_splitters: 0.3.6\n> langgraph_api: 0.0.28\n> langgraph_cli: 0.1.75\n> langgraph_license: Installed. No version info available.\n> langgraph_sdk: 0.1.55\n> langgraph_storage: Installed. No version info available.\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> anthropic[vertexai]: Installed. No version info available.\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> click: 8.1.7\n> cryptography: 43.0.3\n> dataclasses-json<0.7,>=0.5.7: Installed. No version info available.\n> filetype: 1.2.0\n> google-ai-generativelanguage: 0.6.16\n> google-cloud-aiplatform: 1.83.0\n> google-cloud-storage: 2.19.0\n> httpx: 0.28.1\n> httpx-sse: 0.4.0\n> httpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> jsonschema-rs: 0.20.0\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.34: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.41: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.42: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langchain<1.0.0,>=0.3.20: Installed. No version info available.\n> langgraph: 0.3.5\n> langgraph-checkpoint: 2.0.18\n> langsmith-pyo3: Installed. No version info available.\n> langsmith<0.4,>=0.1.125: Installed. No version info available.\n> langsmith<0.4,>=0.1.17: Installed. No version info available.\n> numpy<3,>=1.26.2: Installed. No version info available.\n> openai<2.0.0,>=1.58.1: Installed. No version info available.\n> orjson: 3.10.15\n> packaging: 24.1\n> packaging<25,>=23.2: Installed. No version info available.\n> pydantic: 2.10.6\n> pydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\n> pydantic<3.0.0,>=2.5.2;: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4;: Installed. No version info available.\n> pyjwt: 2.10.1\n> pytest: 8.3.2\n> python-dotenv: 1.0.1\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> requests<3,>=2: Installed. No version info available.\n> rich: 13.9.4\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> sse-starlette: 2.1.3\n> starlette: 0.46.1\n> structlog: 25.1.0\n> tenacity: 9.0.0\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken<1,>=0.7: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> uvicorn: 0.34.0\n> watchfiles: 1.0.4\n> zstandard: 0.23.0\n", "created_at": "2025-03-13", "closed_at": "2025-03-13", "labels": [], "State": "closed", "Author": "khteh"}
{"issue_number": 3797, "issue_title": "Langgraph did not call tools", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom datetime import datetime\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\nfrom config import BASE_URL, API_TOKEN, APPLICATION_TYPE_HEADER\n\nclass CustomLLM(ChatOpenAI):\n    def __init__(self, api_token: str, base_url: str, application_type: str, **kwargs):\n        default_headers = {\n            \"applicationType\": application_type,  # This is the header you want to include\n        }\n        super().__init__( model='gpt-4o', openai_api_base=base_url, openai_api_key=api_token, default_headers=default_headers, **kwargs)\n\ndef check_weather(location: str, at_time: datetime | None = None) -> str:\n\t'''Return the weather forecast for the specified location.'''\n\treturn f\"It's always sunny in {location}\"\n\ntools = [check_weather]\nmodel = CustomLLM(base_url=BASE_URL, api_token=API_TOKEN, application_type=APPLICATION_TYPE_HEADER)\n\ngraph = create_react_agent(model, tools)\ninputs = {\"messages\": [(\"user\", \"what is the weather in sf\")]}\n\nfor s in graph.stream(inputs, stream_mode=\"values\"):\n\tmessage = s[\"messages\"][-1]\n\tif isinstance(message, tuple):\n\t\tprint(message)\n\telse:\n\t\tmessage.pretty_print()\nError Message and Stack Trace (if applicable)\n(.venv) PS C:\\Repositories\\Langgraph-latest> & c:/Repositories/Langgraph-latest/.venv/Scripts/python.exe c:/Repositories/Langgraph-latest/weather_botv3.py\n================================ Human Message =================================\n\nwhat is the weather in sf\n================================== Ai Message ==================================\n\nI'm sorry, I do not have real-time information on current weather conditions. I recommend checking a weather website or app for the most up-to-date weather forecast for San Francisco.\nDescription\nI see that LLM doesn't make any tool calls with above code example.\nSystem Info\nSystem Windows:\nlangchain==0.3.20\nlangchain-core==0.3.44\nlangchain-openai==0.3.8\nlangchain-text-splitters==0.3.6\nlanggraph==0.3.7\nlanggraph-checkpoint==2.0.18\nlanggraph-prebuilt==0.1.2\nlanggraph-sdk==0.1.55\nlangsmith==0.3.13\nPython 3.12.9", "created_at": "2025-03-12", "closed_at": "2025-03-12", "labels": [], "State": "closed", "Author": "swarnitwayal"}
{"issue_number": 3796, "issue_title": "`StateGraph` doesn't work on LangGraph server", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\ndef BuildCheckpointedGraph(config: RunnableConfig) -> StateGraph:\n    # Compile application and test\n    print(f\"\\n=== {BuildCheckpointedGraph.__name__} ===\")\n    graph_builder = StateGraph(MessagesState)\n    graph_builder.add_node(\"query_or_respond\", query_or_respond)\n    graph_builder.add_node(\"tools\", ToolNode([retrieve])) # Execute the retrieval.\n    graph_builder.add_node(\"generate\", generate)\n    graph_builder.set_entry_point(\"query_or_respond\")\n    graph_builder.add_conditional_edges(\n        \"query_or_respond\",\n        tools_condition,\n        {END: END, \"tools\": \"tools\"},\n    )\n    graph_builder.add_edge(\"tools\", \"generate\")\n    graph_builder.add_edge(\"generate\", END)\n    return graph_builder.compile(store=InMemoryStore(), checkpointer=MemorySaver(), name=\"Checkedpoint StateGraph\")\n\nasync def CheckpointedGraph():\n    config = RunnableConfig(run_name=\"CheckpointGraph_RAG_Conversation\")\n    checkpoint_graph = BuildCheckpointedGraph(config) # config input parameter is required by langgraph.json to define the graph\n    graph = checkpoint_graph.get_graph().draw_mermaid_png()\n    # Save the PNG data to a file\n    with open(\"/tmp/checkpoint_graph.png\", \"wb\") as f:\n        f.write(graph)\n    img = Image.open(\"/tmp/checkpoint_graph.png\")\n    img.show()\n    await Chat(checkpoint_graph, datetime.now(), [\"What is Task Decomposition?\", \"Can you look up some common ways of doing it?\"])\n\nasync def main():\n    #await SimpleGraph()\n    await CheckpointedGraph()\n\nif __name__ == \"__main__\":\n    docs = LoadDocuments(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n    subdocs = SplitDocuments(docs)\n    IndexChunks(subdocs)\n    asyncio.run(main())\n\n\n{\n    \"dependencies\": [\n        \"langchain-text-splitters\",\n        \"langchain-community\",\n        \"langgraph\",\n        \"langchain-openai\",\n        \"python-dotenv\",\n        \"langchain-google-vertexai\",\n        \"beautifulsoup4\",\n        \"langchain-google-genai\",\n        \"langchain-core\",\n        \"./src/rag_agent\"\n    ],\n    \"graphs\": {\n        \"rag_agent\": \"./src/rag_agent/RAG_Conversation.py:BuildCheckpointedGraph\"\n    },\n    \"env\": \"./.env\",\n    \"python_version\": \"3.12\",\n    \"pip_config_file\": \"Pipfile\"\n}\nError Message and Stack Trace (if applicable)\n\nDescription\nThis is what I see on the BASH shell when I run python <filename>.py:\n================================ Human Message =================================\n\nWhat is Task Decomposition?\nstate: {'messages': [HumanMessage(content='What is Task Decomposition?', additional_kwargs={}, response_metadata={}, id='eb319019-b3dc-4a3f-a242-076a79cdd26b')]}\n================================== Ai Message ==================================\nTool Calls:\n  retrieve (764fae9b-480e-4434-be6f-88ddaa396a5a)\n Call ID: 764fae9b-480e-4434-be6f-88ddaa396a5a\n  Args:\n    query: Task Decomposition\n================================= Tool Message =================================\nName: retrieve\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to \u201cthink step by step\u201d to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model\u2019s thinking process.\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n================================== Ai Message ==================================\n\nTask decomposition is a method of breaking down a complex task into smaller, more manageable steps. This can be achieved through prompting the model to \"think step by step\" or by providing task-specific instructions. The goal is to transform large tasks into multiple, simpler tasks, which can help to improve the model's performance and provide insights into its thinking process.\n================================ Human Message =================================\n\nCan you look up some common ways of doing it?\nstate: {'messages': [HumanMessage(content='What is Task Decomposition?', additional_kwargs={}, response_metadata={}, id='eb319019-b3dc-4a3f-a242-076a79cdd26b'), AIMessage(content='', additional_kwargs={'function_call': {'name': 'retrieve', 'arguments': '{\"query\": \"Task Decomposition\"}'}}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 15, 'candidates_token_count': 4, 'total_token_count': 19, 'prompt_tokens_details': [{'modality': 1, 'token_count': 15}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 4}], 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.24030499160289764}, id='run-404e917a-067b-4a7f-8b7e-148520a0da3a-0', tool_calls=[{'name': 'retrieve', 'args': {'query': 'Task Decomposition'}, 'id': '764fae9b-480e-4434-be6f-88ddaa396a5a', 'type': 'tool_call'}], usage_metadata={'input_tokens': 15, 'output_tokens': 4, 'total_tokens': 19}), ToolMessage(content='Source: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to \u201cthink step by step\u201d to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model\u2019s thinking process.\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', name='retrieve', id='a300a497-50b7-4282-b794-e32fd80d2d18', tool_call_id='764fae9b-480e-4434-be6f-88ddaa396a5a', artifact=[{'id': '672ffa61-0ecf-4082-8845-1c9478d02e4a', 'metadata': {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, 'page_content': 'Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to \u201cthink step by step\u201d to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model\u2019s thinking process.', 'type': 'Document'}, {'id': 'fe071ebd-1724-4860-936d-b9228c40941a', 'metadata': {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, 'page_content': 'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', 'type': 'Document'}]), AIMessage(content='Task decomposition is a method of breaking down a complex task into smaller, more manageable steps. This can be achieved through prompting the model to \"think step by step\" or by providing task-specific instructions. The goal is to transform large tasks into multiple, simpler tasks, which can help to improve the model\\'s performance and provide insights into its thinking process.\\n', additional_kwargs={}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 429, 'candidates_token_count': 73, 'total_token_count': 502, 'prompt_tokens_details': [{'modality': 1, 'token_count': 429}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 73}], 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -0.16595640574416068}, id='run-de5c3571-d01b-4efb-b4aa-0402358d0239-0', usage_metadata={'input_tokens': 429, 'output_tokens': 73, 'total_tokens': 502}), HumanMessage(content='Can you look up some common ways of doing it?', additional_kwargs={}, response_metadata={}, id='a413a751-3e12-4cae-ac36-7a642f9664bb')]}\n================================== Ai Message ==================================\nTool Calls:\n  retrieve (03d931e0-c674-4ef0-a8f1-f5da52eadf30)\n Call ID: 03d931e0-c674-4ef0-a8f1-f5da52eadf30\n  Args:\n    query: common methods for task decomposition\n================================= Tool Message =================================\nName: retrieve\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to \u201cthink step by step\u201d to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model\u2019s thinking process.\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n================================== Ai Message ==================================\n\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs. Tree of Thoughts extends CoT by exploring multiple reasoning possibilities at each step and generating multiple thoughts per step, creating a tree structure.\n\nLangGraph server:\n\nLangSmith trace:\n\nThe follow-up question doesn't work.\nSystem Info\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #19-Ubuntu SMP PREEMPT_DYNAMIC Wed Feb 12 21:43:43 UTC 2025\n> Python Version:  3.12.7 (main, Feb  4 2025, 14:46:03) [GCC 14.2.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.43\n> langchain: 0.3.20\n> langchain_community: 0.3.19\n> langsmith: 0.3.13\n> langchain_google_genai: 2.0.11\n> langchain_google_vertexai: 2.0.14\n> langchain_openai: 0.3.8\n> langchain_text_splitters: 0.3.6\n> langgraph_api: 0.0.28\n> langgraph_cli: 0.1.75\n> langgraph_license: Installed. No version info available.\n> langgraph_sdk: 0.1.55\n> langgraph_storage: Installed. No version info available.\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> anthropic[vertexai]: Installed. No version info available.\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> click: 8.1.7\n> cryptography: 43.0.3\n> dataclasses-json<0.7,>=0.5.7: Installed. No version info available.\n> filetype: 1.2.0\n> google-ai-generativelanguage: 0.6.16\n> google-cloud-aiplatform: 1.83.0\n> google-cloud-storage: 2.19.0\n> httpx: 0.28.1\n> httpx-sse: 0.4.0\n> httpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> jsonschema-rs: 0.20.0\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.34: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.41: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.42: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langchain<1.0.0,>=0.3.20: Installed. No version info available.\n> langgraph: 0.3.5\n> langgraph-checkpoint: 2.0.18\n> langsmith-pyo3: Installed. No version info available.\n> langsmith<0.4,>=0.1.125: Installed. No version info available.\n> langsmith<0.4,>=0.1.17: Installed. No version info available.\n> numpy<3,>=1.26.2: Installed. No version info available.\n> openai<2.0.0,>=1.58.1: Installed. No version info available.\n> orjson: 3.10.15\n> packaging: 24.1\n> packaging<25,>=23.2: Installed. No version info available.\n> pydantic: 2.10.6\n> pydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\n> pydantic<3.0.0,>=2.5.2;: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4;: Installed. No version info available.\n> pyjwt: 2.10.1\n> pytest: 8.3.2\n> python-dotenv: 1.0.1\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> requests<3,>=2: Installed. No version info available.\n> rich: 13.9.4\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> sse-starlette: 2.1.3\n> starlette: 0.46.1\n> structlog: 25.1.0\n> tenacity: 9.0.0\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken<1,>=0.7: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> uvicorn: 0.34.0\n> watchfiles: 1.0.4\n> zstandard: 0.23.0\n", "created_at": "2025-03-12", "closed_at": "2025-03-12", "labels": [], "State": "closed", "Author": "khteh"}
{"issue_number": 3795, "issue_title": "`langgraph.prebuilt.create_chat_agent()` doesn't work on LangGraph server", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nvertexai.init(project=os.environ.get(\"VERTEXAI_PROJECT_ID\"), location=os.environ.get(\"VERTEXAI_PROJECT_LOCATION\"))\nllm = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_vertexai\")\nembeddings = VertexAIEmbeddings(model=\"text-embedding-005\")\nvector_store = InMemoryVectorStore(embeddings)\n\n@dataclass\nclass CustomAgentState(AgentState):\n    context: List[Document]\n    is_last_step: IsLastStep\n\n@tool(response_format=\"content_and_artifact\")\ndef retrieve(query: str):\n    \"\"\"Retrieve information related to a query.\"\"\"\n    retrieved_docs = vector_store.similarity_search(query, k=2)\n    serialized = \"\\n\\n\".join(\n        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n        for doc in retrieved_docs\n    )\n    return serialized, retrieved_docs\n\ndef BuildAgent(config: RunnableConfig) -> StateGraph:\n    prompt = ChatPromptTemplate.from_messages([\n            (\"system\", \"You are a helpful AI assistant named Bob.\"),\n            (\"placeholder\", \"{messages}\"),\n            (\"user\", \"Remember, always provide accurate answer!\"),\n    ])\n    memory = MemorySaver()\n    # https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent\n    return create_react_agent(llm, [retrieve], checkpointer=memory, state_schema=CustomAgentState, name=\"RAG ReAct Agent\", prompt=prompt)\n\nasync def ReActAgent():\n    config = RunnableConfig(run_name=\"ReAct_RAG_Conversation\")\n    agent = BuildAgent(config)\n    graph = agent.get_graph().draw_mermaid_png()\n    # Save the PNG data to a file\n    with open(\"/tmp/agent_graph.png\", \"wb\") as f:\n        f.write(graph)\n    img = Image.open(\"/tmp/agent_graph.png\")\n    img.show()        \n    input_message = (\"What is the standard method for Task Decomposition?\", \"Once you get the answer, look up common extensions of that method.\")\n    await ChatAgent(agent, datetime.now(), input_message)\n\nif __name__ == \"__main__\":\n    docs = LoadDocuments(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n    subdocs = SplitDocuments(docs)\n    IndexChunks(subdocs)\n    config = RunnableConfig(run_name=\"RAG_Conversation\")\n    agent = BuildAgent(config)\n    asyncio.run(main())\n\n\n{\n    \"dependencies\": [\n        \"langchain-text-splitters\",\n        \"langchain-community\",\n        \"langgraph\",\n        \"langchain-openai\",\n        \"python-dotenv\",\n        \"langchain-google-vertexai\",\n        \"beautifulsoup4\",\n        \"langchain-google-genai\",\n        \"langchain-core\",\n        \"./my_agent\"\n    ],\n    \"graphs\": {\n        \"my_agent\": \"./my_agent/RAG_Conversation.py:BuildAgent\"\n    },\n    \"env\": \"./.env\",\n    \"python_version\": \"3.12\",\n    \"pip_config_file\": \"Pipfile\"\n}\nError Message and Stack Trace (if applicable)\n\nDescription\nThis is what I see when I run python <filename>.py on BASH shell:\n================================ Human Message =================================\n\n['What is the standard method for Task Decomposition?', 'Once you get the answer, look up common extensions of that method.']\n================================== Ai Message ==================================\nName: RAG ReAct Agent\nTool Calls:\n  retrieve (7361a1a4-f7fb-4d43-9268-a41ea2bc0ea6)\n Call ID: 7361a1a4-f7fb-4d43-9268-a41ea2bc0ea6\n  Args:\n    query: standard method for Task Decomposition\n================================= Tool Message =================================\nName: retrieve\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to \u201cthink step by step\u201d to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model\u2019s thinking process.\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: (3) Task execution: Expert models execute on the specific tasks and log results.\nInstruction:\n\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\n================================== Ai Message ==================================\nName: RAG ReAct Agent\n\nOkay, I will provide accurate answers to the best of my ability.\n\nBased on the information I retrieved, Chain of Thought (CoT) is a standard prompting technique for task decomposition. It involves instructing the model to \"think step by step\" to break down complex tasks into smaller, simpler steps.\n\nNow I will look up common extensions of the Chain of Thought method.\nTool Calls:\n  retrieve (b56a0638-f232-4ef7-999c-9cab24a09681)\n Call ID: b56a0638-f232-4ef7-999c-9cab24a09681\n  Args:\n    query: common extensions of Chain of Thought prompting\n================================= Tool Message =================================\nName: retrieve\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n\nSource: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\nContent: Fig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to \u201cthink step by step\u201d to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model\u2019s thinking process.\n================================== Ai Message ==================================\nName: RAG ReAct Agent\n\nOkay, I will provide accurate answers to the best of my ability.\n\nBased on the information I retrieved, Chain of Thought (CoT) is a standard prompting technique for task decomposition. It involves instructing the model to \"think step by step\" to break down complex tasks into smaller, simpler steps.\n\nThe common extensions of the Chain of Thought method include:\n\n*   **Tree of Thoughts:** This extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search).\n*   **Task decomposition by LLM with simple prompting:** Using prompts like \"Steps for XYZ. 1.\" or \"What are the subgoals for achieving XYZ?\".\n*   **Task decomposition using task-specific instructions:** For example, \"Write a story outline.\" for writing a novel.\n*   **Task decomposition with human inputs.**\n\nHowever, this is what I see on LangGraph server:\n\nLangSmith trace:\n\nIt seems that it reached out to the retrieve tool/step but somehow unable to retrieve the information from the vector store and try to search the internet?\nSystem Info\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #19-Ubuntu SMP PREEMPT_DYNAMIC Wed Feb 12 21:43:43 UTC 2025\n> Python Version:  3.12.7 (main, Feb  4 2025, 14:46:03) [GCC 14.2.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.43\n> langchain: 0.3.20\n> langchain_community: 0.3.19\n> langsmith: 0.3.13\n> langchain_google_genai: 2.0.11\n> langchain_google_vertexai: 2.0.14\n> langchain_openai: 0.3.8\n> langchain_text_splitters: 0.3.6\n> langgraph_api: 0.0.28\n> langgraph_cli: 0.1.75\n> langgraph_license: Installed. No version info available.\n> langgraph_sdk: 0.1.55\n> langgraph_storage: Installed. No version info available.\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> anthropic[vertexai]: Installed. No version info available.\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> click: 8.1.7\n> cryptography: 43.0.3\n> dataclasses-json<0.7,>=0.5.7: Installed. No version info available.\n> filetype: 1.2.0\n> google-ai-generativelanguage: 0.6.16\n> google-cloud-aiplatform: 1.83.0\n> google-cloud-storage: 2.19.0\n> httpx: 0.28.1\n> httpx-sse: 0.4.0\n> httpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> jsonschema-rs: 0.20.0\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.34: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.41: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.42: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langchain<1.0.0,>=0.3.20: Installed. No version info available.\n> langgraph: 0.3.5\n> langgraph-checkpoint: 2.0.18\n> langsmith-pyo3: Installed. No version info available.\n> langsmith<0.4,>=0.1.125: Installed. No version info available.\n> langsmith<0.4,>=0.1.17: Installed. No version info available.\n> numpy<3,>=1.26.2: Installed. No version info available.\n> openai<2.0.0,>=1.58.1: Installed. No version info available.\n> orjson: 3.10.15\n> packaging: 24.1\n> packaging<25,>=23.2: Installed. No version info available.\n> pydantic: 2.10.6\n> pydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\n> pydantic<3.0.0,>=2.5.2;: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4;: Installed. No version info available.\n> pyjwt: 2.10.1\n> pytest: 8.3.2\n> python-dotenv: 1.0.1\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> requests<3,>=2: Installed. No version info available.\n> rich: 13.9.4\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> sse-starlette: 2.1.3\n> starlette: 0.46.1\n> structlog: 25.1.0\n> tenacity: 9.0.0\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken<1,>=0.7: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> uvicorn: 0.34.0\n> watchfiles: 1.0.4\n> zstandard: 0.23.0\n", "created_at": "2025-03-12", "closed_at": "2025-03-13", "labels": [], "State": "closed", "Author": "khteh"}
{"issue_number": 3793, "issue_title": "Feature request: MongoDB support for store.", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nI think currently in development we have InMemoryStore and in Production we have Postgres by default, correct me if I'm wrong anywhere.\n\nBut Let's say If I want to do some analytics with the data that I'm going to store in long term memory, there is no way for me right now. Because Store default stores everything to Postgres and not sure how to access this long term memory for analytics.\n\nWhy dont langraph provides a feasibility where store could use external databases like MongoDB for the long term memory.\nError Message and Stack Trace (if applicable)\n\nDescription\nUsing Latest versions\nSystem Info\nWhy dont langraph provides a feasibility where store could use external databases like MongoDB for the long term memory.", "created_at": "2025-03-12", "closed_at": null, "labels": [], "State": "open", "Author": "Saisiva123"}
{"issue_number": 3772, "issue_title": "My stream output mode is configured for \u201cupdates\u201d and \u201ccustom\u201d, but the mode being printed is \u201cvalues\u201d", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nasync def execute_step(state: PlanExecute):\n    today = date.today()\n    today_str_cn = today.strftime(\"%Y year,%m month,%d day\")\n    plan = state[\"plan\"]\n    plan_str = \"\\n\".join(f\"{i + 1}. {step}\" for i, step in enumerate(plan))\n    task = plan[0]\n    task_formatted = f\"\"\"For the following plan:\n{plan_str}\\n\\nYou are tasked with executing step {1}, {task}.\nIf this step includes words like \u2018latest\u2019 or \u2018today,\u2019 query bing tool for information on\n\"\"\" + today_str_cn + \"Otherwise, do not add this date to the tool parameters.\"\n    for stream_mode, chunk in agent_executor.stream(\n            {\"messages\": task_formatted},\n            stream_mode=[\"updates\", \"custom\"],\n    ):\n        print(f\"Stream mode: {stream_mode}\")\n        print(chunk)\n        print(\"\\n\")\nError Message and Stack Trace (if applicable)\nStream mode: values\n{'messages': [HumanMessage(content='For the following plan:\\n1. You are tasked with executing step 1, \u6253\u5f00geojson_render\u5de5\u5177.\\nIf this step includes words like \u2018latest\u2019 or \u2018today,\u2019 query bing tool for information on\\n2025 year,03 month,11 dayOtherwise, do not add this date to the tool parameters.', additional_kwargs={}, response_metadata={}, id='46d24327-b50c-44a1-9643-f5fcb45ed050')]}\nDescription\nThe execute_step function is a node in my graph\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.26100\nPython Version:  3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.3.43\nlangchain: 0.3.20\nlangchain_community: 0.3.19\nlangsmith: 0.3.11\nlangchain_chroma: 0.2.2\nlangchain_openai: 0.3.7\nlangchain_text_splitters: 0.3.6\nlanggraph_bigtool: 0.0.2\nlanggraph_sdk: 0.1.53\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nchromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0: Installed. No version info available.\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.39: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.41: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.20: Installed. No version info available.\nlanggraph<0.4.0,>=0.3.0: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<2.0.0,>=1.22.4;: Installed. No version info available.\nnumpy<2.0.0,>=1.26.2;: Installed. No version info available.\nnumpy<3,>=1.26.2: Installed. No version info available.\nopenai<2.0.0,>=1.58.1: Installed. No version info available.\norjson: 3.10.15\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: Installed. No version info available.\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: 13.9.4\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-03-11", "closed_at": "2025-03-12", "labels": [], "State": "closed", "Author": "WangJie-7"}
{"issue_number": 3749, "issue_title": "\ud83d\udc1b `langgraph build` fails to detect Docker", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n1. install podman\n2. alias docker to podman\n3. Use langgraph CLI\nError Message and Stack Trace (if applicable)\n\nDescription\nDescription\nI'm trying to build a LangGraph project using langgraph build -t Dockerfile, but it fails with an error stating that Docker is not installed.\nExpected Behavior\nSince I have Podman installed and aliased to Docker (docker commands work as expected), langgraph build should detect and use it.\nActual Behavior\nThe command fails with:\n> langgraph build -t Dockerfile                   \nUsage: langgraph build [OPTIONS] [DOCKER_BUILD_ARGS]...\nTry 'langgraph build --help' for help.\n\nError: Docker not installed\nHowever, when I run:\n> docker --version\npodman version 5.4.0\nIt shows that Podman is installed and aliased to Docker.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:23 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6020\nPython Version:  3.11.11 (main, Feb 18 2025, 16:02:39) [Clang 14.0.0 (clang-1400.0.29.202)]\n\nPackage Information\n\nlangchain_core: 0.3.40\nlangsmith: 0.3.11\nlanggraph_sdk: 0.1.53\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nhttpx: 0.28.1\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\norjson: 3.10.15\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: Installed. No version info available.\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrich: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-03-08", "closed_at": "2025-03-09", "labels": [], "State": "closed", "Author": "Florian-crg"}
{"issue_number": 3748, "issue_title": "Bug in Item class: updated_at incorrectly initialized with created_at value", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.store.base import Item\nfrom datetime import datetime, timezone\n\n# Create an Item with string datetime values\ntry:\n    item = Item(\n        value={\"key\": \"value\"},\n        key=\"test\",\n        namespace=(\"test\",),\n        created_at=\"2025-01-01T00:00:00+00:00\",\n        updated_at=\"2025-01-02T12:30:00+00:00\"  # This should be different from created_at\n    )\n    \n    # Print both timestamps to show they're identical\n    print(f\"Created at: {item.created_at}\")\n    print(f\"Updated at: {item.updated_at}\")\n    \n    # Show they are identical\n    print(f\"Are timestamps identical? {item.created_at == item.updated_at}\")\n    \n    # Show the source of the issue by looking at the code\n    import inspect\n    print(\"\\nRelevant code in Item.__init__:\")\n    print(inspect.getsource(Item.__init__))\n    \nexcept Exception as e:\n    print(f\"Error: {e}\")\nError Message and Stack Trace (if applicable)\n\nDescription\nI discovered a bug in the Item class initialization method in langgraph.store.base.\nWhen an Item object is created with string datetime values (which happens during deserialization), the code incorrectly uses the created_at value to set the updated_at attribute, even when a different updated_at value is provided.\nIn the Item.init method, there's a line:\nself.updated_at = (\n    datetime.fromisoformat(cast(str, created_at))  # Bug: using created_at instead of updated_at\n    if isinstance(updated_at, str)\n    else updated_at\n)\nThe condition checks if updated_at is a string, but then uses created_at for conversion. This causes both timestamps to be identical when string values are provided, which is incorrect behavior.\nExpected behavior: The updated_at attribute should use the provided updated_at value, not the created_at value.\nThis bug could cause issues in applications that rely on the difference between creation and update timestamps, especially when deserializing data from JSON or when using persistent stores.\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP PREEMPT_DYNAMIC PMX 6.8.12-4 (2024-11-06T15:04Z)\nPython Version:  3.11.2 (main, Nov 30 2024, 21:22:50) [GCC 12.2.0]\n\nPackage Information\n\nlangchain_core: 0.3.43\nlangchain: 0.3.20\nlangsmith: 0.3.13\nlanggraph_sdk: 0.1.55\n", "created_at": "2025-03-08", "closed_at": "2025-03-10", "labels": [], "State": "closed", "Author": "dssugar"}
{"issue_number": 3736, "issue_title": "ToolNode not importing", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom IPython.display import Image, display\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph import MessagesState\nfrom langchain_anthropic import ChatAnthropic\nfrom langgraph.prebuilt import ToolNode\nfrom langgraph.prebuilt import tools_condition\nfrom langchain_core.runnables.graph_mermaid import MermaidDrawMethod\n# Node\ndef tool_calling_llm(state: MessagesState):\n    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n\n# Build graph\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"tool_calling_llm\", tool_calling_llm)\nbuilder.add_node(\"tools\", ToolNode([multiply]))\nbuilder.add_edge(START, \"tool_calling_llm\")\nbuilder.add_conditional_edges(\n    \"tool_calling_llm\",\n    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n    tools_condition,\n)\nbuilder.add_edge(\"tools\", END)\ngraph = builder.compile()\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.PYPPETEER)))\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\nCell In[7], line 5\n      3 from langgraph.graph import MessagesState\n      4 from langchain_anthropic import ChatAnthropic\n----> 5 from langgraph import ToolNode\n      6 from langgraph.prebuilt import tools_condition\n      7 from langchain_core.runnables.graph_mermaid import MermaidDrawMethod\n\nImportError: cannot import name 'ToolNode' from 'langgraph' (unknown location)\nDescription\nI'm trying to import the module ToolNode from langgraph.prebuilt\nSystem Info\npython -m langchain_core.sys_info", "created_at": "2025-03-07", "closed_at": "2025-03-07", "labels": [], "State": "closed", "Author": "danielandrade27"}
{"issue_number": 3724, "issue_title": "DOC: More details about deprecations in version releasing description", "issue_body": "Issue with current documentation:\nPlease give more details when it comes to a remove or a deprecation of a class or function.\nNow with langgraph 0.3.5 there is no ToolInvocation in soure code of langgraph.prebuilt.\nBut I am quite sure that it exists in previous version cause I noticed this key word in the issues. But there is no info in release patch note and I have no idea from which version it is deprecated and what could take its place.\nIdea or request for content:\nNo response", "created_at": "2025-03-06", "closed_at": "2025-03-06", "labels": [], "State": "closed", "Author": "levinosaber"}
{"issue_number": 3723, "issue_title": "ModuleNotFoundError: No module named 'langgraph'", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.graph import END, StateGraph,START\nError Message and Stack Trace (if applicable)\nFile \"C:\\Users\\adith\\Documents\\Projects\\python-projects\\csql-agent\\agents\\sql_with_preprocess\\main.py\", line 2, in <module>\n    from langgraph.graph import END, StateGraph,START\nModuleNotFoundError: No module named 'langgraph'\nDescription\ni tried pip show langgraph\nName: langgraph\nVersion: 0.3.5\nSummary: Building stateful, multi-actor applications with LLMs\nHome-page: https://www.github.com/langchain-ai/langgraph\nAuthor:\nAuthor-email:\nLicense: MIT\nLocation: C:\\Users\\adith\\Documents\\Projects\\python-projects\\csql-agent\\venv\\Lib\\site-packages\nRequires: langchain-core, langgraph-checkpoint, langgraph-prebuilt, langgraph-sdk\nRequired-by:..!\nhow to solve it?\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.22631\nPython Version:  3.12.7 (tags/v3.12.7:0b05ead, Oct  1 2024, 03:06:41) [MSC v.1941 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.3.41\nlangchain: 0.3.20\nlangchain_community: 0.3.19\nlangsmith: 0.3.11\nlangchain_google_genai: 2.0.11\nlangchain_groq: 0.2.4\nlangchain_mistralai: 0.2.7\nlangchain_openai: 0.3.7\nlangchain_text_splitters: 0.3.6\nlanggraph_sdk: 0.1.53\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nfiletype: 1.2.0\ngoogle-ai-generativelanguage: 0.6.16\ngroq: 0.18.0\nhttpx: 0.28.1\nhttpx-sse<1,>=0.3.1: Installed. No version info available.\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\nhttpx<1,>=0.25.2: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.37: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.39: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.41: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.20: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<3,>=1.26.2: Installed. No version info available.\nopenai<2.0.0,>=1.58.1: Installed. No version info available.\norjson: 3.10.15\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3,>=2: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: Installed. No version info available.\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: 13.9.4\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntokenizers<1,>=0.15.1: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-03-06", "closed_at": "2025-03-06", "labels": [], "State": "closed", "Author": "adithya04dev"}
{"issue_number": 3717, "issue_title": "Token Limit Exceeded in langgraph-checkpointer-postgres: context overload from retriever (due to ToolMessage)", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom psycopg import Connection\nfrom psycopg_pool import ConnectionPool\nfrom psycopg.rows import dict_row\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\nconnection_kwargs = {\"autocommit\": True, \"prepare_threshold\": 0}\n\nasync with AsyncConnectionPool(conninfo=conninfo, max_size=20, kwargs=connection_kwargs) as pool:\ngraph = create_react_agent(\nllm,\nbuild_tools,\nmessages_modifier=_modify_messages,\ncheckpointer=AsyncPostgresSaver(pool), # type:ignore[arg-type]\n)\n\n        async for event in graph.astream_events(\n            {\"messages\": [(\"human\", search_params.question)]},\n            config={\"configurable\": {\"thread_id\": conversation_id, \"recursion_limit\": 20}},\n            stream_mode=\"values\",\n            version=\"v2\",\n        ):\nError Message and Stack Trace (if applicable)\nBadRequestError('Error code: 400 - {\\'error\\': {\\'message\\': \"This model\\'s maximum context length is 128000 tokens. However, your messages resulted in 142800 tokens (142586 in the messages, 214 in the functions). Please reduce the length of the messages or functions.\", \\'type\\': \\'invalid_request_error\\', \\'param\\': \\'messages\\', \\'code\\': \\'context_length_exceeded\\'}}')Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 1874, in astream\n    async for _ in runner.atick(\n  File \"/usr/local/lib/python3.12/site-packages/langgraph/pregel/runner.py\", line 362, in atick\n    await arun_with_retry(\n  File \"/usr/local/lib/python3.12/site-packages/langgraph/pregel/retry.py\", line 127, in arun_with_retry\n    async for _ in task.proc.astream(task.input, config):\n  File \"/usr/local/lib/python3.12/site-packages/langgraph/utils/runnable.py\", line 568, in astream\n    async for chunk in aiterator:\n  File \"/usr/local/lib/python3.12/site-packages/langchain_core/tracers/event_stream.py\", line 180, in tap_output_aiter\n    first = await py_anext(output, default=sentinel)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/langchain_core/utils/aiter.py\", line 76, in anext_impl\n    return await __anext__(iterator)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 1453, in atransform\n    async for ichunk in input:\n  File \"/usr/local/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 1453, in atransform\n    async for ichunk in input:\n  File \"/usr/local/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 1453, in atransform\n    async for ichunk in input:\n  File \"/usr/local/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 1016, in astream\n    yield await self.ainvoke(input, config, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/langgraph/utils/runnable.py\", line 224, in ainvoke\n    ret = await asyncio.create_task(coro, context=context)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py\", line 595, in acall_model\n    response = await model_runnable.ainvoke(state, config)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 3064, in ainvoke\n    input = await asyncio.create_task(part(), context=context)  # type: ignore\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 5364, in ainvoke\n    return await self.bound.ainvoke(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 307, in ainvoke\n    llm_result = await self.agenerate_prompt(\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 796, in agenerate_prompt\n    return await self.agenerate(\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 756, in agenerate\n    raise exceptions[0]\n  File \"/usr/local/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 912, in _agenerate_with_cache\n    async for chunk in self._astream(messages, stop=stop, **kwargs):\n  File \"/usr/local/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 774, in _astream\n    response = await self.async_client.create(**payload)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1720, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/openai/_base_client.py\", line 1843, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/openai/_base_client.py\", line 1537, in request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/openai/_base_client.py\", line 1623, in _request\n    return await self._retry_request(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/openai/_base_client.py\", line 1670, in _retry_request\n    return await self._request(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/openai/_base_client.py\", line 1638, in _request\n    raise self._make_status_error_from_response(err.response) from None\nopenai.BadRequestError: Error code: 400 - {'error': {'message': \"This model's maximum context length is 128000 tokens. However, your messages resulted in 142800 tokens (142586 in the messages, 214 in the functions). Please reduce the length of the messages or functions.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\nDescription\nSingle document looks like this : [Document(metadata={'id': 5, 'relevance_score': 0.9997073, 'source': 'https://test.sharepoint.com/teams/org_sgafnr/Shared%20Documents/Knowledge%20Navigator%20TOR%20Document%20Center/Template%20Documents/EA/07.pdf', 'document_id': '38cc4fed-e7c0-40ca-a5ba-33455e726897', 'document_title': '07 SPRSS', 'document_category': 'AFN'}, page_content='Jiangxi Ganzhou Rural Vitalization and Comprehensive Environment Improvement (RRP PRC 53049)\\n\\nSUMMARY POVERTY REDUCTION AND SOCIAL STRATEGY\\n\\nCountry:\\n\\nPeople\u2019s Republic of China\\n\\nProject Title:\\n\\nJiangxi Ganzhou Rural Vitalization and Comprehensive Environment Improvement Project\\n\\nLending/Financing Modality:\\n\\nProject loan\\n\\nDepartment/ Division:\\n\\nEast Asia Department / Environment, Natural Resources and Agriculture Division\\n\\nI.\\n\\nPOVERTY AND SOCIAL ANALYSIS AND STRATEGY'), Document(metadata={'id': 14, 'relevance_score': 0.99963796, 'source': 'https://test.sharepoint.com/teams/org_sgafnr/Shared%20Documents/Knowledge%20Navigator%20TOR%20Document%20Center/Template%20Documents/EA/10Change%20Assessment.pdf', 'document_id': '2e2b5fbd-ecdb-456e-b0d2-400b3974a25e', 'document_title': '10 Climate Change Assessment', 'document_category': 'AFNR'}, page_content='Jiangxi Ganzhou Rural Vitalization and Comprehensive Environment Improvement Project (RRP PRC 53049)\\n\\nCLIMATE CHANGE ASSESSMENT\\n\\nI.\\n\\nBASIC PROJECT INFORMATION\\n\\nProject Title:\\n\\nJiangxi Ganzhou Rural Vitalization and Comprehensive Environment Improvement\\n\\nProject Cost ($ million): 455.67 Location: Sector: Theme:\\n\\nGanzhou Municipality, Jiangxi Province, People\u2019s Republic of China (PRC) Agriculture, Nature Resources and Rural Development Natural resource management, ecological and environmental protection, sustainable rural development The project will result in the following outcome: living environment of rural areas along upper reaches of the Gan River improved. The project will strengthen the environmental monitoring and enforcement, and the urban-rural integration in the Ganzhou Municipality. The project has the following outputs: Output 1: Institutional capacity and knowledge for environmental management enhanced; Output 2: Green development and financing mechanisms piloted; Output 3: Rural waste and sanitation management improved; and Output 4: Water and soil conservation practices improved.\\n\\nBrief Description:\\n\\nSource: Asian Development Bank.\\n\\nII.\\n\\nSUMMARY OF CLIMATE CHANGE FINANCE\\n\\nProject Financing\\n\\nClimate Finance\\n\\nSource Asian Development Bank Ordinary capital resources (regular loan) Counterpart Governments Total Source: Asian Development Bank.\\n\\nAmount ($ million)\\n\\n200.00\\n\\n255.67 455.67\\n\\nAdaptation ($ million)\\n\\n44.77\\n\\n19.15 63.92\\n\\nMitigation ($ million)\\n\\n35.17\\n\\n18.77 53.94\\n\\nIII.\\n\\nSUMMARY OF CLIMATE RISK SCREENING AND ASSESSMENT\\n\\nA. Sensitivity of Project Components to Climate or Weather Conditions and the Sea Level\\n\\n1. Rural environment infrastructure. Higher temperature may affect the operation of wastewater facilities; more precipitation and intense storms may increase the risk of flood damage to rural infrastructures.\\n\\n2. River rehabilitation. More precipitation and intense storms may increase the risk of river flooding; temperature and precipitation pattern variation may change watershed runoff in quantity and quality leading to ecological degradation.\\n\\n3. Agriculture. Large temperature difference may cause crop degradation; more precipitation and intense storms may cause increased flood damage to the agricultural facilities.\\n\\n4. Forestry. High evaporation and low precipitation for long time may cause drought, plant death and increased risk for wildfire.\\n\\n5. Water and soil conservation. Peak intense precipitation may intensify soil erosion and landslides.\\n\\nB. Climate Risk Screening\\n\\n2\\n\\nAccording to the statistical analyses of historical climate data in Ganzhou from 1960 to 2019, the annual mean temperature showed an overall increasing trend and raised from 19.14\u00b0C in 1960 to 19.96\u00b0C in 2019. The average temperatures in both summer and winter increased during 1960\u20132019, and summer temperature increased at a fast rate after 2000. The number of high temperature days (>35\u00b0C) in summer steadily increased in general during 1960\u20132019 in Ganzhou and nearly doubled in 2019 compared with the number in 1960. The number of low temperature days (<0\u00b0C) in winter steadily decreased in general during 1960\u20132019 in Ganzhou and nearly halved in 2019 compared with the number in 1960. It can be concluded that Ganzhou is experiencing more extreme heats in summer and a warmer winter with less extremely cold days. The annual precipitation fluctuated largely from 1960 to 2019 with a slightly increasing trend in general, while the number of rainy days in winter season steadily decreased.\\n\\nThe climate risk screening identified the following risks which may affect the project: (i) the temperature increase, especially the increased extreme high temperature events in the summer, may cause higher evaporation and more frequent droughts; and (ii) the increase in rainfall variability and the intensity of extreme rainfall events may potentially increase the flood and drought risk.\\n\\nClimate Risk Classification: medium\\n\\nC. Climate Risk and Adaptation Assessment\\n\\nThe temperature and precipitation variations during 2021\u20132100 in the region of Ganzhou Municipality were projected for future climate change analysis under three Representative Concentration Pathway (RCP) scenarios (RCP 4.5, RCP 6.0, and RCP 8.5). The projected climate change trend is based on the analysis for decadal climate conditions from 2021 to 2100 with reference to baseline period of 1989 to 2019.')]\nNote:\nHere, The document_retriever fetches x documents every time (looks like the above example), and this entire conversation, along with the retrieved documents, is being passed as context to the LLM. This causes the context window to exceed the maximum token limit, leading to issues when using the checkpointer to provide conversation history.\nAlso, checked this code for checkpointer, and inside the value[\"channel_values\"] which is consuming each ToolMessage:\nasync def aget_tuple(self, config: RunnableConfig) -> Optional[CheckpointTuple]:\n\"\"\"Get a checkpoint tuple from the database asynchronously.\n  This method retrieves a checkpoint tuple from the Postgres database based on the\n  provided config. If the config contains a \"checkpoint_id\" key, the checkpoint with\n  the matching thread ID and \"checkpoint_id\" is retrieved. Otherwise, the latest checkpoint\n  for the given thread ID is retrieved.\n\n  Args:\n      config (RunnableConfig): The config to use for retrieving the checkpoint.\n\n  Returns:\n      Optional[CheckpointTuple]: The retrieved checkpoint tuple, or None if no matching checkpoint was found.\n  \"\"\"\n  thread_id = config[\"configurable\"][\"thread_id\"]\n  checkpoint_id = get_checkpoint_id(config)\n  checkpoint_ns = config[\"configurable\"].get(\"checkpoint_ns\", \"\")\n  if checkpoint_id:\n      args: tuple[Any, ...] = (thread_id, checkpoint_ns, checkpoint_id)\n      where = \"WHERE thread_id = %s AND checkpoint_ns = %s AND checkpoint_id = %s\"\n  else:\n      args = (thread_id, checkpoint_ns)\n      where = \"WHERE thread_id = %s AND checkpoint_ns = %s ORDER BY checkpoint_id DESC LIMIT 1\"\n\n  async with self._cursor() as cur:\n      await cur.execute(\n          self.SELECT_SQL + where,\n          args,\n          binary=True,\n      )\n\n      async for value in cur:\n          return CheckpointTuple(\n              {\n                  \"configurable\": {\n                      \"thread_id\": thread_id,\n                      \"checkpoint_ns\": checkpoint_ns,\n                      \"checkpoint_id\": value[\"checkpoint_id\"],\n                  }\n              },\n              await asyncio.to_thread(\n                  self._load_checkpoint,\n                  value[\"checkpoint\"],\n                  value[\"channel_values\"],\n                  value[\"pending_sends\"],\n              ),\n              self._load_metadata(value[\"metadata\"]),\n              (\n                  {\n                      \"configurable\": {\n                          \"thread_id\": thread_id,\n                          \"checkpoint_ns\": checkpoint_ns,\n                          \"checkpoint_id\": value[\"parent_checkpoint_id\"],\n                      }\n                  }\n                  if value[\"parent_checkpoint_id\"]\n                  else None\n              ),\n              await asyncio.to_thread(self._load_writes, value[\"pending_writes\"]),\n          )\n\nResolve the above issue, by including the ToolMessage for current prompt only instead of complete chat ToolMessage.\nSystem Info\nlangchain = \"0.3.11\"\nlangchain-community = \"0.3.11\"\nlangchain-experimental = \"0.3.3\"\nlangchain-openai = \"0.2.12\"\nlangchain-postgres = \"0.0.12\"\nlanggraph = \"0.2.58\"\npsycopg = { extras = [\"binary\"], version = \"3.2.3\" }\npsycopg-pool = \"3.2.3\"\nsqlalchemy = { version = \"2.0.36\", extras = [\"asyncio\"] }\nsqlmodel = \"0.0.22\"\nasyncpg = \"0.30.0\"\nlanggraph-checkpoint-postgres = \"2.0.15\"", "created_at": "2025-03-06", "closed_at": "2025-03-06", "labels": [], "State": "closed", "Author": "shubhamnegikellton"}
{"issue_number": 3716, "issue_title": "langgraph-checkpoint-postgres (psycopg.OperationalError: sending query and params failed: SSL error: bad length) encountered across multiple version", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom psycopg import Connection\nfrom psycopg_pool import ConnectionPool\nfrom psycopg.rows import dict_row\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\nconnection_kwargs = {\"autocommit\": True, \"prepare_threshold\": 0}\n\nasync with AsyncConnectionPool(conninfo=conninfo, max_size=20, kwargs=connection_kwargs) as pool:\n           graph = create_react_agent(\n                llm,\n                build_tools,\n                messages_modifier=_modify_messages,\n                checkpointer=AsyncPostgresSaver(pool),  # type:ignore[arg-type]\n            )\n\n            async for event in graph.astream_events(\n                {\"messages\": [(\"human\", search_params.question)]},\n                config={\"configurable\": {\"thread_id\": conversation_id, \"recursion_limit\": 20}},\n                stream_mode=\"values\",\n                version=\"v2\",\n            ):\nError Message and Stack Trace (if applicable)\nINSERT INTO checkpoints ( thread_id, checkpoint_ns, checkpoint_id, parent_checkpoint_id, checkpoint, metadata ) \nVALUES ( ? ) ON CONFLICT ( thread_id, checkpoint_ns, checkpoint_id ) DO \nUPDATE SET checkpoint = EXCLUDED.checkpoint, metadata = EXCLUDED.metadata\n\npsycopg.OperationalError: sending query and params failed: SSL error: bad length\n File \"/app/app/search/ai_models.py\", line 315, in chat\n    async for event in graph.astream_events(\n  File \"/app/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 1386, in astream_events\n    async for event in event_stream:\n  File \"/app/venv/lib/python3.12/site-packages/langchain_core/tracers/event_stream.py\", line 1012, in _astream_events_implementation_v2\n    await task\n  File \"/app/venv/lib/python3.12/site-packages/langchain_core/tracers/event_stream.py\", line 967, in consume_astream\n    async for _ in event_streamer.tap_output_aiter(run_id, stream):\n  File \"/app/venv/lib/python3.12/site-packages/langchain_core/tracers/event_stream.py\", line 203, in tap_output_aiter\n    async for chunk in output:\n  File \"/app/venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 1832, in astream\n    async with AsyncPregelLoop(\n               ^^^^^^^^^^^^^^^^\n  File \"/app/venv/lib/python3.12/site-packages/langgraph/pregel/loop.py\", line 1035, in __aexit__\n    return await asyncio.shield(\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/contextlib.py\", line 754, in __aexit__\n    raise exc_details[1]\n  File \"/usr/local/lib/python3.12/contextlib.py\", line 737, in __aexit__\n    cb_suppress = await cb(*exc_details)\n                  ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/venv/lib/python3.12/site-packages/langgraph/pregel/executor.py\", line 200, in __aexit__\n    raise exc\n  File \"/app/venv/lib/python3.12/site-packages/langgraph/pregel/loop.py\", line 957, in _checkpointer_put_after_previous\n    await cast(BaseCheckpointSaver, self.checkpointer).aput(\n  File \"/app/venv/lib/python3.12/site-packages/langgraph/checkpoint/postgres/aio.py\", line 270, in aput\n    await cur.execute(\n  File \"/app/venv/lib/python3.12/site-packages/ddtrace/contrib/dbapi_async.py\", line 136, in execute\n    return await self._trace_method(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/venv/lib/python3.12/site-packages/ddtrace/contrib/dbapi_async.py\", line 105, in _trace_method\n    return await method(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/venv/lib/python3.12/site-packages/psycopg/cursor_async.py\", line 97, in execute\n    raise ex.with_traceback(None)\npsycopg.OperationalError: sending query and params failed: SSL error: bad length\nSSL SYSCALL error: EOF detected\nDescription\nFaced the below issue with langgraph-checkpoint-postgres:\npsycopg.OperationalError: sending query and params failed: SSL error: bad length\nSSL SYSCALL error: EOF detected\n\nNOTE:\nI have tried with multiple langgraph-checkpoint-postgres i.e 2.0.11, 2.0.9, 2.0.13, 2.0.15\nSystem Info\nlangchain = \"0.3.11\"\nlangchain-community = \"0.3.11\"\nlangchain-experimental = \"0.3.3\"\nlangchain-openai = \"0.2.12\"\nlangchain-postgres = \"0.0.12\"\nlanggraph = \"0.2.58\"\npsycopg = { extras = [\"binary\"], version = \"3.2.3\" }\npsycopg-pool = \"3.2.3\"\nsqlalchemy = { version = \"2.0.36\", extras = [\"asyncio\"] }\nsqlmodel = \"0.0.22\"\nasyncpg = \"0.30.0\"\nlanggraph-checkpoint-postgres = \"2.0.15\"", "created_at": "2025-03-06", "closed_at": null, "labels": [], "State": "open", "Author": "shubhamnegikellton"}
{"issue_number": 3705, "issue_title": "Stream mode is not properly reflected to LangSmith trace", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport os\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n\nfrom langgraph.graph import START, END, StateGraph\nfrom langgraph.graph.message import add_messages\nfrom langgraph.graph.graph import CompiledGraph\nfrom langgraph.graph.state import CompiledStateGraph\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\n# Enable LangSmith\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\nos.environ[\"LANGSMITH_API_KEY\"] = \"<api-key>\"\n\n# Define a graph\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\n\ngraph_builder = StateGraph(State)\n\ntool = TavilySearchResults(max_results=2)\ntools = [tool]\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\nllm_with_tools = llm.bind_tools(tools)\n\n\ndef chatbot(state: State):\n    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n\n\ngraph_builder.add_node(\"chatbot\", chatbot)\ngraph_builder.add_node(\"tools\", ToolNode(tools=[tool]))\ngraph_builder.add_conditional_edges(\n    \"chatbot\",\n    tools_condition,\n)\ngraph_builder.add_edge(\"tools\", \"chatbot\")\ngraph_builder.add_edge(START, \"chatbot\")\n\ngraph = graph_builder.compile()\n\n# Invoke the graph with stream_model=\"updates\"\nevents = graph.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"What is LangGraph?\"}]},\n    stream_mode=\"updates\",\n)\nfor event in events:\n    print(event)\nError Message and Stack Trace (if applicable)\n\nDescription\nWhen invoking a graph with stream_mode=\"updates\", the graph properly returns state update only. However, LangSmith trace includes input state (message) and inconsistent with what's actually returned.\n\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.6.0: Thu Sep 12 23:36:12 PDT 2024; root:xnu-10063.141.1.701.1~1/RELEASE_ARM64_T6020\nPython Version:  3.12.9 (main, Feb  4 2025, 14:38:38) [Clang 16.0.0 (clang-1600.0.26.6)]\n\nPackage Information\n\nlangchain_core: 0.3.34\nlangchain: 0.3.12\nlangchain_community: 0.3.12\nlangsmith: 0.1.125\nlangchain_cohere: 0.3.3\nlangchain_experimental: 0.3.3\nlangchain_faiss: 0.1.1\nlangchain_openai: 0.2.9\nlangchain_text_splitters: 0.3.3\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.12\nasync-timeout: Installed. No version info available.\ncohere: 5.13.3\ndataclasses-json: 0.6.6\nhttpx: 0.27.0\nhttpx-sse: 0.4.0\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nnumpy: 1.26.4\nopenai: 1.55.3\norjson: 3.10.3\npackaging<25,>=23.2: Installed. No version info available.\npandas: 2.2.3\npydantic: 2.9.2\npydantic-settings: 2.5.2\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\nPyYAML: 6.0.2\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nSQLAlchemy: 2.0.36\ntabulate: 0.9.0\ntenacity: 8.5.0\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken: 0.8.0\ntyping-extensions>=4.7: Installed. No version info available.\n", "created_at": "2025-03-06", "closed_at": "2025-03-06", "labels": [], "State": "closed", "Author": "B-Step62"}
{"issue_number": 3675, "issue_title": "GraphInterrupt is not propagated to the caller, graph.invoke() in my case.", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.errors import GraphInterrupt\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.types import interrupt\nfrom pydantic import BaseModel\n\n\n# The overall state of the graph (this is the public state shared across nodes)\nclass OverallState(BaseModel):\n    a: str\n\n\ndef node(state: OverallState):\n    response = interrupt(\"What is the value?\")\n    return {\"a\": \"goodbye\"}\n\nmemory = MemorySaver()\n\n# Build the state graph\nbuilder = StateGraph(OverallState)\nbuilder.add_node(node)  # node_1 is the first node\nbuilder.add_edge(START, \"node\")  # Start the graph with node_1\nbuilder.add_edge(\"node\", END)  # End the graph after node_1\ngraph = builder.compile(checkpointer=memory)\n\n# Test the graph with a valid input\ntry:\n    config = {\"configurable\": {\"thread_id\": \"1\"}}\n    result = graph.invoke({\"a\": \"hello\"}, config)\nexcept GraphInterrupt as interrupt:\n    print(interrupt) # Not Printed\n\nprint(result)\nError Message and Stack Trace (if applicable)\nThere is no error, but to utilise the value which is passed to interrupt it must be caught by the caller.\nDescription\nThe interrupt() method is used for Human-In-The-Loop functionality. As per documentation, values passed to interrupt() can be used by the caller with GraphInterrupt exception. But somehow it is not propogated to the caller and thus value is of not use.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:16 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6000\nPython Version:  3.12.9 (main, Feb  4 2025, 14:38:38) [Clang 16.0.0 (clang-1600.0.26.6)]\n\nPackage Information\n\nlangchain_core: 0.3.40\nlangchain: 0.3.19\nlangchain_community: 0.3.18\nlangsmith: 0.3.11\nlangchain_openai: 0.3.7\nlangchain_text_splitters: 0.3.6\nlanggraph_sdk: 0.1.53\nlanggraph_supervisor: 0.0.2\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.35: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.37: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.39: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.19: Installed. No version info available.\nlanggraph>=0.2.71: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<2,>=1.26.4;: Installed. No version info available.\nnumpy<3,>=1.26.2;: Installed. No version info available.\nopenai<2.0.0,>=1.58.1: Installed. No version info available.\norjson: 3.10.15\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.11.0a2\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: 8.3.4\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: 13.9.4\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-03-04", "closed_at": "2025-03-05", "labels": [], "State": "closed", "Author": "nm177"}
{"issue_number": 3665, "issue_title": "`create_react_agent()` docs are now incorrect/out of date.", "issue_body": "Issue with current documentation:\nWith langgraph 0.3 and prebuilts being split out, the interface for create_react_agent() has changed.\n\nFirst argument has been renamed (from model to llm)\nThere is no longer a checkpointer named arg.\n\nThe documentation is therefore incorrect:\nfrom langgraph.prebuilt import create_react_agent\n\ngraph = create_react_agent(model, tools=tools, checkpointer=memory)\nHow to use a checkpointer with the new interface?  Also, lots of docs are now out of date.\nIdea or request for content:\nNo response", "created_at": "2025-03-03", "closed_at": "2025-03-03", "labels": [], "State": "closed", "Author": "EarningsCall"}
{"issue_number": 3662, "issue_title": "ImportError: cannot import name 'create_react_agent' from 'langgraph.prebuilt' (unknown location)", "issue_body": "Hi,\nI get the same issue as described in your answer 4 days ago:\n\ncan you create a new virtualenv and try again?\n\nOriginally posted by @vbarda in #3631\nI've commented the original issue, but basically: although correctly installed langgraph, create_react_agent is not found for some reason\nThanks", "created_at": "2025-03-03", "closed_at": "2025-03-03", "labels": [], "State": "closed", "Author": "Ibrahimous"}
{"issue_number": 3661, "issue_title": "State Propagation Fails in Multi-Node Graphs (v0.2.74)", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing import Dict, Annotated\nfrom langgraph.graph import StateGraph, END\n\nclass TestState(Dict):\n    input: Annotated[str, lambda x, y: x] = \"\"\n    step: Annotated[str, lambda x, y: x] = \"start\"\n    output: Annotated[str, lambda x, y: x] = \"\"\n\ndef start_node(state: TestState) -> Dict:\n    print(f\"Start Node: Received state {state}\")\n    return {\"step\": \"next\", \"output\": \"Step 1 done\"}\n\ndef next_node(state: TestState) -> Dict:\n    print(f\"Next Node: Received state {state}\")\n    return {\"step\": \"end\", \"output\": \"Step 2 done\"}\n\ngraph = StateGraph(TestState)\ngraph.add_node(\"start\", start_node)\ngraph.add_node(\"next\", next_node)\ngraph.add_edge(\"start\", \"next\")\ngraph.add_edge(\"next\", END)\ngraph.set_entry_point(\"start\")\ncompiled_graph = graph.compile()\n\nstate = TestState(input=\"test input\")\nprint(f\"Invoking with state: {state}\")\nupdates = compiled_graph.invoke(state.copy())\nprint(f\"Updates received: {updates}\")\nstate.update(updates)\nprint(f\"Final state: {state}\")\nError Message and Stack Trace (if applicable)\n\nDescription\nWhen using a multi-node graph with LangGraph v0.2.74, the initial state passed to invoke is reset to empty values before reaching the first node, and subsequent node updates are ignored. This issue does not occur with single-node graphs, where state propagation works as expected.\nObserve the output:\nInvoking with state: {'input': 'test input'}\nStart Node: Received state {'input': '', 'step': '', 'output': ''}\nNext Node: Received state {'input': '', 'step': '', 'output': ''}\nUpdates received: {'input': '', 'step': '', 'output': ''}\nFinal state: {'input': '', 'step': '', 'output': ''}\nExpected Behavior:\nstart_node should receive {'input': 'test input', 'step': 'start', 'output': ''}.\nnext_node should receive {'input': 'test input', 'step': 'next', 'output': 'Step 1 done'}.\nFinal state should be {'input': 'test input', 'step': 'end', 'output': 'Step 2 done'}.\nActual Behavior:\nState resets to empty values before reaching start_node.\nNode updates are ignored, and invoke returns an empty state.\nEnvironment:\nLangGraph Version: 0.2.74\nPython Version: 3.12\nOS:Ubuntu\nSystem Info\npython -m run app.py", "created_at": "2025-03-03", "closed_at": "2025-03-03", "labels": [], "State": "closed", "Author": "thiagu-r"}
{"issue_number": 3660, "issue_title": "\"Cancel Run\" endpoint returns 500 HTTP error if \"wait\" arg is not passed", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\ncurl 'http://127.0.0.1:2024/threads/{thread_id}/runs/{run_id}/cancel' \\\n  --request POST\nError Message and Stack Trace (if applicable)\n03/03/2025 10:37:15\nException in ASGI application\n\n  + Exception Group Traceback (most recent call last):\n  |   File \"/usr/local/lib/python3.11/site-packages/starlette/_utils.py\", line 76, in collapse_excgroups\n  |     yield\n  |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/base.py\", line 174, in __call__\n  |     async with anyio.create_task_group() as task_group:\n  |   File \"/usr/local/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 767, in __aexit__\n  |     raise BaseExceptionGroup(\n  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 409, in run_asgi\n    |     result = await app(  # type: ignore[func-returns-value]\n    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n    |     return await self.app(scope, receive, send)\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/applications.py\", line 112, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n    |     raise exc\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    |     await self.app(scope, receive, _send)\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n    |     await self.simple_response(scope, receive, send, request_headers=headers)\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n    |     await self.app(scope, receive, send)\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/base.py\", line 173, in __call__\n    |     with recv_stream, send_stream, collapse_excgroups():\n    |   File \"/usr/local/lib/python3.11/contextlib.py\", line 158, in __exit__\n    |     self.gen.throw(typ, value, traceback)\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    |     raise exc\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/base.py\", line 175, in __call__\n    |     response = await self.dispatch_func(request, call_next)\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/api/langgraph_license/middleware.py\", line 20, in dispatch\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/base.py\", line 153, in call_next\n    |     raise app_exc\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/base.py\", line 140, in coro\n    |     await self.app(scope, receive_or_disconnect, send_no_error)\n    |   File \"/api/langgraph_api/middleware/http_logger.py\", line 60, in __call__\n    |   File \"/api/langgraph_api/middleware/http_logger.py\", line 54, in __call__\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 714, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 734, in app\n    |     await route.handle(scope, receive, send)\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 460, in handle\n    |     await self.app(scope, receive, send)\n    |   File \"/api/langgraph_api/auth/middleware.py\", line 45, in __call__\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/authentication.py\", line 48, in __call__\n    |     await self.app(scope, receive, send)\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 714, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 734, in app\n    |     await route.handle(scope, receive, send)\n    |   File \"/api/langgraph_api/route.py\", line 125, in handle\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 288, in handle\n    |     await self.app(scope, receive, send)\n    |   File \"/api/langgraph_api/route.py\", line 38, in app\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/api/langgraph_api/route.py\", line 33, in app\n    |   File \"/usr/local/lib/python3.11/site-packages/langgraph_storage/retry.py\", line 34, in wrapper\n    |     return await func(*args, **kwargs)\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/api/langgraph_api/api/runs.py\", line 363, in cancel_run\n    | AttributeError: 'bool' object has no attribute 'lower'\n    +------------------------------------\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 409, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n    return await self.app(scope, receive, send)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/starlette/applications.py\", line 112, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n    raise exc\n  File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n    await self.simple_response(scope, receive, send, request_headers=headers)\n  File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n    await self.app(scope, receive, send)\n  File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/base.py\", line 173, in __call__\n    with recv_stream, send_stream, collapse_excgroups():\n  File \"/usr/local/lib/python3.11/contextlib.py\", line 158, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/usr/local/lib/python3.11/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    raise exc\n  File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/base.py\", line 175, in __call__\n    response = await self.dispatch_func(request, call_next)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/api/langgraph_license/middleware.py\", line 20, in dispatch\n  File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/base.py\", line 153, in call_next\n    raise app_exc\n  File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/base.py\", line 140, in coro\n    await self.app(scope, receive_or_disconnect, send_no_error)\n  File \"/api/langgraph_api/middleware/http_logger.py\", line 60, in __call__\n  File \"/api/langgraph_api/middleware/http_logger.py\", line 54, in __call__\n  File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 714, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 734, in app\n    await route.handle(scope, receive, send)\n  File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 460, in handle\n    await self.app(scope, receive, send)\n  File \"/api/langgraph_api/auth/middleware.py\", line 45, in __call__\n  File \"/usr/local/lib/python3.11/site-packages/starlette/middleware/authentication.py\", line 48, in __call__\n    await self.app(scope, receive, send)\n  File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 714, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 734, in app\n    await route.handle(scope, receive, send)\n  File \"/api/langgraph_api/route.py\", line 125, in handle\n  File \"/usr/local/lib/python3.11/site-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/api/langgraph_api/route.py\", line 38, in app\n  File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/api/langgraph_api/route.py\", line 33, in app\n  File \"/usr/local/lib/python3.11/site-packages/langgraph_storage/retry.py\", line 34, in wrapper\n    return await func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/api/langgraph_api/api/runs.py\", line 363, in cancel_run\nAttributeError: 'bool' object has no attribute 'lower'\nDescription\nThe LangGraph server raises an exception when we call the \"Cancel Run\" endpoint without passing the \"wait\" argument.\nThis should work, since this argument is not flagged as required in the documentation.\nThis was reproduced both with a locally running LangGraph Server and a LangGraph Cloud hosted agent.\nSystem info below show langgraph version 0.2.74 but this was also reproduced after upgrading to 0.3.2\nSystem Info\nThe currently activated Python version 3.10.13 is not supported by the project (^3.11.11).\nTrying to find and use a compatible version.\nUsing python3 (3.11.11)\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:16 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6000\nPython Version:  3.11.11 (main, Dec  5 2024, 17:56:59) [Clang 16.0.0 (clang-1600.0.26.4)]\n\nPackage Information\n\nlangchain_core: 0.3.37\nlangchain: 0.3.19\nlangchain_community: 0.3.18\nlangsmith: 0.2.11\nlangchain_anthropic: 0.2.4\nlangchain_fireworks: 0.2.7\nlangchain_milvus: 0.1.8\nlangchain_openai: 0.2.14\nlangchain_text_splitters: 0.3.6\nlangchainhub: 0.1.21\nlanggraph_api: 0.0.26\nlanggraph_cli: 0.1.73\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.53\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.12\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nanthropic: 0.46.0\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nclick: 8.1.8\ncryptography: 43.0.3\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\ndefusedxml: 0.7.1\nfireworks-ai: 0.15.12\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\njsonschema-rs: 0.25.1\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.35: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.37: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.19: Installed. No version info available.\nlanggraph: 0.2.74\nlanggraph-checkpoint: 2.0.16\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<2,>=1.26.4;: Installed. No version info available.\nnumpy<3,>=1.26.2;: Installed. No version info available.\nopenai: 1.63.2\norjson: 3.10.15\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npyjwt: 2.10.1\npymilvus: 2.5.4\npython-dotenv: 1.0.1\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nSQLAlchemy<3,>=1.4: Installed. No version info available.\nsse-starlette: 2.1.3\nstarlette: 0.45.3\nstructlog: 24.4.0\ntenacity: 9.0.0\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken: 0.9.0\ntypes-requests: 2.32.0.20241016\ntyping-extensions>=4.7: Installed. No version info available.\nuvicorn: 0.34.0\nwatchfiles: 1.0.4\nzstandard: Installed. No version info available.\n", "created_at": "2025-03-03", "closed_at": "2025-03-03", "labels": [], "State": "closed", "Author": "mobiware"}
{"issue_number": 3657, "issue_title": "Branch Conditions Limited to Preceding Node's Input Schema Instead of Full Graph State", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.graph import StateGraph, START, END\nfrom typing import TypedDict, Literal\n\nclass OverallState(TypedDict):\n    property_in_input: str \n    property_not_in_input: str \n    property_control: str\n\nclass InputState(TypedDict):\n    property_in_input: str \n\ndef node1(input: OverallState):\n    print(\"node1 overall state\" , input)\n    return {\n        \"property_control\": \"route_to_node2\",\n    }\n\ndef intermediate(input: InputState):\n    print(\"intermediate node with input state\" , input)\n    return {\n        \"property_control\": \"route_to_end\",\n    }\n    \ndef node2(input: OverallState):\n    print(\"node2 overall state\" , input)\n    return {\n        \"property_control\": \"completed\",\n    }\n\ndef router(state: OverallState) -> Literal[\"node2\", \"END\"]:\n    print(\"Router function state:\", state)\n    if state[\"property_control\"] == \"route_to_node2\":\n        return \"node2\"\n    else:\n        return END\n\ngraph_builder = StateGraph(OverallState)\ngraph_builder.add_node(\"node1\", node1)\ngraph_builder.add_node(\"intermediate\", intermediate, input=InputState)\ngraph_builder.add_node(\"node2\", node2)\n\ngraph_builder.add_edge(START, \"node1\")\ngraph_builder.add_edge(\"node1\", \"intermediate\")\ngraph_builder.add_conditional_edges(\"intermediate\", router, {\n    \"node2\": \"node2\",\n    END: END\n})\ngraph_builder.add_edge(\"node2\", END)\n\ngraph = graph_builder.compile()\nresult = graph.invoke({\n    \"property_in_input\": \"value\",\n    \"property_not_in_input\": \"value\",\n    \"property_control\": \"initial_value\"\n})\nError Message and Stack Trace (if applicable)\n\nDescription\nDescription\nWhen using a branch condition in LangGraph, the branch condition function only has access to fields defined in the source node's input schema, not the full graph schema. This breaks the expected behavior where branch conditions should have access to the complete state.\nReproduction Example\nfrom langgraph.graph import StateGraph, START, END\nfrom typing import TypedDict, Literal\n\nclass OverallState(TypedDict):\n    property_in_input: str \n    property_not_in_input: str \n    property_control: str\n\nclass InputState(TypedDict):\n    property_in_input: str \n\ndef node1(input: OverallState):\n    print(\"node1 overall state\" , input)\n    return {\n        \"property_control\": \"route_to_node2\",\n    }\n\ndef intermediate(input: InputState):\n    print(\"intermediate node with input state\" , input)\n    return {\n        \"property_control\": \"route_to_end\",\n    }\n    \ndef node2(input: OverallState):\n    print(\"node2 overall state\" , input)\n    return {\n        \"property_control\": \"completed\",\n    }\n\ndef router(state: OverallState) -> Literal[\"node2\", \"END\"]:\n    print(\"Router function state:\", state)\n    if state[\"property_control\"] == \"route_to_node2\":\n        return \"node2\"\n    else:\n        return END\n\ngraph_builder = StateGraph(OverallState)\ngraph_builder.add_node(\"node1\", node1)\ngraph_builder.add_node(\"intermediate\", intermediate, input=InputState)\ngraph_builder.add_node(\"node2\", node2)\n\ngraph_builder.add_edge(START, \"node1\")\ngraph_builder.add_edge(\"node1\", \"intermediate\")\ngraph_builder.add_conditional_edges(\"intermediate\", router, {\n    \"node2\": \"node2\",\n    END: END\n})\ngraph_builder.add_edge(\"node2\", END)\n\ngraph = graph_builder.compile()\nresult = graph.invoke({\n    \"property_in_input\": \"value\",\n    \"property_not_in_input\": \"value\",\n    \"property_control\": \"initial_value\"\n}) \nCurrent Behavior\nThe router function only receives properties that are defined in the source node's input schema, plus any updates from that node. In this example, property_not_in_input is lost when the router is called, despite the router function expecting the complete OverallState.\nOutput:\nnode1 overall state {'property_in_input': 'value', 'property_not_in_input': 'value', 'property_control': 'initial_value'}\nintermediate node with input state {'property_in_input': 'value'}\nRouter function state: {'property_control': 'route_to_end', 'property_in_input': 'value'}\nExpected Behavior\nThe router function should receive the complete graph state (all properties in OverallState), regardless of what fields the source node uses or updates.\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #26~22.04.1-Ubuntu SMP Thu Jul 11 22:33:04 UTC 2024\nPython Version:  3.12.1 (main, Dec 12 2024, 22:30:56) [GCC 9.4.0]\n\nPackage Information\n\nlangchain_core: 0.3.40\nlangchain: 0.3.19\nlangsmith: 0.1.147\nlangchain_google_vertexai: 2.0.10\nlangchain_openai: 0.2.2\nlangchain_pinecone: 0.2.0\nlangchain_text_splitters: 0.3.6\nlanggraph_sdk: 0.1.53\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.9.5\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nanthropic[vertexai]: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\ngoogle-cloud-aiplatform: 1.81.0\ngoogle-cloud-storage: 2.19.0\nhttpx: 0.27.2\nhttpx-sse: 0.4.0\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.35: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy: 1.26.4\nnumpy<2,>=1.26.4;: Installed. No version info available.\nnumpy<3,>=1.26.2;: Installed. No version info available.\nopenai: 1.64.0\norjson: 3.10.15\npackaging<25,>=23.2: Installed. No version info available.\npinecone-client: 5.0.1\npydantic: 2.10.6\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken: 0.9.0\ntyping-extensions>=4.7: Installed. No version info available.\n", "created_at": "2025-03-03", "closed_at": "2025-03-11", "labels": ["bug"], "State": "closed", "Author": "AI091"}
{"issue_number": 3656, "issue_title": "ImportError: cannot import name 'create_react_agent' from 'langgraph.prebuilt' (unknown location)", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langchain_openai import ChatOpenAI\n\nfrom langgraph.agents import create_react_agent\n\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph_swarm import create_handoff_tool, create_swarm\n#from langchain.agents import AgentExecutor, create_react_agent\n\nmodel = ChatOpenAI(model=\"gpt-4o\")\n\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\nalice = create_react_agent(\n    model,\n    [add, create_handoff_tool(agent_name=\"Bob\")],\n    prompt=\"You are Alice, an addition expert.\",\n    name=\"Alice\",\n)\nError Message and Stack Trace (if applicable)\n\u279c  swarm python -m venv venv\n\u279c  swarm source venv/bin/activate\n(venv) \u279c  swarm pip install langgraph-swarm\n(venv) \u279c  pip install langchain-openai\n(venv) \u279c  swarm pip show langgraph\nName: langgraph\nVersion: 0.3.2\nSummary: Building stateful, multi-actor applications with LLMs\nHome-page: https://www.github.com/langchain-ai/langgraph\nAuthor:\nAuthor-email:\nLicense: MIT\nLocation: /Users/binu.b.varghese/.pyenv/versions/3.12.8/lib/python3.12/site-packages\nRequires: langchain-core, langgraph-checkpoint, langgraph-prebuilt, langgraph-sdk\nRequired-by: langgraph-swarm\n(venv) \u279c  swarm pip show langgraph-swarm\nName: langgraph-swarm\nVersion: 0.0.4\nSummary: An implementation of a multi-agent swarm using LangGraph\nHome-page:\nAuthor:\nAuthor-email: Vadym Barda <19161700+vbarda@users.noreply.github.com >\nLicense:\nLocation: /Users/binu.b.varghese/.pyenv/versions/3.12.8/lib/python3.12/site-packages\nRequires: langchain-core, langgraph\nRequired-by:\n(venv) \u279c  swarm python main.py\nTraceback (most recent call last):\n  File \"/Users/binu.b.varghese/source/agent/swarm/main.py\", line 3, in <module>\n    from langgraph.agents import create_react_agent\nModuleNotFoundError: No module named 'langgraph.agents'\nDescription\nI created a new venv. Installed the packages as above. I am getting import error Traceback (most recent call last):\nFile \"/Users/binu.b.varghese/source/agent/swarm/main.py\", line 3, in \nfrom langgraph.agents import create_react_agent\nModuleNotFoundError: No module named 'langgraph.agents'\nSystem Info\n(venv) \u279c  swarm python -m langchain_core.sys_info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:22:00 PST 2025; root:xnu-11215.81.4~3/RELEASE_X86_64\nPython Version:  3.12.8 (main, Dec 17 2024, 23:59:18) [Clang 16.0.0 (clang-1600.0.26.4)]\n\nPackage Information\n\nlangchain_core: 0.3.40\nlangchain: 0.3.19\nlangchain_community: 0.3.13\nlangsmith: 0.3.8\nlangchain_openai: 0.3.7\nlangchain_text_splitters: 0.3.6\nlanggraph_sdk: 0.1.48\nlanggraph_swarm: 0.0.4\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.11\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\ndataclasses-json: 0.6.7\nhttpx: 0.28.1\nhttpx-sse: 0.4.0\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<0.4.0,>=0.3.40: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.35: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.39: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlanggraph<0.4.0,>=0.3.1: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy: 2.2.0\nnumpy<2,>=1.26.4;: Installed. No version info available.\nnumpy<3,>=1.26.2;: Installed. No version info available.\nopenai<2.0.0,>=1.58.1: Installed. No version info available.\norjson: 3.10.12\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic-settings: 2.7.0\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: 8.3.4\nPyYAML: 6.0.2\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: 13.9.4\nSQLAlchemy: 2.0.36\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity: 9.0.0\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-03-02", "closed_at": "2025-03-03", "labels": [], "State": "closed", "Author": "callbinuvarghese"}
{"issue_number": 3648, "issue_title": "Node with subgraph streams current state in `updates` mode (and not the delta update)", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom operator import add\nfrom platform import node\nfrom typing import Annotated, TypedDict\n\nfrom langgraph.graph import END, START, StateGraph\nfrom langgraph.graph.state import CompiledStateGraph\n\n\nclass InputState(TypedDict):\n    var1: str\n    var2: str\n    list: Annotated[list[str], add]\n\n\nclass OutputState(TypedDict):\n    list: Annotated[list[str], add]\n\n\nclass OverallState(InputState, OutputState):\n    out1: str\n    out2: str\n\n\ndef create_subgraph(name: str, subgraph: CompiledStateGraph | None) -> CompiledStateGraph:\n    builder = StateGraph(OverallState, input=InputState, output=OutputState)\n\n    def node1(state):\n        return {\n            \"out1\": state[\"var1\"],\n            \"var2\": state[\"var1\"] + state[\"var2\"],\n            \"list\": [f\"{name}:node1\"],\n        }\n\n    def node2(state):\n        return {\n            \"out1\": state[\"var1\"],\n            \"out2\": state[\"var2\"],\n            \"list\": [f\"{name}:node2\"],\n        }\n\n    builder.add_node(\"node1\", subgraph or node1)\n    builder.add_node(node2)\n\n    builder.add_edge(START, \"node1\")\n    builder.add_edge(\"node1\", \"node2\")\n    builder.add_edge(\"node2\", END)\n\n    return builder.compile(name=name)\n\n\ndef main():\n\n    subgraph1 = create_subgraph(\"subgraph1\", None)\n    subgraph2 = create_subgraph(\"subgraph2\", subgraph1)\n    graph = create_subgraph(\"graph\", subgraph2)\n\n    for chunk in graph.stream(\n        {\"var1\": \"var1\", \"var2\": \"var2\", \"list\": [\"initial\"]},\n        stream_mode=[\"updates\", \"values\"],\n        subgraphs=True,\n    ):\n        print(chunk)\n\n\nmain()\nError Message and Stack Trace (if applicable)\n# Stream output\n((), 'values', {'var1': 'var1', 'var2': 'var2', 'list': ['initial']})\n(('node1:824093e1-7fb4-e5be-2468-7b93bf79df79',), 'values', {'list': ['initial']})\n(('node1:824093e1-7fb4-e5be-2468-7b93bf79df79', 'node1:fc669ad4-101d-3251-9956-f1510a141c2b'), 'values', {'list': ['initial']})\n(('node1:824093e1-7fb4-e5be-2468-7b93bf79df79', 'node1:fc669ad4-101d-3251-9956-f1510a141c2b'), 'updates', {'node1': {'list': ['subgraph1:node1']}})\n(('node1:824093e1-7fb4-e5be-2468-7b93bf79df79', 'node1:fc669ad4-101d-3251-9956-f1510a141c2b'), 'values', {'list': ['initial', 'subgraph1:node1']})\n(('node1:824093e1-7fb4-e5be-2468-7b93bf79df79', 'node1:fc669ad4-101d-3251-9956-f1510a141c2b'), 'updates', {'node2': {'list': ['subgraph1:node2']}})\n(('node1:824093e1-7fb4-e5be-2468-7b93bf79df79', 'node1:fc669ad4-101d-3251-9956-f1510a141c2b'), 'values', {'list': ['initial', 'subgraph1:node1', 'subgraph1:node2']})\n(('node1:824093e1-7fb4-e5be-2468-7b93bf79df79',), 'updates', {'node1': {'list': ['initial', 'subgraph1:node1', 'subgraph1:node2']}})\n(('node1:824093e1-7fb4-e5be-2468-7b93bf79df79',), 'values', {'list': ['initial', 'initial', 'subgraph1:node1', 'subgraph1:node2']})\n(('node1:824093e1-7fb4-e5be-2468-7b93bf79df79',), 'updates', {'node2': {'list': ['subgraph2:node2']}})\n(('node1:824093e1-7fb4-e5be-2468-7b93bf79df79',), 'values', {'list': ['initial', 'initial', 'subgraph1:node1', 'subgraph1:node2', 'subgraph2:node2']})\n((), 'updates', {'node1': {'list': ['initial', 'initial', 'subgraph1:node1', 'subgraph1:node2', 'subgraph2:node2']}})\n((), 'values', {'var1': 'var1', 'var2': 'var2', 'list': ['initial', 'initial', 'initial', 'subgraph1:node1', 'subgraph1:node2', 'subgraph2:node2']})\n((), 'updates', {'node2': {'out1': 'var1', 'out2': 'var2', 'list': ['graph:node2']}})\n((), 'values', {'var1': 'var1', 'var2': 'var2', 'list': ['initial', 'initial', 'initial', 'subgraph1:node1', 'subgraph1:node2', 'subgraph2:node2', 'graph:node2'], 'out1': 'var1', 'out2': 'var2'})\nDescription\nA node with a subgraph streams the full state in \"updates\" mode, so in channels with reducer (like a list operator.add) the global state gets corrupted with duplications.\nExample output\nYou can see that the \"initial\" value of the list channel gets triplicated in this nested graph\nexpected:  'list': ['initial', 'subgraph1:node1', 'subgraph1:node2', 'subgraph2:node2', 'graph:node2']\nreceived:  'list': ['initial', 'initial', 'initial', 'subgraph1:node1', 'subgraph1:node2', 'subgraph2:node2', 'graph:node2']\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP Tue Nov 5 00:21:55 UTC 2024\nPython Version:  3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0]\n\nPackage Information\n\nlangchain_core: 0.3.35\nlangchain: 0.3.18\nlangchain_community: 0.3.17\nlangsmith: 0.3.6\nlangchain_openai: 0.3.6\nlangchain_text_splitters: 0.3.6\nlanggraph_sdk: 0.1.51\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.35: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.18: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<2,>=1.26.4;: Installed. No version info available.\nnumpy<3,>=1.26.2;: Installed. No version info available.\nopenai<2.0.0,>=1.58.1: Installed. No version info available.\norjson: 3.10.15\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: 8.3.3\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: 13.9.4\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-02-28", "closed_at": "2025-03-03", "labels": [], "State": "closed", "Author": "ianchi"}
{"issue_number": 3645, "issue_title": "DOC: example usage for SyncRunsClient.list shows a SyncRunsClient.delete example", "issue_body": "Issue with current documentation:\nThe Example Usage for SyncRunsClient.list in the documentation shows an example of using the SyncRunsClient.delete() method\n\nIdea or request for content:\nNo response", "created_at": "2025-02-28", "closed_at": "2025-02-28", "labels": [], "State": "closed", "Author": "mobiware"}
{"issue_number": 3644, "issue_title": "SyncRunsClient.cancel raises orjson.JSONDecodeError", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom orjson import JSONDecodeError\nfrom langgraph_sdk import get_sync_client\n\nclient = get_sync_client()\n\nthread_id = \"\"  # Use actual thread id\nrun_id = \"\"     # Use actual run id\n\ntry:\n    client.runs.cancel(\n        thread_id=thread_id,\n        run_id=run_id,\n    )\nexcept JSONDecodeError as e:\n    print(e)\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\n  ... (removed application specific frames)\n    client.runs.cancel(\n  File \"/usr/local/lib/python3.10/site-packages/langgraph_sdk/client.py\", line 3944, in cancel\n    return self.http.post(\n  File \"/usr/local/lib/python3.10/site-packages/langgraph_sdk/client.py\", line 2423, in post\n    return decode_json(r)\n  File \"/usr/local/lib/python3.10/site-packages/langgraph_sdk/client.py\", line 2520, in decode_json\n    return orjson.loads(body if body else None)\norjson.JSONDecodeError: Input must be bytes, bytearray, memoryview, or str: line 1 column 1 (char 0)\nDescription\nWhen calling SyncRunsClient.cancel, we get an orjson.JSONDecodeError exception.\nNo error is seen in the LangGraph server and the run is successfully canceled.\nSystem Info\nThis is a bug in langgraph-sdk, and langchain_core is not installed in the application that uses the LangGraph SDK", "created_at": "2025-02-28", "closed_at": "2025-03-04", "labels": [], "State": "closed", "Author": "mobiware"}
{"issue_number": 3637, "issue_title": "No module named 'langgraph.prebuilt'", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n# \u5b9a\u4e49\u5de5\u5177\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n\n# \u521d\u59cb\u5316\u641c\u7d22\u5de5\u5177\nsearch = TavilySearchResults(max_results=2)\n\n# \u5c06\u5de5\u5177\u653e\u5165\u5217\u8868\ntools = [search]\n\n# \u5bfc\u5165 OllamaLLM\nfrom langchain_ollama import OllamaLLM\n\n# \u521d\u59cb\u5316\u6a21\u578b\nmodel = OllamaLLM(model=\"deepseek-r1:14b\")\n\n# \u5bfc\u5165 LangGraph \u76f8\u5173\u6a21\u5757\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_core.messages import HumanMessage\n\n# \u521b\u5efa LangGraph \u4ee3\u7406\nagent = create_react_agent(model, tools)\n\n# \u6d4b\u8bd5\u4ee3\u7406\nresponse = agent.invoke({\"input\": \"What is the weather in San Francisco?\"})\nprint(response)\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[10], line 17\n     14 model = OllamaLLM(model=\"deepseek-r1:14b\")\n     16 # \u5bfc\u5165 LangGraph \u76f8\u5173\u6a21\u5757\n---> 17 from langgraph.prebuilt import create_react_agent\n     18 from langchain_core.messages import HumanMessage\n     20 # \u521b\u5efa LangGraph \u4ee3\u7406\n\nModuleNotFoundError: No module named 'langgraph.prebuilt'\nDescription\nThere are no files named 'langgraphit.py' in the project, and the code in the example on the official website will have the same error\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:06 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T8103\nPython Version:  3.13.2 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 12:55:35) [Clang 14.0.6 ]\n\nPackage Information\n\nlangchain_core: 0.3.40\nlangchain: 0.3.19\nlangchain_community: 0.3.18\nlangsmith: 0.3.11\nlangchain_agent: Installed. No version info available.\nlangchain_anthropic: 0.3.8\nlangchain_chatbot: Installed. No version info available.\nlangchain_chroma: 0.2.2\nlangchain_ollama: 0.2.3\nlangchain_openai: 0.3.7\nlangchain_simple_llm: Installed. No version info available.\nlangchain_text_splitters: 0.3.6\nlangchain_vector: Installed. No version info available.\nlanggraph_sdk: 0.1.53\nlangserve: 0.3.1\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nanthropic<1,>=0.47.0: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nchromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0: Installed. No version info available.\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nfastapi: 0.115.8\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.35: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.37: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.39: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.19: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<2,>=1.26.4;: Installed. No version info available.\nnumpy<2.0.0,>=1.22.4;: Installed. No version info available.\nnumpy<2.0.0,>=1.26.2;: Installed. No version info available.\nnumpy<3,>=1.26.2;: Installed. No version info available.\nollama: 0.4.7\nopenai<2.0.0,>=1.58.1: Installed. No version info available.\norjson: 3.10.15\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: Installed. No version info available.\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: 13.9.4\nSQLAlchemy<3,>=1.4: Installed. No version info available.\nsse-starlette: 2.2.1\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-02-28", "closed_at": "2025-02-28", "labels": [], "State": "closed", "Author": "qianYuanJ"}
{"issue_number": 3631, "issue_title": "Langgraph Swam + create_react_agent", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport datetime\nfrom collections import defaultdict\nfrom typing import Callable\n\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain_aws import ChatBedrock\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph_swarm import create_handoff_tool, create_swarm\nmodel = ChatBedrock(\n    region_name=\"us-east-1\",\n    model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n    provider=\"anthropic\",\n    model_kwargs={\"max_tokens\": 4000}\n)\n# Mock data for tools\nRESERVATIONS = defaultdict(lambda: {\"flight_info\": {}, \"hotel_info\": {}})\n\nTOMORROW = (datetime.date.today() + datetime.timedelta(days=1)).isoformat()\nFLIGHTS = [\n    {\n        \"departure_airport\": \"BOS\",\n        \"arrival_airport\": \"JFK\",\n        \"airline\": \"Jet Blue\",\n        \"date\": TOMORROW,\n        \"id\": \"1\",\n    }\n]\nHOTELS = [\n    {\n        \"location\": \"New York\",\n        \"name\": \"McKittrick Hotel\",\n        \"neighborhood\": \"Chelsea\",\n        \"id\": \"1\",\n    }\n]\n\n\n# Flight tools\ndef search_flights(\n    departure_airport: str,\n    arrival_airport: str,\n    date: str,\n) -> list[dict]:\n    \"\"\"Search flights.\n\n    Args:\n        departure_airport: 3-letter airport code for the departure airport. If unsure, use the biggest airport in the area\n        arrival_airport: 3-letter airport code for the arrival airport. If unsure, use the biggest airport in the area\n        date: YYYY-MM-DD date\n    \"\"\"\n    # return all flights for simplicity\n    return FLIGHTS\n\n\ndef book_flight(\n    flight_id: str,\n    config: RunnableConfig,\n) -> str:\n    \"\"\"Book a flight.\"\"\"\n    user_id = config[\"configurable\"].get(\"user_id\")\n    flight = [flight for flight in FLIGHTS if flight[\"id\"] == flight_id][0]\n    RESERVATIONS[user_id][\"flight_info\"] = flight\n    return \"Successfully booked flight\"\n\n\n# Hotel tools\ndef search_hotels(location: str) -> list[dict]:\n    \"\"\"Search hotels.\n\n    Args:\n        location: offical, legal city name (proper noun)\n    \"\"\"\n    # return all hotels for simplicity\n    return HOTELS\n\n\ndef book_hotel(\n    hotel_id: str,\n    config: RunnableConfig,\n) -> str:\n    \"\"\"Book a hotel\"\"\"\n    user_id = config[\"configurable\"].get(\"user_id\")\n    hotel = [hotel for hotel in HOTELS if hotel[\"id\"] == hotel_id][0]\n    RESERVATIONS[user_id][\"hotel_info\"] = hotel\n    return \"Successfully booked hotel\"\n\n\n# Define handoff tools\ntransfer_to_hotel_assistant = create_handoff_tool(\n    agent_name=\"hotel_assistant\",\n    description=\"Transfer user to the hotel-booking assistant that can search for and book hotels.\",\n)\ntransfer_to_flight_assistant = create_handoff_tool(\n    agent_name=\"flight_assistant\",\n    description=\"Transfer user to the flight-booking assistant that can search for and book flights.\",\n)\n\n\n# Define agent prompt\ndef make_prompt(base_system_prompt: str) -> Callable[[dict, RunnableConfig], list]:\n    def prompt(state: dict, config: RunnableConfig) -> list:\n        user_id = config[\"configurable\"].get(\"user_id\")\n        current_reservation = RESERVATIONS[user_id]\n        system_prompt = (\n            base_system_prompt\n            + f\"\\n\\nUser's active reservation: {current_reservation}\"\n            + f\"Today is: {datetime.datetime.now()}\"\n        )\n        return [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n\n    return prompt\n\n\n# Define agents\nflight_assistant = create_react_agent(\n    model,\n    [search_flights, book_flight, transfer_to_hotel_assistant],\n    prompt=make_prompt(\"You are a flight booking assistant\"),\n    name=\"flight_assistant\",\n)\n\nhotel_assistant = create_react_agent(\n    model,\n    [search_hotels, book_hotel, transfer_to_flight_assistant],\n    prompt=make_prompt(\"You are a hotel booking assistant\"),\n    name=\"hotel_assistant\",\n)\n\n# Compile and run!\ncheckpointer = InMemorySaver()\nbuilder = create_swarm([flight_assistant, hotel_assistant], default_active_agent=\"flight_assistant\")\n\n# Important: compile the swarm with a checkpointer to remember\n# previous interactions and last active agent\napp = builder.compile(checkpointer=checkpointer)\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\nresult = app.invoke({\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"i am looking for a flight from boston to ny tomorrow\"\n        }\n    ],\n}, config)\nprint(result)\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\n  File \"C:\\Users\\johpisca1\\OneDrive - Publicis Groupe\\IntelligentEngine\\intelligence_engine\\swarm.py\", line 8, in <module>\n    from langgraph.prebuilt import create_react_agent\nImportError: cannot import name 'create_react_agent' from 'langgraph.prebuilt' (unknown location)\nDescription\nCannot run swarm with langgraph 0.3.1. Having issues importing create_react_agent.\nTried to import as:\n\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.prebuilt.chat_agent_executor import create_react_agent\n\nSystem Info\nRan code locally\n$ pip show langgraph\nName: langgraph\nVersion: 0.3.1\nSummary: Building stateful, multi-actor applications with LLMs\nHome-page: https://www.github.com/langchain-ai/langgraph\nAuthor:\nAuthor-email:\nLicense: MIT\nLocation: C:\\Users\\johpisca1\\OneDrive - Publicis Groupe\\IntelligentEngine\\venv\\Lib\\site-packages\nRequires: langchain-core, langgraph-checkpoint, langgraph-prebuilt, langgraph-sdk\nRequired-by: langgraph-swarm\n$ pip show langgraph-swarm\nName: langgraph-swarm\nVersion: 0.0.3\nSummary: An implementation of a multi-agent swarm using LangGraph\nHome-page:\nAuthor:\nAuthor-email: Vadym Barda <19161700+vbarda@users.noreply.github.com >\nLicense:\nLocation: C:\\Users\\johpisca1\\OneDrive - Publicis Groupe\\IntelligentEngine\\venv\\Lib\\site-packages\nRequires: langgraph, langgraph-prebuilt\nRequired-by:", "created_at": "2025-02-27", "closed_at": "2025-02-27", "labels": ["question"], "State": "closed", "Author": "johnpiscani"}
{"issue_number": 3630, "issue_title": "Langgraph 0.3 causing issues with Langgraph cli & api", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\npoetry add \"langgraph-cli[inmem]\nError Message and Stack Trace (if applicable)\n[tool.poetry.dependencies]\npython = \">=3.11.0,<4.0\"\nlanggraph = \">=0.3.0,<0.4.0\"\nDescription\nwhen running poetry add \"langgraph-cli[inmem]\"\npoetry add \"langgraph-cli[inmem]\"\nUsing version ^0.1.73 for langgraph-cli\n\nUpdating dependencies\nResolving dependencies... (0.3s)\n\nBecause no versions of langgraph-cli match >0.1.73,<0.2.0\n and langgraph-cli[inmem] (0.1.73) depends on langgraph-api (>=0.0.26,<0.1.0), langgraph-cli[inmem] (>=0.1.73,<0.2.0) requires langgraph-api (>=0.0.26,<0.1.0).\nBecause langgraph-api (0.0.26) depends on langgraph (>=0.2.56,<0.3.0)\n and no versions of langgraph-api match >0.0.26,<0.1.0, langgraph-api (>=0.0.26,<0.1.0) requires langgraph (>=0.2.56,<0.3.0).\nThus, langgraph-cli[inmem] (>=0.1.73,<0.2.0) requires langgraph (>=0.2.56,<0.3.0).\nSo, because maestro-agent depends on both langgraph (>=0.3.0,<0.4.0) and langgraph-cli[inmem] (^0.1.73), version solving failed.\n\nand when trying to add just langgraph-api\npoetry add langgraph-api\nUsing version ^0.0.26 for langgraph-api\n\nUpdating dependencies\nResolving dependencies... (0.0s)\n\nBecause no versions of langgraph-api match >0.0.26,<0.0.27\n and langgraph-api (0.0.26) depends on langgraph (>=0.2.56,<0.3.0), langgraph-api (>=0.0.26,<0.0.27) requires langgraph (>=0.2.56,<0.3.0).\nSo, because maestro-agent depends on both langgraph (>=0.3.0,<0.4.0) and langgraph-api (^0.0.26), version solving failed.\n\nSystem Info\nMacOS", "created_at": "2025-02-27", "closed_at": "2025-02-27", "labels": [], "State": "closed", "Author": "pixelcatgg"}
{"issue_number": 3619, "issue_title": "Adding unwanted conditional edge in the graph workflow", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\ndef routing_function(state:State):\n    if state[\"execution_result\"]==\"Success\":\n        return END \n    else:\n        return \"handel_error\"\n    \n\ngraph_builder.add_edge(START,\"generate_component\")\n\ngraph_builder.add_edge(\"generate_component\",\"execute_component\")    \ngraph_builder.add_conditional_edges(\"execute_component\",routing_function)\ngraph_builder.add_edge(\"handel_error\",\"execute_component\")\n\n\ncomponent_details=\"A button component with a click event that logs 'Button Clicked!' to the console.\"\ngraph=graph_builder.compile()\nstate=State()\nError Message and Stack Trace (if applicable)\nI am getting issue with flow graph image which is generated after the graph is compiled\nDescription\n\nI am not adding any conditional edge between generate_component and execute_component but I am getting an conditional edge.\nSystem Info\naiohappyeyeballs==2.4.6\naiohttp==3.11.13\naiosignal==1.3.2\nannotated-types==0.7.0\nanyio==4.8.0\nattrs==25.1.0\ncertifi==2025.1.31\ncharset-normalizer==3.4.1\nclick==8.1.8\ncolorama==0.4.6\ndistro==1.9.0\nfastapi==0.115.8\nfrozenlist==1.5.0\ngreenlet==3.1.1\ngrpclib==0.4.7\nh11==0.14.0\nh2==4.2.0\nhpack==4.1.0\nhttpcore==1.0.7\nhttpx==0.28.1\nhyperframe==6.1.0\nidna==3.10\njiter==0.8.2\njsonpatch==1.33\njsonpointer==3.0.0\nlangchain==0.3.19\nlangchain-core==0.3.40\nlangchain-openai==0.3.7\nlangchain-text-splitters==0.3.6\nlanggraph==0.3.0\nlanggraph-checkpoint==2.0.16\nlanggraph-sdk==0.1.53\nlangsmith==0.3.11\nmarkdown-it-py==3.0.0\nmdurl==0.1.2\nmodal==0.73.72\nmsgpack==1.1.0\nmultidict==6.1.0\nnumpy==2.2.3\nopenai==1.64.0\norjson==3.10.15\npackaging==24.2\npillow==11.1.0\npropcache==0.3.0\nprotobuf==5.29.3\npydantic==2.10.6\npydantic_core==2.27.2\nPygments==2.19.1\npython-dotenv==1.0.1\nPyYAML==6.0.2\nregex==2024.11.6\nrequests==2.32.3\nrequests-toolbelt==1.0.0\nrich==13.9.4\nshellingham==1.5.4\nsigtools==4.0.1\nsniffio==1.3.1\nSQLAlchemy==2.0.38\nstarlette==0.45.3\nsynchronicity==0.9.11\ntenacity==9.0.0\ntiktoken==0.9.0\ntoml==0.10.2\ntqdm==4.67.1\ntyper==0.15.1\ntypes-certifi==2021.10.8.3\ntypes-toml==0.10.8.20240310\ntyping_extensions==4.12.2\nurllib3==2.3.0\nuvicorn==0.34.0\nwatchfiles==1.0.4\nyarl==1.18.3\nzstandard==0.23.0", "created_at": "2025-02-27", "closed_at": "2025-02-27", "labels": [], "State": "closed", "Author": "SubrahmanyamNaidu"}
{"issue_number": 3618, "issue_title": "updated state can not be catched during workflow execution", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\ndef main():\n    graph = entity_extract_workflow.build_workflow()\n    state = {\n        \"document_path\": PDF_PATH,\n        \"schema_tool\": schema_reader_tool,\n    }\n    try:\n        events = graph.stream(state, config={\"recursion_limit\": 5000000000})\n        for s in events:\n            print(s)\n        print(\"----\")\n    except Exception as e:\n        logger.get_logger().error(f\"An error occurred: {e}\")\n        now = datetime.datetime.now()\n        output_data = {'entities': state.get('entities', [])}\n        path = Path(__file__).resolve().parent / 'knowledge_data' / f'{now.strftime(\"%Y_%m_%d_%H_%M_\")}entities.json'\n        with open(path, 'w', encoding='utf-8') as f:\n            json.dump(output_data, f, ensure_ascii=False, indent=2)\n        logger.get_logger().info(f\"Entities saved to {path}\")\nError Message and Stack Trace (if applicable)\n\nDescription\nI have encountered an issue where the state variable remains unchanged from its initial values and does not get updated during the execution of the workflow. As a result, when I try to access the state in the try block, it does not reflect the updates from the workflow.\nHere is a simplified version of my code:\nhow can i catch the updated state outside?\nSystem Info\nhow can i catch the updated state?", "created_at": "2025-02-27", "closed_at": "2025-03-11", "labels": ["invalid"], "State": "closed", "Author": "funnykeke"}
{"issue_number": 3617, "issue_title": "Parallel execution of nodes do not seem to work.", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n\nimport operator\nfrom typing import Annotated, TypedDict\n\nfrom langgraph.graph import END, START, StateGraph\n\n\nclass State(TypedDict):\n    aggregate: Annotated[list, operator.add]\n\n\nclass ReturnNodeValue:\n    def __init__(self, node_secret: str) -> None:\n        self._value = node_secret\n\n    def __call__(self, state: State) -> State:\n        print(f\"Adding {self._value} to {state['aggregate']}\")\n        return {\"aggregate\": [self._value]}\n\n\nbuilder = StateGraph(State)\nfor s in \"abcd\":\n    builder.add_node(s, ReturnNodeValue(f\"I'm {s.upper()}\"))\n\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(START, \"b\")\nbuilder.add_edge(\"a\", \"c\")\nbuilder.add_edge(\"b\", \"d\")\nbuilder.add_edge([\"c\", \"d\"], END)\n\ngraph = builder.compile()\ngraph.get_graph().draw_mermaid_png(output_file_path=\"example.png\")\n\ngraph.invoke({\"aggregate\": []})\nError Message and Stack Trace (if applicable)\n\nDescription\nI get the following.\nAdding I'm A to []\nAdding I'm B to []\nAdding I'm C to [\"I'm A\", \"I'm B\"]\nAdding I'm D to [\"I'm A\", \"I'm B\"]\n{'aggregate': [\"I'm A\", \"I'm B\", \"I'm C\", \"I'm D\"]}\nHowever, I expect \"I'm A\" and \"I'm C\" to be added independently to pathway of \"I'm B\" and \"I'm D\". They need to merged only in the end. So I would expect\nAdding I'm A to []\nAdding I'm C to [\"I'm A\"]\nAdding I'm B to []\nAdding I'm D to [\"I'm B\"]\n{'aggregate': [\"I'm A\", \"I'm C\", \"I'm B\", \"I'm D\"]}\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version: 20.04.1-Ubuntu\nPython Version:  3.10.16\n\nPackage Information\n\nlangchain_core: 0.3.37\nlangchain: 0.3.10\nlangchain_community: 0.3.10\nlangsmith: 0.1.147\nlangchain_anthropic: 0.3.0\nlangchain_astradb: 0.5.2\nlangchain_aws: 0.2.7\nlangchain_chroma: 0.1.4\nlangchain_cohere: 0.3.3\nlangchain_elasticsearch: 0.3.0\nlangchain_experimental: 0.3.4\nlangchain_google_calendar_tools: 0.0.1\nlangchain_google_community: 2.0.3\nlangchain_google_genai: 2.0.6\nlangchain_google_vertexai: 2.0.7\nlangchain_groq: 0.2.1\nlangchain_milvus: 0.1.7\nlangchain_mistralai: 0.2.3\nlangchain_mongodb: 0.2.0\nlangchain_nvidia: Installed. No version info available.\nlangchain_nvidia_ai_endpoints: 0.3.5\nlangchain_ollama: 0.2.1\nlangchain_openai: 0.3.6\nlangchain_pinecone: 0.2.2\nlangchain_tests: 0.3.11\nlangchain_text_splitters: 0.3.6\nlangchain_unstructured: 0.1.5\nlangchainhub: 0.1.21\nlanggraph_sdk: 0.1.51\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.10.11\nanthropic: 0.45.2\nanthropic[vertexai]: Installed. No version info available.\nastrapy: 1.5.2\nasync-timeout: 4.0.3\nbeautifulsoup4: 4.12.3\nboto3: 1.34.162\nchromadb: 0.5.23\ncohere: 5.13.12\ndataclasses-json: 0.6.7\ndb-dtypes: Installed. No version info available.\ndefusedxml: 0.7.1\nelasticsearch[vectorstore-mmr]: Installed. No version info available.\nfastapi: 0.115.8\nfiletype: 1.2.0\ngapic-google-longrunning: Installed. No version info available.\ngoogle-api-core: 2.24.1\ngoogle-api-python-client: 2.154.0\ngoogle-api-python-client>=2.104.0: Installed. No version info available.\ngoogle-auth-httplib2: 0.2.0\ngoogle-auth-oauthlib: 1.2.1\ngoogle-auth-oauthlib>=1.1.0: Installed. No version info available.\ngoogle-cloud-aiplatform: 1.79.0\ngoogle-cloud-bigquery: 3.29.0\ngoogle-cloud-bigquery-storage: Installed. No version info available.\ngoogle-cloud-contentwarehouse: Installed. No version info available.\ngoogle-cloud-core: 2.4.1\ngoogle-cloud-discoveryengine: Installed. No version info available.\ngoogle-cloud-documentai: Installed. No version info available.\ngoogle-cloud-documentai-toolbox: Installed. No version info available.\ngoogle-cloud-speech: Installed. No version info available.\ngoogle-cloud-storage: 2.19.0\ngoogle-cloud-texttospeech: Installed. No version info available.\ngoogle-cloud-translate: Installed. No version info available.\ngoogle-cloud-vision: Installed. No version info available.\ngoogle-generativeai: 0.8.4\ngooglemaps: Installed. No version info available.\ngroq: 0.18.0\ngrpcio: 1.67.1\nhttpx: 0.27.2\nhttpx-sse: 0.4.0\nhttpx<1,>=0.25.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.35: Installed. No version info available.\nlangchain>=0.0.335: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nnumpy: 1.26.4\nnumpy<2.0.0,>=1.24.0;: Installed. No version info available.\nnumpy<3,>=1.26.2;: Installed. No version info available.\nollama: 0.4.7\nopenai<2.0.0,>=1.58.1: Installed. No version info available.\norjson: 3.10.15\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npandas: 2.2.2\npillow: 11.0.0\npinecone: 5.4.2\nprotobuf>=4.25.0: Installed. No version info available.\npyarrow: 17.0.0\npydantic: 2.10.6\npydantic-settings: 2.4.0\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npymilvus: 2.5.4\npymongo: 4.10.1\npytest-asyncio<1,>=0.20: Installed. No version info available.\npytest-socket<1,>=0.6.0: Installed. No version info available.\npytest<9,>=7: Installed. No version info available.\npytz>=2023.3.post1: Installed. No version info available.\nPyYAML: 6.0.2\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nSQLAlchemy: 2.0.38\nsyrupy<5,>=4: Installed. No version info available.\ntabulate: 0.9.0\ntenacity: 8.5.0\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntokenizers: 0.20.3\ntypes-requests: 2.32.0.20241016\ntyping-extensions>=4.7: Installed. No version info available.\nunstructured-client: 0.25.9\nunstructured[all-docs]: Installed. No version info available.\n", "created_at": "2025-02-27", "closed_at": "2025-02-27", "labels": [], "State": "closed", "Author": "tkbala"}
{"issue_number": 3594, "issue_title": "Unexpected Connection in LangChain StateGraph Visualization", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\ngraph_builder = StateGraph(State)\ngraph_builder.add_node(query_or_respond)\ngraph_builder.add_node(tools)\ngraph_builder.add_node(entry_node)\ngraph_builder.add_node(summarize_conversation)\n\ngraph_builder.set_entry_point(\"entry_node\")\ngraph_builder.add_conditional_edges(\n    \"query_or_respond\",\n    main_node_to_tool_summarize_or_end,\n)\ngraph_builder.add_edge(\"tools\", \"query_or_respond\")\ngraph_builder.add_edge(\"entry_node\", \"query_or_respond\")\ngraph_builder.add_edge(\"summarize_conversation\", END)\n\ndef main_node_to_tool_summarize_or_end(state: State):\n    messages_key: str = \"messages\"\n    if isinstance(state, dict) and (messages := state.get(messages_key, [])):\n        ai_message = messages[-1]\n    else:\n        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n        return \"tools\"\n    if should_summarize(state):\n        return \"summarize_conversation\"\n    return END\nError Message and Stack Trace (if applicable)\n\nDescription\nUnexpected Connection in LangChain StateGraph Visualization\nI'm building a conversation flow with LangChain's StateGraph, but I'm seeing an unexpected connection in the visualization that I didn't explicitly add in my code.\n\nMy Issue\nI have a graph with several nodes:\n\nentry_node\nquery_or_respond\ntools\nsummarize_conversation\n\nAccording to my graph visualization, there's a dotted line connection from query_or_respond back to entry_node, but I never added this connection in my code.\nMy Code\nHere's the relevant section of my code:\ngraph_builder = StateGraph(State)\ngraph_builder.add_node(query_or_respond)\ngraph_builder.add_node(tools)\ngraph_builder.add_node(entry_node)\ngraph_builder.add_node(summarize_conversation)\n\ngraph_builder.set_entry_point(\"entry_node\")\ngraph_builder.add_conditional_edges(\n    \"query_or_respond\",\n    main_node_to_tool_summarize_or_end,\n)\ngraph_builder.add_edge(\"tools\", \"query_or_respond\")\ngraph_builder.add_edge(\"entry_node\", \"query_or_respond\")\ngraph_builder.add_edge(\"summarize_conversation\", END)\nThe conditional function:\ndef main_node_to_tool_summarize_or_end(state: State):\n    messages_key: str = \"messages\"\n    if isinstance(state, dict) and (messages := state.get(messages_key, [])):\n        ai_message = messages[-1]\n    else:\n        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n        return \"tools\"\n    if should_summarize(state):\n        return \"summarize_conversation\"\n    return END\nMy Question\nWhy is there a connection from query_or_respond back to entry_node in the visualization when I didn't add it in my code? Is there some implicit connection being created, or is this a visualization bug?\nHas anyone encountered this issue before with StateGraph? Any insights would be greatly appreciated!\nEnvironment\n\nLangChain version: [your version]\nPython version: [your version]\n\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:23 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T8122\nPython Version:  3.13.2 (main, Feb  4 2025, 14:51:09) [Clang 16.0.0 (clang-1600.0.26.6)]\n\nPackage Information\n\nlangchain_core: 0.3.36\nlangchain: 0.3.19\nlangchain_community: 0.3.17\nlangsmith: 0.3.8\nlangchain_chroma: 0.2.2\nlangchain_ollama: 0.2.3\nlangchain_openai: 0.3.6\nlangchain_text_splitters: 0.3.6\nlanggraph_sdk: 0.1.51\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nchromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0: Installed. No version info available.\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.35: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.18: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<2,>=1.26.4;: Installed. No version info available.\nnumpy<2.0.0,>=1.22.4;: Installed. No version info available.\nnumpy<2.0.0,>=1.26.2;: Installed. No version info available.\nnumpy<3,>=1.26.2;: Installed. No version info available.\nollama: 0.4.7\nopenai<2.0.0,>=1.58.1: Installed. No version info available.\norjson: 3.10.15\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: Installed. No version info available.\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: 13.9.4\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-02-26", "closed_at": "2025-02-26", "labels": [], "State": "closed", "Author": "4nur4g"}
{"issue_number": 3590, "issue_title": "Messages stream out of order when routing using `Command.PARENT`", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.graph import StateGraph, START, MessagesState\nfrom langgraph.types import Command\nfrom langchain_core.messages import BaseMessage, AIMessage\n\ndef call_model(state):\n    # simulate calling an LLM\n    return {\"messages\": AIMessage(content=\"Why hello there!\", id=\"123\")}\n\ndef route(state):\n    return Command(goto=\"node_2\", graph=Command.PARENT)\n\nsubgraph = (\n    StateGraph(MessagesState)\n    .add_node(call_model)\n    .add_node(route)\n    .add_edge(START, \"call_model\")\n    .add_edge(\"call_model\", \"route\")\n    .compile()\n)\n\ngraph = StateGraph(MessagesState).add_node(\"node_1\", subgraph).add_node(\"node_2\", lambda state: state).add_edge(START, \"node_1\").compile()\n\nfor chunk in graph.stream({\"messages\": \"hi\"}, stream_mode=\"messages\", subgraphs=False):\n    message: BaseMessage = chunk[0]\n    print(message.pretty_print())\nError Message and Stack Trace (if applicable)\n\nDescription\nIssue reported by a community member.\nCurrent (incorrect) behavior\nThe AIMessage is output first, followed by the HumanMessage\n================================== Ai Message ==================================\n\nWhy hello there!\nNone\n================================ Human Message =================================\n\nhi\nNone\n\nExpected behavior:\nThe HumanMessage should stream first, followed by the AIMessage.\n================================== Ai Message ==================================\n\nWhy hello there!\nNone\n================================ Human Message =================================\n\nhi\nNone\n\nSystem Info\nSystem Information\n------------------\n> OS:  Darwin\n> OS Version:  Darwin Kernel Version 24.2.0: Fri Dec  6 19:03:40 PST 2024; root:xnu-11215.61.5~2/RELEASE_ARM64_T6041\n> Python Version:  3.11.11 (main, Jan 13 2025, 16:27:48) [Clang 16.0.0 (clang-1600.0.26.6)]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.30\n> langchain: 0.3.18\n> langchain_community: 0.3.17\n> langsmith: 0.1.138\n> langchain_anthropic: 0.2.4\n> langchain_cohere: 0.4.2\n> langchain_experimental: 0.3.4\n> langchain_fireworks: 0.2.7\n> langchain_mistralai: 0.2.6\n> langchain_nomic: 0.1.4\n> langchain_ollama: 0.2.3\n> langchain_openai: 0.3.4\n> langchain_text_splitters: 0.3.6\n> langgraph_sdk: 0.1.51\n", "created_at": "2025-02-26", "closed_at": "2025-02-26", "labels": [], "State": "closed", "Author": "benjamincburns"}
{"issue_number": 3587, "issue_title": "Unexpected Behavior of State Reducer Function in Subgraph", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph\nfrom langgraph.graph import END, START\nfrom typing import Annotated, TypedDict\n\n\ndef update_dialog_stack(left: list[str], right: str | None) -> list[str]:\n    \"\"\"Push or pop the state.\"\"\"\n    if right is None:\n        return left\n    if right == \"pop\":\n        return left[:-1]\n    return left + [right]\n\n\nclass State(TypedDict):\n    dialog_state: Annotated[list[str], update_dialog_stack]\n\n\ndef node_a_child(state):\n    return {\"dialog_state\": \"a_child_state\"}\n\ndef node_b_child(state):\n    return {\"dialog_state\": \"b_child_state\"}\n\n\n\nsub_builder = StateGraph(State)\n\nsub_builder.add_node(\"node_a_child\", node_a_child)\nsub_builder.add_edge(START, \"node_a_child\")\n\nsub_builder.add_node(\"node_b_child\", node_b_child) \nsub_builder.add_edge(\"node_a_child\", \"node_b_child\")\n\nsub_builder.add_edge(\"node_b_child\", END)\n\n\nsub_graph = sub_builder.compile()\n\n\ndef node_a_parent(state):\n    return {\"dialog_state\": \"a_parent_state\"}\n\ndef node_b_parent(state):\n    return {\"dialog_state\": \"pop\"}\n\n\nmain_builder = StateGraph(State)\n\nmain_builder.add_node(\"node_a_parent\", node_a_parent)\nmain_builder.add_edge(START, \"node_a_parent\")\n\nmain_builder.add_node(\"subgraph\", sub_graph)\nmain_builder.add_edge(\"node_a_parent\", \"subgraph\")\n\nmain_builder.add_node(\"node_b_parent\", node_b_parent)\nmain_builder.add_edge(\"subgraph\", \"node_b_parent\")\n\n\nmain_builder.add_edge(\"node_b_parent\", END)\n\n\ncheckpointer_temp = MemorySaver()\nmain_graph = main_builder.compile(checkpointer_temp, name=\"parent\")\n\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": 1,\n    }\n}\n\nresult = main_graph.invoke(input={\"dialog_state\": \"init_state\"}, config=config, subgraphs=True, debug=True)\nError Message and Stack Trace (if applicable)\n\nDescription\nI am trying to update a state from a subgraph node. The dialog_state state key is annotated with the update_dialog_stack reducer function, which is taken verbatim from the LangGraph Customer Support Bot tutorial.\nThe expected behavior for the dialog_state key should be the following:\n[init_state] \u2192 [init_state, a_parent_state] \u2192 [init_state, a_parent_state, a_child_state] \u2192 [init_state, a_parent_state, a_child_state, b_child_state] \u2192 [init_state, a_parent_state, a_child_state].\nBelow is the history of the actual state updates provided.\n{'dialog_state': []}\n\n[0:tasks] Starting 1 task for step 0:\n- __start__ -> {'dialog_state': 'init_state'}\n\n[0:writes] Finished step 0 with writes to 1 channel:\n- dialog_state -> 'init_state'\n\n[0:checkpoint] State at the end of step 0:\n{'dialog_state': ['init_state']}\n\n[1:tasks] Starting 1 task for step 1:\n- node_a_parent -> {'dialog_state': ['init_state']}\n\n[1:writes] Finished step 1 with writes to 1 channel:\n- dialog_state -> 'a_parent_state'\n\n[1:checkpoint] State at the end of step 1:\n{'dialog_state': ['init_state', 'a_parent_state']}\n\n[2:tasks] Starting 1 task for step 2:\n- subgraph -> {'dialog_state': ['init_state', 'a_parent_state']}\n\n[2:writes] Finished step 2 with writes to 1 channel:\n- dialog_state -> [['init_state', 'a_parent_state'], 'a_child_state', 'b_child_state']\n\n[2:checkpoint] State at the end of step 2:\n{'dialog_state': ['init_state',\n                  'a_parent_state',\n                  [[...], 'a_child_state', 'b_child_state']]}\n\n[3:tasks] Starting 1 task for step 3:\n- node_b_parent -> {'dialog_state': ['init_state',\n                  'a_parent_state',\n                  [['init_state', 'a_parent_state'],\n                   'a_child_state',\n                   'b_child_state']]}\n\n[3:writes] Finished step 3 with writes to 1 channel:\n- dialog_state -> 'pop'\n\n[3:checkpoint] State at the end of step 3:\n{'dialog_state': ['init_state', 'a_parent_state']}```\n\n### System Info\n\nSystem Information\n------------------\n> OS:  Darwin\n> OS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:16 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6000\n> Python Version:  3.12.7 (main, Oct  1 2024, 02:05:46) [Clang 15.0.0 (clang-1500.3.9.4)]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.36\n> langchain: 0.3.19\n> langsmith: 0.2.11\n> langchain_openai: 0.3.6\n> langchain_text_splitters: 0.3.6\n> langgraph_sdk: 0.1.51\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> httpx: 0.28.1\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.34: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.35: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langsmith-pyo3: Installed. No version info available.\n> langsmith<0.4,>=0.1.125: Installed. No version info available.\n> langsmith<0.4,>=0.1.17: Installed. No version info available.\n> numpy<2,>=1.26.4;: Installed. No version info available.\n> numpy<3,>=1.26.2;: Installed. No version info available.\n> openai<2.0.0,>=1.58.1: Installed. No version info available.\n> orjson: 3.10.15\n> packaging<25,>=23.2: Installed. No version info available.\n> pydantic: 2.10.5\n> pydantic<3.0.0,>=2.5.2;: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4;: Installed. No version info available.\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> requests<3,>=2: Installed. No version info available.\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken<1,>=0.7: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> zstandard: Installed. No version info available.\n", "created_at": "2025-02-25", "closed_at": "2025-04-15", "labels": [], "State": "closed", "Author": "iamolvr"}
{"issue_number": 3576, "issue_title": "create_react_agent: System prompt not part of generate_structured_response call with `response_format` (for structured output)", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport os\n\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\nfrom pydantic import BaseModel, Field\n\n\nclass AgentResponse(BaseModel):\n    agent_response_text: str = Field(description=\"The agent's response\")\n\n\ndef bug_reproduce():\n    llm = ChatOpenAI(model='gpt-4o', api_key=os.getenv(\"OPEN_AI_KEY\"), temperature=0)\n    agent = create_react_agent(\n        llm, tools=[], prompt=\"Always answer in German.\",\n        response_format=AgentResponse\n    )\n    result = agent.invoke({\"messages\": \"Which animal is best for riding? Return only the name.\"})\n    for m in result['messages']:\n        m.pretty_print()\n    print(\"==//==\")\n    print(result['structured_response'])\n\n\nbug_reproduce()\nError Message and Stack Trace (if applicable)\nRunning the example code results in this output:\n\n\n================================ Human Message =================================\n\nWhich animal is best for riding? Return only the name.\n================================== Ai Message ==================================\n\nPferd.\n==//==\nagent_response_text='Horse'\nDescription\nThe last generate_structured_response step of the agent does not contain the system message submitted as prompt= (in our case \"Always answer in German.\").\nThe last message in messages is in German, but the structured output is in English since the system prompt does not make it into the final call to produce the structured output AgentResponse.\nExpected behaviour is that it will be part of the message stack.\nA workaround is to submit the system prompt explicitly during invoke().\nSystem Info\nlangchain==0.3.19\nlangchain-openai==0.3.3\nlanggraph==0.2.72", "created_at": "2025-02-25", "closed_at": "2025-02-25", "labels": [], "State": "closed", "Author": "andreaskaltenbach"}
{"issue_number": 3570, "issue_title": "Can't go to a node in the parent graph using Command.PARENT", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport random\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import StateGraph\nfrom langgraph.graph import END, START\nfrom langgraph.types import Command\nfrom typing import Literal, Annotated, TypedDict\nfrom langgraph.graph.message import AnyMessage\nfrom langgraph.graph.message import add_messages\n\n\n\n\nclass State(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n    personal_data: dict\n    dialog_state: str\n\n\n\ndef node_a_child(state):\n    return {\"dialog_state\": \"b_child_state\"}\n    # return {\"registration_status\": True, \"dialog_state\": \"b_child_state\"}\n\ndef node_b_child(state) -> Command[Literal[\"node_c_child\", \"node_d_child\"]]:\n    value = random.choice([0, 1])\n    return Command(\n        goto=\"node_c_child\" if value == 0 else \"node_d_child\"\n    )\n\ndef node_c_child(state):\n    return Command(\n        graph=Command.PARENT,\n        goto=\"node_sibling\",\n    )\n\ndef node_d_child(state):\n    return {\"dialog_state\": \"d_child_state\"}\n\n\n\n\nsub_builder = StateGraph(State)\n\nsub_builder.add_node(\"node_a_child\", node_a_child)\nsub_builder.add_edge(START, \"node_a_child\")\n\nsub_builder.add_node(\"node_b_child\", node_b_child) \nsub_builder.add_edge(\"node_a_child\", \"node_b_child\")\n\nsub_builder.add_node(\"node_c_child\", node_c_child)\nsub_builder.add_edge(\"node_c_child\", END)\n\n\nsub_builder.add_node(\"node_d_child\", node_d_child) \nsub_builder.add_edge(\"node_d_child\", END)\n\nsub_graph = sub_builder.compile(checkpointer=True)\n\n\n\n\ndef node_a_parent(state):\n    return {\"dialog_state\": \"a_parent_state\"}\n\ndef node_b_parent(state):\n    return {\"dialog_state\": \"pop\"}\n\ndef node_sibling(state):\n    return {\"dialog_state\": \"sibling\"}\n\nmain_builder = StateGraph(State)\n\nmain_builder.add_node(\"node_a_parent\", node_a_parent)\nmain_builder.add_edge(START, \"node_a_parent\")\n\nmain_builder.add_node(\"subgraph\", sub_graph)\nmain_builder.add_edge(\"node_a_parent\", \"subgraph\")\n\nmain_builder.add_node(\"node_b_parent\", node_b_parent)\nmain_builder.add_edge(\"subgraph\", \"node_b_parent\")\n\n\nmain_builder.add_edge(\"node_b_parent\", END)\n\nmain_builder.add_node(\"node_sibling\", node_sibling)\nmain_builder.add_edge(\"node_sibling\", END)\n\n\ncheckpointer_temp = MemorySaver()\n\nmain_graph = main_builder.compile(checkpointer_temp, name=\"parent\")\n\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": 1,\n    }\n}\n\n\nresult = main_graph.invoke(input={\"dialog_state\": [\"init_state\"]}, config=config, subgraphs=True, debug=True)\nError Message and Stack Trace (if applicable)\n{\n\t\"name\": \"ParentCommand\",\n\t\"message\": \"Command(graph='subgraph', update=[('messages', []), ('dialog_state', 'a_parent_state'), ('messages', []), ('dialog_state', 'b_child_state')], goto='node_sibling')\",\n\t\"stack\": \"---------------------------------------------------------------------------\nParentCommand                             Traceback (most recent call last)\nCell In[46], line 8\n      1 config = {\n      2     \\\"configurable\\\": {\n      3         \\\"thread_id\\\": 1,\n      4     }\n      5 }\n----> 8 result = main_graph.invoke(input={\\\"dialog_state\\\": [\\\"init_state\\\"]}, config=config, subgraphs=True, debug=True)\n      9 result\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2124, in Pregel.invoke(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\n   2122 else:\n   2123     chunks = []\n-> 2124 for chunk in self.stream(\n   2125     input,\n   2126     config,\n   2127     stream_mode=stream_mode,\n   2128     output_keys=output_keys,\n   2129     interrupt_before=interrupt_before,\n   2130     interrupt_after=interrupt_after,\n   2131     debug=debug,\n   2132     **kwargs,\n   2133 ):\n   2134     if stream_mode == \\\"values\\\":\n   2135         latest = chunk\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1779, in Pregel.stream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\n   1773     # Similarly to Bulk Synchronous Parallel / Pregel model\n   1774     # computation proceeds in steps, while there are channel updates.\n   1775     # Channel updates from step N are only visible in step N+1\n   1776     # channels are guaranteed to be immutable for the duration of the step,\n   1777     # with channel updates applied only at the transition between steps.\n   1778     while loop.tick(input_keys=self.input_channels):\n-> 1779         for _ in runner.tick(\n   1780             loop.tasks.values(),\n   1781             timeout=self.step_timeout,\n   1782             retry_policy=self.retry_policy,\n   1783             get_waiter=get_waiter,\n   1784         ):\n   1785             # emit output\n   1786             yield from output()\n   1787 # emit output\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/runner.py:302, in PregelRunner.tick(self, tasks, reraise, timeout, retry_policy, get_waiter)\n    300 yield\n    301 # panic on failure or timeout\n--> 302 _panic_or_proceed(\n    303     futures.done.union(f for f, t in futures.items() if t is not None),\n    304     panic=reraise,\n    305 )\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/runner.py:619, in _panic_or_proceed(futs, timeout_exc_cls, panic)\n    617         # raise the exception\n    618         if panic:\n--> 619             raise exc\n    620 if inflight:\n    621     # if we got here means we timed out\n    622     while inflight:\n    623         # cancel all pending tasks\n\nFile /opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:340, in Future._invoke_callbacks(self)\n    338 for callback in self._done_callbacks:\n    339     try:\n--> 340         callback(self)\n    341     except Exception:\n    342         LOGGER.exception('exception calling callback for %r', self)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/runner.py:88, in FuturesDict.on_done(self, task, fut)\n     82 def on_done(\n     83     self,\n     84     task: PregelExecutableTask,\n     85     fut: F,\n     86 ) -> None:\n     87     try:\n---> 88         self.callback(task, _exception(fut))\n     89     finally:\n     90         with self.lock:\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/runner.py:551, in PregelRunner.commit(self, task, exception)\n    549         self.put_writes(task.id, interrupts)\n    550 elif isinstance(exception, GraphBubbleUp):\n--> 551     raise exception\n    552 else:\n    553     # save error to checkpointer\n    554     self.put_writes(task.id, [(ERROR, exception)])\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/executor.py:83, in BackgroundExecutor.done(self, task)\n     81 \\\"\\\"\\\"Remove the task from the tasks dict when it's done.\\\"\\\"\\\"\n     82 try:\n---> 83     task.result()\n     84 except GraphBubbleUp:\n     85     # This exception is an interruption signal, not an error\n     86     # so we don't want to re-raise it on exit\n     87     self.tasks.pop(task)\n\nFile /opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:449, in Future.result(self, timeout)\n    447     raise CancelledError()\n    448 elif self._state == FINISHED:\n--> 449     return self.__get_result()\n    451 self._condition.wait(timeout)\n    453 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\nFile /opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401, in Future.__get_result(self)\n    399 if self._exception:\n    400     try:\n--> 401         raise self._exception\n    402     finally:\n    403         # Break a reference cycle with the exception in self._exception\n    404         self = None\n\nFile /opt/homebrew/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/thread.py:58, in _WorkItem.run(self)\n     55     return\n     57 try:\n---> 58     result = self.fn(*self.args, **self.kwargs)\n     59 except BaseException as exc:\n     60     self.future.set_exception(exc)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/retry.py:40, in run_with_retry(task, retry_policy, configurable)\n     38     task.writes.clear()\n     39     # run the task\n---> 40     return task.proc.invoke(task.input, config)\n     41 except ParentCommand as exc:\n     42     ns: str = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/utils/runnable.py:546, in RunnableSeq.invoke(self, input, config, **kwargs)\n    542 config = patch_config(\n    543     config, callbacks=run_manager.get_child(f\\\"seq:step:{i + 1}\\\")\n    544 )\n    545 if i == 0:\n--> 546     input = step.invoke(input, config, **kwargs)\n    547 else:\n    548     input = step.invoke(input, config)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2124, in Pregel.invoke(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\n   2122 else:\n   2123     chunks = []\n-> 2124 for chunk in self.stream(\n   2125     input,\n   2126     config,\n   2127     stream_mode=stream_mode,\n   2128     output_keys=output_keys,\n   2129     interrupt_before=interrupt_before,\n   2130     interrupt_after=interrupt_after,\n   2131     debug=debug,\n   2132     **kwargs,\n   2133 ):\n   2134     if stream_mode == \\\"values\\\":\n   2135         latest = chunk\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1779, in Pregel.stream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\n   1773     # Similarly to Bulk Synchronous Parallel / Pregel model\n   1774     # computation proceeds in steps, while there are channel updates.\n   1775     # Channel updates from step N are only visible in step N+1\n   1776     # channels are guaranteed to be immutable for the duration of the step,\n   1777     # with channel updates applied only at the transition between steps.\n   1778     while loop.tick(input_keys=self.input_channels):\n-> 1779         for _ in runner.tick(\n   1780             loop.tasks.values(),\n   1781             timeout=self.step_timeout,\n   1782             retry_policy=self.retry_policy,\n   1783             get_waiter=get_waiter,\n   1784         ):\n   1785             # emit output\n   1786             yield from output()\n   1787 # emit output\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/runner.py:240, in PregelRunner.tick(self, tasks, reraise, timeout, retry_policy, get_waiter)\n    238     self.commit(t, None)\n    239 except Exception as exc:\n--> 240     self.commit(t, exc)\n    241     if reraise and futures:\n    242         # will be re-raised after futures are done\n    243         fut: concurrent.futures.Future = concurrent.futures.Future()\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/runner.py:551, in PregelRunner.commit(self, task, exception)\n    549         self.put_writes(task.id, interrupts)\n    550 elif isinstance(exception, GraphBubbleUp):\n--> 551     raise exception\n    552 else:\n    553     # save error to checkpointer\n    554     self.put_writes(task.id, [(ERROR, exception)])\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/runner.py:230, in PregelRunner.tick(self, tasks, reraise, timeout, retry_policy, get_waiter)\n    228 t = tasks[0]\n    229 try:\n--> 230     run_with_retry(\n    231         t,\n    232         retry_policy,\n    233         configurable={\n    234             CONFIG_KEY_SEND: partial(writer, t),\n    235             CONFIG_KEY_CALL: partial(call, t),\n    236         },\n    237     )\n    238     self.commit(t, None)\n    239 except Exception as exc:\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/pregel/retry.py:40, in run_with_retry(task, retry_policy, configurable)\n     38     task.writes.clear()\n     39     # run the task\n---> 40     return task.proc.invoke(task.input, config)\n     41 except ParentCommand as exc:\n     42     ns: str = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/utils/runnable.py:548, in RunnableSeq.invoke(self, input, config, **kwargs)\n    546             input = step.invoke(input, config, **kwargs)\n    547         else:\n--> 548             input = step.invoke(input, config)\n    549 # finish the root run\n    550 except BaseException as e:\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/utils/runnable.py:310, in RunnableCallable.invoke(self, input, config, **kwargs)\n    308 else:\n    309     context.run(_set_config_context, config)\n--> 310     ret = context.run(self.func, *args, **kwargs)\n    311 if isinstance(ret, Runnable) and self.recurse:\n    312     return ret.invoke(input, config)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/graph/graph.py:94, in Branch._route(self, input, config, reader, writer)\n     92 else:\n     93     value = input\n---> 94 result = self.path.invoke(value, config)\n     95 return self._finish(writer, input, result, config)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/utils/runnable.py:310, in RunnableCallable.invoke(self, input, config, **kwargs)\n    308 else:\n    309     context.run(_set_config_context, config)\n--> 310     ret = context.run(self.func, *args, **kwargs)\n    311 if isinstance(ret, Runnable) and self.recurse:\n    312     return ret.invoke(input, config)\n\nFile ~/Library/Caches/pypoetry/virtualenvs/backend-ZFbskOdg-py3.12/lib/python3.12/site-packages/langgraph/graph/state.py:895, in _control_branch(value)\n    893 for command in commands:\n    894     if command.graph == Command.PARENT:\n--> 895         raise ParentCommand(command)\n    896     if isinstance(command.goto, Send):\n    897         rtn.append(command.goto)\n\nParentCommand: Command(graph='subgraph', update=[('messages', []), ('dialog_state', 'a_parent_state'), ('messages', []), ('dialog_state', 'b_child_state')], goto='node_sibling')\"\n}\nDescription\nI am trying to implement a handoff where a subgraph transfers the user to another node in the parent graph (a sibling node in this case) using a Command, but I keep encountering an error. There is no difference whether the graph and subgraph share the same state or only a single key, whether a reducer function is applied or not\u2014the transition simply doesn\u2019t occur. To me, this seems somewhat illogical. I have experimented with various target nodes (e.g., node_a_parent), but the result remains the same\u2014a ParentCommand exception.\u201d\n\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:16 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6000\nPython Version:  3.12.7 (main, Oct  1 2024, 02:05:46) [Clang 15.0.0 (clang-1500.3.9.4)]\n\nPackage Information\n\nlangchain_core: 0.3.36\nlangchain: 0.3.19\nlangsmith: 0.2.11\nlangchain_openai: 0.3.6\nlangchain_text_splitters: 0.3.6\nlanggraph_sdk: 0.1.51\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nhttpx: 0.28.1\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.35: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<2,>=1.26.4;: Installed. No version info available.\nnumpy<3,>=1.26.2;: Installed. No version info available.\nopenai<2.0.0,>=1.58.1: Installed. No version info available.\norjson: 3.10.15\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.5\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: Installed. No version info available.\n", "created_at": "2025-02-24", "closed_at": null, "labels": ["investigate"], "State": "open", "Author": "iamolvr"}
{"issue_number": 3569, "issue_title": "Error when running `langgraph dev` after upgrade 0.1.71 -> 0.1.73", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n`langgraph dev`\nError Message and Stack Trace (if applicable)\n(.venv) ~\\PycharmProjects\\Lighthouse.AGENTIC git:[summarisation-poc]\nlanggraph dev\nINFO:langgraph_api.cli:\n\n        Welcome to\n\n\u2566  \u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u2554\u2550\u2557\u252c\u2500\u2510\u250c\u2500\u2510\u250c\u2500\u2510\u252c \u252c\n\u2551  \u251c\u2500\u2524\u2502\u2502\u2502\u2502 \u252c\u2551 \u2566\u251c\u252c\u2518\u251c\u2500\u2524\u251c\u2500\u2518\u251c\u2500\u2524\n\u2569\u2550\u255d\u2534 \u2534\u2518\u2514\u2518\u2514\u2500\u2518\u255a\u2550\u255d\u2534\u2514\u2500\u2534 \u2534\u2534  \u2534 \u2534\n\n- \ud83d\ude80 API: http://127.0.0.1:2024\n- \ud83c\udfa8 Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n- \ud83d\udcda API Docs: http://127.0.0.1:2024/docs\n\nThis in-memory server is designed for development and testing.\nFor production use, please use LangGraph Cloud.\n\n\nException in thread Thread-2 (_open_browser):\nTraceback (most recent call last):\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n    self.run()\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\threading.py\", line 982, in run\n    self._target(*self._args, **self._kwargs)\n  File \"C:\\Users\\[USERNAME]\\PycharmProjects\\Lighthouse.AGENTIC\\.venv\\Lib\\site-packages\\langgraph_api\\cli.py\", line 218, in _open_browser\n    with urllib.request.urlopen(f\"{local_url}/ok\") as response:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\urllib\\request.py\", line 216, in urlopen\n    return opener.open(url, data, timeout)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\urllib\\request.py\", line 519, in open\n    response = self._open(req, data)\n               ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\urllib\\request.py\", line 536, in _open\n    result = self._call_chain(self.handle_open, protocol, protocol +\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\urllib\\request.py\", line 496, in _call_chain\n    result = func(*args)\n             ^^^^^^^^^^^\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\urllib\\request.py\", line 1377, in http_open\n    return self.do_open(http.client.HTTPConnection, req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\urllib\\request.py\", line 1352, in do_open\n    r = h.getresponse()\n        ^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py\", line 1395, in getresponse\n    response.begin()\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py\", line 325, in begin\n    version, status, reason = self._read_status()\n                              ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py\", line 286, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\socket.py\", line 706, in readinto\n    return self._sock.recv_into(b)\n           ^^^^^^^^^^^^^^^^^^^^^^^\nConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\nDescription\nA working langgraph implementation when run on 0.1.71, now returns the following error on the langgraph dev command after upgrading to 0.1.73\nSystem Info\n(.venv) ~\\PycharmProjects\\Lighthouse.AGENTIC git:[summarisation-poc]\npython -m langchain_core.sys_info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.26100\nPython Version:  3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.3.34\nlangchain: 0.3.18\nlangsmith: 0.3.8\nlangchain_openai: 0.3.5\nlangchain_text_splitters: 0.3.6\nlanggraph_api: 0.0.23\nlanggraph_cli: 0.1.73\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.51\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nclick: 8.1.8\ncryptography: 43.0.3\nhttpx: 0.28.1\njsonpatch<2.0,>=1.33: Installed. No version info available.\njsonschema-rs: 0.25.1\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlanggraph: 0.2.74\nlanggraph-checkpoint: 2.0.12\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<2,>=1.26.4;: Installed. No version info available.\nnumpy<3,>=1.26.2;: Installed. No version info available.\nopenai<2.0.0,>=1.58.1: Installed. No version info available.\norjson: 3.10.15\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npyjwt: 2.10.1\npytest: Installed. No version info available.\npython-dotenv: 1.0.1\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: Installed. No version info available.\nSQLAlchemy<3,>=1.4: Installed. No version info available.\nsse-starlette: 2.1.3\nstarlette: 0.45.3\nstructlog: 24.4.0\ntenacity: 9.0.0\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nuvicorn: 0.34.0\nwatchfiles: 1.0.4\nzstandard: 0.23.0\n", "created_at": "2025-02-24", "closed_at": "2025-02-25", "labels": [], "State": "closed", "Author": "simon-lighthouse"}
{"issue_number": 3564, "issue_title": "State not injected in tool", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.graph import StateGraph, START, END, MessagesState\nfrom langchain.tools.base import StructuredTool\nfrom langgraph.prebuilt import ToolNode\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.types import StreamWriter\n\nclass CustomAgentState(MessagesState):\n    folder_path: str\n\nclass CustomAgent:\n    def __init__(self, path: str):\n       self._folder_path = path\n       \n       workflow = StateGraph(CustomAgentState)\n\n       self._tools = [\n            StructuredTool.from_function(\n                coroutine=self._request_files_tool,\n                name=\"request_files_tool\",\n                description=\"The tool for requesting content of one or multiple files by their path.\"\n            )\n        ]\n\n        self._llm = ChatAnthropic(model=..., \n                                  api_key=..., \n                                  temperature=0, \n                                  streaming=True).bind_tools(self._tools)\n\n        \n        workflow.add_node(\"invoke_llm\", self._invoke_llm_node)\n        workflow.add_node(\"tools\", ToolNode(self._tools))\n        \n        workflow.add_edge(START, \"invoke_llm\")\n        workflow.add_conditional_edges(\"invoke_llm\", self._intelligent_routing, [\"tools\", END])\n        workflow.add_edge(\"tools\", \"invoke_llm\")\n\n        memory = MemorySaver()\n        self._chain = workflow.compile(checkpointer=memory)\n\n    async def invoke(self, prompt: str, context: str = None) -> AsyncIterator[str]:\n        ...\n        chain_input = {\n            \"messages\": [system_prompt, user_prompt],\n            \"folder_path\": self._folder_path\n        } \n\n       async for output_chunk in self._chain.astream(input=chain_input, config=config, stream_mode=\"custom\"):\n             yield output_chunk\n      \n       ...\n\n    async def _intelligent_routing(self, state: CustomAgentState) -> str:\n       ... state injected correctly\n\n    async def _invoke_llm_node(self, state: CustomAgentState, writer: StreamWriter):\n       ... state injected correctly\n   \n    # PROBLEMATIC TOOL\n    async def _request_files_tool(self, file_paths: list[str], state: CustomAgentState) -> List[Tuple[str, str]]:\n       ... state is empty\n       # tried using Annotated[dict, CustomAgentState] as well but tool calling stops working\n\n# Invoking\nagent = CustomAgent(\"/some/custom/path\")\n\nasync for output_chunk in agent.invoke(prompt=msg.text, context=context):\n  ...\nError Message and Stack Trace (if applicable)\n\nDescription\nI'm trying to pass the Graph's state to an async tool, as described in the docs, but doesn't work and I get an empty state always even if it's correctly injected in other nodes which are not defined as a tool.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:16 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6000\nPython Version:  3.13.2 (v3.13.2:4f8bb3947cf, Feb  4 2025, 11:51:10) [Clang 15.0.0 (clang-1500.3.9.4)]\n\nPackage Information\n\nlangchain_core: 0.3.37\nlangchain: 0.3.19\nlangsmith: 0.3.8\nlangchain_anthropic: 0.3.7\nlangchain_text_splitters: 0.3.6\nlanggraph_sdk: 0.1.51\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nanthropic<1,>=0.45.0: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nhttpx: 0.28.1\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.35: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<2,>=1.26.4;: Installed. No version info available.\nnumpy<3,>=1.26.2;: Installed. No version info available.\norjson: 3.10.15\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: Installed. No version info available.\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: Installed. No version info available.\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-02-24", "closed_at": "2025-02-24", "labels": [], "State": "closed", "Author": "georgeberar"}
{"issue_number": 4382, "issue_title": "Cannot use model 'gpt-3.5-turbo-instruct' with deployment on LangGraph Platform", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nI use the code from prebuilt react-agent to load chat models:\n\ndef load_chat_model(fully_specified_name: str) -> BaseChatModel:\n    \"\"\"Load a chat model from a fully specified name.\n\n    Args:\n        fully_specified_name (str): String in the format 'provider/model'.\n    \"\"\"\n    provider, model = fully_specified_name.split(\"/\", maxsplit=1)\n    return init_chat_model(model, model_provider=provider)\nError Message and Stack Trace (if applicable)\nopenai.NotFoundError: Error code: 404 - {'error': {'message': 'This is not a chat model and thus not supported in the v1/chat/completions endpoint. Did you mean to use v1/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}}\nDescription\nI run a LangGraph app deployed on LangGraph Platform. Using other 'built-in' models works just fine, but not the gpt-3.5-turbo-instruct model.\nI understand from the error message above that I would have to use another endpoint (v1/completions instead of v1/chat/completions). But I don't know how to do that, as there is no setting in the LangGraph Platform deployment process to specify endpoints.\nApart from that I want to use different models in different nodes in my graph, i.e. changing the model endpoint for the whole app would not do the trick. I would need to specify different endpoints in each node.\nThank you!\nSystem Info\nLangGraph Platform deployment", "created_at": "2025-04-23", "closed_at": "2025-04-23", "labels": [], "State": "closed", "Author": "SPINDRIFTAI"}
{"issue_number": 4381, "issue_title": "langgraph new project has error ModuleNotFoundError: No module named 'configuration'", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nlanggraph new\nlanggraph dev\nError Message and Stack Trace (if applicable)\n2025-04-23T06:09:13.198294Z [error    ] Traceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/starlette/routing.py\", line 692, in lifespan\n    async with self.lifespan_context(app) as maybe_state:\n               ~~~~~~~~~~~~~~~~~~~~~^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/contextlib.py\", line 214, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langgraph_runtime_inmem/lifespan.py\", line 43, in lifespan\n    await collect_graphs_from_env(True)\n  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langgraph_api/graph.py\", line 322, in collect_graphs_from_env\n    graph = await run_in_executor(None, _graph_from_spec, spec)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/runnables/config.py\", line 616, in run_in_executor\n    return await asyncio.get_running_loop().run_in_executor(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<2 lines>...\n    )\n    ^\n  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/thread.py\", line 59, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_core/runnables/config.py\", line 607, in wrapper\n    return func(*args, **kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langgraph_api/graph.py\", line 362, in _graph_from_spec\n    modspec.loader.exec_module(module)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^\n  File \"<frozen importlib._bootstrap_external>\", line 1026, in exec_module\n  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n  File \".../genai/test/src/agent/graph.py\", line 11, in <module>\n    from configuration import Configuration\nModuleNotFoundError: No module named 'configuration'\nCould not import python module for graph:\nGraphSpec(id='agent', path='./src/agent/graph.py', module=None, variable='graph', config={}, description=None)\nThis error likely means you haven't installed your project and its dependencies yet. Before running the server, install your project:\n\nIf you are using requirements.txt:\npython -m pip install -r requirements.txt\n\nIf you are using pyproject.toml or setuptools:\npython -m pip install -e .\n\nMake sure to run this command from your project's root directory (where your setup.py or pyproject.toml is located)\nDescription\nAfter using \"langgraph new\" to create a new langgraph project, then run \"langgraph dev\" to run it, it throws the error as listed\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.4.0: Fri Apr 11 18:32:05 PDT 2025; root:xnu-11417.101.15~117/RELEASE_ARM64_T8132\nPython Version:  3.13.3 (main, Apr  8 2025, 13:54:08) [Clang 16.0.0 (clang-1600.0.26.6)]\n\nPackage Information\n\nlangchain_core: 0.3.55\nlangsmith: 0.3.33\nlanggraph_sdk: 0.1.63\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nhttpx: 0.28.1\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nopenai-agents: Installed. No version info available.\nopentelemetry-api: Installed. No version info available.\nopentelemetry-exporter-otlp-proto-http: Installed. No version info available.\nopentelemetry-sdk: Installed. No version info available.\norjson: 3.10.16\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.11.3\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: Installed. No version info available.\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrich: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-04-23", "closed_at": null, "labels": ["question"], "State": "open", "Author": "MichaelLi65535"}
{"issue_number": 4377, "issue_title": "Comand does not actually hand off to the respective Agent", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing import Literal\n\nfrom dotenv import load_dotenv\nfrom langchain_core.messages import SystemMessage, AIMessage\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\n\nimport os\n\nfrom langgraph.graph import StateGraph, START, MessagesState, END\nfrom langgraph.prebuilt import ToolNode\nfrom langgraph.types import Command\nfrom utils.configs import SupervisorResponseStructure, supervisor_agent_prompt, addition_agent_prompt, multiplication_agent_prompt\n\nload_dotenv()\n\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n\n@tool\ndef add_numbers(a: int, b: int):\n    '''This helps in adding two numbers.'''\n    return a+b\n\n@tool\ndef multiply_numbers(a: int, b: int):\n    '''This helps in multiplying two numbers.'''\n    return a*b\n\nmodel =  ChatOpenAI(model=\"gpt-4o\", openai_api_key=os.getenv(\"OPEN_AI_API_KEY\"))\n\n# supervisor agent\ndef supervisor_agent(state: MessagesState) -> Command[Literal['addition_expert_agent', 'multiplication_expert_agent', END]]:\n   supervisor_model = model.with_structured_output(SupervisorResponseStructure)\n\n   response = supervisor_model.invoke([SystemMessage(content = supervisor_agent_prompt)] + state['messages'])\n\n   if response['agent_to_call'] and response['agent_to_call'] in ['addition_expert_agent', 'multiplication_expert_agent']:\n       agent_to_call = response['agent_to_call']\n\n       return Command(goto=agent_to_call, update={\"messages\": [ AIMessage(content=f\"Successfully transferred to the agent: {agent_to_call}\")]})\n\n   else:\n       return {\"messages\": [AIMessage(content=response[\"message\"])]}\n\n\n# Addition expert agent\ndef addition_expert_agent(state: MessagesState):\n\n    def call_model(state: MessagesState):\n        response = model.bind_tools([add_numbers]).invoke([SystemMessage(content=addition_agent_prompt)] + state['messages'])\n        if len(response.tool_calls) > 0:\n            return Command(goto='tool_node', update={'messages': [response]})\n\n        return Command(update={'messages': [response]})\n\n    tool_node = ToolNode([add_numbers])\n\n    addition_expert_graph = StateGraph(MessagesState)\n    addition_expert_graph.add_node('call_model', call_model)\n    addition_expert_graph.add_node('tool_node', tool_node)\n\n    addition_expert_graph.add_edge(START, 'call_model')\n    addition_expert_graph.add_edge('tool_node', 'call_model')\n\n    return addition_expert_graph.compile()\n\n\n# Multiplication expert agent\ndef multiplication_expert_agent(state: MessagesState):\n    def call_model(state: MessagesState):\n        response = model.bind_tools([add_numbers]).invoke(\n            [SystemMessage(content=multiplication_agent_prompt)] + state['messages'])\n        if len(response.tool_calls) > 0:\n            return Command(goto='tool_node', update={'messages': [response]})\n\n        return Command(update={'messages': [response]})\n\n    tool_node = ToolNode([multiply_numbers])\n\n    multiplication_expert_graph = StateGraph(MessagesState)\n    multiplication_expert_graph.add_node('call_model', call_model)\n    multiplication_expert_graph.add_node('tool_node', tool_node)\n\n    multiplication_expert_graph.add_edge(START, 'call_model')\n    multiplication_expert_graph.add_edge('tool_node', 'call_model')\n\n    return multiplication_expert_graph.compile()\n\n\nsupervisor_graph = StateGraph(MessagesState)\n\n\nsupervisor_graph.add_node('supervisor_node', supervisor_agent)\nsupervisor_graph.add_node('addition_expert_agent', addition_expert_agent)\nsupervisor_graph.add_node('multiplication_expert_agent', multiplication_expert_agent)\n\nsupervisor_graph.add_edge(START, 'supervisor_node')\n\nsupervisor_agent = supervisor_graph.compile()\n\n\n# Utils/configs.py file\n\nfrom typing import TypedDict, Literal, Union\n\nfrom pydantic import Field\n\nsupervisor_agent_prompt = '''You act like a supervisor/receptionist who would transfer the incoming user requests either to the Addition expert(addition_expert_agent) or the Multiplication expert(multiplication_expert_agent).\nYour job is to transfer the incoming requests that's all and if there is no need to transfer to any agent then just simply respond'''\n\naddition_agent_prompt = '''You are an export in adding two numbers and you dont know any except the addition of two numbers.'''\nmultiplication_agent_prompt = '''You are an export in multiplying two numbers and you dont know any except the multiplication of two numbers.'''\n\n\nclass SupervisorResponseStructure(TypedDict):\n    agent_to_call: Union[None, Literal['addition_expert_agent', 'multiplication_expert_agent']] = Field(description=\"The agent that suppose to be called\")\n    message: Union[None, str] =  Field(description=\"The message that needs to be conveyed to the user.\")\nError Message and Stack Trace (if applicable)\n\nDescription\nI have two agents (addition and multiplication experts) and the supervisor agent, once the supervisor hands off to that respective agent, the user question is not sent to that respective agent instead its passing from the supervisor agent.\nExpected behavior: once the supervisor agents hands off to the respective agent then in the next turn the user request should be passed to that respective agent instead of passing through the first node.\n\nSystem Info\nUsing langgraph 0.3.31", "created_at": "2025-04-22", "closed_at": "2025-04-23", "labels": [], "State": "closed", "Author": "Saisiva123"}
{"issue_number": 4376, "issue_title": "Error: KeyError('tool_call_id') within create_react_agent() and create_supervisor() structure", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langchain_core.messages import BaseMessage\nfrom langgraph.graph.message import add_messages\nfrom pydantic import BaseModel, Field\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom typing import (\n    Annotated,\n    Any,\n    Callable,\n    Literal,\n    Optional,\n    Sequence,\n    Type,\n    TypedDict,\n    TypeVar,\n    Union,\n    cast,\n)\n\nimport httpx\nimport pandas as pd\nfrom IPython.display import display\nfrom langchain.output_parsers import PydanticOutputParser\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.messages import HumanMessage, SystemMessage, ToolMessage\nfrom langchain_openai import AzureChatOpenAI\nfrom langgraph_supervisor import create_supervisor\nfrom langgraph.graph import END, START, MessagesState, StateGraph\nfrom langgraph.managed import IsLastStep, RemainingSteps\nfrom langgraph.prebuilt import ToolNode, create_react_agent\n\ntools = [\n    tk.get_fund_attributes,\n    tk.get_fund_holdings, \n    ]\n\nclass CustomState(TypedDict):\n    today: str\n    messages: Annotated[list[BaseMessage], add_messages]\n    fund_holding_filename: str\n    fund_attribute_filename: str\n\nchat_llm_4o = AzureChatOpenAI(\n    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n    openai_api_version = os.getenv(\"AZURE_VERSION\"),\n    deployment_name = \"gpt-4o\",\n    openai_api_key = os.getenv(\"OPENAI_API_KEY\"),\n    http_client = httpx.Client(verify=False),\n    model_kwargs={\n        'parallel_tool_calls': False}\n)\n\n\nprompt = \"You are a helpful FactSet expert, who can pass in a comp list of factset identifiers and get required factset fields saved to a file.\"\n\n\n\nfds_agent = create_react_agent(\n    model=chat_llm_4o,\n    tools=tools,\n    state_schema=CustomState,\n    prompt=prompt,\n    name=\"fds_expert\",\n)\n\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.store.memory import InMemoryStore\n\ncheckpointer = InMemorySaver()\nstore = InMemoryStore()\n\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nworkflow = create_supervisor(\n    [fds_agent],\n    model=llm,\n    prompt=(\n        \"You are a team supervisor managing a fds expert and a merge expert. \"\n        \"Always call merge expert after everything is finished with fds expert. \"\n        \"If there are multiple files to merge, provide two file paths first, then use the intermediate merged filepath to merge with next file. Do not re-use files that is already merged in the last step.\"\n    ),\n    output_mode=\"full_history\",\n    state_schema=CustomStateDF,\n    supervisor_name=\"supervisor\",\n    tools=[tk.python_repl_tool]\n)\n\napp = workflow.compile(\n    checkpointer=checkpointer,\n)\n\ninputs = {\n    \"messages\": [(\"user\", \"\"\"I want latest Vanguard's fund attributes and fund holdings at 13F level of 24Q4. Output one excel file with merged attributes and holdings.\n                  \n                  \"\"\")],\n    \"today\": \"April 14, 2025\",\n}\n\nstate = app.invoke(input=inputs,\n                   config=config)\n\n\n### My tool return would be:\nreturn Command(\n        update={\n            \"fund_holding_filename\": fund_holding_path,\n            \"messages\": [ToolMessage(f\"The fund holding info can be accessed at the Excel file located in the filepath: {fund_holding_path}. Primary key of this dataframe is 'holder_id', foreign key is 'sedol' linking the funds to their holding companies.\")]\n        }\n    )\nError Message and Stack Trace (if applicable)\n================================ Human Message =================================\n\nI want latest Vanguard's fund attributes and fund holdings at 13F level of 24Q4. Output one excel file with merged attributes and holdings.\n                  \n                  \n================================== Ai Message ==================================\nName: supervisor\nTool Calls:\n  transfer_to_fds_expert (call_KasSL9j8RbRQwwAy9OOOLWkn)\n Call ID: call_KasSL9j8RbRQwwAy9OOOLWkn\n  Args:\n================================= Tool Message =================================\nName: transfer_to_fds_expert\n\nSuccessfully transferred to fds_expert\n================================== Ai Message ==================================\nName: fds_expert\nTool Calls:\n  name_mapper (call_Bf0fkSeSEaL9FtFkfUSRkGHX)\n Call ID: call_Bf0fkSeSEaL9FtFkfUSRkGHX\n  Args:\n    query: Give me factset identifier of Vanguard at 13F level\n================================= Tool Message =================================\nName: name_mapper\n\n\"The FactSet identifier for Vanguard at the 13F level is:\\n\\n- **Holder ID**: F72998\\n- **Holder Name**: The Vanguard Group Inc\\n- **Fund Level**: 13F\"\n================================== Ai Message ==================================\nName: fds_expert\nTool Calls:\n  get_fund_attributes (call_pa98KOJOIvBKhLE1wtNokw1n)\n Call ID: call_pa98KOJOIvBKhLE1wtNokw1n\n  Args:\n    all_ids: ['F72998']\n    quarters: ['2024-12-31']\n================================= Tool Message =================================\nName: get_fund_attributes\n\nError: KeyError('tool_call_id')\n Please fix your mistakes.\n================================== Ai Message ==================================\nName: fds_expert\nTool Calls:\n  get_fund_attributes (call_kid0Ag9x2XPEGZT5fdM0Kkto)\n Call ID: call_kid0Ag9x2XPEGZT5fdM0Kkto\n  Args:\n    all_ids: ['F72998']\n    quarters: ['2024-12-31']\n================================= Tool Message =================================\nName: get_fund_attributes\n\nError: KeyError('tool_call_id')\n Please fix your mistakes.\n================================== Ai Message ==================================\nName: fds_expert\n\nIt seems that a technical issue occurred while pulling the fund attributes. Let me try again to address it.\nTool Calls:\n  get_fund_attributes (call_zuw37u3xYcC55vsNBIgwMwdw)\n Call ID: call_zuw37u3xYcC55vsNBIgwMwdw\n  Args:\n    all_ids: ['F72998']\n    quarters: ['2024-12-31']\n================================= Tool Message =================================\nName: get_fund_attributes\n\nError: KeyError('tool_call_id')\n Please fix your mistakes.\n================================== Ai Message ==================================\nName: fds_expert\n\nI encountered a persistent issue in retrieving Vanguard's fund attributes. I\u2019ll now proceed to try fetching the fund holdings.\nTool Calls:\n  get_fund_holdings (call_bjfOJbaBkPlxQGD5GGS385ub)\n Call ID: call_bjfOJbaBkPlxQGD5GGS385ub\n  Args:\n    all_ids: ['F72998']\n    quarters: ['2024-12-31']\n================================= Tool Message =================================\nName: get_fund_holdings\n\nError: KeyError('tool_call_id')\n Please fix your mistakes.\n================================== Ai Message ==================================\nName: fds_expert\n\nIt seems multiple errors are hindering the operations. Could you clarify or provide direction for alternative approaches?\n================================== Ai Message ==================================\nName: fds_expert\n\nTransferring back to supervisor\nTool Calls:\n  transfer_back_to_supervisor (e09eda9b-9e4f-42aa-adca-ad1c1717cc85)\n Call ID: e09eda9b-9e4f-42aa-adca-ad1c1717cc85\n  Args:\n================================= Tool Message =================================\nName: transfer_back_to_supervisor\n\nSuccessfully transferred back to supervisor\n================================== Ai Message ==================================\nName: supervisor\nTool Calls:\n  transfer_to_merge_expert (call_gedAZNqojI2zDJ9cBSq4t7ZP)\n Call ID: call_gedAZNqojI2zDJ9cBSq4t7ZP\n  Args:\n================================= Tool Message =================================\nName: transfer_to_merge_expert\n\nSuccessfully transferred to merge_expert\n================================== Ai Message ==================================\nName: merge_expert\n\nPlease provide the file paths for Vanguard's 'fund attributes' and 'fund holdings' datasets for 24Q4, so I can merge them into one output file.\n================================== Ai Message ==================================\nName: merge_expert\n\nTransferring back to supervisor\nTool Calls:\n  transfer_back_to_supervisor (f15410c0-6ac9-4d95-930e-155915cb10c4)\n Call ID: f15410c0-6ac9-4d95-930e-155915cb10c4\n  Args:\n================================= Tool Message =================================\nName: transfer_back_to_supervisor\n\nSuccessfully transferred back to supervisor\n================================== Ai Message ==================================\nName: supervisor\n\nThe FDS expert faced technical errors when extracting Vanguard's fund attributes and holdings for 24Q4, hence no file paths exist. Would you like me to escalate or try another approach?\nDescription\nKeyerror: \"tool_call_id\" within create_react_agent() and create_supervisor() structure\nSystem Info\nlangchain                                        0.3.19\nlangchain-anthropic                              0.3.8\nlangchain-community                              0.3.18\nlangchain-core                                   0.3.41\nlangchain-experimental                           0.3.4\nlangchain-openai                                 0.3.7\nlangchain-text-splitters                         0.3.6\nlanggraph                                        0.3.5\nlanggraph-checkpoint                             2.0.16\nlanggraph-codeact                                0.1.2\nlanggraph-prebuilt                               0.1.1\nlanggraph-sdk                                    0.1.53\nlanggraph-supervisor                             0.0.6\nlangsmith                                        0.3.11\npydantic                                         2.7.4\npydantic_core                                    2.18.4\npydantic-extra-types                             2.9.0\npydantic-settings                                2.5.2", "created_at": "2025-04-22", "closed_at": "2025-04-23", "labels": ["question"], "State": "closed", "Author": "PANhuihuihuihui"}
{"issue_number": 4372, "issue_title": "Tools returning a Command are missing from messages-streaming", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing import Annotated, Any\n\nfrom langchain_core.messages import ToolMessage\nfrom langchain_core.runnables.config import RunnableConfig\nfrom langchain_core.tools import tool\nfrom langchain_core.tools.base import InjectedToolCallId\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\nfrom langgraph.types import Command\n\nUSER_INFO = [\n    {\"user_id\": \"1\", \"name\": \"Bob Dylan\", \"location\": \"New York, NY\"},\n    {\"user_id\": \"2\", \"name\": \"Taylor Swift\", \"location\": \"Beverly Hills, CA\"},\n]\n\nUSER_ID_TO_USER_INFO = {info[\"user_id\"]: info for info in USER_INFO}\n\n\nclass State(AgentState):\n    # updated by the tool\n    user_info: dict[str, Any]\n\n\ndef main() -> None:\n    @tool\n    def lookup_user_info(tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig):\n        \"\"\"Use this to look up user information to better assist them with their questions.\"\"\"\n        user_id = config.get(\"configurable\", {}).get(\"user_id\")\n        if user_id is None:\n            raise ValueError(\"Please provide user ID\")\n\n        if user_id not in USER_ID_TO_USER_INFO:\n            raise ValueError(f\"User '{user_id}' not found\")\n\n        user_info = USER_ID_TO_USER_INFO[user_id]\n        return Command(\n            update={\n                # update the state keys\n                \"user_info\": user_info,\n                # update the message history\n                \"messages\": [\n                    ToolMessage(\n                        \"Successfully looked up user information\", tool_call_id=tool_call_id\n                    )\n                ],\n            }\n        )\n\n    def prompt(state: State):\n        user_info = state.get(\"user_info\")\n        if user_info is None:\n            return state[\"messages\"]\n\n        system_msg = (\n            f\"User name is {user_info['name']}. User lives in {user_info['location']}\"\n        )\n        return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n\n    model = ChatOpenAI(model=\"gpt-4o\")\n\n    agent = create_react_agent(\n        model,\n        # pass the tool that can update state\n        [lookup_user_info],\n        state_schema=State,\n        # pass dynamic prompt function\n        prompt=prompt,\n    )\n\n    agent_input = {\"messages\": [(\"user\", \"hi, where do I live?\")]}\n    agent_config = {\"configurable\": {\"user_id\": \"1\"}}\n\n    invoke_result = agent.invoke(\n        agent_input,\n        agent_config,\n    )\n\n    # print(invoke_result)\n\n    for chunk in agent.stream(agent_input, agent_config, stream_mode='messages'):\n        print(chunk)\n\n\nif __name__ == '__main__':\n    main()\nError Message and Stack Trace (if applicable)\nThere are no streaming messages from the tool command response.\nDescription\nThe issue is ToolNode returns an array of Command and the fix from #4111 is not properly addressing that.\nThis code works, but it may require more polish:\ndef on_chain_end(\n        self,\n        response: Any,\n        *,\n        run_id: UUID,\n        parent_run_id: Optional[UUID] = None,\n        **kwargs: Any,\n    ) -> Any:\n        if meta := self.metadata.pop(run_id, None):\n            if isinstance(response, Command):\n                response = response.update\n\n            if isinstance(response, Sequence) and any(\n                isinstance(value, Command) for value in response\n            ):\n                response = [\n                    value.update if isinstance(value, Command) else value\n                    for value in response\n                ]\n            def _find_and_emit(value, the_type, try_dir=True):\n                nonlocal recur_count\n                recur_count += 1\n                if recur_count > 100:\n                    raise AssertionError(\"Something is wrong! current value: \" + str(value))\n                if isinstance(value, the_type):\n                    self._emit(meta, value, dedupe=True)\n                elif isinstance(value, Sequence) and not isinstance(value, str):\n                    for item in value:\n                        _find_and_emit(item, the_type)\n                elif isinstance(value, dict):\n                    for item in value.values():\n                        _find_and_emit(item, the_type)\n                elif try_dir and hasattr(value, \"__dir__\") and callable(value.__dir__):\n                    for key in dir(value):\n                        try:\n                            item = getattr(value, key)\n                            _find_and_emit(item, the_type, try_dir=False)\n                        except AttributeError:\n                            pass\n\n            _find_and_emit(response, BaseMessage)\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:16 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6000\nPython Version:  3.11.11 (main, Mar 11 2025, 17:41:13) [Clang 20.1.0 ]\n\nPackage Information\n\nlangchain_core: 0.3.54\nlangchain: 0.3.23\nlangsmith: 0.3.32\nlangchain_anthropic: 0.3.12\nlangchain_groq: 0.3.2\nlangchain_text_splitters: 0.3.8\nlanggraph_sdk: 0.1.61\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nanthropic<1,>=0.49.0: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\ngroq<1,>=0.4.1: Installed. No version info available.\nhttpx: 0.28.1\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-azure-ai;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.49: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.51: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.53: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-perplexity;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.8: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nopenai-agents: Installed. No version info available.\nopentelemetry-api: 1.32.1\nopentelemetry-exporter-otlp-proto-http: 1.32.1\nopentelemetry-sdk: 1.32.1\norjson: 3.10.16\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.11.3\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: Installed. No version info available.\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: 14.0.0\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-04-22", "closed_at": "2025-04-23", "labels": ["bug"], "State": "closed", "Author": "injeniero"}
{"issue_number": 4369, "issue_title": "DOC: <can't render TABLE in API refrence page>", "issue_body": "Issue with current documentation:\nhttps://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.graph.CompiledGraph.astream_events\nin ATTENTION part:\n\nIdea or request for content:\nNo response", "created_at": "2025-04-22", "closed_at": null, "labels": [], "State": "open", "Author": "674019130"}
{"issue_number": 4366, "issue_title": "DOC: <Failed to build document server build-typedoc error>", "issue_body": "Issue with current documentation:\nSo I am trying serve Lnagraph documentation locally on Win10.\nwhen I run make serve-docs I get this:\ncd ../libs/sdk-js && yarn --silent concat-md --decrease-title-levels --ignore=js_ts_sdk_ref.md --start-title-level-at 2 docs > ../../docs/docs/cloud/reference/sdk/js_ts_sdk_ref.md 2>/dev/null The system cannot find the path specified. Makefile:4: recipe for target 'build-typedoc' failed make: *** [build-typedoc] Error 1\nPreviously I had to install yarn and npm manually since I didn't know they are required and the doc didn't bother to mention it (at lest I don't see one). The above error occurred after having done all of that.\nNot sure if it supports Windows at all from the look of it.  dev/null smells linux box.\nIdea or request for content:\nNo response", "created_at": "2025-04-22", "closed_at": null, "labels": [], "State": "open", "Author": "aungkhant0911"}
{"issue_number": 4360, "issue_title": "`test_state_schema_optional_values` throws `PydanticForbiddenQualifier` with latest Pydantic (2.11.1)", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n# N/A, this is a build error\nError Message and Stack Trace (if applicable)\n> FAILED tests/test_state.py::test_state_schema_optional_values[False] - pydantic.errors.PydanticForbiddenQualifier: The annotation 'Required[str]' ...\n       > FAILED tests/test_state.py::test_state_schema_optional_values[True] - pydantic.errors.PydanticForbiddenQualifier: The annotation 'Required[str]' ...\n\n\nin [`test_state.py`:133-152](https://github.com/langchain-ai/langgraph/blob/c7306f7aed68334a4287b4eed9c05444e4b095a6/libs/langgraph/tests/test_state.py#L133-L152)\nDescription\nWhile building langgraph for nixpkgs, the following error appears:\n       > FAILED tests/test_state.py::test_state_schema_optional_values[False] -  pydantic.errors.PydanticForbiddenQualifier: The annotation 'Required[str]' contains the 'typing.Required' type qualifier, which is invalid in the context it is defined.\n       > FAILED tests/test_state.py::test_state_schema_optional_values[True] - pydantic.errors.PydanticForbiddenQualifier: The annotation 'Required[str]' contains the 'typing.Required' type qualifier, which is invalid in the context it is defined.\nProbable cause\nThe affected test code is > 6 months old, so I looked at the dependencies.\nnixpkgs is currently on pydantic 2.11.1 and pydantic-core 2.33.0.\npoetry.lock uses an older pydantic:\n[[package]]\nname = \"pydantic\"\nversion = \"2.9.2\"\ndescription = \"Data validation using Python type hints\"\noptional = false\npython-versions = \">=3.8\"\ngroups = [\"main\", \"dev\"]\nfiles = [\n    {file = \"pydantic-2.9.2-py3-none-any.whl\", hash = \"sha256:f048cec7b26778210e28a0459867920654d48e5e62db0958433636cde4254f12\"},\n    {file = \"pydantic-2.9.2.tar.gz\", hash = \"sha256:d155cef71265d1e9807ed1c32b4c8deec042a44a50a4188b25ac67ecd81a9c0f\"},\n]\n\n\n[package.dependencies]\nannotated-types = \">=0.6.0\"\npydantic-core = \"2.23.4\"\ntyping-extensions = [\n    {version = \">=4.6.1\", markers = \"python_version < \\\"3.13\\\"\"},\n    {version = \">=4.12.2\", markers = \"python_version >= \\\"3.13\\\"\"},\n]\nSystem Info\nn/a", "created_at": "2025-04-21", "closed_at": null, "labels": [], "State": "open", "Author": "sarahec"}
{"issue_number": 4355, "issue_title": "Unable to update state when using `interrupt` for a HITL implementation", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nclass ConceptInputs(TypedDict):\n    case_name: str = None\n    fav_fruit: str = None\n    fav_book: str = None\n    total_budget: int = None\n    additional_instruction: str = None\n    generated_draft: str = None\n\n\n    def remaining_fields(self) -> List[str]:\n        return [field for field, value in self.model_dump().items() \n                if (value in [None, '', []]) \n                and (field not in ['additional_instruction', 'generated_draft'])]\n\n\ndef first_node(state: ConceptInputs):\n    case_name = state['case_name']\n    if not case_name:\n        case_name = interrupt(\n            {\"Please provide a name for the concept case to be created.\"}\n        )\n        print(f\"[human feedback] Case Name: {case_name.title()}\")\n        return Command(update={\"case_name\": case_name}, goto=\"second_node\")\n    \n    remaining_fields = state.remaining_fields()\n    if len(remaining_fields) > 0:\n        field_to_ask = remaining_fields[0]\n        field_value = interrupt(\n            {f\"Please provide a value for {field_to_ask.replace('_',' ').title()}\"}\n        )\n        print(f\"[human feedback] {field_to_ask}: {field_value}\")\n        if len(remaining_fields[1:]) > 0:\n            return Command(update={f\"{field_to_ask}\": {field_value}}, goto=\"second_node\")\n    else:\n        return state\n\n\ndef second_node(state: ConceptInputs):\n    remaining_fields = state.remaining_fields()\n    if len(remaining_fields) > 0:\n        field_to_ask = remaining_fields[0]\n        field_value = interrupt(\n            {f\"Please provide a value for {field_to_ask.replace('_',' ').title()}\"}\n        )\n        print(f\"[human feedback] {field_to_ask}: {field_value}\")\n        if len(remaining_fields[1:]) > 0:\n            return Command(update={f\"{field_to_ask}\": {field_value}}, goto=\"first_node\")\n        else:\n            return Command(update={f\"{field_to_ask}\": {field_value}}, goto=\"generate_node\")\n    else:\n        return state\n\n    \ndef generate_node(state: ConceptInputs):\n    result = \"A quick brown fox jumps over the lazy dog.\"\n    return {\"generate_draft\": result}\n\ngraph_builder = StateGraph(ConceptInputs)\ngraph_builder.add_node(\"first_node\", first_node)\ngraph_builder.add_node(\"second_node\", second_node)\ngraph_builder.add_node(\"generate_node\", generate_node)\n\n# **Define the Flow**\ngraph_builder.add_edge(START, \"first_node\")\ngraph_builder.add_edge(\"first_node\", \"second_node\")\ngraph_builder.add_edge(\"second_node\", \"first_node\")\ngraph_builder.add_edge(\"second_node\", \"generate_node\")\n\ngraph_builder.set_finish_point(\"generate_node\")\n\n# **Enable Interrupt Mechanism**\ncheckpointer = MemorySaver()\ngraph = graph_builder.compile(checkpointer=checkpointer)\n\n# **Thread Configuration**\nthread_config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n\n# **Start the Graph Execution**\ninitial_state = {\n    'case_name': None,\n    'fav_fruit': None,\n    'fav_book': None,\n    'total_budget': None,\n    'additional_instruction': None,\n    'generated_draft': None\n}\n\nprint(initial_state)\n\nfor chunk in graph.stream(initial_state, config=thread_config):\n    for node_id, value in chunk.items():\n        # print(\"-----\")\n        # print(f\"[Node]: {node_id}\")\n        # print(f\"[Output]: {value}\")\n        # print(list(value[0].value)[0])\n\n        # If we reach an interrupt, continuously ask for human feedback\n        if node_id == \"__interrupt__\":\n            while True:\n                user_feedback = input(f\"{list(value[0].value)[0]}\")\n\n                # Resume the graph execution with the user's feedback\n                graph.invoke(Command(resume=user_feedback), config=thread_config)\n\n                # Exit loop if user says \"done\"\n                if user_feedback.lower() == \"done\":\n                    break\nError Message and Stack Trace (if applicable)\nKeyError                                  Traceback (most recent call last)\nCell In[132], line 35\n     32 user_feedback = input(f\"{list(value[0].value)[0]}\")\n     34 # Resume the graph execution with the user's feedback\n---> 35 graph.invoke(Command(resume=user_feedback), config=thread_config)\n     37 # Exit loop if user says \"done\"\n     38 if user_feedback.lower() == \"done\":\n\nFile d:\\WORKSPACE\\.venv1110\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2683, in Pregel.invoke(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\n   2681 else:\n   2682     chunks = []\n-> 2683 for chunk in self.stream(\n   2684     input,\n   2685     config,\n   2686     stream_mode=stream_mode,\n   2687     output_keys=output_keys,\n   2688     interrupt_before=interrupt_before,\n   2689     interrupt_after=interrupt_after,\n   2690     debug=debug,\n   2691     **kwargs,\n   2692 ):\n   2693     if stream_mode == \"values\":\n   2694         latest = chunk\n\nFile d:\\WORKSPACE\\.venv1110\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2331, in Pregel.stream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\n   2325     # Similarly to Bulk Synchronous Parallel / Pregel model\n   2326     # computation proceeds in steps, while there are channel updates.\n   2327     # Channel updates from step N are only visible in step N+1\n   2328     # channels are guaranteed to be immutable for the duration of the step,\n   2329     # with channel updates applied only at the transition between steps.\n   2330     while loop.tick(input_keys=self.input_channels):\n-> 2331         for _ in runner.tick(\n   2332             loop.tasks.values(),\n   2333             timeout=self.step_timeout,\n   2334             retry_policy=self.retry_policy,\n   2335             get_waiter=get_waiter,\n   2336         ):\n   2337             # emit output\n   2338             yield from output()\n   2339 # emit output\n\nFile d:\\WORKSPACE\\.venv1110\\Lib\\site-packages\\langgraph\\pregel\\runner.py:146, in PregelRunner.tick(self, tasks, reraise, timeout, retry_policy, get_waiter)\n    144 t = tasks[0]\n    145 try:\n--> 146     run_with_retry(\n    147         t,\n    148         retry_policy,\n    149         configurable={\n    150             CONFIG_KEY_CALL: partial(\n    151                 _call,\n    152                 weakref.ref(t),\n    153                 retry=retry_policy,\n    154                 futures=weakref.ref(futures),\n    155                 schedule_task=self.schedule_task,\n    156                 submit=self.submit,\n    157                 reraise=reraise,\n    158             ),\n    159         },\n    160     )\n    161     self.commit(t, None)\n    162 except Exception as exc:\n\nFile d:\\WORKSPACE\\.venv1110\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40, in run_with_retry(task, retry_policy, configurable)\n     38     task.writes.clear()\n     39     # run the task\n---> 40     return task.proc.invoke(task.input, config)\n     41 except ParentCommand as exc:\n     42     ns: str = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n\nFile d:\\WORKSPACE\\.venv1110\\Lib\\site-packages\\langgraph\\utils\\runnable.py:606, in RunnableSeq.invoke(self, input, config, **kwargs)\n    602 config = patch_config(\n    603     config, callbacks=run_manager.get_child(f\"seq:step:{i + 1}\")\n    604 )\n    605 if i == 0:\n--> 606     input = step.invoke(input, config, **kwargs)\n    607 else:\n    608     input = step.invoke(input, config)\n\nFile d:\\WORKSPACE\\.venv1110\\Lib\\site-packages\\langgraph\\utils\\runnable.py:371, in RunnableCallable.invoke(self, input, config, **kwargs)\n    369 else:\n    370     with set_config_context(config) as context:\n--> 371         ret = context.run(self.func, *args, **kwargs)\n    372 if isinstance(ret, Runnable) and self.recurse:\n    373     return ret.invoke(input, config)\n\nCell In[130], line 2, in first_node(state)\n      1 def first_node(state: ConceptInputs):\n----> 2     case_name = state['case_name']\n      3     if not case_name:\n      4         case_name = interrupt(\n      5             {\"Please provide a name for the concept case to be created.\"}\n      6         )\n\nKeyError: 'case_name'\nDuring task with name 'first_node' and id '09592458-bf56-3ebf-4a06-64e01b16b385'\nDescription\nI tried a number of tweaks, went through the whole documentation on Human In The Loop multiple times, none of the examples mentioned in the documentation seems to have this issue. In fact this code is very much inspired from one of LangGraph's Officia YT videos.\nI could be wrong in stating this as an issue but please direct me what is it that I'm missing out here. I want to be able to interrogate the user to get a bunch of values basis the keys in my ConceptInputs pydantic model. But it seems the langgraph isn't able to read the pydantic model at all and always returns an empty dict with none of those keys mentioned in the pydantic model.\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.22631\nPython Version:  3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.3.49\nlangchain: 0.3.20\nlangchain_community: 0.3.19\nlangsmith: 0.3.21\nlangchain_chroma: 0.2.2\nlangchain_groq: 0.2.5\nlangchain_huggingface: 0.1.2\nlangchain_ollama: 0.2.3\nlangchain_openai: 0.3.9\nlangchain_text_splitters: 0.3.6\nlanggraph_sdk: 0.1.60\nlanggraph_supervisor: 0.0.15\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nchromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0: Installed. No version info available.\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\ngroq<1,>=0.4.1: Installed. No version info available.\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\nhuggingface-hub: 0.29.3\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.41: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.42: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.45: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.20: Installed. No version info available.\nlanggraph-prebuilt<0.2.0,>=0.1.7: Installed. No version info available.\nlanggraph<0.4.0,>=0.3.5: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<2.0.0,>=1.22.4;: Installed. No version info available.\nnumpy<2.0.0,>=1.26.2;: Installed. No version info available.\nnumpy<3,>=1.26.2: Installed. No version info available.\nollama: 0.4.7\nopenai-agents: Installed. No version info available.\nopenai<2.0.0,>=1.66.3: Installed. No version info available.\nopentelemetry-api: 1.31.0\nopentelemetry-exporter-otlp-proto-http: Installed. No version info available.\nopentelemetry-sdk: 1.31.0\norjson: 3.10.16\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.11.1\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: Installed. No version info available.\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: 13.9.4\nsentence-transformers: 3.4.1\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntokenizers: 0.21.1\ntransformers: 4.50.1\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-04-21", "closed_at": "2025-04-22", "labels": [], "State": "closed", "Author": "amansingh9097"}
{"issue_number": 4353, "issue_title": "InMemoryStore when using LangGraph dev", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nembed_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs={'device': \"cuda\"})\n\nstore = InMemoryStore(\n    index={\"embed\": embed_model}\n)\n\nmanage_memory_tool = create_manage_memory_tool(\n    namespace=(\n        \"email_assistant\", \n        \"{langgraph_user_id}\",\n        \"collection\"\n    )\n)\nsearch_memory_tool = create_search_memory_tool(\n    namespace=(\n        \"email_assistant\",\n        \"{langgraph_user_id}\",\n        \"collection\"\n    )\n)\n\ntools= [\n    write_email, \n    schedule_meeting,\n    check_calendar_availability,\n    manage_memory_tool,\n    search_memory_tool\n]\nresponse_agent = create_react_agent(\n    llm,\n    tools=tools,\n    prompt=create_prompt,\n    # Use this to ensure the store is passed to the agent \n    store=store\n)\nError Message and Stack Trace (if applicable)\nFile \"/home/aptikal/abdalfar/InterviewSim/email-agent/.venv/lib/python3.11/site-packages/langgraph_api/graph.py\", line 455, in _graph_from_spec\n    raise ValueError(\nValueError: Heads up! Your graph 'email_agent' from './src/agent/graph.py' includes a custom store (type <class 'langgraph.store.memory.InMemoryStore'>). With LangGraph API, persistence is handled automatically by the platform, so providing a custom store (type <class 'langgraph.store.memory.InMemoryStore'>) here isn't necessary and will be ignored when deployed.\n\nTo simplify your setup and use the built-in persistence, please remove the custom store (type <class 'langgraph.store.memory.InMemoryStore'>) from your graph definition. If you are looking to customize which postgres database to connect to, please set the `POSTGRES_URI` environment variable. See https://langchain-ai.github.io/langgraph/cloud/reference/env_var/#postgres_uri_custom for more details.\nDescription\nI'm trying to create an email agent that use this function :\nfrom langgraph.store.memory import InMemoryStore \nI'm getting this error when doing langgraph dev, which supposed to run the code locally and not using langgraph cloud api as mentioned in documentation :\nThe langgraph dev command starts a lightweight development server that requires no Docker installation. This server is ideal for rapid development and testing, with features like:\n\nHot reloading: Changes to your code are automatically detected and reloaded\nDebugger support: Attach your IDE's debugger for line-by-line debugging\n- In-memory state with local persistence: Server state is stored in memory for speed but persisted locally between restarts\n\nBut i'm getting this error saying i need to use Postgres database instead of InMemoryStore,although in other projects i implemnted the same function and worked\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP PREEMPT_DYNAMIC Debian 6.1.112-1 (2024-09-30)\nPython Version:  3.11.2 (main, Nov 30 2024, 21:22:50) [GCC 12.2.0]\n\nPackage Information\n\nlangchain_core: 0.3.47\nlangchain: 0.3.21\nlangsmith: 0.3.18\nlangchain_anthropic: 0.3.10\nlangchain_huggingface: 0.1.2\nlangchain_ollama: 0.3.0\nlangchain_openai: 0.3.9\nlangchain_text_splitters: 0.3.7\nlanggraph_api: 0.1.9\nlanggraph_cli: 0.2.5\nlanggraph_license: Installed. No version info available.\nlanggraph_runtime: Installed. No version info available.\nlanggraph_runtime_inmem: 0.0.4\nlanggraph_sdk: 0.1.61\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nanthropic<1,>=0.49.0: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nblockbuster: 1.5.24\nclick: 8.1.8\ncloudpickle: 3.1.1\ncryptography: 44.0.2\nhttpx: 0.28.1\nhuggingface-hub: 0.30.2\njsonpatch<2.0,>=1.33: Installed. No version info available.\njsonschema-rs: 0.29.1\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-azure-ai;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.45: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.47: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.7: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlanggraph: 0.3.18\nlanggraph-checkpoint: 2.0.24\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nollama<1,>=0.4.4: Installed. No version info available.\nopenai-agents: Installed. No version info available.\nopenai<2.0.0,>=1.66.3: Installed. No version info available.\nopentelemetry-api: Installed. No version info available.\nopentelemetry-exporter-otlp-proto-http: Installed. No version info available.\nopentelemetry-sdk: Installed. No version info available.\norjson: 3.10.15\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npyjwt: 2.10.1\npytest: Installed. No version info available.\npython-dotenv: 1.1.0\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: Installed. No version info available.\nsentence-transformers: 4.1.0\nSQLAlchemy<3,>=1.4: Installed. No version info available.\nsse-starlette: 2.1.3\nstarlette: 0.46.2\nstructlog: 25.2.0\ntenacity: 9.0.0\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntokenizers: 0.21.1\ntransformers: 4.51.3\ntyping-extensions>=4.7: Installed. No version info available.\nuvicorn: 0.34.2\nwatchfiles: 1.0.5\nzstandard: 0.23.0\n", "created_at": "2025-04-21", "closed_at": "2025-04-21", "labels": [], "State": "closed", "Author": "farouk09"}
{"issue_number": 4351, "issue_title": "LANGCHAIN ACADEMY CERTIFICAT", "issue_body": "Issue with current documentation:\nI am writing to kindly request a correction on the course certificate issued for the 'Introduction to langgraph' completed on 19/04/2025. The name of the participant has been incorrectly listed as [GOOGLE USER], whereas the correct full name is [Mohamed Aziz Derbel].\nI would appreciate it if you could issue an updated certificate reflecting the accurate information. Please let me know if you require any supporting documentation or identification to proceed with the correction.\nIdea or request for content:\nMore details:\n. Correct full name: Mohamed Aziz Derbel\n. The course name: Introduction to LangGraph\n. The completion date: 19/04/2025\n. The current name on the certificate: Google User\n. CERTIFICAT ID: h7vosd3ox9", "created_at": "2025-04-20", "closed_at": "2025-04-21", "labels": [], "State": "closed", "Author": "azizbtk"}
{"issue_number": 4343, "issue_title": "LangGraph PostgresStore bug: index `fields` parameter ignored due to incorrect naming", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport os\nfrom langgraph.store.postgres import PostgresStore\nfrom langgraph.store.postgres.base import PostgresIndexConfig\nfrom langgraph.store.base import PutOp\nfrom langchain_core.embeddings import FakeEmbeddings\n\nsize = 1024\nembeddings = FakeEmbeddings(size=size)\n\ndb_string = os.getenv(\"DB_CONN_STRING\")\nassert db_string is not None, \"DB_CONN_STRING environment variable is not set\"\n\nindex: PostgresIndexConfig = {\n    \"dims\": size,\n    \"embed\": embeddings,\n    \"fields\": [\"text\"], # <--- Ignored!\n    #\"text_fields\": [\"text\"], # <--- Correct behaviour, but mismatch with `PostgresIndexConfig` type.\n}\n\n\nwith PostgresStore.from_conn_string(\n    db_string,\n    index=index,\n) as store:\n    print(store.index_config)\n\n    preps = store._prepare_batch_PUT_queries(\n        [\n            (\n                1,\n                PutOp(\n                    namespace=(\"documents\", \"user123\"),\n                    key=\"report1\",\n                    value={\n                        \"text\": \"hello world\",\n                        \"metadata\": {\"id\": \"1\"},\n                        \"user_ids\": [\"1\"],\n                    },\n                ),\n            )\n        ]\n    )\n    for _prep in preps:\n        for prep in _prep:\n            print(\"=\" * 20)\n            print(prep)\nError Message and Stack Trace (if applicable)\n\nDescription\nI am using the langgraph.store.postgres.base.PostgresStore implementation and noticed that all fields were embedded, not just the fields I specified in the IndexConfig.\nAfter digging into the code, I found this:\ndef _ensure_index_config(\n    index_config: PostgresIndexConfig,\n) -> tuple[Optional[\"Embeddings\"], PostgresIndexConfig]:\n    index_config = index_config.copy()\n    tokenized: list[tuple[str, Union[Literal[\"$\"], list[str]]]] = []\n    tot = 0\n    text_fields = index_config.get(\"text_fields\") or [\"$\"]\n\nNote that the property \"text_fields\", rather than \"fields\" is extracted from the index_config.\nHowever, the IndexConfig class does not have a property \"text_fields\", but \"fields\" instead.\nThis mismatch of the typing and documentation should be resolved.\nSystem Info\nuv pip show langgraph-checkpoint-postgres\nName: langgraph-checkpoint-postgres\nVersion: 2.0.19\nLocation: /Users/vincent.min/Projects/akgentic-framework/.venv/lib/python3.12/site-packages\nRequires: langgraph-checkpoint, orjson, psycopg, psycopg-pool\nRequired-by: tools", "created_at": "2025-04-18", "closed_at": "2025-04-18", "labels": [], "State": "closed", "Author": "VMinB12"}
{"issue_number": 4331, "issue_title": "The root layer is missing an `__init__.py` file", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nDescription\nAs stated in the title, specifically, this directory is missing an __init__.py file.\n\nIs there any particular reason we can't add it?\nI accidentally discovered today that missing it causes some VSCode extensions to skip analyzing the contents of this library.\nI also found that this project recursively contains py.typed files.\nAs far as I know, it should only be needed in the root directory. At least, langchain_core only has it in the root directory.\nCould this be related to the missing __init__.py in the root directory?\nSystem Info\nlanggraph==0.3.31", "created_at": "2025-04-17", "closed_at": "2025-04-17", "labels": [], "State": "closed", "Author": "gbaian10"}
{"issue_number": 4322, "issue_title": "Pandas Dataframe not handled when checkpointer is used", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\nfrom langgraph.checkpoint.memory import MemorySaver\n\nsample_df = pd.DataFrame({\"foo\": [1, 2, 3]})\n\n# Define subgraph\nclass SubgraphState(BaseModel):\n    model_config = {\"arbitrary_types_allowed\": True, \"use_enum_values\": True}\n\n    df: Dict[str, pd.DataFrame]\n\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\n        \"df\": {\"foo\": sample_df},\n    }\n\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\n\nsubgraph_builder.add_node(subgraph_node_1)\n\nsubgraph = subgraph_builder.compile()\n\n\n# Define parent graph\nclass ParentState(BaseModel):\n    model_config = {\"arbitrary_types_allowed\": True, \"use_enum_values\": True}\n\n    df: Dict[str, pd.DataFrame] = Field(default_factory=dict)\n\n\nbuilder = StateGraph(ParentState)\nbuilder.add_edge(START, \"subgraph\")\n\nbuilder.add_node(\"subgraph\", subgraph)\n\ngraph = builder.compile(checkpointer=MemorySaver())\n\nresult = graph.invoke(\n    {}, config={\"configurable\": {\"thread_id\": \"testing\"}}\n)\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[20], line 1\n----> 1 result = graph.invoke(\n      2     {\"foo\": \"world!\"}, config={\"configurable\": {\"thread_id\": \"testing\"}}\n      3 )\n\nFile ~/Documents/PlotCode/plot-social-listening-functions/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:2669, in Pregel.invoke(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\n   2667 else:\n   2668     chunks = []\n-> 2669 for chunk in self.stream(\n   2670     input,\n   2671     config,\n   2672     stream_mode=stream_mode,\n   2673     output_keys=output_keys,\n   2674     interrupt_before=interrupt_before,\n   2675     interrupt_after=interrupt_after,\n   2676     debug=debug,\n   2677     **kwargs,\n   2678 ):\n   2679     if stream_mode == \"values\":\n   2680         latest = chunk\n\nFile ~/Documents/PlotCode/plot-social-listening-functions/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:2323, in Pregel.stream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\n   2317     # Similarly to Bulk Synchronous Parallel / Pregel model\n   2318     # computation proceeds in steps, while there are channel updates.\n   2319     # Channel updates from step N are only visible in step N+1\n   2320     # channels are guaranteed to be immutable for the duration of the step,\n   2321     # with channel updates applied only at the transition between steps.\n   2322     while loop.tick(input_keys=self.input_channels):\n-> 2323         for _ in runner.tick(\n   2324             loop.tasks.values(),\n   2325             timeout=self.step_timeout,\n   2326             retry_policy=self.retry_policy,\n   2327             get_waiter=get_waiter,\n   2328         ):\n   2329             # emit output\n   2330             yield from output()\n   2331 # emit output\n\nFile ~/Documents/PlotCode/plot-social-listening-functions/.venv/lib/python3.10/site-packages/langgraph/pregel/runner.py:146, in PregelRunner.tick(self, tasks, reraise, timeout, retry_policy, get_waiter)\n    144 t = tasks[0]\n    145 try:\n--> 146     run_with_retry(\n    147         t,\n    148         retry_policy,\n    149         configurable={\n    150             CONFIG_KEY_CALL: partial(\n    151                 _call,\n    152                 weakref.ref(t),\n    153                 retry=retry_policy,\n    154                 futures=weakref.ref(futures),\n    155                 schedule_task=self.schedule_task,\n    156                 submit=self.submit,\n    157                 reraise=reraise,\n    158             ),\n    159         },\n    160     )\n    161     self.commit(t, None)\n    162 except Exception as exc:\n\nFile ~/Documents/PlotCode/plot-social-listening-functions/.venv/lib/python3.10/site-packages/langgraph/pregel/retry.py:40, in run_with_retry(task, retry_policy, configurable)\n     38     task.writes.clear()\n     39     # run the task\n---> 40     return task.proc.invoke(task.input, config)\n     41 except ParentCommand as exc:\n     42     ns: str = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n\nFile ~/Documents/PlotCode/plot-social-listening-functions/.venv/lib/python3.10/site-packages/langgraph/utils/runnable.py:600, in RunnableSeq.invoke(self, input, config, **kwargs)\n    596 config = patch_config(\n    597     config, callbacks=run_manager.get_child(f\"seq:step:{i + 1}\")\n    598 )\n    599 if i == 0:\n--> 600     input = step.invoke(input, config, **kwargs)\n    601 else:\n    602     input = step.invoke(input, config)\n\nFile ~/Documents/PlotCode/plot-social-listening-functions/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:2669, in Pregel.invoke(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\n   2667 else:\n   2668     chunks = []\n-> 2669 for chunk in self.stream(\n   2670     input,\n   2671     config,\n   2672     stream_mode=stream_mode,\n   2673     output_keys=output_keys,\n   2674     interrupt_before=interrupt_before,\n   2675     interrupt_after=interrupt_after,\n   2676     debug=debug,\n   2677     **kwargs,\n   2678 ):\n   2679     if stream_mode == \"values\":\n   2680         latest = chunk\n\nFile ~/Documents/PlotCode/plot-social-listening-functions/.venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:2264, in Pregel.stream(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\n   2260 if \"custom\" in stream_modes:\n   2261     config[CONF][CONFIG_KEY_STREAM_WRITER] = lambda c: stream.put(\n   2262         ((), \"custom\", c)\n   2263     )\n-> 2264 with SyncPregelLoop(\n   2265     input,\n   2266     input_model=self.input_model,\n   2267     stream=StreamProtocol(stream.put, stream_modes),\n   2268     config=config,\n   2269     store=store,\n   2270     checkpointer=checkpointer,\n   2271     nodes=self.nodes,\n   2272     specs=self.channels,\n   2273     output_keys=output_keys,\n   2274     stream_keys=self.stream_channels_asis,\n   2275     interrupt_before=interrupt_before_,\n   2276     interrupt_after=interrupt_after_,\n   2277     manager=run_manager,\n   2278     debug=debug,\n   2279 ) as loop:\n   2280     # create runner\n   2281     runner = PregelRunner(\n   2282         submit=config[CONF].get(\n   2283             CONFIG_KEY_RUNNER_SUBMIT, weakref.WeakMethod(loop.submit)\n   (...)\n   2287         node_finished=config[CONF].get(CONFIG_KEY_NODE_FINISHED),\n   2288     )\n   2289     # enable subgraph streaming\n\nFile ~/Documents/PlotCode/plot-social-listening-functions/.venv/lib/python3.10/site-packages/langgraph/pregel/loop.py:1007, in SyncPregelLoop.__exit__(self, exc_type, exc_value, traceback)\n   1000 def __exit__(\n   1001     self,\n   1002     exc_type: Optional[Type[BaseException]],\n   (...)\n   1005 ) -> Optional[bool]:\n   1006     # unwind stack\n-> 1007     return self.stack.__exit__(exc_type, exc_value, traceback)\n\nFile /opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py:576, in ExitStack.__exit__(self, *exc_details)\n    572 try:\n    573     # bare \"raise exc_details[1]\" replaces our carefully\n    574     # set-up context\n    575     fixed_ctx = exc_details[1].__context__\n--> 576     raise exc_details[1]\n    577 except BaseException:\n    578     exc_details[1].__context__ = fixed_ctx\n\nFile /opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/contextlib.py:561, in ExitStack.__exit__(self, *exc_details)\n    559 assert is_sync\n    560 try:\n--> 561     if cb(*exc_details):\n    562         suppressed_exc = True\n    563         pending_raise = False\n\nFile ~/Documents/PlotCode/plot-social-listening-functions/.venv/lib/python3.10/site-packages/langgraph/pregel/executor.py:120, in BackgroundExecutor.__exit__(self, exc_type, exc_value, traceback)\n    118     continue\n    119 try:\n--> 120     task.result()\n    121 except concurrent.futures.CancelledError:\n    122     pass\n\nFile /opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:451, in Future.result(self, timeout)\n    449     raise CancelledError()\n    450 elif self._state == FINISHED:\n--> 451     return self.__get_result()\n    453 self._condition.wait(timeout)\n    455 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\nFile /opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:403, in Future.__get_result(self)\n    401 if self._exception:\n    402     try:\n--> 403         raise self._exception\n    404     finally:\n    405         # Break a reference cycle with the exception in self._exception\n    406         self = None\n\nFile ~/Documents/PlotCode/plot-social-listening-functions/.venv/lib/python3.10/site-packages/langgraph/pregel/executor.py:83, in BackgroundExecutor.done(self, task)\n     81 \"\"\"Remove the task from the tasks dict when it's done.\"\"\"\n     82 try:\n---> 83     task.result()\n     84 except GraphBubbleUp:\n     85     # This exception is an interruption signal, not an error\n     86     # so we don't want to re-raise it on exit\n     87     self.tasks.pop(task)\n\nFile /opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:451, in Future.result(self, timeout)\n    449     raise CancelledError()\n    450 elif self._state == FINISHED:\n--> 451     return self.__get_result()\n    453 self._condition.wait(timeout)\n    455 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\nFile /opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:403, in Future.__get_result(self)\n    401 if self._exception:\n    402     try:\n--> 403         raise self._exception\n    404     finally:\n    405         # Break a reference cycle with the exception in self._exception\n    406         self = None\n\nFile /opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/thread.py:58, in _WorkItem.run(self)\n     55     return\n     57 try:\n---> 58     result = self.fn(*self.args, **self.kwargs)\n     59 except BaseException as exc:\n     60     self.future.set_exception(exc)\n\nFile ~/Documents/PlotCode/plot-social-listening-functions/.venv/lib/python3.10/site-packages/langgraph/checkpoint/memory/__init__.py:448, in InMemorySaver.put_writes(self, config, writes, task_id, task_path)\n    442 if inner_key[1] >= 0 and outer_writes_ and inner_key in outer_writes_:\n    443     continue\n    445 self.writes[outer_key][inner_key] = (\n    446     task_id,\n    447     c,\n--> 448     self.serde.dumps_typed(v),\n    449     task_path,\n    450 )\n\nFile ~/Documents/PlotCode/plot-social-listening-functions/.venv/lib/python3.10/site-packages/langgraph/checkpoint/serde/jsonplus.py:207, in JsonPlusSerializer.dumps_typed(self, obj)\n    205 if \"valid UTF-8\" in str(exc):\n    206     return \"json\", self.dumps(obj)\n--> 207 raise exc\n\nFile ~/Documents/PlotCode/plot-social-listening-functions/.venv/lib/python3.10/site-packages/langgraph/checkpoint/serde/jsonplus.py:203, in JsonPlusSerializer.dumps_typed(self, obj)\n    201 else:\n    202     try:\n--> 203         return \"msgpack\", _msgpack_enc(obj)\n    204     except ormsgpack.MsgpackEncodeError as exc:\n    205         if \"valid UTF-8\" in str(exc):\n\nFile ~/Documents/PlotCode/plot-social-listening-functions/.venv/lib/python3.10/site-packages/langgraph/checkpoint/serde/jsonplus.py:623, in _msgpack_enc(data)\n    622 def _msgpack_enc(data: Any) -> bytes:\n--> 623     return ormsgpack.packb(data, default=_msgpack_default, option=_option)\n\nTypeError: Type is not msgpack serializable: DataFrame\nDescription\nTry running this notebook:\n\nwith checkpoint -> You'll get an error\nwithout checkpoint -> Works fine\n\nSystem Info\nlangchain==0.3.21\nlangchain-community==0.3.20\nlangchain-core==0.3.49\nlangchain-text-splitters==0.3.7\nlanggraph==0.3.18\nlanggraph-checkpoint==2.0.23\nlanggraph-prebuilt==0.1.4\nlanggraph-sdk==0.1.58", "created_at": "2025-04-17", "closed_at": "2025-04-17", "labels": [], "State": "closed", "Author": "minki-j"}
{"issue_number": 4316, "issue_title": "SchemaCoercionMapper has incorrect deserialization behavior.", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing import List, Generic, TypeVar, Optional, Annotated, Union, Literal\n\nfrom langchain_core.messages import AnyMessage, HumanMessage, AIMessage\nfrom pydantic import BaseModel, Tag, Field\nfrom langgraph.graph.schema_utils import SchemaCoercionMapper\n\n\n# generic\nclass MyMessage(BaseModel):\n    msg: List[AnyMessage]\n\n\ndata = {\n    \"msg\": [\n        {\n            \"type\": \"human\",\n            \"content\": \"Hello\"\n        },\n        {\n            \"type\": \"ai\",\n            \"content\": \"Hi there!\"\n        }\n    ]\n}\nMyMessage.model_validate(data)\nmapper = SchemaCoercionMapper(MyMessage)\nresult = mapper(data)\nassert isinstance(result, MyMessage)\nassert isinstance(result.msg, list)\nassert len(result.msg) == 2\nassert isinstance(result.msg[0], (HumanMessage))\nassert isinstance(result.msg[1], (AIMessage))\n\nT = TypeVar(\"T\")\n\n\nclass Dog(BaseModel):\n    type: Literal[\"dog\"]\n    age: int\n\n\nclass Cat(BaseModel):\n    type: Literal[\"cat\"]\n    name: str\n\n\n# ==== Annotated + Tag + discriminator ====\nTaggedPet = Annotated[\n    Union[\n        Annotated[Dog, Tag(tag=\"dog\")],\n        Annotated[Cat, Tag(tag=\"cat\")],\n    ],\n    Field(discriminator=\"type\"),\n]\n\n\nclass Box(BaseModel, Generic[T]):\n    content: Optional[T]\n\n\nclass Crate(BaseModel, Generic[T]):\n    payload: Box[T]\n\n\n# generic and polymorphic\nclass Warehouse(BaseModel):\n    cage: Crate[TaggedPet]\n\n\ndata2 = {\n    \"cage\": {\n        \"payload\": {\n            \"content\": {\n                \"type\": \"dog\",\n                \"age\": 8\n            }\n        }\n    }\n}\n\n# Pydantic v2 is ok\nassert isinstance(Warehouse.model_validate(data2).cage.payload.content, Dog)\n\n\nmapper2 = SchemaCoercionMapper(Warehouse)\nresult2 = mapper2(data2)\n\nassert isinstance(result2.cage.payload.content, Dog)\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\n  File \"/Users/baishanglin/pycharmProjects/beike/new_workspace/test_mark_trunk.py\", line 27, in <module>\n    result = mapper(data)\n             ^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/envs/new_workspace/lib/python3.11/site-packages/langgraph/graph/schema_utils.py\", line 87, in __call__\n    return self.coerce(input_data, depth)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/envs/new_workspace/lib/python3.11/site-packages/langgraph/graph/schema_utils.py\", line 103, in coerce\n    processed[k] = fn(v, depth - 1) if fn else v\n                   ^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/envs/new_workspace/lib/python3.11/site-packages/langgraph/graph/schema_utils.py\", line 144, in list_coercer\n    return [sub(x, d - 1) for x in v]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/envs/new_workspace/lib/python3.11/site-packages/langgraph/graph/schema_utils.py\", line 144, in <listcomp>\n    return [sub(x, d - 1) for x in v]\n            ^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/envs/new_workspace/lib/python3.11/site-packages/langgraph/graph/schema_utils.py\", line 219, in union_coercer\n    return sp(v, d - 1)\n           ^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/envs/new_workspace/lib/python3.11/site-packages/langgraph/graph/schema_utils.py\", line 133, in <lambda>\n    return lambda v, d: mapper.coerce(v, d) if isinstance(v, dict) else v\n                        ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/envs/new_workspace/lib/python3.11/site-packages/langgraph/graph/schema_utils.py\", line 103, in coerce\n    processed[k] = fn(v, depth - 1) if fn else v\n                   ^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/envs/new_workspace/lib/python3.11/site-packages/langgraph/graph/schema_utils.py\", line 229, in <lambda>\n    return lambda v, _d: adapter_fn(v)\n                         ^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/envs/new_workspace/lib/python3.11/site-packages/pydantic/type_adapter.py\", line 412, in validate_python\n    return self.validator.validate_python(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 1 validation error for literal['ai']\n  Input should be 'ai' [type=literal_error, input_value='human', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.10/v/literal_error\nDescription\nWhen I used Pydantic to store state, I encountered a strange issue. Starting from version 0.3.27, the polymorphic type I implemented (modeled after AnyMessage) could no longer be correctly deserialized \u2014 the value under the state key was stored as a raw dictionary instead of the expected type. After upgrading to version 0.3.30, this issue turned into the aforementioned error. Upon investigation, I found that the root cause was that SchemaCoercionMapper failed to correctly handle generics.\nI submitted a PR to fix this issue. #4317\nSystem Info\nenv:   python v3.11.11  Conda\nlanggraph                                0.3.30\nlangchain                                0.3.19\nlangchain-core                           0.3.37\npydantic                                 2.10.6\npydantic_core                            2.27.2\npydantic-settings                        2.7.1", "created_at": "2025-04-17", "closed_at": null, "labels": [], "State": "open", "Author": "littlebai3618"}
{"issue_number": 4313, "issue_title": "Orchestrator sends lengthy input to tool causing json.decoder.JSONDecodeError", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nclass ToolInput(BaseModel):\n    query: str \n    class Config:\n        extra = \"allow\"\n\n@tool(\"CodingAgent\", args_schema=ToolInput, return_direct=True)\n@log_calls\nasync def process_query(query: str):\n   pass\nError Message and Stack Trace (if applicable)\n\nDescription\nHi community,\nI have been playing with orchestrator with function calling type agentic workflow. There are cases where orchestrator needs to send lengthy input to a given tool, and that causes Json decoding issue. My understanding is when orchestrator is doing function calling, it's basically generating a json which includes what tool to call and what inputs to the tool. When it comes to lengthy input (e.g., user might send a lengthy code to orchestrator asking for explanation or correction, and it passes it to a coding agent), the input could be missing \" or } in the end etc., causing the error - json.decoder.JSONDecodeError: Unterminated string starting at.\nAny suggestion please?\nThanks.\nSystem Info\n\"langchain<0.4,>=0.3.9\",\n\"langchain-community<0.4,>=0.3.9\",\n\"langchain-core<0.4,>=0.3\",\n\"langchain-openai<0.3,>=0.2.11\",\n\"langgraph<0.3,>=0.2.56\",", "created_at": "2025-04-17", "closed_at": null, "labels": ["invalid"], "State": "open", "Author": "haolx"}
{"issue_number": 4305, "issue_title": "Inconsistent reducer behavior for Optional, nullable and regular data type", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing import Annotated, Optional\n\nfrom pydantic import BaseModel\nfrom langgraph.graph import StateGraph, START\n\ndef reducer(current, update):\n    return update(current)\n\nadd_one = lambda x: x + 1\n\nclass State1(BaseModel):\n    foo: Annotated[int, reducer] = 1\n    boo: Annotated[int, reducer] = 1\n\nclass State2(BaseModel):\n    foo: Annotated[Optional[int], reducer] = 1\n    boo: Annotated[int, reducer] = 1\n\nclass State3(BaseModel):\n    foo: Annotated[int | None, reducer] = 1\n    boo: Annotated[int, reducer] = 1\n\ndef my_node(state):\n    return {\n        \"foo\": add_one,\n        \"boo\": add_one,\n    }\n\ngraph = (\n    StateGraph(State1)\n    .add_node(my_node)\n    .add_edge(START, \"my_node\")\n    .compile()\n)\n\ngraph = (\n    StateGraph(State1)\n    .add_node(my_node)\n    .add_edge(START, \"my_node\")\n    .compile()\n)\n\ninput_state = {}\nprint(f\"The state 1 output: {graph.invoke(input_state)}\")\nprint(f\"The reducer address: {reducer}\")\nprint(f\"The update lambda address: {add_one}\")\n\ngraph = (\n    StateGraph(State2)\n    .add_node(my_node)\n    .add_edge(START, \"my_node\")\n    .compile()\n)\n\ninput_state = {}\nprint(f\"The state 2 output: {graph.invoke(input_state)}\")\nprint(f\"The reducer address: {reducer}\")\nprint(f\"The update lambda address: {add_one}\")\n\ngraph = (\n    StateGraph(State3)\n    .add_node(my_node)\n    .add_edge(START, \"my_node\")\n    .compile()\n)\n\ninput_state = {}\nprint(f\"The state 3 output: {graph.invoke(input_state)}\")\nprint(f\"The reducer address: {reducer}\")\nprint(f\"The update lambda address: {add_one}\")\nError Message and Stack Trace (if applicable)\n\nDescription\nLangGraph ver 0.3.30\nObserved inconsistent reducer behavior on types with different annotations. See the example code above. User defined a custom reducer that takes in a lambda to update the current value of the state.\n\n\n\ndescription\nbehavior\n\n\n\n\nState1: all fields are normal.\npydantic default value ignored. Reducer applied\n\n\nState2: foo is optional\nReducer not applied, raw lambda passed back\n\n\nState3: foo is nullable\nReducer not applied, raw lambda passed back\n\n\n\nAdditional observation\nIf adding a no-op node after my_node, you will observe different behavior between State2 and State3:\ndef no_op(state):\n    return {}\n\ngraph = (\n    StateGraph(State3)\n    .add_node(my_node)\n    .add_edge(START, \"my_node\")\n    .add_node(no_op)\n    .add_edge(\"my_node\", \"no_op\")\n    .compile()\n)\n\n\n\ndescription\nbehavior\n\n\n\n\nState2: foo is optional\nReducer not applied, raw lambda passed back\n\n\nState3: foo is nullable\nGet error. Pydantic validation error\n\n\n\nIf value is set in input_state for example\ninput_state = {\n    \"foo\": 1\n}\nThen you will also observe different behaviors for all three types.\n\n\n\ndescription\nbehavior\n\n\n\n\nState1: all fields are normal\nReducer called on int. Error\n\n\nState2: foo is optional\nworks as expected, default value used\n\n\nState3: foo is nullable\nworks as expected, default value used\n\n\n\nSystem Info\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.1.0: Thu Oct 10 21:02:26 PDT 2024; root:xnu-11215.41.3~2/RELEASE_ARM64_T8122\nPython Version:  3.11.11 (main, Dec  3 2024, 17:20:40) [Clang 16.0.0 (clang-1600.0.26.4)]\n\nPackage Information\n\nlangchain_core: 0.3.44\nlangsmith: 0.3.13\nlanggraph_sdk: 0.1.55\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nhttpx: 0.28.1\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\norjson: 3.10.15\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: Installed. No version info available.\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrich: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-04-16", "closed_at": null, "labels": [], "State": "open", "Author": "xiangyuwang-mai"}
{"issue_number": 4294, "issue_title": "Raised exceptions in @task doesn't get caught", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport asyncio\nimport logging\n\nfrom langgraph.func import entrypoint\nfrom langgraph.func import task\n\n\n@task()\nasync def my_task(number: int):\n    await asyncio.sleep(1)\n    return number * 2\n\n\n@task()\nasync def task_with_exception(number: int):\n    await asyncio.sleep(1)\n    raise Exception(\"This is a test exception\")\n\n\n@entrypoint()\nasync def my_workflow(number: int):\n    await my_task(number)\n    try:\n        await task_with_exception(number)\n    except Exception as e:\n        logging.error(f\"Error in task_with_exception: {e}\")\n    await my_task(number)\n    return \"done\"\n\n\nasync def main():\n    await my_workflow.ainvoke(1)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nError Message and Stack Trace (if applicable)\nERROR:root:Error in task_with_exception: This is a test exception\nTraceback (most recent call last):\n  File \"/Users/ahmadilaiwi/Documents/ilaiwi/WebApp/apps/codegen2/src/components/shared/types/tests/main.py\", line 36, in <module>\n    asyncio.run(main())\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/runners.py\", line 195, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 691, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/Users/ahmadilaiwi/Documents/ilaiwi/WebApp/apps/codegen2/src/components/shared/types/tests/main.py\", line 32, in main\n    await my_workflow.ainvoke(1)\n  File \"/Users/ahmadilaiwi/Documents/ilaiwi/WebApp/apps/codegen2/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 2773, in ainvoke\n    async for chunk in self.astream(\n  File \"/Users/ahmadilaiwi/Documents/ilaiwi/WebApp/apps/codegen2/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 2655, in astream\n    async for _ in runner.atick(\n  File \"/Users/ahmadilaiwi/Documents/ilaiwi/WebApp/apps/codegen2/src/components/shared/types/tests/main.py\", line 24, in my_workflow\n    await task_with_exception(number)\n  File \"/Users/ahmadilaiwi/Documents/ilaiwi/WebApp/apps/codegen2/.venv/lib/python3.12/site-packages/langgraph/pregel/retry.py\", line 127, in arun_with_retry\n    return await task.proc.ainvoke(task.input, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ahmadilaiwi/Documents/ilaiwi/WebApp/apps/codegen2/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py\", line 671, in ainvoke\n    input = await asyncio.create_task(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ahmadilaiwi/Documents/ilaiwi/WebApp/apps/codegen2/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py\", line 439, in ainvoke\n    ret = await self.afunc(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ahmadilaiwi/Documents/ilaiwi/WebApp/apps/codegen2/src/components/shared/types/tests/main.py\", line 17, in task_with_exception\n    raise Exception(\"This is a test exception\")\nException: This is a test exception\nDuring task with name 'task_with_exception' and id '5fb8df1b-1f89-e80e-e9a5-8874a3516972'\nDescription\n\nrun the code\nthe code should not error but it does\n\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:22 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6041\nPython Version:  3.12.9 (main, Feb  4 2025, 14:38:38) [Clang 16.0.0 (clang-1600.0.26.6)]\n\nPackage Information\n\nlangchain_core: 0.3.52\nlangchain: 0.3.23\nlangsmith: 0.3.30\nlangchain_anthropic: 0.3.9\nlangchain_google_genai: 2.0.10\nlangchain_openai: 0.3.12\nlangchain_text_splitters: 0.3.8\nlanggraph_checkpoint_dynamodb: Installed. No version info available.\nlanggraph_sdk: 0.1.61\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nanthropic<1,>=0.47.0: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nfiletype: 1.2.0\ngoogle-generativeai: 0.8.4\nhttpx: 0.27.2\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-azure-ai;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.41: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.49: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.51: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-perplexity;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.8: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nopenai-agents: Installed. No version info available.\nopenai<2.0.0,>=1.68.2: Installed. No version info available.\nopentelemetry-api: Installed. No version info available.\nopentelemetry-exporter-otlp-proto-http: Installed. No version info available.\nopentelemetry-sdk: Installed. No version info available.\norjson: 3.10.16\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.11.3\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: 8.3.5\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: 14.0.0\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-04-16", "closed_at": null, "labels": [], "State": "open", "Author": "Ilaiwi"}
{"issue_number": 4289, "issue_title": "`INVALID_CHAT_HISTORY` exception", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nasync for step in current_app.agent.astream(\n            {\"messages\": [{\"role\": \"user\", \"content\": user_input['message']}]},\n            stream_mode=\"values\", # Use this to stream all values in the state after each step.\n            config = config, # This is needed by Checkpointer\n        ):\n            step[\"messages\"][-1].pretty_print()\nError Message and Stack Trace (if applicable)\n2025-04-16 13:32:06 ERROR    /invoke exception! Found AIMessages with tool_calls that do not have a corresponding ToolMessage. Here are the first few of those tool calls: [{'name': 'HealthcareCypher', 'args': {'query': 'Which physician has treated the most patients covered by Cigna?'}, 'id': '930c507b-3c80-4d92-a960-ce5d669aea78', 'type': 'tool_call'}].\n\nEvery tool call (LLM requesting to call a tool) in the message history MUST have a corresponding ToolMessage (result of a tool invocation to return to the LLM) - this is required by most LLM providers.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_CHAT_HISTORY\nTraceback (most recent call last):\n  File \"/usr/src/Python/rag-agent/src/controllers/HomeController.py\", line 138, in invoke\n    async for step in current_app.agent.astream(\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 2651, in astream\n    async for _ in runner.atick(\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py\", line 762, in acall_model\n    state = _get_model_input_state(state)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py\", line 733, in _get_model_input_state\n    _validate_chat_history(messages)\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py\", line 260, in _validate_chat_history\n    raise ValueError(error_message)\nValueError: Found AIMessages with tool_calls that do not have a corresponding ToolMessage. Here are the first few of those tool calls: [{'name': 'HealthcareCypher', 'args': {'query': 'Which physician has treated the most patients covered by Cigna?'}, 'id': '930c507b-3c80-4d92-a960-ce5d669aea78', 'type': 'tool_call'}].\n\nEvery tool call (LLM requesting to call a tool) in the message history MUST have a corresponding ToolMessage (result of a tool invocation to return to the LLM) - this is required by most LLM providers.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_CHAT_HISTORY\nDescription\nThis could be due to previous run crashed or user terminated the session abruptly by, say, CTRL-C.\n(1) Why did the library persist inconsistent data into the database?\n(2) How to recover from this?\nDoes this refer to the checkpoint DB in PostgreSQL? Which table? Any documentation on the tables? What do they persist and what functionality they provide?\nI also notice that there is no timestamp column in all of the checkpoint DB tables. How do we SELECT and ORDER BY the table data?\nSystem Info\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #21-Ubuntu SMP PREEMPT_DYNAMIC Wed Feb 19 16:50:40 UTC 2025\n> Python Version:  3.12.7 (main, Feb  4 2025, 14:46:03) [GCC 14.2.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.51\n> langchain: 0.3.23\n> langchain_community: 0.3.21\n> langsmith: 0.3.27\n> langchain_chroma: 0.2.2\n> langchain_cli: 0.0.36\n> langchain_google_genai: 2.1.2\n> langchain_google_vertexai: 2.0.9\n> langchain_neo4j: 0.4.0\n> langchain_nomic: 0.1.4\n> langchain_ollama: 0.3.1\n> langchain_openai: 0.3.12\n> langchain_text_splitters: 0.3.8\n> langgraph_sdk: 0.1.61\n> langserve: 0.3.1\n\nOther Dependencies\n------------------\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> anthropic[vertexai]: Installed. No version info available.\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0: Installed. No version info available.\n> dataclasses-json<0.7,>=0.5.7: Installed. No version info available.\n> fastapi: 0.115.9\n> filetype: 1.2.0\n> gitpython<4,>=3: Installed. No version info available.\n> google-ai-generativelanguage: 0.6.17\n> google-cloud-aiplatform: 1.71.1\n> google-cloud-storage: 2.19.0\n> gritql<1.0.0,>=0.2.0: Installed. No version info available.\n> httpx: 0.27.2\n> httpx-sse: 0.4.0\n> httpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-azure-ai;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.49: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.51: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-perplexity;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.8: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langchain<1.0.0,>=0.3.23: Installed. No version info available.\n> langserve[all]>=0.0.51: Installed. No version info available.\n> langsmith-pyo3: Installed. No version info available.\n> langsmith<0.4,>=0.1.125: Installed. No version info available.\n> langsmith<0.4,>=0.1.17: Installed. No version info available.\n> neo4j: 5.28.1\n> neo4j-graphrag: 1.6.1\n> nomic: 3.4.1\n> numpy<2.0.0,>=1.22.4;: Installed. No version info available.\n> numpy<2.0.0,>=1.26.2;: Installed. No version info available.\n> numpy<3,>=1.26.2: Installed. No version info available.\n> ollama<1,>=0.4.4: Installed. No version info available.\n> openai-agents: Installed. No version info available.\n> openai<2.0.0,>=1.68.2: Installed. No version info available.\n> opentelemetry-api: 1.31.1\n> opentelemetry-exporter-otlp-proto-http: Installed. No version info available.\n> opentelemetry-sdk: 1.31.1\n> orjson: 3.10.16\n> packaging: 24.2\n> packaging<25,>=23.2: Installed. No version info available.\n> pillow: 10.4.0\n> pydantic: 2.9.2\n> pydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\n> pydantic<3.0.0,>=2.5.2;: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4;: Installed. No version info available.\n> pytest: 8.3.5\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> requests<3,>=2: Installed. No version info available.\n> rich: 14.0.0\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> sse-starlette: 1.8.2\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken<1,>=0.7: Installed. No version info available.\n> tomlkit>=0.12: Installed. No version info available.\n> typer[all]<1.0.0,>=0.9.0: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> uvicorn<1.0,>=0.23: Installed. No version info available.\n> zstandard: 0.23.0\n", "created_at": "2025-04-16", "closed_at": "2025-04-16", "labels": [], "State": "closed", "Author": "khteh"}
{"issue_number": 4275, "issue_title": "Mermaid.ink Timeout Error with Single-Character Node Names", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n%%capture --no-stderr\n%pip install -U langgraph typing-extensions\nfrom IPython.display import Image\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph\n\nclass ClientContext(TypedDict):\n    client_id: str\n\ngraph_builder = StateGraph(ClientContext)\n\ndef chatbot(state: ClientContext):\n    return {\"messages\": \"Yam\"}\n\ngraph_builder.add_node(\"A\", chatbot)\ngraph_builder.set_entry_point(\"A\")\ngraph_builder.set_finish_point(\"A\")\ngraph = graph_builder.compile()\n\ngraph_image = Image(graph.get_graph().draw_mermaid_png())\nError Message and Stack Trace (if applicable)\nTimeoutError                              Traceback (most recent call last)\n\n[/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py](https://localhost:8080/#) in _make_request(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\n    533         try:\n--> 534             response = conn.getresponse()\n    535         except (BaseSSLError, OSError) as e:\n\n22 frames\n\nTimeoutError: The read operation timed out\n\n\nThe above exception was the direct cause of the following exception:\n\nReadTimeoutError                          Traceback (most recent call last)\n\nReadTimeoutError: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)\n\n\nDuring handling of the above exception, another exception occurred:\n\nReadTimeout                               Traceback (most recent call last)\n\n[/usr/local/lib/python3.11/dist-packages/requests/adapters.py](https://localhost:8080/#) in send(self, request, stream, timeout, verify, cert, proxies)\n    711                 raise SSLError(e, request=request)\n    712             elif isinstance(e, ReadTimeoutError):\n--> 713                 raise ReadTimeout(e, request=request)\n    714             elif isinstance(e, _InvalidHeader):\n    715                 raise InvalidHeader(e, request=request)\n\nReadTimeout: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)\nDescription\nI\u2019m encountering a timeout error when generating a Mermaid diagram via langgraph with a single-character node identifier. I followed the instructions in the LangGraph visualization documentation but still received the same error. The error is reproduced both on my computer, as in this Google Colab example.\nSystem Info\nSystem Information\n------------------\n> OS:  Darwin\n> OS Version:  Darwin Kernel Version 24.4.0: Wed Mar 19 21:12:54 PDT 2025; root:xnu-11417.101.15~1/RELEASE_ARM64_T8103\n> Python Version:  3.13.2 (v3.13.2:4f8bb3947cf, Feb  4 2025, 11:51:10) [Clang 15.0.0 (clang-1500.3.9.4)]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.51\n> langchain: 0.3.23\n> langchain_community: 0.3.21\n> langsmith: 0.3.24\n> langchain_anthropic: 0.3.10\n> langchain_experimental: 0.3.4\n> langchain_openai: 0.3.12\n> langchain_text_splitters: 0.3.8\n> langgraph_api: 0.0.46\n> langgraph_cli: 0.1.89\n> langgraph_license: Installed. No version info available.\n> langgraph_sdk: 0.1.61\n> langgraph_storage: Installed. No version info available.\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> anthropic<1,>=0.49.0: Installed. No version info available.\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> blockbuster: 1.5.24\n> click: 8.1.8\n> cloudpickle: 3.1.1\n> cryptography: 44.0.2\n> dataclasses-json<0.7,>=0.5.7: Installed. No version info available.\n> httpx: 0.28.1\n> httpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> jsonschema-rs: 0.29.1\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-azure-ai;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.45: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.49: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.51: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-perplexity;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.8: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langchain<1.0.0,>=0.3.23: Installed. No version info available.\n> langgraph: 0.3.30\n> langgraph-checkpoint: 2.0.24\n> langsmith-pyo3: Installed. No version info available.\n> langsmith<0.4,>=0.1.125: Installed. No version info available.\n> langsmith<0.4,>=0.1.17: Installed. No version info available.\n> numpy<3,>=1.26.2: Installed. No version info available.\n> openai-agents: Installed. No version info available.\n> openai<2.0.0,>=1.68.2: Installed. No version info available.\n> opentelemetry-api: Installed. No version info available.\n> opentelemetry-exporter-otlp-proto-http: Installed. No version info available.\n> opentelemetry-sdk: Installed. No version info available.\n> orjson: 3.10.16\n> packaging: 24.2\n> packaging<25,>=23.2: Installed. No version info available.\n> pydantic: 2.11.2\n> pydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\n> pydantic<3.0.0,>=2.5.2;: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4;: Installed. No version info available.\n> pyjwt: 2.10.1\n> pytest: Installed. No version info available.\n> python-dotenv: 1.1.0\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> requests<3,>=2: Installed. No version info available.\n> rich: Installed. No version info available.\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> sse-starlette: 2.1.3\n> starlette: 0.46.1\n> structlog: 25.2.0\n> tenacity: 9.1.2\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken<1,>=0.7: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> uvicorn: 0.34.0\n> watchfiles: 1.0.4\n> zstandard: 0.23.0\n", "created_at": "2025-04-15", "closed_at": "2025-04-15", "labels": [], "State": "closed", "Author": "Omer80"}
{"issue_number": 4272, "issue_title": "DOC: (subgraph.ipynb); HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)?", "issue_body": "I'm following this guideline, I wrote this script like\nimport os\n# from langchain_openai import ChatOpenAI\n# from dotenv import load_dotenv\nfrom langgraph.graph import START, StateGraph\nfrom typing import TypedDict\nfrom langchain_core.runnables.graph import MermaidDrawMethod\n# load_dotenv()\n# os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n\n\nclass SubgraphState(TypedDict):\n    foo: str  # note that this key is shared with the parent graph state\n    bar: str\n\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"bar\": \"bar\"}\n\n\ndef subgraph_node_2(state: SubgraphState):\n    # note that this node is using a state key ('bar') that is only available in the subgraph\n    # and is sending update on the shared state key ('foo')\n    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph = subgraph_builder.compile()\n\n\n# Define parent graph\nclass ParentState(TypedDict):\n    foo: str\n\n\ndef node_1(state: ParentState):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\n\nbuilder = StateGraph(ParentState)\nbuilder.add_node(\"node_1\", node_1)\n# note that we're adding the compiled subgraph as a node to the parent graph\nbuilder.add_node(\"node_2\", subgraph)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\ngraph = builder.compile()\n\n# save graph's scheme\ntry:\n    with open('subgraphs-scheme.png', 'wb') as f:\n        f.write(graph.get_graph().draw_mermaid_png(\n            draw_method=MermaidDrawMethod.API,\n        ))\nexcept Exception as e:\n    # This requires some extra dependencies and is optional\n    print(f\"L\u1ed7i khi l\u01b0u bi\u1ec3u \u0111\u1ed3: {e}\")\n\nfor chunk in graph.stream({\"foo\": \"foo\"}):\n    print(chunk)\n\nfor chunk in graph.stream({\"foo\": \"foo\"}, subgraphs=True):\n    print(chunk)\nI don't know why it raise the error\nL\u1ed7i khi l\u01b0u bi\u1ec3u \u0111\u1ed3: HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)\n\nbecause I have did many example and all of them work fine, save to png file.\nimport os\nfrom langchain_openai import ChatOpenAI\nfrom typing import Literal\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_core.tools import tool\nfrom dotenv import load_dotenv\n\nload_dotenv()\nos.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n# For this tutorial we will use custom tool that returns pre-defined values for weather in two cities (NYC & SF)\n\n\n@tool\ndef get_weather(city: Literal[\"nyc\", \"sf\"]):\n    \"\"\"Use this to get weather information.\"\"\"\n    if city == \"nyc\":\n        return \"It might be cloudy in nyc\"\n    elif city == \"sf\":\n        return \"It's always sunny in sf\"\n    else:\n        raise AssertionError(\"Unknown city\")\n\n\ntools = [get_weather]\n\n# Define the graph\ngraph = create_react_agent(model, tools=tools)\n\n# save graph's scheme\ntry:\n    with open('parallel-extras-step-scheme.png', 'wb') as f:\n        f.write(graph.get_graph().draw_mermaid_png())\nexcept Exception as e:\n    # This requires some extra dependencies and is optional\n    print(f\"L\u1ed7i khi l\u01b0u bi\u1ec3u \u0111\u1ed3: {e}\")\n\n\ndef print_stream(stream):\n    for s in stream:\n        message = s[\"messages\"][-1]\n        if isinstance(message, tuple):\n            print(message)\n        else:\n            message.pretty_print()\n\n\ninputs = {\"messages\": [(\"user\", \"what is the weather in sf\")]}\nprint_stream(graph.stream(inputs, stream_mode=\"values\"))\n\nthe snippet code is ok\nversion lib\n\nlangchain                                0.3.19\nlangchain-anthropic                      0.3.10\nlangchain-chroma                         0.2.2\nlangchain-community                      0.3.18\nlangchain-core                           0.3.51\nlangchain-experimental                   0.3.4\nlangchain-ollama                         0.2.3\nlangchain-openai                         0.3.7\nlangchain-text-splitters                 0.3.6\nlangdetect                               1.0.9\nlanggraph                                0.3.30\nlanggraph-checkpoint                     2.0.16\nlanggraph-prebuilt                       0.1.8\nlanggraph-sdk                            0.1.53\nlangmem                                  0.0.22\nlangsmith                                0.3.11\npython                         3.12.9\n", "created_at": "2025-04-15", "closed_at": "2025-04-15", "labels": [], "State": "closed", "Author": "david101-hunter"}
{"issue_number": 4260, "issue_title": "react-ui: useStreamContext must be used within a LoadExternalComponent", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n// weatherComponent.tsx\n\nimport { useStreamContext } from \"@langchain/langgraph-sdk/react-ui\";\n\nconst WeatherComponent: React.FunctionComponent<{ city: string }> = (props) => {\n  const { submit } = useStreamContext();\n  return (\n    <>\n      <div>Weather for {props.city}</div>\n\n      <button\n        onClick={() => {\n          const newMessage = {\n            type: \"human\",\n            content: `What's the weather in ${props.city}?`,\n          };\n\n          submit({ messages: [newMessage] });\n        }}\n      >\n        Retry\n      </button>\n    </>\n  );\n};\n\nexport default WeatherComponent;\n\n\n\n// page.tsx\n\"use client\";\n\nimport { useStream } from \"@langchain/langgraph-sdk/react\";\nimport type { Message } from \"@langchain/langgraph-sdk\";\nimport { LoadExternalComponent, UIMessage } from \"@langchain/langgraph-sdk/react-ui\";\nimport WeatherComponent from \"./weatherComponent\";\n\nconst clientComponents = {\n  weather: WeatherComponent,\n} as unknown as Record<string, React.FunctionComponent>;\n\n\nexport default function Chat() {\n  const thread = useStream<{ messages: Message[], ui: UIMessage[] }>({\n    apiUrl: \"http://localhost:2024\",\n    assistantId: \"ui\",\n    messagesKey: \"messages\",\n  });\n\n  return (\n    <div>\n      <div>\n        {thread.messages\n        .filter((message) => message.type === \"human\" || message.type === \"ai\")\n        .map((message) => (\n          <div key={message.id}>\n            <p><strong>{message.type}:</strong>  {message.content as string}</p>\n            {thread.values?.ui\n            ?.filter((ui) => ui.metadata?.message_id === message.id)\n            .map((ui) => (\n              <LoadExternalComponent\n                key={ui.id}\n                stream={thread}\n                message={ui}\n                components={clientComponents}\n                />\n            ))}\n          </div>\n          \n        ))}\n      </div>\n\n      <form\n        onSubmit={(e) => {\n          e.preventDefault();\n\n          const form = e.target as HTMLFormElement;\n          const message = new FormData(form).get(\"message\") as string;\n\n          form.reset();\n          thread.submit({ messages: [{ type: \"human\", content: message }] });\n        }}\n      >\n        <input type=\"text\" name=\"message\" />\n\n        {thread.isLoading ? (\n          <button key=\"stop\" type=\"button\" onClick={() => thread.stop()}>\n            Stop\n          </button>\n        ) : (\n          <button key=\"send\" type=\"submit\">\n            Send\n          </button>\n        )}\n      </form>\n    </div>\n  );\n}\nError Message and Stack Trace (if applicable)\nuseStreamContext@http://localhost:3000/_next/static/chunks/node_modules_432c4f4b._.js:27037:15\nWeatherComponent@http://localhost:3000/_next/static/chunks/src_app_chat_c0a2b125._.js:19:228\nLoadExternalComponent@http://localhost:3000/_next/static/chunks/node_modules_432c4f4b._.js:27129:174\n[project]/src/app/chat/page.tsx [app-client] (ecmascript)/Chat/<.children<.children</<.children<@http://localhost:3000/_next/static/chunks/src_app_chat_c0a2b125._.js:121:313\n[project]/src/app/chat/page.tsx [app-client] (ecmascript)/Chat/<.children<.children<@http://localhost:3000/_next/static/chunks/src_app_chat_c0a2b125._.js:121:101\nChat@http://localhost:3000/_next/static/chunks/src_app_chat_c0a2b125._.js:99:112\nClientPageRoot@http://localhost:3000/_next/static/chunks/node_modules_next_dist_1a6ee436._.js:2061:50\n\nDescription\nI'm trying to use Generate UI with LangGraph, using Python on the backend and Next/React on the frontend. According to the documentation, it is possible to define my components on the frontend using the component prop.\n\nRefs:\n\nhttps://langchain-ai.github.io/langgraph/cloud/how-tos/generative_ui_react/#provide-custom-components-on-the-client-side\nhttps://langchain-ai.github.io/langgraph/cloud/how-tos/generative_ui_react/#access-and-interact-with-the-thread-state-from-the-ui-component\n\nHowever, when I use useStreamContext, it shows me the following error:\n\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #59-Ubuntu SMP PREEMPT_DYNAMIC Sat Mar 15 17:40:59 UTC 2025\nPython Version:  3.13.2 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:02) [GCC 11.2.0]\n\nPackage Information\n\nlangchain_core: 0.3.51\nlangsmith: 0.3.30\nlangchain_openai: 0.3.0\nlanggraph_api: 0.1.2\nlanggraph_cli: 0.2.3\nlanggraph_license: Installed. No version info available.\nlanggraph_runtime: Installed. No version info available.\nlanggraph_runtime_inmem: 0.0.3\nlanggraph_sdk: 0.1.61\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nblockbuster: 1.5.24\nclick: 8.1.8\ncloudpickle: 3.1.1\ncryptography: 44.0.1\nhttpx: 0.28.1\njsonpatch<2.0,>=1.33: Installed. No version info available.\njsonschema-rs: 0.29.1\nlanggraph: 0.3.28\nlanggraph-checkpoint: 2.0.24\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nopenai: 1.72.0\nopenai-agents: Installed. No version info available.\nopentelemetry-api: Installed. No version info available.\nopentelemetry-exporter-otlp-proto-http: Installed. No version info available.\nopentelemetry-sdk: Installed. No version info available.\norjson: 3.10.16\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.11.3\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npyjwt: 2.10.1\npytest: Installed. No version info available.\npython-dotenv: 1.0.1\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrich: Installed. No version info available.\nsse-starlette: 2.1.3\nstarlette: 0.41.3\nstructlog: 25.2.0\ntenacity: 9.1.2\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken: 0.9.0\ntyping-extensions>=4.7: Installed. No version info available.\nuvicorn: 0.34.0\nwatchfiles: 1.0.5\nzstandard: 0.23.0\n", "created_at": "2025-04-12", "closed_at": "2025-04-18", "labels": [], "State": "closed", "Author": "nicobytes"}
{"issue_number": 4259, "issue_title": "Packaging Error: langgraph-checkpoint-sqlite wheel installs code under incorrect 'Langgraph' folder, causing ModuleNotFoundError", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n**Code to Reproduce**\n\n1.  Create a fresh Conda environment (e.g., `conda create -n test-env python=3.12 -y` and `conda activate test-env`).\n2.  Install the package: `pip install langgraph-checkpoint-sqlite==2.0.6`\n3.  Run the following Python code:\n\n\ntry:\n    # Attempt to import the class using the correct path for LangGraph >= 0.2.0\n    from langgraph_checkpoint_sqlite.sqlite import SqliteSaver\n    print(\"Import successful!\") # This line will not be reached\nexcept ModuleNotFoundError as e:\n    print(f\"Import failed: {e}\")\n    # Verify installation location (optional, for debugging)\n    import sys\n    import site\n    print(f\"Python Executable: {sys.executable}\")\n    print(f\"Site Packages Paths: {site.getsitepackages()}\")\n    # You can also add a pip check here if desired\n    # import subprocess\n    # try:\n    #     result = subprocess.run(['pip', 'show', 'langgraph-checkpoint-sqlite'], capture_output=True, text=True, check=True)\n    #     print(\"\\n--- pip show langgraph-checkpoint-sqlite ---\")\n    #     print(result.stdout)\n    # except Exception as check_e:\n    #     print(f\"\\nError running pip show: {check_e}\")\n\n\n\n**Expected Behavior**\nThe import should succeed without error if the package installed correctly.\n\n**Actual Behavior**\nThe code fails with the error:\n`Import failed: ModuleNotFoundError: No module named 'langgraph_checkpoint_sqlite'`\n(Even though `pip list` shows the package is installed).\nError Message and Stack Trace (if applicable)\n\nDescription\nWhen installing langgraph-checkpoint-sqlite (version 2.0.6, potentially others) using pip within a Conda environment on Windows, the installation reports success, but the package cannot be imported, resulting in ModuleNotFoundError: No module named 'langgraph_checkpoint_sqlite'.\nDetailed Symptoms\npip install langgraph-checkpoint-sqlite completes without error messages.\npip list shows langgraph-checkpoint-sqlite installed at the correct version.\nInspecting the site-packages directory reveals that only the metadata folder (langgraph_checkpoint_sqlite-X.Y.Z.dist-info) is created. The actual package code folder (langgraph_checkpoint_sqlite) containing the necessary .py files is missing.\nRunning python -c \"import langgraph_checkpoint_sqlite\" fails with ModuleNotFoundError.\nRoot Cause Analysis\nAfter extensive troubleshooting (see below), the root cause was identified by manually inspecting the wheel file downloaded from PyPI:\nDownloaded langgraph_checkpoint_sqlite-2.0.6-py3-none-any.whl from https://pypi.org/project/langgraph-checkpoint-sqlite/2.0.6/#files\nRenamed the .whl to .zip and extracted its contents.\nObservation: The extracted archive contains the checkpointer code files, but they are incorrectly located under a folder named Langgraph. The structure found was Langgraph/checkpoint/sqlite/...\nExpected Structure: The code files should be located directly under a top-level folder named langgraph_checkpoint_sqlite within the archive (e.g., langgraph_checkpoint_sqlite/checkpoint/sqlite/...).\nBecause the code files are not located where the package metadata (.dist-info/RECORD) expects them, pip fails to copy them into site-packages during installation, even though it successfully installs the metadata and reports success.\nSystem Info\nOS: Windows 10 (also likely affects Windows 11)\nPython: 3.12 (via Miniconda/Anaconda)\nEnvironment Manager: Conda\npip version: Latest (tried upgrading pip, setuptools, wheel)\nlanggraph-checkpoint-sqlite version: 2.0.6 (potentially others)\nNote: This issue was reproduced even in a completely fresh Conda environment created with conda create --name test-env python=3.12.\nTroubleshooting Steps Attempted (Unsuccessful)\nVerifying correct Conda environment activation.\nVerifying sys.path includes the correct site-packages.\nUsing full paths to pip.exe and python.exe.\npip install --force-reinstall --no-cache-dir.\nInstalling directly from the local .whl file.\nRunning install commands as Administrator.\nUpgrading pip, setuptools, wheel.\nCompletely removing and recreating the Conda environment.", "created_at": "2025-04-12", "closed_at": "2025-04-12", "labels": [], "State": "closed", "Author": "MichaelKoliopoulos"}
{"issue_number": 4258, "issue_title": "`llama3.3` with `Chroma` `retriever_tool` hits `KeyError: 'tools'` when calling `.astream()`", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nChroma vector store:\n        self.vector_store = Chroma(client = self._client, collection_name = self._collection, embedding_function = self._embeddings)\n        # https://api.python.langchain.com/en/latest/tools/langchain.tools.retriever.create_retriever_tool.html\n        self.retriever_tool = create_retriever_tool(\n            self.vector_store.as_retriever(),\n            \"Retrieve information related to a query\",\n            \"Search and return information about the query from the documents available in the store\",\n        )\nAgent:\n    async def Agent(self, state: CustomAgentState, config: RunnableConfig):\n        \"\"\"\n        Invokes the agent model to generate a response based on the current state. Given\n        the question, it will decide to retrieve using the retriever tool, or simply end.\n\n        Args:\n            state (messages): The current state\n\n        Returns:\n            dict: The updated state with the agent response appended to messages\n        \"\"\"\n        logging.info(f\"\\n=== {self.Agent.__name__} ===\")\n        logging.debug(f\"state: {state}\")\n        response = await self._llm.with_config(config).ainvoke(state[\"messages\"])#, config)\n        # MessageState appends messages to state instead of overwriting\n        return {\"messages\": [response]}\n\nCustomAgentState:\n@dataclass\nclass CustomAgentState(AgentState):\n    context: List[Document]\n    is_last_step: IsLastStep\n\nStateGraph:\n    self._vectorStore = VectorStore(model=appconfig.EMBEDDING_MODEL, chunk_size=1000, chunk_overlap=0)\n    self._llm = init_chat_model(appconfig.LLM_RAG_MODEL, model_provider=\"ollama\", base_url=appconfig.OLLAMA_URI, streaming=True).bind_tools([self._vectorStore.retriever_tool])\n---\n    graph_builder = StateGraph(CustomAgentState)\n    graph_builder.add_node(\"Agent\", self.Agent)\n    graph_builder.add_node(\"Retrieve\", ToolNode([self._vectorStore.retriever_tool])) # Execute the retrieval.\n    graph_builder.add_node(\"Rewrite\", self.Rewrite)\n    graph_builder.add_node(\"Generate\", self.Generate)\n    graph_builder.add_edge(START, \"Agent\")\n    #graph_builder.set_entry_point(\"query_or_respond\")\n    graph_builder.add_conditional_edges(\n        \"Agent\",\n        # Assess agent decision\n        tools_condition,\n        {\n            \"\"\"\n            Translate the condition outputs to nodes in our graph\n            which node to go to based on the output of the conditional edge function - tools_condition.\n            \"\"\"\n            \"tools\": \"Retrieve\",\n            END: END\n        },\n    )\n    # Edges taken after the `action` node is called.\n    graph_builder.add_conditional_edges(\n        \"Retrieve\",\n        # Assess agent decision\n        self.GradeDocuments,\n    )\n    graph_builder.add_edge(\"Generate\", END)\n    graph_builder.add_edge(\"Rewrite\", \"Agent\")\n    self._graph = graph_builder.compile(store=self._in_memory_store, name=self._name)\n\nCalling the graph .astream:\n        async for step in self._graph.astream(\n            {\"messages\": [{\"role\": \"user\", \"content\": message}]},\n            stream_mode=\"values\",\n            config = config\n        ):\n            step[\"messages\"][-1].pretty_print()\n\nError Message and Stack Trace (if applicable)\n================================ Human Message =================================\n\nHello, who are you?\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/src/Python/rag-agent/src/rag_agent/GraphRAG.py\", line 352, in <module>\n    asyncio.run(main())\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/usr/src/Python/rag-agent/src/rag_agent/GraphRAG.py\", line 348, in main\n    await graph.TestDirectResponseWithoutRetrieval(config, \"Hello, who are you?\")\n  File \"/usr/src/Python/rag-agent/src/rag_agent/GraphRAG.py\", line 299, in TestDirectResponseWithoutRetrieval\n    async for step in self._graph.astream(\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 2651, in astream\n    async for _ in runner.atick(\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/graph/branch.py\", line 193, in _aroute\n    return self._finish(writer, input, result, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/graph/branch.py\", line 208, in _finish\n    r if isinstance(r, Send) else self.ends[r] for r in result\n                                  ~~~~~~~~~^^^\nKeyError: 'tools'\nDuring task with name 'Agent' and id '8a1c5e4f-294d-ea6a-9e45-af305c707b6b'\nLogs:\n=== Agent ===\n2025-04-12 12:37:19 DEBUG    state: {'messages': [HumanMessage(content='Hello, who are you?', additional_kwargs={}, response_metadata={}, id='641ef09b-a834-48ad-a1f6-dbd387e0b68a')], 'is_last_step': False, 'remaining_steps': 24}\n\nDescription\nI follow this tutorial: https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/#graph and hit the error.\nSystem Info\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #21-Ubuntu SMP PREEMPT_DYNAMIC Wed Feb 19 16:50:40 UTC 2025\n> Python Version:  3.12.7 (main, Feb  4 2025, 14:46:03) [GCC 14.2.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.51\n> langchain: 0.3.23\n> langchain_community: 0.3.21\n> langsmith: 0.3.27\n> langchain_chroma: 0.2.2\n> langchain_cli: 0.0.36\n> langchain_google_genai: 2.1.2\n> langchain_google_vertexai: 2.0.9\n> langchain_neo4j: 0.4.0\n> langchain_nomic: 0.1.4\n> langchain_ollama: 0.3.1\n> langchain_openai: 0.3.12\n> langchain_text_splitters: 0.3.8\n> langgraph_sdk: 0.1.61\n> langserve: 0.3.1\n\nOther Dependencies\n------------------\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> anthropic[vertexai]: Installed. No version info available.\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0: Installed. No version info available.\n> dataclasses-json<0.7,>=0.5.7: Installed. No version info available.\n> fastapi: 0.115.9\n> filetype: 1.2.0\n> gitpython<4,>=3: Installed. No version info available.\n> google-ai-generativelanguage: 0.6.17\n> google-cloud-aiplatform: 1.71.1\n> google-cloud-storage: 2.19.0\n> gritql<1.0.0,>=0.2.0: Installed. No version info available.\n> httpx: 0.27.2\n> httpx-sse: 0.4.0\n> httpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-azure-ai;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.49: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.51: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-perplexity;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.8: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langchain<1.0.0,>=0.3.23: Installed. No version info available.\n> langserve[all]>=0.0.51: Installed. No version info available.\n> langsmith-pyo3: Installed. No version info available.\n> langsmith<0.4,>=0.1.125: Installed. No version info available.\n> langsmith<0.4,>=0.1.17: Installed. No version info available.\n> neo4j: 5.28.1\n> neo4j-graphrag: 1.6.1\n> nomic: 3.4.1\n> numpy<2.0.0,>=1.22.4;: Installed. No version info available.\n> numpy<2.0.0,>=1.26.2;: Installed. No version info available.\n> numpy<3,>=1.26.2: Installed. No version info available.\n> ollama<1,>=0.4.4: Installed. No version info available.\n> openai-agents: Installed. No version info available.\n> openai<2.0.0,>=1.68.2: Installed. No version info available.\n> opentelemetry-api: 1.31.1\n> opentelemetry-exporter-otlp-proto-http: Installed. No version info available.\n> opentelemetry-sdk: 1.31.1\n> orjson: 3.10.16\n> packaging: 24.2\n> packaging<25,>=23.2: Installed. No version info available.\n> pillow: 10.4.0\n> pydantic: 2.9.2\n> pydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\n> pydantic<3.0.0,>=2.5.2;: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4;: Installed. No version info available.\n> pytest: 8.3.5\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> requests<3,>=2: Installed. No version info available.\n> rich: 14.0.0\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> sse-starlette: 1.8.2\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken<1,>=0.7: Installed. No version info available.\n> tomlkit>=0.12: Installed. No version info available.\n> typer[all]<1.0.0,>=0.9.0: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> uvicorn<1.0,>=0.23: Installed. No version info available.\n> zstandard: 0.23.0\n", "created_at": "2025-04-12", "closed_at": "2025-04-12", "labels": [], "State": "closed", "Author": "khteh"}
{"issue_number": 4253, "issue_title": "Langgraph Studio re-run from here fails after langgraph dev auto reload", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nMy env setup:\nconda create -n langgraph-test -y python=3.12 pip\nconda activate langgraph-test\nProject setup and starting langgraph studio:\npip install langgraph-cli[inmem]\nlanggraph new --template new-langgraph-project-python .\npip install -e .\nlanggraph dev\nThen I perform following steps:\n\nClick submit (works as expected)\nMake some small change in code, like add new line\nClick re-run from here on some node (does not work)\n\nWhat happens: Re-run seems to start at first, but stops after a second without any errors. All results from nodes bellow clicked one are removed.\nError Message and Stack Trace (if applicable)\nNo errors\nDescription\nI'm experiencing an issue with the re-run from here feature on locally developed graphs. When I run langgraph dev and use re-run from here without making any code modifications, everything works as expected. However, if I make even a small change to the code, the re-run from here feature stops working for any checkpoints created before the reload.\nIn other words, I would like to use re-run from here as shown here on video: run graph -> make modification in code -> re-run from here. unfortunately it doesn't work for me.\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #59~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Mar 19 17:07:41 UTC 2\nPython Version:  3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0]\n\nPackage Information\n\nlangchain_core: 0.3.51\nlangsmith: 0.3.30\nlanggraph_api: 0.1.2\nlanggraph_cli: 0.2.3\nlanggraph_license: Installed. No version info available.\nlanggraph_runtime: Installed. No version info available.\nlanggraph_runtime_inmem: 0.0.3\nlanggraph_sdk: 0.1.61\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nblockbuster: 1.5.24\nclick: 8.1.8\ncloudpickle: 3.1.1\ncryptography: 44.0.2\nhttpx: 0.28.1\njsonpatch<2.0,>=1.33: Installed. No version info available.\njsonschema-rs: 0.29.1\nlanggraph: 0.3.28\nlanggraph-checkpoint: 2.0.24\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nopenai-agents: Installed. No version info available.\nopentelemetry-api: Installed. No version info available.\nopentelemetry-exporter-otlp-proto-http: Installed. No version info available.\nopentelemetry-sdk: Installed. No version info available.\norjson: 3.10.16\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.11.3\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npyjwt: 2.10.1\npytest: Installed. No version info available.\npython-dotenv: 1.1.0\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrich: Installed. No version info available.\nsse-starlette: 2.1.3\nstarlette: 0.46.1\nstructlog: 25.2.0\ntenacity: 9.1.2\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nuvicorn: 0.34.0\nwatchfiles: 1.0.5\nzstandard: 0.23.0\n", "created_at": "2025-04-11", "closed_at": null, "labels": [], "State": "open", "Author": "klima7"}
{"issue_number": 4231, "issue_title": "`langgraph.errors.GraphRecursionError` ReAct agent keeps calling the `save_memory` tool repeatedly!", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nThe tool in question:\n@tool\nasync def save_memory(memory: str, *, config: Annotated[RunnableConfig, InjectedToolArg], store: Annotated[BaseStore, InjectedStore()]) -> str:\n    \"\"\"\n    Save the given memory for the current user.\n    This should only be used after you have exhausted all other tools to accomplish your task. After saving the memory for the current user, you should return to the user with your answer.\n    \"\"\"\n    # This is a **tool** the model can use to save memories to storage\n    config = ensure_config(config)\n    user_id = config.get(\"configurable\", {}).get(\"user_id\")\n    namespace = (\"memories\", user_id)\n    store.put(namespace, f\"memory_{len(await store.asearch(namespace))}\", {\"data\": memory})\n    return f\"Saved memory: {memory}\"\nThe ReAct agent:\n    _prompt = ChatPromptTemplate.from_messages([\n                (\"system\", \"You are a helpful AI assistant named Bob.\"),\n                (\"placeholder\", \"{messages}\"),\n                (\"human\", \"Remember, always provide accurate answer!\"),\n        ])\n        self._tools = [self._vectorStore.retriever_tool, ground_search, save_memory]\n        self._agent = create_react_agent(self._llm, self._tools, store = in_memory_store, checkpointer = MemorySaver(), \n                               config_schema = Configuration, state_schema = CustomAgentState, name = self._name, prompt = self._prompt)\n\nApplication invoking the ReAct agent:\n    async def ChatAgent(self, config: RunnableConfig, messages: List[tuple]): #messages: List[str]):\n        logging.info(f\"\\n=== {self.ChatAgent.__name__} ===\")\n        async for event in self._agent.with_config({\"user_id\": uuid7str()}).astream(\n            #{\"messages\": [{\"role\": \"user\", \"content\": messages}]}, This works with gemini-2.0-flash\n           {\"messages\": messages}, # This works with Ollama llama3.3\n            stream_mode=\"values\", # Use this to stream all values in the state after each step.\n            config=config, # This is needed by Checkpointer\n        ):\n            event[\"messages\"][-1].pretty_print()\n\nasync def main():\n    config = RunnableConfig(run_name=\"RAG ReAct Agent\", thread_id=datetime.now())\n    rag = RAGAgent(config)\n    await rag.CreateGraph()\n    input_message = [(\"human\", \"What is the standard method for Task Decomposition?\"), (\"human\", \"Once you get the answer, look up common extensions of that method.\")]\n    await rag.ChatAgent(config, input_message)\n\nError Message and Stack Trace (if applicable)\nThe console output:\n================================ Human Message =================================\n\nOnce you get the answer, look up common extensions of that method.\n================================== Ai Message ==================================\nName: RAG ReAct Agent\nTool Calls:\n  Retrieve information related to a query (88674e09-6a8c-47d3-87e8-3eb26a12402c)\n Call ID: 88674e09-6a8c-47d3-87e8-3eb26a12402c\n  Args:\n    query: standard method for Task Decomposition\n================================= Tool Message =================================\nName: Retrieve information related to a query\n\nFig. 1. Overview of a LLM-powered autonomous agent system.\nComponent One: Planning#\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\nTask Decomposition#\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to \u201cthink step by step\u201d to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model\u2019s thinking process.\n\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\nThe system comprises of 4 stages:\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\nInstruction:\n\nFig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\n================================== Ai Message ==================================\nName: RAG ReAct Agent\nTool Calls:\n  save_memory (c81987b3-9884-4ccd-b4e6-a101bd764235)\n Call ID: c81987b3-9884-4ccd-b4e6-a101bd764235\n  Args:\n    memory: Always provide an accurate answer\n================================= Tool Message =================================\nName: save_memory\n\nSaved memory: Always provide an accurate answer\n================================== Ai Message ==================================\nName: RAG ReAct Agent\nTool Calls:\n  save_memory (a81f12e9-15fe-4088-bf28-6fc99c6c3ff7)\n Call ID: a81f12e9-15fe-4088-bf28-6fc99c6c3ff7\n  Args:\n    memory: Always provide an accurate answer\n================================= Tool Message =================================\nName: save_memory\n\nSaved memory: Always provide an accurate answer\n================================== Ai Message ==================================\nName: RAG ReAct Agent\nTool Calls:\n  save_memory (58c99b54-1afa-47ef-8938-d78f72f75835)\n Call ID: 58c99b54-1afa-47ef-8938-d78f72f75835\n  Args:\n    memory: Always provide an accurate answer\n================================= Tool Message =================================\nName: save_memory\n\nSaved memory: Always provide an accurate answer\n================================== Ai Message ==================================\nName: RAG ReAct Agent\nTool Calls:\n  save_memory (0913b79e-a834-4730-b199-580fd0aa6abe)\n Call ID: 0913b79e-a834-4730-b199-580fd0aa6abe\n  Args:\n    memory: Always provide an accurate answer\n================================= Tool Message =================================\nName: save_memory\n\nSaved memory: Always provide an accurate answer\n================================== Ai Message ==================================\nName: RAG ReAct Agent\nTool Calls:\n  save_memory (61c44153-38c4-49ce-9974-18c56ad9e2e5)\n Call ID: 61c44153-38c4-49ce-9974-18c56ad9e2e5\n  Args:\n    memory: Always provide an accurate answer\n================================= Tool Message =================================\nName: save_memory\n\nSaved memory: Always provide an accurate answer\n================================== Ai Message ==================================\nName: RAG ReAct Agent\nTool Calls:\n  save_memory (bb004ff6-6052-4462-92d7-6a14d4738d32)\n Call ID: bb004ff6-6052-4462-92d7-6a14d4738d32\n  Args:\n    memory: Always provide an accurate answer\n================================= Tool Message =================================\nName: save_memory\n\nSaved memory: Always provide an accurate answer\n================================== Ai Message ==================================\nName: RAG ReAct Agent\nTool Calls:\n  save_memory (e759e92f-e7aa-400b-b186-bfdceeeb8257)\n Call ID: e759e92f-e7aa-400b-b186-bfdceeeb8257\n  Args:\n    memory: Always provide an accurate answer\n================================= Tool Message =================================\nName: save_memory\n\nSaved memory: Always provide an accurate answer\n================================== Ai Message ==================================\nName: RAG ReAct Agent\nTool Calls:\n  save_memory (bc7353a7-b4fd-4287-9dfb-9b67695d7b17)\n Call ID: bc7353a7-b4fd-4287-9dfb-9b67695d7b17\n  Args:\n    memory: Always provide an accurate answer\n================================= Tool Message =================================\nName: save_memory\n\nSaved memory: Always provide an accurate answer\n================================== Ai Message ==================================\nName: RAG ReAct Agent\nTool Calls:\n  save_memory (0e631708-ed77-422d-8d91-075e14712850)\n Call ID: 0e631708-ed77-422d-8d91-075e14712850\n  Args:\n    memory: Always provide an accurate answer\n================================= Tool Message =================================\nName: save_memory\n\nSaved memory: Always provide an accurate answer\n================================== Ai Message ==================================\nName: RAG ReAct Agent\nTool Calls:\n  save_memory (60e6a66f-00e3-48b5-bc5b-28c725e6eb65)\n Call ID: 60e6a66f-00e3-48b5-bc5b-28c725e6eb65\n  Args:\n    memory: Always provide an accurate answer\n================================= Tool Message =================================\nName: save_memory\n\nSaved memory: Always provide an accurate answer\n================================== Ai Message ==================================\nName: RAG ReAct Agent\nTool Calls:\n  save_memory (84eccb2d-d7e1-4714-bdcc-05fb40643dfc)\n Call ID: 84eccb2d-d7e1-4714-bdcc-05fb40643dfc\n  Args:\n    memory: Always provide an accurate answer\n================================= Tool Message =================================\nName: save_memory\n\nSaved memory: Always provide an accurate answer\n================================== Ai Message ==================================\n\nSorry, need more steps to process this request.\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/src/Python/rag-agent/src/rag_agent/RAGAgent.py\", line 173, in <module>\n    asyncio.run(main())\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/usr/src/Python/rag-agent/src/rag_agent/RAGAgent.py\", line 170, in main\n    await rag.ChatAgent(config, input_message)\n  File \"/usr/src/Python/rag-agent/src/rag_agent/RAGAgent.py\", line 141, in ChatAgent\n    async for event in self._agent.with_config({\"user_id\": uuid7str()}).astream(\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 2648, in astream\n    raise GraphRecursionError(msg)\nlanggraph.errors.GraphRecursionError: Recursion limit of 25 reached without hitting a stop condition. You can increase the limit by setting the `recursion_limit` config key.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/GRAPH_RECURSION_LIMIT\nDescription\nI have a RAG ReAct agent which uses 3 tools, one to retrieve information to answer user question from Chroma vector DB, one to use GoogleSearch and the third one is to save the user memory.\nThe first problem I face is that it keeps calling the save_memory tool after it has retrieved the required information from Chroma vector DB to answer user's question until it hits the recursion limit.\nThe second problem I face is that the ReAct agent skips the first \"Human Message\" and goes straight to the second one in the list.\nSystem Info\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #21-Ubuntu SMP PREEMPT_DYNAMIC Wed Feb 19 16:50:40 UTC 2025\n> Python Version:  3.12.7 (main, Feb  4 2025, 14:46:03) [GCC 14.2.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.51\n> langchain: 0.3.23\n> langchain_community: 0.3.21\n> langsmith: 0.3.27\n> langchain_chroma: 0.2.2\n> langchain_cli: 0.0.36\n> langchain_google_genai: 2.1.2\n> langchain_google_vertexai: 2.0.9\n> langchain_neo4j: 0.4.0\n> langchain_nomic: 0.1.4\n> langchain_ollama: 0.3.1\n> langchain_openai: 0.3.12\n> langchain_text_splitters: 0.3.8\n> langgraph_sdk: 0.1.61\n> langserve: 0.3.1\n\nOther Dependencies\n------------------\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> anthropic[vertexai]: Installed. No version info available.\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0: Installed. No version info available.\n> dataclasses-json<0.7,>=0.5.7: Installed. No version info available.\n> fastapi: 0.115.9\n> filetype: 1.2.0\n> gitpython<4,>=3: Installed. No version info available.\n> google-ai-generativelanguage: 0.6.17\n> google-cloud-aiplatform: 1.71.1\n> google-cloud-storage: 2.19.0\n> gritql<1.0.0,>=0.2.0: Installed. No version info available.\n> httpx: 0.27.2\n> httpx-sse: 0.4.0\n> httpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-azure-ai;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.49: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.51: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-perplexity;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.8: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langchain<1.0.0,>=0.3.23: Installed. No version info available.\n> langserve[all]>=0.0.51: Installed. No version info available.\n> langsmith-pyo3: Installed. No version info available.\n> langsmith<0.4,>=0.1.125: Installed. No version info available.\n> langsmith<0.4,>=0.1.17: Installed. No version info available.\n> neo4j: 5.28.1\n> neo4j-graphrag: 1.6.1\n> nomic: 3.4.1\n> numpy<2.0.0,>=1.22.4;: Installed. No version info available.\n> numpy<2.0.0,>=1.26.2;: Installed. No version info available.\n> numpy<3,>=1.26.2: Installed. No version info available.\n> ollama<1,>=0.4.4: Installed. No version info available.\n> openai-agents: Installed. No version info available.\n> openai<2.0.0,>=1.68.2: Installed. No version info available.\n> opentelemetry-api: 1.31.1\n> opentelemetry-exporter-otlp-proto-http: Installed. No version info available.\n> opentelemetry-sdk: 1.31.1\n> orjson: 3.10.16\n> packaging: 24.2\n> packaging<25,>=23.2: Installed. No version info available.\n> pillow: 10.4.0\n> pydantic: 2.9.2\n> pydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\n> pydantic<3.0.0,>=2.5.2;: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4;: Installed. No version info available.\n> pytest: 8.3.5\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> requests<3,>=2: Installed. No version info available.\n> rich: 14.0.0\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> sse-starlette: 1.8.2\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken<1,>=0.7: Installed. No version info available.\n> tomlkit>=0.12: Installed. No version info available.\n> typer[all]<1.0.0,>=0.9.0: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> uvicorn<1.0,>=0.23: Installed. No version info available.\n> zstandard: 0.23.0\n", "created_at": "2025-04-10", "closed_at": "2025-04-10", "labels": [], "State": "closed", "Author": "khteh"}
{"issue_number": 4230, "issue_title": "run_id and other config parameters are not persisted when calling an agent", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\ncompiled_graph = graph.compile(checkpointer=AsyncPostgresSaver(...))\nstream_agent = compiled_graph.astream(\n    {\"messages\": [query]},\n    config={\n        \"callbacks\": [CallbackHandler(user_id=user_id, session_id=thread_id)],\n        \"configurable\": {\"thread_id\": thread_id},\n        \"run_id\": run_id,\n        \"run_name\": \"chat_agent\"}\n    },\n    stream_mode=\"messages\",\n)\nError Message and Stack Trace (if applicable)\n\nDescription\nI noticed that only thread_id and checkpoint_id are being persisted. However, I would also expect run_id (and potentially other config parameters) to be saved, as it\u2019s critical for features like attaching user feedback to a specific run later on.\nFeature Request:\nPlease consider persisting additional config values such as run_id in the database or checkpoint. This would make it easier to track and interact with specific runs across sessions or when implementing post-run features like feedback collection.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.4.0: Wed Mar 19 21:17:32 PDT 2025; root:xnu-11417.101.15~1/RELEASE_ARM64_T6030\nPython Version:  3.11.11 (main, Dec  3 2024, 17:20:40) [Clang 16.0.0 (clang-1600.0.26.4)]\n\nPackage Information\n\nlangchain_core: 0.3.51\nlangchain: 0.3.23\nlangchain_community: 0.3.21\nlangsmith: 0.3.27\nlangchain_aws: 0.2.15\nlangchain_google_vertexai: 2.0.15\nlangchain_openai: 0.3.12\nlangchain_text_splitters: 0.3.8\nlangchain_weaviate: 0.0.4\nlanggraph_sdk: 0.1.61\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nanthropic[vertexai]: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nboto3: 1.37.0\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\ngoogle-cloud-aiplatform: 1.83.0\ngoogle-cloud-storage: 2.19.0\nhttpx: 0.28.1\nhttpx-sse: 0.4.0\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-azure-ai;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.49: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.51: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-perplexity;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.8: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.23: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy: 1.26.4\nnumpy<3,>=1.26.2: Installed. No version info available.\nopenai-agents: Installed. No version info available.\nopenai<2.0.0,>=1.68.2: Installed. No version info available.\nopentelemetry-api: Installed. No version info available.\nopentelemetry-exporter-otlp-proto-http: Installed. No version info available.\nopentelemetry-sdk: Installed. No version info available.\norjson: 3.10.16\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: 8.3.5\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: Installed. No version info available.\nsimsimd: 6.2.1\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nweaviate-client: 4.12.1\nzstandard: 0.23.0\n", "created_at": "2025-04-10", "closed_at": null, "labels": [], "State": "open", "Author": "Leo310"}
{"issue_number": 4229, "issue_title": "ReAct agent `astream` created with `create_react_agent` always retrieve ONLY the last message in the provided message list.", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nself._agent = create_react_agent(self._llm, TOOLS, store = self._in_memory_store, config_schema = Configuration, state_schema=AgentState, name=\"Healthcare ReAct Agent\", prompt=self._prompt)\n\ninput_message = [\"What is the wait time at Wallace-Hamilton?\", \"Which hospital has the shortest wait time?\"]\nasync for event in self._agent.with_config({\"user_id\": uuid7str()}).astream(\n    #{\"messages\": [{\"role\": \"user\", \"content\": messages}]}, This works with gemini-2.0-flash\n    {\"messages\": messages}, # This works with Ollama llama3.3\n    stream_mode=\"values\", # Use this to stream all values in the state after each step.\n    config=config, # This is needed by Checkpointer\n):\n    event[\"messages\"][-1].pretty_print()\nError Message and Stack Trace (if applicable)\nNot error message but as seen from the cosole, it only retrieves and processes the last message in the list:\n\nLoading .env environment variables...\n================================ Human Message =================================\n\nWhich hospital has the shortest wait time?\nDescription\nSomehow related to langchain-ai/langchain#30663\nSystem Info\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #21-Ubuntu SMP PREEMPT_DYNAMIC Wed Feb 19 16:50:40 UTC 2025\n> Python Version:  3.12.7 (main, Feb  4 2025, 14:46:03) [GCC 14.2.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.51\n> langchain: 0.3.23\n> langchain_community: 0.3.21\n> langsmith: 0.3.27\n> langchain_chroma: 0.2.2\n> langchain_cli: 0.0.36\n> langchain_google_genai: 2.1.2\n> langchain_google_vertexai: 2.0.9\n> langchain_neo4j: 0.4.0\n> langchain_nomic: 0.1.4\n> langchain_ollama: 0.3.1\n> langchain_openai: 0.3.12\n> langchain_text_splitters: 0.3.8\n> langgraph_sdk: 0.1.61\n> langserve: 0.3.1\n\nOther Dependencies\n------------------\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> anthropic[vertexai]: Installed. No version info available.\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0: Installed. No version info available.\n> dataclasses-json<0.7,>=0.5.7: Installed. No version info available.\n> fastapi: 0.115.9\n> filetype: 1.2.0\n> gitpython<4,>=3: Installed. No version info available.\n> google-ai-generativelanguage: 0.6.17\n> google-cloud-aiplatform: 1.71.1\n> google-cloud-storage: 2.19.0\n> gritql<1.0.0,>=0.2.0: Installed. No version info available.\n> httpx: 0.27.2\n> httpx-sse: 0.4.0\n> httpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-azure-ai;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.49: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.51: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-perplexity;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.8: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langchain<1.0.0,>=0.3.23: Installed. No version info available.\n> langserve[all]>=0.0.51: Installed. No version info available.\n> langsmith-pyo3: Installed. No version info available.\n> langsmith<0.4,>=0.1.125: Installed. No version info available.\n> langsmith<0.4,>=0.1.17: Installed. No version info available.\n> neo4j: 5.28.1\n> neo4j-graphrag: 1.6.1\n> nomic: 3.4.1\n> numpy<2.0.0,>=1.22.4;: Installed. No version info available.\n> numpy<2.0.0,>=1.26.2;: Installed. No version info available.\n> numpy<3,>=1.26.2: Installed. No version info available.\n> ollama<1,>=0.4.4: Installed. No version info available.\n> openai-agents: Installed. No version info available.\n> openai<2.0.0,>=1.68.2: Installed. No version info available.\n> opentelemetry-api: 1.31.1\n> opentelemetry-exporter-otlp-proto-http: Installed. No version info available.\n> opentelemetry-sdk: 1.31.1\n> orjson: 3.10.16\n> packaging: 24.2\n> packaging<25,>=23.2: Installed. No version info available.\n> pillow: 10.4.0\n> pydantic: 2.9.2\n> pydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\n> pydantic<3.0.0,>=2.5.2;: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4;: Installed. No version info available.\n> pytest: 8.3.5\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> requests<3,>=2: Installed. No version info available.\n> rich: 14.0.0\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> sse-starlette: 1.8.2\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken<1,>=0.7: Installed. No version info available.\n> tomlkit>=0.12: Installed. No version info available.\n> typer[all]<1.0.0,>=0.9.0: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> uvicorn<1.0,>=0.23: Installed. No version info available.\n> zstandard: 0.23.0\n", "created_at": "2025-04-10", "closed_at": "2025-04-10", "labels": [], "State": "closed", "Author": "khteh"}
{"issue_number": 4228, "issue_title": "I keep getting `AsyncConnectionPool` RuntimeWarning", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nTop-level main.py:\napp.db_pool = AsyncConnectionPool(...)\nawait app.db_pool.open()\n\nThe rest of the code I use:\n\nasync with AsyncConnectionPool(...)\nError Message and Stack Trace (if applicable)\nIt will be helpful if the following `RuntimeWarning` show the source line:\n\n/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/psycopg_pool/pool_async.py:142: RuntimeWarning: opening the async pool AsyncConnectionPool in the constructor is deprecated and will not be supported anymore in a future release. Please use `await pool.open()`, or use the pool as context manager using: `async with AsyncConnectionPool(...) as pool: `...\n  warnings.warn(\nDescription\nNo idea where the warning comes from.\nSystem Info\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #21-Ubuntu SMP PREEMPT_DYNAMIC Wed Feb 19 16:50:40 UTC 2025\n> Python Version:  3.12.7 (main, Feb  4 2025, 14:46:03) [GCC 14.2.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.51\n> langchain: 0.3.23\n> langchain_community: 0.3.21\n> langsmith: 0.3.27\n> langchain_chroma: 0.2.2\n> langchain_cli: 0.0.36\n> langchain_google_genai: 2.1.2\n> langchain_google_vertexai: 2.0.9\n> langchain_neo4j: 0.4.0\n> langchain_nomic: 0.1.4\n> langchain_ollama: 0.3.1\n> langchain_openai: 0.3.12\n> langchain_text_splitters: 0.3.8\n> langgraph_sdk: 0.1.61\n> langserve: 0.3.1\n\nOther Dependencies\n------------------\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> anthropic[vertexai]: Installed. No version info available.\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0: Installed. No version info available.\n> dataclasses-json<0.7,>=0.5.7: Installed. No version info available.\n> fastapi: 0.115.9\n> filetype: 1.2.0\n> gitpython<4,>=3: Installed. No version info available.\n> google-ai-generativelanguage: 0.6.17\n> google-cloud-aiplatform: 1.71.1\n> google-cloud-storage: 2.19.0\n> gritql<1.0.0,>=0.2.0: Installed. No version info available.\n> httpx: 0.27.2\n> httpx-sse: 0.4.0\n> httpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-azure-ai;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.49: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.51: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-perplexity;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.8: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langchain<1.0.0,>=0.3.23: Installed. No version info available.\n> langserve[all]>=0.0.51: Installed. No version info available.\n> langsmith-pyo3: Installed. No version info available.\n> langsmith<0.4,>=0.1.125: Installed. No version info available.\n> langsmith<0.4,>=0.1.17: Installed. No version info available.\n> neo4j: 5.28.1\n> neo4j-graphrag: 1.6.1\n> nomic: 3.4.1\n> numpy<2.0.0,>=1.22.4;: Installed. No version info available.\n> numpy<2.0.0,>=1.26.2;: Installed. No version info available.\n> numpy<3,>=1.26.2: Installed. No version info available.\n> ollama<1,>=0.4.4: Installed. No version info available.\n> openai-agents: Installed. No version info available.\n> openai<2.0.0,>=1.68.2: Installed. No version info available.\n> opentelemetry-api: 1.31.1\n> opentelemetry-exporter-otlp-proto-http: Installed. No version info available.\n> opentelemetry-sdk: 1.31.1\n> orjson: 3.10.16\n> packaging: 24.2\n> packaging<25,>=23.2: Installed. No version info available.\n> pillow: 10.4.0\n> pydantic: 2.9.2\n> pydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\n> pydantic<3.0.0,>=2.5.2;: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4;: Installed. No version info available.\n> pytest: 8.3.5\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> requests<3,>=2: Installed. No version info available.\n> rich: 14.0.0\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> sse-starlette: 1.8.2\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken<1,>=0.7: Installed. No version info available.\n> tomlkit>=0.12: Installed. No version info available.\n> typer[all]<1.0.0,>=0.9.0: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> uvicorn<1.0,>=0.23: Installed. No version info available.\n> zstandard: 0.23.0\n", "created_at": "2025-04-10", "closed_at": "2025-04-10", "labels": [], "State": "closed", "Author": "khteh"}
{"issue_number": 4227, "issue_title": "CompiledStateGraph.steam demo returns with None", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\ngraph_builder = StateGraph(State)\n\nllm = ChatOllama(model=\"llama3.1:latest\")\n\ndef chatbot(state: State):\n    return {\"message\":[llm.invoke(state[\"messages\"])]}\n\ngraph_builder.add_node(\"chatbot\", chatbot)\ngraph_builder.add_edge(START, \"chatbot\")\ngraph_builder.add_edge(\"chatbot\", END)\ngraph = graph_builder.compile()\ndef stream_graph_updates(user_input: str):\n    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n        for value in event.values():\n            print(\"Assistant:\", value[\"messages\"][-1].content)\n\nwhile True:\n    try:\n        user_input = input(\"User: \")\n        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n            print(\"Goodbye!\")\n            break\n        stream_graph_updates(user_input)\n    except:\n        user_input = \"What do you know about LangGraph?\"\n        print(\"User: \" + user_input)\n        stream_graph_updates(user_input)\n        break\nError Message and Stack Trace (if applicable)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[15], line 13\n     12         break\n---> 13     stream_graph_updates(user_input)\n     14 except:\n\nCell In[15], line 5\n      4 for value in event.values():\n----> 5     print(\"Assistant:\", value[\"messages\"][-1].content)\n\nTypeError: 'NoneType' object is not subscriptable\n\nDuring handling of the above exception, another exception occurred:\n\nTypeError                                 Traceback (most recent call last)\nCell In[15], line 17\n     15 user_input = \"What do you know about LangGraph?\"\n     16 print(\"User: \" + user_input)\n---> 17 stream_graph_updates(user_input)\n     18 break\n\nCell In[15], line 5\n      3 print(event)\n      4 for value in event.values():\n----> 5     print(\"Assistant:\", value[\"messages\"][-1].content)\n\nTypeError: 'NoneType' object is not subscriptable\nDescription\nI'm studying basics from [(https://langchain-ai.github.io/langgraph/tutorials/introduction/#part-1-build-a-basic-chatbot)] and I only change the ChatAnthropic to ChatOllama. I tried to print(event) returned by graph.steam and it turned out to be {'chatbot': None}\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #21~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Feb 24 16:52:15 UTC 2\nPython Version:  3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:56:27) [GCC 11.2.0]\n\nPackage Information\n\nlangchain_core: 0.3.49\nlangchain: 0.3.21\nlangchain_community: 0.3.20\nlangsmith: 0.3.27\nlangchain_anthropic: 0.3.10\nlangchain_experimental: 0.3.4\nlangchain_huggingface: 0.1.2\nlangchain_ollama: 0.3.0\nlangchain_text_splitters: 0.3.7\nlanggraph_sdk: 0.1.60\nlanggraph_test: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nanthropic<1,>=0.49.0: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\nhuggingface-hub: 0.29.3\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-azure-ai;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.45: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.47: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.7: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.21: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<3,>=1.26.2: Installed. No version info available.\nollama<1,>=0.4.4: Installed. No version info available.\nopenai-agents: Installed. No version info available.\nopentelemetry-api: Installed. No version info available.\nopentelemetry-exporter-otlp-proto-http: Installed. No version info available.\nopentelemetry-sdk: Installed. No version info available.\norjson: 3.10.15\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: Installed. No version info available.\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: 13.9.4\nsentence-transformers: 3.4.1\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntokenizers: 0.21.1\ntransformers: 4.49.0\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-04-10", "closed_at": "2025-04-12", "labels": ["question"], "State": "closed", "Author": "LZY-SPCA"}
{"issue_number": 4226, "issue_title": "`tools_condition` throws KeyError in my code", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n@dataclass\nclass CustomAgentState(AgentState):\n    context: List[Document]\n    is_last_step: IsLastStep\n\n\n            graph_builder = StateGraph(CustomAgentState)\n            graph_builder.add_node(\"Agent\", self.Agent)\n            graph_builder.add_node(\"Retrieve\", ToolNode([self._vectorStore.retriever_tool])) # Execute the retrieval.\n            graph_builder.add_node(\"Rewrite\", self.Rewrite)\n            graph_builder.add_node(\"Generate\", self.Generate)\n            graph_builder.add_edge(START, \"Agent\")\n            #graph_builder.set_entry_point(\"query_or_respond\")\n            graph_builder.add_conditional_edges(\n                \"Agent\",\n                # Assess agent decision\n                tools_condition,\n                {\n                    \"\"\"\n                    Translate the condition outputs to nodes in our graph\n                    which node to go to based on the output of the conditional edge function - tools_condition.\n                    \"\"\"\n                    \"tools\": \"Retrieve\",\n                    END: END\n                },\n            )\n            # Edges taken after the `action` node is called.\n            graph_builder.add_conditional_edges(\n                \"Retrieve\",\n                # Assess agent decision\n                self.GradeDocuments,\n            )\n            graph_builder.add_edge(\"Generate\", END)\n            graph_builder.add_edge(\"Rewrite\", \"Agent\")\n            self._graph = graph_builder.compile(store=self._in_memory_store, name=\"Checkedpoint StateGraph RAG\")\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/src/Python/rag-agent/src/rag_agent/GraphRAG.py\", line 352, in <module>\n    asyncio.run(main())\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/usr/src/Python/rag-agent/src/rag_agent/GraphRAG.py\", line 348, in main\n    await graph.TestDirectResponseWithoutRetrieval(config, \"Hello, who are you?\")\n  File \"/usr/src/Python/rag-agent/src/rag_agent/GraphRAG.py\", line 303, in TestDirectResponseWithoutRetrieval\n    async for step in self._graph.astream(\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 2651, in astream\n    async for _ in runner.atick(\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/graph/branch.py\", line 193, in _aroute\n    return self._finish(writer, input, result, config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/graph/branch.py\", line 208, in _finish\n    r if isinstance(r, Send) else self.ends[r] for r in result\n                                  ~~~~~~~~~^^^\nKeyError: 'tools'\nDuring task with name 'Agent' and id '33fcf59e-ff5d-fb29-d8b3-f4ef9c98ff36'\nDescription\nI follow this tutorial: https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_agentic_rag/#nodes-and-edges\nSystem Info\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #21-Ubuntu SMP PREEMPT_DYNAMIC Wed Feb 19 16:50:40 UTC 2025\n> Python Version:  3.12.7 (main, Feb  4 2025, 14:46:03) [GCC 14.2.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.51\n> langchain: 0.3.23\n> langchain_community: 0.3.21\n> langsmith: 0.3.27\n> langchain_chroma: 0.2.2\n> langchain_cli: 0.0.36\n> langchain_google_genai: 2.1.2\n> langchain_google_vertexai: 2.0.9\n> langchain_neo4j: 0.4.0\n> langchain_nomic: 0.1.4\n> langchain_ollama: 0.3.1\n> langchain_openai: 0.3.12\n> langchain_text_splitters: 0.3.8\n> langgraph_sdk: 0.1.61\n> langserve: 0.3.1\n\nOther Dependencies\n------------------\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> anthropic[vertexai]: Installed. No version info available.\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0: Installed. No version info available.\n> dataclasses-json<0.7,>=0.5.7: Installed. No version info available.\n> fastapi: 0.115.9\n> filetype: 1.2.0\n> gitpython<4,>=3: Installed. No version info available.\n> google-ai-generativelanguage: 0.6.17\n> google-cloud-aiplatform: 1.71.1\n> google-cloud-storage: 2.19.0\n> gritql<1.0.0,>=0.2.0: Installed. No version info available.\n> httpx: 0.27.2\n> httpx-sse: 0.4.0\n> httpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-azure-ai;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.49: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.51: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-perplexity;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.8: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langchain<1.0.0,>=0.3.23: Installed. No version info available.\n> langserve[all]>=0.0.51: Installed. No version info available.\n> langsmith-pyo3: Installed. No version info available.\n> langsmith<0.4,>=0.1.125: Installed. No version info available.\n> langsmith<0.4,>=0.1.17: Installed. No version info available.\n> neo4j: 5.28.1\n> neo4j-graphrag: 1.6.1\n> nomic: 3.4.1\n> numpy<2.0.0,>=1.22.4;: Installed. No version info available.\n> numpy<2.0.0,>=1.26.2;: Installed. No version info available.\n> numpy<3,>=1.26.2: Installed. No version info available.\n> ollama<1,>=0.4.4: Installed. No version info available.\n> openai-agents: Installed. No version info available.\n> openai<2.0.0,>=1.68.2: Installed. No version info available.\n> opentelemetry-api: 1.31.1\n> opentelemetry-exporter-otlp-proto-http: Installed. No version info available.\n> opentelemetry-sdk: 1.31.1\n> orjson: 3.10.16\n> packaging: 24.2\n> packaging<25,>=23.2: Installed. No version info available.\n> pillow: 10.4.0\n> pydantic: 2.9.2\n> pydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\n> pydantic<3.0.0,>=2.5.2;: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4;: Installed. No version info available.\n> pytest: 8.3.5\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> requests<3,>=2: Installed. No version info available.\n> rich: 14.0.0\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> sse-starlette: 1.8.2\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken<1,>=0.7: Installed. No version info available.\n> tomlkit>=0.12: Installed. No version info available.\n> typer[all]<1.0.0,>=0.9.0: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> uvicorn<1.0,>=0.23: Installed. No version info available.\n> zstandard: 0.23.0\n", "created_at": "2025-04-10", "closed_at": null, "labels": ["question"], "State": "open", "Author": "khteh"}
{"issue_number": 4218, "issue_title": "blockbuster causes blocking error via tiktoken in async context with latest langgraph-api", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nasync def retrieve_relevant_docs(state: ChatState, config: RunnableConfig) -> dict:\n    \"\"\"Retrieve relevant documents based on the latest user message.\"\"\"\n    #configurable = ChatConfigurable.from_runnable_config(config)\n    last_user_message_obj = next(\n            (m for m in reversed(state.messages) if isinstance(m, HumanMessage)), None)\n    if last_user_message_obj:\n        last_user_message = last_user_message_obj.content  # Extract the content (string)\n        print(f\"last User message is ---> {last_user_message}\")\n        #We retrieve our docs here based on last user message content\n        retrieved_docs = vector_store.similarity_search(last_user_message)\n        print(f\"RETRIEVED DOCS:::::::::::::::::: {retrieved_docs}\")\n        return {\"documents\": retrieved_docs}\n    else:\n        return {\"documents\": \"<documents></documents>\"}\nError Message and Stack Trace (if applicable)\n2025-04-09T11:17:34.218820Z [info     ] POST /threads/be8f27cd-e846-4913-8976-d4e24399f8e7/history 200 5ms [langgraph_api.server] api_variant=local_dev latency_ms=5 method=POST path=/threads/be8f27cd-e846-4913-8976-d4e24399f8e7/history path_params={'thread_id': 'be8f27cd-e846-4913-8976-d4e24399f8e7'} proto=1.1 query_string= req_header={'host': '127.0.0.1:2024', 'connection': 'keep-alive', 'content-length': '14', 'x-auth-scheme': 'langsmith', 'sec-ch-ua-platform': '\"Linux\"', 'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36', 'sec-ch-ua': '\"Chromium\";v=\"134\", \"Not:A-Brand\";v=\"24\", \"Google Chrome\";v=\"134\"', 'content-type': 'application/json', 'sec-ch-ua-mobile': '?0', 'accept': '*/*', 'origin': 'https://smith.langchain.com', 'sec-fetch-site': 'cross-site', 'sec-fetch-mode': 'cors', 'sec-fetch-dest': 'empty', 'accept-encoding': 'gzip, deflate, br, zstd', 'accept-language': 'en-US,en;q=0.9'} res_header={'content-length': '2', 'content-type': 'application/json'} route=/threads/{thread_id}/history status=200 thread_name=MainThread\n2025-04-09T11:17:35.158316Z [info     ] Starting background run        [langgraph_api.worker] api_variant=local_dev run_attempt=1 run_created_at=2025-04-09T11:17:34.191979+00:00 run_id=1f015343-c41d-6a4b-b702-348d3daac89c run_queue_ms=965 run_started_at=2025-04-09T11:17:35.157432+00:00 thread_name=asyncio_0\nlast User message is ---> testing\n2025-04-09T11:17:35.224181Z [error    ] Background run failed. Exception: Blocking call to os.listdir [langgraph_api.worker] api_variant=local_dev graph_id=chatbot run_attempt=1 run_created_at=2025-04-09T11:17:34.191979+00:00 run_ended_at=2025-04-09T11:17:35.223977+00:00 run_exec_ms=66 run_id=1f015343-c41d-6a4b-b702-348d3daac89c run_started_at=2025-04-09T11:17:35.157432+00:00 thread_name=asyncio_1\nTraceback (most recent call last):\n  File \"/home/c/anaconda3/envs/test/lib/python3.11/site-packages/langgraph_api/worker.py\", line 149, in worker\n    await asyncio.wait_for(consume(stream, run_id), BG_JOB_TIMEOUT_SECS)\n  File \"/home/c/anaconda3/envs/test/lib/python3.11/asyncio/tasks.py\", line 489, in wait_for\n    return fut.result()\n           ^^^^^^^^^^^^\n  File \"/home/c/anaconda3/envs/test/lib/python3.11/site-packages/langgraph_api/stream.py\", line 268, in consume\n    raise e from None\n  File \"/home/c/anaconda3/envs/test/lib/python3.11/site-packages/langgraph_api/stream.py\", line 258, in consume\n    async for mode, payload in stream:\n  File \"/home/c/anaconda3/envs/test/lib/python3.11/site-packages/langgraph_api/stream.py\", line 209, in astream_state\n    event = await wait_if_not_done(anext(stream, sentinel), done)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/c/anaconda3/envs/test/lib/python3.11/site-packages/langgraph_api/asyncio.py\", line 82, in wait_if_not_done\n    raise e.exceptions[0] from None\n  File \"/home/c/anaconda3/envs/test/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 2651, in astream\n    async for _ in runner.atick(\n  File \"/home/c/GitHub/rag-graph-long-mem/./src/chatbot/graph.py\", line 154, in retrieve_relevant_docs\n    retrieved_docs = vector_store.similarity_search(last_user_message)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/c/anaconda3/envs/test/lib/python3.11/site-packages/langchain_postgres/vectorstores.py\", line 942, in similarity_search\n    embedding = self.embeddings.embed_query(query)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/c/anaconda3/envs/test/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 629, in embed_query\n    return self.embed_documents([text])[0]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/c/anaconda3/envs/test/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 588, in embed_documents\n    return self._get_len_safe_embeddings(texts, engine=engine)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/c/anaconda3/envs/test/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 480, in _get_len_safe_embeddings\n    _iter, tokens, indices = self._tokenize(texts, _chunk_size)\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/c/anaconda3/envs/test/lib/python3.11/site-packages/langchain_openai/embeddings/base.py\", line 420, in _tokenize\n    encoding = tiktoken.encoding_for_model(model_name)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/c/anaconda3/envs/test/lib/python3.11/site-packages/tiktoken/model.py\", line 110, in encoding_for_model\n    return get_encoding(encoding_name_for_model(model_name))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/c/anaconda3/envs/test/lib/python3.11/site-packages/tiktoken/registry.py\", line 75, in get_encoding\n    _find_constructors()\n  File \"/home/c/anaconda3/envs/test/lib/python3.11/site-packages/tiktoken/registry.py\", line 41, in _find_constructors\n    for mod_name in _available_plugin_modules():\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/c/anaconda3/envs/test/lib/python3.11/site-packages/tiktoken/registry.py\", line 28, in _available_plugin_modules\n    for _, mod_name, _ in plugin_mods:\n  File \"/home/c/anaconda3/envs/test/lib/python3.11/pkgutil.py\", line 130, in iter_modules\n    for name, ispkg in iter_importer_modules(i, prefix):\n  File \"/home/c/anaconda3/envs/test/lib/python3.11/pkgutil.py\", line 151, in _iter_file_finder_modules\n    filenames = os.listdir(importer.path)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/c/anaconda3/envs/test/lib/python3.11/site-packages/blockbuster/blockbuster.py\", line 109, in wrapper\n    raise BlockingError(func_name)\nblockbuster.blockbuster.BlockingError: Blocking call to os.listdir\nDuring task with name 'retrieve_relevant_docs' and id 'e4bc5b5e-6853-492c-3db2-031bde24ea20'\nHeads up! LangGraph identified a synchronous blocking call in your code. When running in an ASGI web server, blocking calls can degrade performance for everyone since they tie up the event loop.\n\nHere are your options to fix this:\n\n1. Best approach: Convert any blocking code to use async/await patterns\n   For example, use 'await aiohttp.get()' instead of 'requests.get()'\n\n2. Quick fix: Move blocking operations to a separate thread\n   Example: 'await asyncio.to_thread(your_blocking_function)'\n\n3. Override (if you can't change the code):\n   - For development: Run 'langgraph dev --allow-blocking'\n   - For deployment: Set 'BG_JOB_ISOLATED_LOOPS=true' environment variable\n\nThese blocking operations can prevent health checks and slow down other runs in your deployment. Following these recommendations will help keep your LangGraph application running smoothly!\nDescription\nWhen upgrading to the latest langgraph-cli and langgraph-api, I encountered this blocking error:\nblockbuster.blockbuster.BlockingError: Blocking call to os.listdir\nIt originates from tiktoken.encoding_for_model(...) \u2192 pkgutil.iter_modules \u2192 os.listdir.\nThis occurs when using langgraph-cli[inmem] during a background run (a vector store embedding call), causing a crash unless I run with --allow-blocking. However, doing so leads to further async-related issues down the line.\nReverting to older versions of langgraph-cli and langgraph-api (pre-blockbuster integration) resolves the issue.\nEnvironment:\nOS: Ubuntu 24.10\nPython: 3.11\nUsing: langgraph-cli[inmem]\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #21-Ubuntu SMP PREEMPT_DYNAMIC Wed Feb 19 16:50:40 UTC 2025\nPython Version:  3.11.11 (main, Dec 11 2024, 16:28:39) [GCC 11.2.0]\n\nPackage Information\n\nlangchain_core: 0.3.51\nlangchain: 0.3.23\nlangsmith: 0.3.27\nlangchain_anthropic: 0.3.10\nlangchain_openai: 0.3.12\nlangchain_pinecone: 0.2.5\nlangchain_postgres: 0.0.14\nlangchain_tests: 0.3.17\nlangchain_text_splitters: 0.3.8\nlanggraph_api: 0.0.48\nlanggraph_cli: 0.1.89\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.61\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<3.11,>=3.10: Installed. No version info available.\nanthropic<1,>=0.49.0: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nasyncpg: 0.30.0\nblockbuster: 1.5.24\nclick: 8.1.8\ncloudpickle: 3.1.1\ncryptography: 44.0.2\nhttpx: 0.28.1\nhttpx<1,>=0.25.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\njsonschema-rs: 0.29.1\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-azure-ai;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.45: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.49: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.51: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-perplexity;: Installed. No version info available.\nlangchain-tests<1.0.0,>=0.3.7: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.8: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlanggraph: 0.3.27\nlanggraph-checkpoint: 2.0.24\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy: 1.26.4\nnumpy<3,>=1.26.2: Installed. No version info available.\nnumpy>=1.26.4: Installed. No version info available.\nopenai-agents: Installed. No version info available.\nopenai<2.0.0,>=1.68.2: Installed. No version info available.\nopentelemetry-api: Installed. No version info available.\nopentelemetry-exporter-otlp-proto-http: Installed. No version info available.\nopentelemetry-sdk: Installed. No version info available.\norjson: 3.10.16\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npgvector: 0.3.6\npinecone[async]<7.0.0,>=6.0.0: Installed. No version info available.\npsycopg: 3.2.6\npsycopg-pool: 3.2.6\npydantic: 2.11.3\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npyjwt: 2.10.1\npytest: 8.3.5\npytest-asyncio<1,>=0.20: Installed. No version info available.\npytest-socket<1,>=0.6.0: Installed. No version info available.\npytest<9,>=7: Installed. No version info available.\npython-dotenv: 1.1.0\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: Installed. No version info available.\nsqlalchemy: 2.0.40\nSQLAlchemy<3,>=1.4: Installed. No version info available.\nsse-starlette: 2.1.3\nstarlette: 0.46.1\nstructlog: 25.2.0\nsyrupy<5,>=4: Installed. No version info available.\ntenacity: 9.1.2\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nuvicorn: 0.34.0\nwatchfiles: 1.0.5\nzstandard: 0.23.0\n", "created_at": "2025-04-09", "closed_at": "2025-04-09", "labels": [], "State": "closed", "Author": "coralreefman"}
{"issue_number": 4217, "issue_title": "create_react_agent fails to resume after external update_state with ToolMessage (tried with astream/ainvoke)", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n# To run this example, first of all install dependencies with poetry. \n# After that, just run:\n# poetry run python minimal_langgraph_test_openai.py\n\nimport asyncio\nimport os\nimport uuid\nimport logging\nfrom dotenv import load_dotenv\n\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage, AIMessage, ToolMessage\nfrom pydantic import BaseModel, Field\nfrom langchain_core.tools import tool\n\nfrom langgraph.checkpoint.sqlite import SqliteSaver \nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.prebuilt import create_react_agent\n\nload_dotenv()\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nLLM_MODEL = \"gpt-4o\" \n\n\nclass DataInputArgs(BaseModel):\n    \"\"\"Schema for arguments passed to the data processing tool.\"\"\"\n    data: str = Field(description=\"The data string that needs to be processed.\")\n\n@tool\ndef get_data_externally() -> str:\n    \"\"\"\n    Retrieves data from a simulated external source when not provided initially by the user. \n    Use this tool FIRST when the user asks to process data but has NOT provided the data in their message.\n    This tool obtains the data needed by the 'process_data' tool.\n    \"\"\"\n    logger.info(\"--- SIMULATED TOOL CALL: get_data_externally ---\")\n    return \"Placeholder: Data retrieval initiated externally.\" \n\n\n@tool(args_schema=DataInputArgs)\ndef process_data(data: str) -> str:\n    \"\"\"\n    Processes the provided data string. Requires the 'data' argument containing the string.\n    Use this tool ONLY AFTER 'get_data_externally' has successfully run and the data \n    has been provided back to you, or if the user provided the data directly.\n    \"\"\"\n    logger.info(f\"--- SIMULATED TOOL EXECUTION: process_data ---\")\n    logger.info(f\"  Received data: {data[:100]}...\")\n    processed_result = f\"Successfully processed data: '{data}'\"\n    logger.info(f\"  Returning: {processed_result}\")\n    return processed_result\n\n\nllm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n\n\ncheckpointer = MemorySaver()\n\ntools = [get_data_externally, process_data]\n\nprompt_template = (\n    \"You are an agent tasked with processing data. \"\n    \"You have the following tools available: 'get_data_externally', 'process_data'.\\n\"\n    \"IMPORTANT FLOW: When asked to process data:\\n\"\n    \"1. Check if the user provided the data in their message.\\n\"\n    \"2. If the data IS provided, directly use 'process_data' with that data.\\n\"\n    \"3. If the data IS NOT provided, YOU MUST FIRST use the 'get_data_externally' tool to retrieve the data.\\n\"\n    \"4. After 'get_data_externally' runs and you receive the data back (as a ToolMessage), YOU MUST THEN use 'process_data' with the retrieved data.\\n\"\n    \"Do not answer directly about processing data if you haven't processed it using the tools and the specific data.\"\n)\n\ngraph = create_react_agent(\n    llm,\n    tools=tools,\n    checkpointer=checkpointer,\n    prompt=prompt_template,\n    interrupt_before=[\"tools\"] \n)\n\nasync def simulate_bot_flow():\n    thread_id = str(uuid.uuid4())\n    config = {\"configurable\": {\"thread_id\": thread_id}}\n    logger.info(f\"Starting simulation with Thread ID: {thread_id}\")\n\n    user_input = \"process the external data\" # Input que no incluye los datos\n    logger.info(f\"Initial User Input: '{user_input}'\")\n\n    get_data_tool_call_id = None\n    interrupted_for_get_data = False\n    \n    print(\"\\nRUNNING INITIAL STREAM...\")\n    try:\n        async for chunk in graph.astream({\"messages\": [(\"user\", user_input)]}, config=config, stream_mode=\"updates\"):\n            logger.info(f\"Initial Chunk Keys: {list(chunk.keys())}\")\n            \n            agent_chunk = chunk.get('agent')\n            if agent_chunk and isinstance(agent_chunk.get('messages', [None])[-1], AIMessage):\n                last_ai_msg = agent_chunk['messages'][-1]\n                if last_ai_msg.tool_calls:\n                    logger.info(f\"  Agent wants to call: {last_ai_msg.tool_calls}\")\n                    if last_ai_msg.tool_calls[0].get('name') == 'get_data_externally':\n                        get_data_tool_call_id = last_ai_msg.tool_calls[0].get('id')\n                        logger.info(f\"  Captured get_data_externally tool_call_id: {get_data_tool_call_id}\")\n\n            if \"__interrupt__\" in chunk:\n                logger.info(\"  >>> Interrupt Detected <<<\")\n                if get_data_tool_call_id: \n                    interrupted_for_get_data = True\n                    logger.info(\"  Interrupt is for 'get_data_externally'. SIMULATING PAUSE.\")\n                    break\n                else:\n                    logger.warning(\"  Interrupt detected, but couldn't confirm it was for 'get_data_externally'.\")\n        \n        if not interrupted_for_get_data:\n            logger.error(\"Simulation Failed: Did not interrupt for 'get_data_externally'. Check LLM decision/prompt.\")\n            return\n\n        print(\"\\nSIMULATING EXTERNAL DATA FETCH AND STATE UPDATE...\")\n        simulated_data_content = f\"This is the externally fetched data for {thread_id}\"\n        logger.info(f\"  Simulated data content: '{simulated_data_content}'\")\n        \n        tool_message = ToolMessage(content=simulated_data_content, tool_call_id=get_data_tool_call_id)\n        logger.info(f\"  Constructed ToolMessage for tool_call_id {get_data_tool_call_id}\")\n\n        graph.update_state(config, {\"messages\": [tool_message]})\n        logger.info(\"  Graph state updated synchronously.\")\n\n        try:\n            current_state = await graph.aget_state(config)\n            logger.info(\"  --- State Before Resume ---\")\n            messages = current_state.values.get(\"messages\", [])\n            for msg in messages[-3:]: \n                logger.info(f\"    {type(msg).__name__}: {str(msg)[:200]}...\") # Limitar longitud\n            logger.info(\"  -------------------------\")\n        except Exception as e:\n             logger.error(f\"  Error getting state before resume: {e}\")\n\n        print(\"\\nATTEMPTING RESUMPTION with astream(None, stream_mode='debug')...\")\n        resumption_chunks_received = []\n        agent_called_process_data = False\n        final_response = None\n\n        try:\n            async for chunk in graph.astream(None, config=config, stream_mode=\"debug\"):\n                logger.info(\"  +++ Received DEBUG chunk after resume +++\")\n                logger.info(chunk) \n                resumption_chunks_received.append(chunk)\n\n                if isinstance(chunk, dict) and 'agent' in chunk.get('node', ''):\n                     output_messages = chunk.get('output', {}).get('messages', [])\n                     if output_messages and isinstance(output_messages[-1], AIMessage):\n                          ai_output : AIMessage = output_messages[-1]\n                          if ai_output.tool_calls:\n                              for tc in ai_output.tool_calls:\n                                   if tc.get('name') == 'process_data':\n                                       logger.info(\"  >>> Agent correctly decided to call 'process_data' after resume! <<<\")\n                                       agent_called_process_data = True\n                          elif ai_output.content:\n                               logger.warning(f\"  Agent produced content instead of calling process_data: {ai_output.content}\")\n                               final_response = ai_output.content\n\n            logger.info(\"Resumption stream finished.\")\n\n        except Exception as e_resume:\n             logger.error(f\"ERROR during resumption stream: {e_resume}\", exc_info=True)\n\n\n        print(\"\\n--- SIMULATION RESULTS ---\")\n        if not resumption_chunks_received:\n            logger.error(\"FAILURE: No chunks received after resumption.\")\n        elif agent_called_process_data:\n            logger.info(\"SUCCESS (Potentially): Agent decided to call 'process_data'. Further steps (interrupt, execution, final answer) should follow.\")\n        else:\n            logger.warning(\"FAILURE: Resumption finished, but agent did not call 'process_data'.\")\n            if final_response:\n                logger.warning(f\"  Agent's final response was: {final_response}\")\n            logger.warning(\"  Check DEBUG chunks above to see what the agent node did (if anything).\")\n\n\n    except Exception as e:\n        logger.error(f\"Unhandled exception in simulation: {e}\", exc_info=True)\n\nif __name__ == \"__main__\":\n    if not os.getenv(\"OPENAI_API_KEY\"):\n        print(\"Error: OPENAI_API_KEY environment variable not set.\")\n    else:\n        asyncio.run(simulate_bot_flow())\nError Message and Stack Trace (if applicable)\n2025-04-09 11:38:42,503 - INFO - Starting simulation with Thread ID: e4dc31f9-aa01-4be0-bb5d-fbae01af2861\n2025-04-09 11:38:42,503 - INFO - Initial User Input: 'process the external data'\n\nRUNNING INITIAL STREAM...\n2025-04-09 11:38:43,847 - INFO - HTTP Request: POST https://... \"HTTP/1.1 200 OK\"\n2025-04-09 11:38:43,855 - INFO - Initial Chunk Keys: ['agent']\n2025-04-09 11:38:43,855 - INFO -   Agent wants to call: [{'name': 'get_data_externally', 'args': {}, 'id': 'call_ZSN33LoRAWXnSjL0ya2LhAtQ', 'type': 'tool_call'}]    \n2025-04-09 11:38:43,855 - INFO -   Captured get_data_externally tool_call_id: call_ZSN33LoRAWXnSjL0ya2LhAtQ\n2025-04-09 11:38:43,855 - INFO - Initial Chunk Keys: ['__interrupt__']\n2025-04-09 11:38:43,855 - INFO -   >>> Interrupt Detected <<<\n2025-04-09 11:38:43,855 - INFO -   Interrupt is for 'get_data_externally'. SIMULATING PAUSE.\n\nSIMULATING EXTERNAL DATA FETCH AND STATE UPDATE...\n2025-04-09 11:38:43,857 - INFO -   Simulated data content: 'This is the externally fetched data for e4dc31f9-aa01-4be0-bb5d-fbae01af2861'\n2025-04-09 11:38:43,857 - INFO -   Constructed ToolMessage for tool_call_id call_ZSN33LoRAWXnSjL0ya2LhAtQ\n2025-04-09 11:38:43,858 - INFO -   Graph state updated synchronously.\n2025-04-09 11:38:43,858 - INFO -   --- State Before Resume ---\n2025-04-09 11:38:43,859 - INFO -     HumanMessage: content='process the external data' additional_kwargs={} response_metadata={} id='fe8e6950-181a-4697-9daa-6acca75e532c'...\n2025-04-09 11:38:43,859 - INFO -     AIMessage: content='' additional_kwargs={'tool_calls': [{'id': 'call_ZSN33LoRAWXnSjL0ya2LhAtQ', 'function': {'arguments': '{}', 'name': 'get_data_externally'}, 'type': 'function'}], 'refusal': None} response_met...\n2025-04-09 11:38:43,859 - INFO -     ToolMessage: content='This is the externally fetched data for e4dc31f9-aa01-4be0-bb5d-fbae01af2861' id='45fa5e80-53aa-4e60-b9db-a95e71be34a7' tool_call_id='call_ZSN33LoRAWXnSjL0ya2LhAtQ'...\n2025-04-09 11:38:43,860 - INFO -   -------------------------\n\nATTEMPTING RESUMPTION with astream(None, stream_mode='debug')...\n2025-04-09 11:38:43,861 - INFO -   +++ Received DEBUG chunk after resume +++\n2025-04-09 11:38:43,861 - INFO - {'type': 'checkpoint', 'timestamp': '2025-04-09T09:38:43.858381+00:00', 'step': 2, 'payload': {'config': {'tags': [], 'metadata': ChainMap({'thread_id': 'e4dc31f9-aa01-4be0-bb5d-fbae01af2861'}), 'callbacks': None, 'recursion_limit': 25, 'configurable': {'checkpoint_ns': '', 'thread_id': 'e4dc31f9-aa01-4be0-bb5d-fbae01af2861', 'checkpoint_id': '1f015266-d80c-6e09-8002-23d48d4404f4'}}, 'parent_config': {'configurable': {'thread_id': 'e4dc31f9-aa01-4be0-bb5d-fbae01af2861', 'checkpoint_ns': '', 'checkpoint_id': '1f015266-d804-6539-8001-54e10a63b667'}}, 'values': {'messages': [HumanMessage(content='process the external data', additional_kwargs={}, response_metadata={}, id='fe8e6950-181a-4697-9daa-6acca75e532c'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ZSN33LoRAWXnSjL0ya2LhAtQ', 'function': {'arguments': '{}', 'name': 'get_data_externally'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 314, 'total_tokens': 328, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_65792305e4', 'id': 'chatcmpl-BKMBG0VWT1HORCWdKy6TPMnqyGx5i', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, id='run-a7fb3038-b000-4510-92ae-1dc87694d33b-0', tool_calls=[{'name': 'get_data_externally', 'args': {}, 'id': 'call_ZSN33LoRAWXnSjL0ya2LhAtQ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 314, 'output_tokens': 14, 'total_tokens': 328, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='This is the externally fetched data for e4dc31f9-aa01-4be0-bb5d-fbae01af2861', id='45fa5e80-53aa-4e60-b9db-a95e71be34a7', tool_call_id='call_ZSN33LoRAWXnSjL0ya2LhAtQ')]}, 'metadata': {'source': 'update', 'writes': {'agent': {'messages': [ToolMessage(content='This is the externally fetched data for e4dc31f9-aa01-4be0-bb5d-fbae01af2861', id='45fa5e80-53aa-4e60-b9db-a95e71be34a7', tool_call_id='call_ZSN33LoRAWXnSjL0ya2LhAtQ')]}}, 'step': 2, 'parents': {}, 'thread_id': 'e4dc31f9-aa01-4be0-bb5d-fbae01af2861'}, 'next': [], 'tasks': []}}\n2025-04-09 11:38:43,861 - INFO - Resumption stream finished.\n\n--- SIMULATION RESULTS ---\n2025-04-09 11:38:43,862 - WARNING - FAILURE: Resumption finished, but agent did not call 'process_data'.\n2025-04-09 11:38:43,862 - WARNING -   Check DEBUG chunks above to see what the agent node did (if anything).\nDescription\nHi,\nI'm encountering an issue where a graph created using create_react_agent fails to continue its execution loop after its state is updated externally with a ToolMessage and execution is resumed via astream(None, ...) or ainvoke(None, ...).\nThe goal is to implement a two-step tool process:\nAn initial tool (get_data_externally) is called by the agent when data is missing.\nExecution is interrupted (interrupt_before=[\"tools\"]).\nThe application fetches the data externally (simulated in the MRE).\nThe application updates the graph state using graph.update_state() to add a ToolMessage containing the fetched data, linked to the first tool call ID.\nThe application resumes execution using graph.astream(None, ...) or graph.ainvoke(None, ...).\nSteps to Reproduce:\nRun the provided Minimal Reproducible Example script below.\nExpected Behavior:\nAfter graph.update_state adds the ToolMessage and graph.astream(None, ...) (or ainvoke) is called:\nThe graph execution should resume.\nThe internal routing should follow the tools -> agent edge (which exists in the graph structure, confirmed via visualization).\nThe agent node (LLM) should be invoked again with the updated state (including the new ToolMessage).\nThe LLM should see the fetched data in the ToolMessage and, following the prompt, decide to call the second tool (process_data) with this data.\nSubsequent chunks (for the process_data tool call, interrupt, execution, final response) should be streamed or included in the final ainvoke result.\nActual Behavior:\nAfter graph.update_state adds the ToolMessage and graph.astream(None, ...) or ainvoke(None, ...) is called:\nThe graph execution terminates prematurely.\nWhen using astream(None, ..., stream_mode=\"debug\"), the only chunk yielded after resumption is a single {'type': 'checkpoint', ...} chunk, confirming the state update was processed internally by the checkpointer.\nNo further chunks indicating the execution of the agent node or any subsequent steps are received. The stream finishes immediately after the checkpoint chunk.\nWhen using ainvoke(None, ...), the call completes successfully but returns a final state identical to the state before the ainvoke call (i.e., ending with the injected ToolMessage). It does not execute the process_data tool or generate a final response based on it.\nThe agent loop does not continue as expected.\nAdditional Context:\nThe agent graph structure generated by create_react_agent (confirmed via visualization) correctly includes the tools -> agent loopback edge.\nThe issue has been reproduced with both MemorySaver and SqliteSaver.\nUsing ainvoke(None, ...) instead of astream(None, ...) for resumption also fails to continue the agent loop, returning the state immediately after the ToolMessage was added.\nUsing different prompts (including highly explicit ones describing the flow) did not resolve the issue.\nSystem Info\nO.S: Windows 11\nPython version: 3.12.3\nModel: GPT-4o (2024-05-13)\nMain dependencies:\nname = \"langgraph\"\nversion = \"0.3.27\"\nname = \"langchain\"\nversion = \"0.3.23\"\nname = \"langchain-openai\"\nversion = \"0.3.12\"\nname = \"openai\"\nversion = \"1.72.0\"\nname = \"pydantic\"\nversion = \"2.11.3\"", "created_at": "2025-04-09", "closed_at": "2025-04-09", "labels": [], "State": "closed", "Author": "ericzon"}
{"issue_number": 4214, "issue_title": "`CompiledGraph` with `AsyncPostgresSaver` used later in the application throws different kinds of exceptions!", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nasync def CreateGraph(self) -> CompiledGraph:\n        logging.debug(f\"\\n=== {self.CreateGraph.__name__} ===\")\n        try:\n            _connection_kwargs = {\n                \"autocommit\": True,\n                \"prepare_threshold\": 0,\n            }\n            pool = AsyncConnectionPool(\n                conninfo = appconfig.POSTGRESQL_DATABASE_URI,\n                max_size = appconfig.DB_MAX_CONNECTIONS,\n                kwargs = _connection_kwargs,\n            )\n            # Create the AsyncPostgresSaver\n            checkpointer = AsyncPostgresSaver(pool)\n            #checkpointer = await GetAsyncCheckpointer()\n            if __name__ == \"__main__\":\n                print(\"checkpointer.setup()\")\n                await checkpointer.setup()\n            self._agent = create_react_agent(self._llm, self._tools, store = in_memory_store, checkpointer = checkpointer, config_schema = Configuration, state_schema = CustomAgentState, name = self._name, prompt = self._prompt)\n        except Exception as e:\n            logging.exception(f\"Exception! {e}\")\n        return self._agent\n\n    async def ChatAgent(self, config: RunnableConfig, messages: List[tuple]): #messages: List[str]):\n        logging.info(f\"\\n=== {self.ChatAgent.__name__} ===\")\n        async for event in self._agent.with_config({\"user_id\": uuid7str()}).astream(\n            #{\"messages\": [{\"role\": \"user\", \"content\": messages}]}, This works with gemini-2.0-flash\n            {\"messages\": messages}, # This works with Ollama llama3.3\n            stream_mode=\"values\", # Use this to stream all values in the state after each step.\n            config=config, # This is needed by Checkpointer\n        ):\n            event[\"messages\"][-1].pretty_print()\n\nasync def main():\n    config = RunnableConfig(run_name=\"RAG ReAct Agent\", thread_id=datetime.now())\n    rag = RAGAgent(config)\n    await rag.CreateGraph()\n    print(f\"args: {args}\")\n    input_message = [(\"human\", \"What is the standard method for Task Decomposition?\"), (\"human\", \"Once you get the answer, look up common extensions of that method.\")]\n    await rag.ChatAgent(config, input_message)\nError Message and Stack Trace (if applicable)\nWhen I use the AsyncConnectionPool as a local var this way, with async with, I get the following warning AND error:\n/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/psycopg_pool/pool_async.py:142: RuntimeWarning: opening the async pool AsyncConnectionPool in the constructor is deprecated and will not be supported anymore in a future release. Please use `await pool.open()`, or use the pool as context manager using: `async with AsyncConnectionPool(...) as pool: `...\n  warnings.warn(\nargs: Namespace(load_urls=False)\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/src/Python/rag-agent/src/rag_agent/RAGAgent.py\", line 171, in <module>\n    asyncio.run(main())\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/usr/src/Python/rag-agent/src/rag_agent/RAGAgent.py\", line 168, in main\n    await rag.ChatAgent(config, input_message)\n  File \"/usr/src/Python/rag-agent/src/rag_agent/RAGAgent.py\", line 139, in ChatAgent\n    async for event in self._agent.with_config({\"user_id\": uuid7str()}).astream(\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 2574, in astream\n    async with AsyncPregelLoop(\n               ^^^^^^^^^^^^^^^^\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/pregel/loop.py\", line 1135, in __aenter__\n    saved = await self.checkpointer.aget_tuple(self.checkpoint_config)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/checkpoint/postgres/aio.py\", line 187, in aget_tuple\n    await cur.execute(\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/psycopg/cursor_async.py\", line 97, in execute\n    raise ex.with_traceback(None)\npsycopg.errors.UndefinedFunction: operator does not exist: text = timestamp without time zone\nLINE 34: from checkpoints WHERE thread_id = $1 AND checkpoint_ns = $2...\n                                          ^\nHINT:  No operator matches the given name and argument types. You might need to add explicit type casts.\nWhen I use async with AsyncConnectionPool in CreateGraph, I get a different error:\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/src/Python/rag-agent/src/rag_agent/RAGAgent.py\", line 171, in <module>\n    asyncio.run(main())\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/usr/src/Python/rag-agent/src/rag_agent/RAGAgent.py\", line 168, in main\n    await rag.ChatAgent(config, input_message)\n  File \"/usr/src/Python/rag-agent/src/rag_agent/RAGAgent.py\", line 139, in ChatAgent\n    async for event in self._agent.with_config({\"user_id\": uuid7str()}).astream(\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 2574, in astream\n    async with AsyncPregelLoop(\n               ^^^^^^^^^^^^^^^^\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/pregel/loop.py\", line 1135, in __aenter__\n    saved = await self.checkpointer.aget_tuple(self.checkpoint_config)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/checkpoint/postgres/aio.py\", line 186, in aget_tuple\n    async with self._cursor() as cur:\n               ^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/checkpoint/postgres/aio.py\", line 328, in _cursor\n    async with _ainternal.get_connection(self.conn) as conn:\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/checkpoint/postgres/_ainternal.py\", line 21, in get_connection\n    async with conn.connection() as conn:\n               ^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__\n    return await anext(self.gen)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/psycopg_pool/pool_async.py\", line 195, in connection\n    conn = await self.getconn(timeout=timeout)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/psycopg_pool/pool_async.py\", line 222, in getconn\n    self._check_open_getconn()\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/psycopg_pool/pool_async.py\", line 122, in _check_open_getconn\n    super()._check_open_getconn()\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/psycopg_pool/base.py\", line 141, in _check_open_getconn\n    raise PoolClosed(f\"the pool {self.name!r} is already closed\")\npsycopg_pool.PoolClosed: the pool 'pool-1' is already closed\n\nDescription\n#4193\nTrying to use PostgreSQL as checkpointer for a CompiledGraph / ReAct Agent.\nSystem Info\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #21-Ubuntu SMP PREEMPT_DYNAMIC Wed Feb 19 16:50:40 UTC 2025\n> Python Version:  3.12.7 (main, Feb  4 2025, 14:46:03) [GCC 14.2.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.51\n> langchain: 0.3.23\n> langchain_community: 0.3.21\n> langsmith: 0.3.26\n> langchain_chroma: 0.2.2\n> langchain_cli: 0.0.36\n> langchain_google_genai: 2.1.2\n> langchain_google_vertexai: 2.0.19\n> langchain_neo4j: 0.4.0\n> langchain_nomic: 0.1.4\n> langchain_ollama: 0.3.1\n> langchain_openai: 0.3.12\n> langchain_text_splitters: 0.3.8\n> langgraph_sdk: 0.1.61\n> langserve: 0.3.1\n\nOther Dependencies\n------------------\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> anthropic[vertexai]: Installed. No version info available.\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0: Installed. No version info available.\n> dataclasses-json<0.7,>=0.5.7: Installed. No version info available.\n> fastapi: 0.115.9\n> filetype: 1.2.0\n> gitpython<4,>=3: Installed. No version info available.\n> google-ai-generativelanguage: 0.6.17\n> google-cloud-aiplatform: 1.87.0\n> google-cloud-storage: 2.19.0\n> gritql<1.0.0,>=0.2.0: Installed. No version info available.\n> httpx: 0.28.1\n> httpx-sse: 0.4.0\n> httpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-azure-ai;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.49: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.51: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-perplexity;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.8: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langchain<1.0.0,>=0.3.23: Installed. No version info available.\n> langserve[all]>=0.0.51: Installed. No version info available.\n> langsmith-pyo3: Installed. No version info available.\n> langsmith<0.4,>=0.1.125: Installed. No version info available.\n> langsmith<0.4,>=0.1.17: Installed. No version info available.\n> neo4j: 5.28.1\n> neo4j-graphrag: 1.6.1\n> nomic: 3.4.1\n> numpy<2.0.0,>=1.22.4;: Installed. No version info available.\n> numpy<2.0.0,>=1.26.2;: Installed. No version info available.\n> numpy<3,>=1.26.2: Installed. No version info available.\n> ollama<1,>=0.4.4: Installed. No version info available.\n> openai-agents: Installed. No version info available.\n> openai<2.0.0,>=1.68.2: Installed. No version info available.\n> opentelemetry-api: 1.31.1\n> opentelemetry-exporter-otlp-proto-http: Installed. No version info available.\n> opentelemetry-sdk: 1.31.1\n> orjson: 3.10.16\n> packaging: 24.2\n> packaging<25,>=23.2: Installed. No version info available.\n> pillow: 10.4.0\n> pydantic: 2.11.2\n> pydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\n> pydantic<3.0.0,>=2.5.2;: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4;: Installed. No version info available.\n> pytest: 8.3.5\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> requests<3,>=2: Installed. No version info available.\n> rich: 14.0.0\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> sse-starlette: 1.8.2\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken<1,>=0.7: Installed. No version info available.\n> tomlkit>=0.12: Installed. No version info available.\n> typer[all]<1.0.0,>=0.9.0: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> uvicorn<1.0,>=0.23: Installed. No version info available.\n> validators: 0.34.0\n> zstandard: 0.23.0\n", "created_at": "2025-04-09", "closed_at": "2025-04-10", "labels": [], "State": "closed", "Author": "khteh"}
{"issue_number": 4198, "issue_title": "TypeError when using pydantic-based state with generic fields like `list[str]`", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.graph import END, START, StateGraph\nfrom pydantic import BaseModel\n\n\nclass MyState(BaseModel):\n    mylist: list[str]\n\n\ndef fill_state(state: MyState) -> dict:\n    return {\"mylist\": [\"a1\", \"b2\", \"c3\"]}\n\n\nbuilder = (\n    StateGraph(MyState)\n    .add_node(fill_state.__name__, fill_state)\n    .add_edge(START, fill_state.__name__)\n    .add_edge(fill_state.__name__, END)\n)\n\ngraph = builder.compile()\n\nif __name__ == \"__main__\":\n    out = graph.invoke({\"mylist\": []})\n    print(out)\nError Message and Stack Trace (if applicable)\nTraceback (most recent call last):\n  File \"/PATHTOCONDAENV/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/PATHTOCONDAENV/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/HOMEDIR/.vscode-server/extensions/ms-python.debugpy-2025.6.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py\", line 71, in <module>\n    cli.main()\n  File \"/HOMEDIR/.vscode-server/extensions/ms-python.debugpy-2025.6.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py\", line 501, in main\n    run()\n  File \"/HOMEDIR/.vscode-server/extensions/ms-python.debugpy-2025.6.0-linux-x64/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py\", line 351, in run_file\n    runpy.run_path(target, run_name=\"__main__\")\n  File \"/HOMEDIR/.vscode-server/extensions/ms-python.debugpy-2025.6.0-linux-x64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py\", line 310, in run_path\n    return _run_module_code(code, init_globals, run_name, pkg_name=pkg_name, script_name=fname)\n  File \"/HOMEDIR/.vscode-server/extensions/ms-python.debugpy-2025.6.0-linux-x64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py\", line 127, in _run_module_code\n    _run_code(code, mod_globals, init_globals, mod_name, mod_spec, pkg_name, script_name)\n  File \"/HOMEDIR/.vscode-server/extensions/ms-python.debugpy-2025.6.0-linux-x64/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py\", line 118, in _run_code\n    exec(code, run_globals)\n  File \"/HOMEDIR/dev_stuff/ragservices/opencanvas-python/langgraph_issue.py\", line 23, in <module>\n    out = graph.invoke({\"mylist\": []})\n  File \"/PATHTOCONDAENV/lib/python3.10/site-packages/langgraph/pregel/__init__.py\", line 2688, in invoke\n    for chunk in self.stream(\n  File \"/PATHTOCONDAENV/lib/python3.10/site-packages/langgraph/pregel/__init__.py\", line 2339, in stream\n    while loop.tick(input_keys=self.input_channels):\n  File \"/PATHTOCONDAENV/lib/python3.10/site-packages/langgraph/pregel/loop.py\", line 496, in tick\n    self.tasks = prepare_next_tasks(\n  File \"/PATHTOCONDAENV/lib/python3.10/site-packages/langgraph/pregel/algo.py\", line 473, in prepare_next_tasks\n    if task := prepare_single_task(\n  File \"/PATHTOCONDAENV/lib/python3.10/site-packages/langgraph/pregel/algo.py\", line 732, in prepare_single_task\n    val = next(\n  File \"/PATHTOCONDAENV/lib/python3.10/site-packages/langgraph/pregel/algo.py\", line 964, in _proc_input\n    val = proc.mapper(val)\n  File \"/PATHTOCONDAENV/lib/python3.10/site-packages/langgraph/graph/schema_utils.py\", line 78, in __call__\n    return self.coerce(input_data, depth)\n  File \"/PATHTOCONDAENV/lib/python3.10/site-packages/langgraph/graph/schema_utils.py\", line 87, in coerce\n    self._field_coercers = {\n  File \"/PATHTOCONDAENV/lib/python3.10/site-packages/langgraph/graph/schema_utils.py\", line 88, in <dictcomp>\n    n: self._build_coercer(t, depth - 1) for n, t in self._fields.items()\n  File \"/PATHTOCONDAENV/lib/python3.10/site-packages/langgraph/graph/schema_utils.py\", line 117, in _build_coercer\n    if is_class_ and issubclass(field_type, BaseModelV1):\n  File \"/PATHTOCONDAENV/lib/python3.10/abc.py\", line 123, in __subclasscheck__\n    return _abc_subclasscheck(cls, subclass)\nTypeError: issubclass() arg 1 must be a class\nDescription\nI'm trying to use a pydantic model as the graph state, and one of the fields is a list[str].\nI expect to have to issues with it, however a TypeError is raised as shown in the trace.\nThe code example WORKS IF:\n\nreplace list[str] with just list\nuse TypedDict instead of BaseModel\n\nThe code example works with langgraph==0.2.67, however fails with langgraph==0.3.9 and langgraph==0.3.25.\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #137-Ubuntu SMP Fri Nov 8 15:21:01 UTC 2024\nPython Version:  3.10.16 | packaged by conda-forge | (main, Dec  5 2024, 14:16:10) [GCC 13.3.0]\n\nPackage Information\n\nlangchain_core: 0.3.51\nlangchain: 0.3.23\nlangchain_community: 0.3.21\nlangsmith: 0.3.26\nlangchain_google_vertexai: 2.0.19\nlangchain_openai: 0.2.14\nlangchain_text_splitters: 0.3.8\nlanggraph_issue: Installed. No version info available.\nlanggraph_sdk: 0.1.61\nlangserve: 0.3.1\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nanthropic[vertexai]: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nfastapi: 0.115.12\ngoogle-cloud-aiplatform: 1.87.0\ngoogle-cloud-storage: 2.19.0\nhttpx: 0.28.1\nhttpx-sse: 0.4.0\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-azure-ai;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.51: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-perplexity;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.8: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.23: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<3,>=1.26.2: Installed. No version info available.\nopenai: 1.63.2\nopenai-agents: Installed. No version info available.\nopentelemetry-api: Installed. No version info available.\nopentelemetry-exporter-otlp-proto-http: Installed. No version info available.\nopentelemetry-sdk: Installed. No version info available.\norjson: 3.10.16\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.11.2\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: 8.3.5\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: 13.9.4\nSQLAlchemy<3,>=1.4: Installed. No version info available.\nsse-starlette: 2.2.1\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken: 0.9.0\ntyping-extensions>=4.7: Installed. No version info available.\nvalidators: 0.34.0\nzstandard: 0.23.0\n", "created_at": "2025-04-08", "closed_at": null, "labels": [], "State": "open", "Author": "AmevinLS"}
{"issue_number": 4194, "issue_title": "`PostgresSaver` checkpointer hits `RuntimeError: cannot join current thread` in `setup()`", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\ncheckpointer = GetCheckpointer()\n   if __name__ == \"__main__\":\n       print(\"checkpointer.setup()\")\n       checkpointer.setup()\nError Message and Stack Trace (if applicable)\ncheckpointer.setup()\nargs: Namespace(load_urls=False)\nException ignored in: <function ConnectionPool.__del__ at 0x740c7af45800>\nTraceback (most recent call last):\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/psycopg_pool/pool.py\", line 111, in __del__\n    gather(*workers, timeout=5.0, timeout_hint=hint)\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/psycopg_pool/_acompat.py\", line 167, in gather\n    t.join(timeout)\n  File \"/usr/lib/python3.12/threading.py\", line 1146, in join\n    raise RuntimeError(\"cannot join current thread\")\nRuntimeError: cannot join current thread\nDescription\nDue to #4193 I switched back to use MemorySaver() but the code line calling PostgresSaver  instance setup() hits the error. I just comment out this code snippet for now since I don't use it but MemorySaver instead.\nSystem Info\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #21-Ubuntu SMP PREEMPT_DYNAMIC Wed Feb 19 16:50:40 UTC 2025\n> Python Version:  3.12.7 (main, Feb  4 2025, 14:46:03) [GCC 14.2.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.49\n> langchain: 0.3.22\n> langchain_community: 0.3.20\n> langsmith: 0.3.21\n> langchain_chroma: 0.2.2\n> langchain_cli: 0.0.36\n> langchain_google_genai: 2.1.2\n> langchain_google_vertexai: 2.0.18\n> langchain_neo4j: 0.4.0\n> langchain_ollama: 0.3.0\n> langchain_openai: 0.3.11\n> langchain_text_splitters: 0.3.7\n> langgraph_sdk: 0.1.60\n> langserve: 0.3.1\n\nOther Dependencies\n------------------\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> anthropic[vertexai]: Installed. No version info available.\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0: Installed. No version info available.\n> dataclasses-json<0.7,>=0.5.7: Installed. No version info available.\n> fastapi: 0.115.12\n> filetype: 1.2.0\n> gitpython<4,>=3: Installed. No version info available.\n> google-ai-generativelanguage: 0.6.17\n> google-cloud-aiplatform: 1.87.0\n> google-cloud-storage: 2.19.0\n> gritql<1.0.0,>=0.2.0: Installed. No version info available.\n> httpx: 0.27.2\n> httpx-sse: 0.4.0\n> httpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-azure-ai;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.45: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.47: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.49: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.7: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langchain<1.0.0,>=0.3.21: Installed. No version info available.\n> langserve[all]>=0.0.51: Installed. No version info available.\n> langsmith-pyo3: Installed. No version info available.\n> langsmith<0.4,>=0.1.125: Installed. No version info available.\n> langsmith<0.4,>=0.1.17: Installed. No version info available.\n> neo4j: 5.28.1\n> neo4j-graphrag: 1.6.1\n> numpy<2.0.0,>=1.22.4;: Installed. No version info available.\n> numpy<2.0.0,>=1.26.2;: Installed. No version info available.\n> numpy<3,>=1.26.2: Installed. No version info available.\n> ollama<1,>=0.4.4: Installed. No version info available.\n> openai-agents: Installed. No version info available.\n> openai<2.0.0,>=1.68.2: Installed. No version info available.\n> opentelemetry-api: 1.31.1\n> opentelemetry-exporter-otlp-proto-http: Installed. No version info available.\n> opentelemetry-sdk: 1.31.1\n> orjson: 3.10.16\n> packaging: 24.2\n> packaging<25,>=23.2: Installed. No version info available.\n> pydantic: 2.9.2\n> pydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\n> pydantic<3.0.0,>=2.5.2;: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4;: Installed. No version info available.\n> pytest: 8.3.5\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> requests<3,>=2: Installed. No version info available.\n> rich: 14.0.0\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> sse-starlette: 1.8.2\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken<1,>=0.7: Installed. No version info available.\n> tomlkit>=0.12: Installed. No version info available.\n> typer[all]<1.0.0,>=0.9.0: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> uvicorn<1.0,>=0.23: Installed. No version info available.\n> validators: 0.34.0\n> zstandard: 0.23.0```\n", "created_at": "2025-04-08", "closed_at": "2025-04-09", "labels": ["invalid"], "State": "closed", "Author": "khteh"}
{"issue_number": 4193, "issue_title": "`PostgresSaver` checkpointer throws `NotImplementedError` exception!", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\ndef GetCheckpointer():\n    return PostgresSaver(ConnectionPool(\n        conninfo = config.POSTGRESQL_DATABASE_URI,\n        max_size = config.DB_MAX_CONNECTIONS,\n        kwargs = _connection_kwargs,\n    ))\n\n    async def CreateGraph(self) -> CompiledGraph:\n        logging.debug(f\"\\n=== {self.CreateGraph.__name__} ===\")\n        try:\n            checkpointer = GetCheckpointer()\n            if __name__ == \"__main__\":\n                checkpointer.setup()\n            self._agent = create_react_agent(self._llm, self._tools, store=self._vectorStore.vector_store, checkpointer=checkpointer, config_schema=Configuration, state_schema=CustomAgentState, name=self._name, prompt=self._prompt)\n        except ResourceExhausted as e:\n            logging.exception(f\"google.api_core.exceptions.ResourceExhausted {e}\")\n        return self._agent\n\n    async def ChatAgent(self, config: RunnableConfig, messages: List[tuple]): #messages: List[str]):\n        logging.info(f\"\\n=== {self.ChatAgent.__name__} ===\")\n        async for event in self._agent.with_config({\"user_id\": uuid7str()}).astream(\n            #{\"messages\": [{\"role\": \"user\", \"content\": messages}]}, This works with gemini-2.0-flash\n            {\"messages\": messages},\n            stream_mode=\"values\", # Use this to stream all values in the state after each step.\n            config=config, # This is needed by Checkpointer\n        ):\n            event[\"messages\"][-1].pretty_print()\n\nasync def main():\n  config = RunnableConfig(run_name=\"RAG ReAct Agent\", thread_id=datetime.now())\n  input_message = [(\"human\", \"What is the standard method for Task Decomposition?\"), (\"human\", \"Once you get the answer, look up common extensions of that method.\")]\n  await rag.ChatAgent(config, input_message)\nError Message and Stack Trace (if applicable)\nasyncio.run(main())\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 194, in run\n    return runner.run(main)\n           ^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n    return self._loop.run_until_complete(task)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n    return future.result()\n           ^^^^^^^^^^^^^^^\n  File \"/usr/src/Python/rag-agent/src/rag_agent/RAGAgent.py\", line 148, in main\n    await rag.ChatAgent(config, input_message)\n  File \"/usr/src/Python/rag-agent/src/rag_agent/RAGAgent.py\", line 119, in ChatAgent\n    async for event in self._agent.with_config({\"user_id\": uuid7str()}).astream(\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 2565, in astream\n    async with AsyncPregelLoop(\n               ^^^^^^^^^^^^^^^^\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/pregel/loop.py\", line 1117, in __aenter__\n    saved = await self.checkpointer.aget_tuple(self.checkpoint_config)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/checkpoint/base/__init__.py\", line 345, in aget_tuple\n    raise NotImplementedError\nNotImplementedError\nDescription\nTrying to use PostgreSQL as checkpointer.\nSystem Info\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #21-Ubuntu SMP PREEMPT_DYNAMIC Wed Feb 19 16:50:40 UTC 2025\n> Python Version:  3.12.7 (main, Feb  4 2025, 14:46:03) [GCC 14.2.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.49\n> langchain: 0.3.22\n> langchain_community: 0.3.20\n> langsmith: 0.3.21\n> langchain_chroma: 0.2.2\n> langchain_cli: 0.0.36\n> langchain_google_genai: 2.1.2\n> langchain_google_vertexai: 2.0.18\n> langchain_neo4j: 0.4.0\n> langchain_ollama: 0.3.0\n> langchain_openai: 0.3.11\n> langchain_text_splitters: 0.3.7\n> langgraph_sdk: 0.1.60\n> langserve: 0.3.1\n\nOther Dependencies\n------------------\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> anthropic[vertexai]: Installed. No version info available.\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> chromadb!=0.5.10,!=0.5.11,!=0.5.12,!=0.5.4,!=0.5.5,!=0.5.7,!=0.5.9,<0.7.0,>=0.4.0: Installed. No version info available.\n> dataclasses-json<0.7,>=0.5.7: Installed. No version info available.\n> fastapi: 0.115.12\n> filetype: 1.2.0\n> gitpython<4,>=3: Installed. No version info available.\n> google-ai-generativelanguage: 0.6.17\n> google-cloud-aiplatform: 1.87.0\n> google-cloud-storage: 2.19.0\n> gritql<1.0.0,>=0.2.0: Installed. No version info available.\n> httpx: 0.27.2\n> httpx-sse: 0.4.0\n> httpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-azure-ai;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core!=0.3.0,!=0.3.1,!=0.3.10,!=0.3.11,!=0.3.12,!=0.3.13,!=0.3.14,!=0.3.2,!=0.3.3,!=0.3.4,!=0.3.5,!=0.3.6,!=0.3.7,!=0.3.8,!=0.3.9,<0.4.0,>=0.2.43: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.45: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.47: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.49: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.7: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langchain<1.0.0,>=0.3.21: Installed. No version info available.\n> langserve[all]>=0.0.51: Installed. No version info available.\n> langsmith-pyo3: Installed. No version info available.\n> langsmith<0.4,>=0.1.125: Installed. No version info available.\n> langsmith<0.4,>=0.1.17: Installed. No version info available.\n> neo4j: 5.28.1\n> neo4j-graphrag: 1.6.1\n> numpy<2.0.0,>=1.22.4;: Installed. No version info available.\n> numpy<2.0.0,>=1.26.2;: Installed. No version info available.\n> numpy<3,>=1.26.2: Installed. No version info available.\n> ollama<1,>=0.4.4: Installed. No version info available.\n> openai-agents: Installed. No version info available.\n> openai<2.0.0,>=1.68.2: Installed. No version info available.\n> opentelemetry-api: 1.31.1\n> opentelemetry-exporter-otlp-proto-http: Installed. No version info available.\n> opentelemetry-sdk: 1.31.1\n> orjson: 3.10.16\n> packaging: 24.2\n> packaging<25,>=23.2: Installed. No version info available.\n> pydantic: 2.9.2\n> pydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\n> pydantic<3.0.0,>=2.5.2;: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4;: Installed. No version info available.\n> pytest: 8.3.5\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> requests<3,>=2: Installed. No version info available.\n> rich: 14.0.0\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> sse-starlette: 1.8.2\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken<1,>=0.7: Installed. No version info available.\n> tomlkit>=0.12: Installed. No version info available.\n> typer[all]<1.0.0,>=0.9.0: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> uvicorn<1.0,>=0.23: Installed. No version info available.\n> validators: 0.34.0\n> zstandard: 0.23.0\n", "created_at": "2025-04-08", "closed_at": "2025-04-08", "labels": [], "State": "closed", "Author": "khteh"}
{"issue_number": 4184, "issue_title": "Pydantic graph state no longer deserializes UUIDs correctly", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing import Any, Dict\nfrom uuid import UUID\n\nfrom pydantic import BaseModel\nfrom langgraph.graph import StateGraph\n\n\nclass State(BaseModel):\n    id: UUID\n\n\ndef my_node(state: State) -> Dict[str, Any]:\n    assert isinstance(state.id, UUID), type(state.id)  # assertion error\n    print(state)\n\n    return {\"id\": state.id}\n\n\nworkflow = StateGraph(State)\nworkflow.add_node(\"my_node\", my_node)\nworkflow.add_edge(\"__start__\", \"my_node\")\ngraph = workflow.compile()\n\ngraph.invoke({\"id\": \"fe096781-5601-53d2-b2f6-0d3403f7e9ca\"})\nError Message and Stack Trace (if applicable)\n/main.py:463: UserWarning: Pydantic serializer warnings:\n  PydanticSerializationUnexpectedValue(Expected `uuid` - serialized value may not be as expected [input_value='fe096781-5601-53d2-b2f6-0d3403f7e9ca', input_type=str])\n  return self.__pydantic_serializer__.to_python(\nDescription\n\nI'm storing UUIDs in my Pydantic graph state\nI expect UUIDs to be deserialized into UUID objects (as per normal Pydantic behavior and like they used to in previous LangGraph versions)\nThe UUIDs are actually incorrectly deserialized into str objects\n\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP PREEMPT_DYNAMIC Mon, 10 Mar 2025 01:49:31 +0000\nPython Version:  3.12.8 (main, Dec 19 2024, 14:33:20) [Clang 18.1.8 ]\n\nPackage Information\n\nlangchain_core: 0.3.51\nlangsmith: 0.3.24\nlanggraph_api: 0.0.46\nlanggraph_cli: 0.1.89\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.61\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nblockbuster: 1.5.24\nclick: 8.1.8\ncloudpickle: 3.1.1\ncryptography: 44.0.2\nhttpx: 0.28.1\njsonpatch<2.0,>=1.33: Installed. No version info available.\njsonschema-rs: 0.29.1\nlanggraph: 0.3.25\nlanggraph-checkpoint: 2.0.24\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nopenai-agents: Installed. No version info available.\nopentelemetry-api: Installed. No version info available.\nopentelemetry-exporter-otlp-proto-http: Installed. No version info available.\nopentelemetry-sdk: Installed. No version info available.\norjson: 3.10.16\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.11.2\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npyjwt: 2.10.1\npytest: Installed. No version info available.\npython-dotenv: 1.1.0\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrich: Installed. No version info available.\nsse-starlette: 2.1.3\nstarlette: 0.46.1\nstructlog: 25.2.0\ntenacity: 9.1.2\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nuvicorn: 0.34.0\nwatchfiles: 1.0.4\nzstandard: 0.23.0\n", "created_at": "2025-04-06", "closed_at": "2025-04-09", "labels": [], "State": "closed", "Author": "optimalstrategy"}
{"issue_number": 4183, "issue_title": "Node has been removed from the graph.", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nCreate a new langgraph project via langgraph new\nDescription\nState is not persisting when shutting down the server then starting it again.\nFirst run langgraph dev\nYou will see state is working as intended in langgraph studio and in Agent Chat UI.\nStop the server. Restart it.\nYou will notice that state is now empty for any previous thread. Messages and anything else in state is now empty and a message shows next to each node \"Node has been removed from the graph.\"\nAfter server restarted:\n\n\nI updated the langgraph cli recently and other deps then this started happening. I have another graph on older deps and it is setup very similar but does not have this problem.\nAdditionally, using rerun from here does not work either and will cause the graph to not return a response at all.\n\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:16 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6000\nPython Version:  3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 12:55:12) [Clang 14.0.6 ]\n\nPackage Information\n\nlangchain_core: 0.3.51\nlangchain: 0.3.23\nlangchain_community: 0.3.17\nlangsmith: 0.3.24\nlangchain_anthropic: 0.3.7\nlangchain_fireworks: 0.2.7\nlangchain_google_genai: 2.0.9\nlangchain_openai: 0.3.4\nlangchain_text_splitters: 0.3.8\nlanggraph_api: 0.0.46\nlanggraph_cli: 0.1.89\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.61\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.12\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nanthropic<1,>=0.45.0: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nblockbuster: 1.5.24\nclick: 8.1.8\ncloudpickle: 3.1.1\ncryptography: 43.0.3\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nfiletype: 1.2.0\nfireworks-ai: 0.15.12\ngoogle-generativeai: 0.8.4\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\njsonschema-rs: 0.25.1\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-azure-ai;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.34: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.51: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-perplexity;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.8: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.18: Installed. No version info available.\nlanggraph: 0.3.25\nlanggraph-checkpoint: 2.0.24\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<2,>=1.26.4;: Installed. No version info available.\nnumpy<3,>=1.26.2;: Installed. No version info available.\nopenai: 1.61.1\nopenai-agents: Installed. No version info available.\nopenai<2.0.0,>=1.58.1: Installed. No version info available.\nopentelemetry-api: Installed. No version info available.\nopentelemetry-exporter-otlp-proto-http: Installed. No version info available.\nopentelemetry-sdk: Installed. No version info available.\norjson: 3.10.16\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.11.2\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npyjwt: 2.10.1\npytest: Installed. No version info available.\npython-dotenv: 1.0.1\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: Installed. No version info available.\nSQLAlchemy<3,>=1.4: Installed. No version info available.\nsse-starlette: 2.1.3\nstarlette: 0.45.3\nstructlog: 24.4.0\ntenacity: 9.1.2\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nuvicorn: 0.34.0\nwatchfiles: 1.0.4\nzstandard: 0.23.0\n", "created_at": "2025-04-06", "closed_at": null, "labels": [], "State": "open", "Author": "richTheCreator"}
{"issue_number": 4182, "issue_title": "State not properly shared between parent graph and react_agent subgraph", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.graph import StateGraph\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\nfrom langgraph.prebuilt import create_react_agent\n# ... other imports ...\n\ndef first_node(state: AgentState):\n    return {\n        \"messages\": [AIMessage(content=\"fake llm response\")],\n        \"remaining_steps\": state[\"remaining_steps\"],\n    }\n\n# Setup graph\nbuilder = StateGraph(AgentState)\nbuilder.add_node(\"agent\", agent)  # subgraph\nbuilder.add_node(\"first\", first_node)\nbuilder.set_entry_point(\"first\")\nbuilder.add_edge(\"first\", \"agent\")\nbuilder.add_edge(\"agent\", END)\n\ngraph = builder.compile()\n\n# Run with initial state\nstate: AgentState = {\"remaining_steps\": 8, \"messages\":[HumanMessage(content=\"Hello\")], \"is_last_step\": False}\nasync for x in graph.astream(state, stream_mode=\"updates\", debug=True):\n    print(x)\nError Message and Stack Trace (if applicable)\n[-1:checkpoint] State at the end of step -1:\n{'messages': []}\n[0:tasks] Starting 1 task for step 0:\n- __start__ -> {'is_last_step': False,\n 'messages': [HumanMessage(content='Hello', additional_kwargs={}, response_metadata={})],\n 'remaining_steps': 8}\n[0:writes] Finished step 0 with writes to 1 channel:\n- messages -> [HumanMessage(content='Hello', additional_kwargs={}, response_metadata={})]\n[0:checkpoint] State at the end of step 0:\n{'messages': [HumanMessage(content='Hello', additional_kwargs={}, response_metadata={}, id='bbc644a0-e573-458e-b258-367c7ba7c256')]}\n[1:tasks] Starting 1 task for step 1:\n- first -> {'is_last_step': False,\n 'messages': [HumanMessage(content='Hello', additional_kwargs={}, response_metadata={}, id='bbc644a0-e573-458e-b258-367c7ba7c256')],\n 'remaining_steps': 24}\nDescription\nExpected Behavior:\nThe remaining_steps value should be shared between the parent graph and subgraph\nActual Behavior:\nThe remaining_steps value gets reset to the default value (25-1=24)\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.20348\nPython Version:  3.12.8 (tags/v3.12.8:2dc476b, Dec  3 2024, 19:30:04) [MSC v.1942 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.3.45\nlangchain: 0.3.21\nlangchain_community: 0.3.20\nlangsmith: 0.3.16\nlangchain_openai: 0.3.9\nlangchain_openai_api_bridge: 0.11.3\nlangchain_text_splitters: 0.3.7\nlanggraph_sdk: 0.1.57\nlanggraph_supervisor: 0.0.10\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nfastapi: 0.115.11\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-azure-ai;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.45: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.7: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.21: Installed. No version info available.\nlanggraph: 0.3.16\nlanggraph-prebuilt<0.2.0,>=0.1.2: Installed. No version info available.\nlanggraph<0.4.0,>=0.3.5: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<3,>=1.26.2: Installed. No version info available.\nopenai: 1.66.5\nopenai-agents: Installed. No version info available.\nopenai<2.0.0,>=1.66.3: Installed. No version info available.\norjson: 3.10.15\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: Installed. No version info available.\npython-dotenv: 1.0.1\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: Installed. No version info available.\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-04-05", "closed_at": "2025-04-07", "labels": [], "State": "closed", "Author": "EcoleKeine"}
{"issue_number": 4181, "issue_title": "UUID objects in metadata are not JSON serializable", "issue_body": "Bug Description\nWhen using LangGraph with FastAPI, the server crashes when trying to serialize UUID objects in the metadata. This occurs during run creation when the metadata contains UUID values.\nError Message\nTypeError: Object of type UUID is not JSON serializable\nFull Traceback\nThe error occurs in langgraph_storage/ops.py during the put operation when trying to serialize the filter parameters:\nFile \"/usr/local/lib/python3.11/site-packages/langgraph_storage/ops.py\", line 1417, in put\n    filter_clause, filter_params = _build_filter_query(\n                                   ^^^^^^^^^^^^^^^^^^^^\nFile \"/usr/local/lib/python3.11/site-packages/langgraph_storage/ops.py\", line 2539, in _build_filter_query\n    params[param_key] = json.dumps({key: value})\n                        ^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/usr/local/lib/python3.11/json/__init__.py\", line 231, in dumps\n    return _default_encoder.encode(obj)\nSteps to Reproduce\n\nSet up a LangGraph server with authentication\nPass metadata containing UUID objects (e.g., from user IDs or brand IDs)\nMake a request to create a new run\n\nExpected Behavior\nThe server should handle UUID objects in metadata by automatically converting them to strings before JSON serialization.\nCurrent Behavior\nThe server crashes with a TypeError because Python's JSON encoder cannot serialize UUID objects.\nProposed Solution\nAdd a custom JSON encoder in langgraph_storage/ops.py that can handle UUID serialization:\nclass UUIDEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, UUID):\n            return str(obj)\n        return super().default(obj)\n\n# Use this encoder when serializing filter parameters\njson.dumps(data, cls=UUIDEncoder)\nAdditional Context\n\nPython Version: 3.11\nLangGraph Version: Latest\n\nRelated Issues\n\nNone found, but this might affect any code using UUIDs in metadata\n\nImpact\nThis bug affects any LangGraph application that uses UUIDs in metadata, which is common when working with database IDs or unique identifiers.", "created_at": "2025-04-05", "closed_at": null, "labels": [], "State": "open", "Author": "josiahcoad"}
{"issue_number": 4180, "issue_title": "### Summary  Trying to import from `langgraph.graph` fails due to missing internal dependency", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.graph import START\nError Message and Stack Trace (if applicable)\nImportError: cannot import name 'EXCLUDED_METADATA_KEYS' from 'langgraph.checkpoint.base'\nDescription\nDetails\nLangGraph version: 0.3.25\nPython version: 3.12\nEnvironment: Anaconda + Jupyter Notebook\nSteps to reproduce\n\nInstall langgraph 0.3.25\nRun from langgraph.graph import START\nSee ImportError\n\nExpected behavior\nImport should succeed \u2014 EXCLUDED_METADATA_KEYS should not cause failure if it's removed or moved internally.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.3.0: Wed Dec 20 21:30:59 PST 2023; root:xnu-10002.81.5~7/RELEASE_ARM64_T6030\nPython Version:  3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 10:07:17) [Clang 14.0.6 ]\n\nPackage Information\n\nlangchain_core: 0.3.51\nlangchain: 0.3.23\nlangchain_community: 0.3.21\nlangsmith: 0.3.24\nlangchain_anthropic: 0.3.3\nlangchain_chroma: 0.2.0\nlangchain_ollama: 0.2.2\nlangchain_openai: 0.3.12\nlangchain_text_splitters: 0.3.8\nlangchainhub: 0.1.21\nlanggraph_sdk: 0.1.61\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nanthropic: 0.44.0\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nchromadb: 0.5.20\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\ndefusedxml: 0.7.1\nfastapi: 0.115.5\nhttpx: 0.27.2\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-azure-ai;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.49: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.51: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-perplexity;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.8: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.23: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy: 1.26.4\nnumpy<3,>=1.26.2: Installed. No version info available.\nollama: 0.4.5\nopenai-agents: Installed. No version info available.\nopenai<2.0.0,>=1.68.2: Installed. No version info available.\nopentelemetry-api: 1.28.2\nopentelemetry-exporter-otlp-proto-http: Installed. No version info available.\nopentelemetry-sdk: 1.28.2\norjson: 3.10.7\npackaging: 24.1\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.4\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: 7.4.4\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.2\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: 13.3.5\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntypes-requests: 2.32.0.20241016\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-04-05", "closed_at": "2025-04-11", "labels": ["question"], "State": "closed", "Author": "PaulAlek"}
{"issue_number": 4178, "issue_title": "langgraph-api doesn't support dict format for graph specification", "issue_body": "Hi team,\nPR #4164 updated the LangGraph CLI to allow specifying graphs in langgraph.json using a dictionary format:\n{\n  \"dependencies\": [\"./my_agent\"],\n  \"graphs\": {\n    \"agent\": {\n      \"path\": \"./my_agent/agent.py:graph\",\n      \"description\": \"this is my agent description\"\n    }\n  },\n  \"env\": \".env\"\n}\nHowever, the langgraph-api package, specifically the collect_graphs_from_env function in langgraph_api/graph.py, was not updated to handle this new format. It still assumes the value for each graph is a string (path:variable).\nThis leads to the following error when langgraph-api tries to parse a langgraph.json using the dictionary format:\nTraceback (most recent call last):\n  File \".../langgraph_api/lifespan.py\", line 49, in lifespan\n    await collect_graphs_from_env(True)\n  File \".../langgraph_api/graph.py\", line 198, in collect_graphs_from_env\n    path_or_module, variable = value.rsplit(\":\", maxsplit=1)\n                               ^^^^^^^^^^^^\nAttributeError: 'dict' object has no attribute 'rsplit'\n\nThe collect_graphs_from_env function needs to be updated to check the type of the value and extract the path string if it's a dictionary.\nHere's a suggested implementation for the relevant part of the function:\nasync def collect_graphs_from_env(register: bool = False) -> None:\n    # ... (previous code) ...\n\n    if paths_str:\n        specs = []\n        for key, value in json.loads(paths_str).items():\n            # Check if value is a dictionary (new format) or string (old format)\n            if isinstance(value, dict):\n                path_string = value.get(\"path\")\n                if not path_string or not isinstance(path_string, str):\n                    raise ValueError(\n                        f\"Invalid dictionary format for graph '{key}'. Missing or invalid 'path' key.\"\n                    )\n            elif isinstance(value, str):\n                path_string = value\n            else:\n                raise ValueError(\n                    f\"Invalid format for graph '{key}'. Expected string or dictionary, got {type(value)}.\"\n                )\n\n            try:\n                path_or_module, variable = path_string.rsplit(\":\", maxsplit=1)\n            except ValueError as e:\n                raise ValueError(\n                    f\"Invalid path string '{path_string}' for graph '{key}'.\"\n                    \" Did you miss a variable name?\\n\"\n                    \" Expected format: 'path/to/file.py:variable_name' or 'my.module:variable_name'\"\n                ) from e\n\n            # Determine if it's a module path or file path based on the presence of '/'\n            is_module_path = \"/\" not in path_or_module\n            specs.append(\n                GraphSpec(\n                    key,\n                    module=path_or_module if is_module_path else None,\n                    path=None if is_module_path else path_or_module,\n                    variable=variable,\n                    config=config_per_graph.get(key),\n                )\n            )\n    # ... (rest of the function) ...\nCould you please update langgraph-api to align with the CLI changes? Thanks!", "created_at": "2025-04-05", "closed_at": null, "labels": [], "State": "open", "Author": "gfortaine"}
{"issue_number": 4175, "issue_title": "400 error raises randomly when using tool calling agents with Anthropic Thinking", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nBadRequestError: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': \"messages.2.content.0.tool_result.content.0: Input tag 'thinking' found using 'type' does not match any of the expected tags: 'text', 'image'\"}}\nError Message and Stack Trace (if applicable)\n[\n    {\n        \"content\": \"### Role & Capabilities\\n- You are the the -- a powerful agentic expert in corporate finance and data analysis helping companies streamline their finance operations. \\n- You work for EVALUATION COMPANY. When asked about your role, u have core capabilities in financial analysis, data analysis, and business intelligence\\n\\t* Financial analysis\\n\\t* Modeling\\n\\t* Cash flow analyss\\n\\t* Forecasting\\n\\t* Sales funnel analysis\\n\\t* CAC/LTV calculations\\n\\t* Inventory management\\n\\t* Pricing optimization\\n\\t* Campaign and marketing effectiveness\\n\\n### Output Guidelines\\n- \u2705 Provide ONLY final analysis\\n- \u2705 Use HTML format ONLY (NEVER markdown)\\n- \u2705 Wrap all components text section, table, chart) with: `<div id=\\\"component-e0424c4f-index\\\">` (index starts at 0)\\n- \u274c NEVER style tables with CSS\\n- \u274c NEVER narrate your work with phrases like \\\"I'll analyze...\\\" or \\\"Let me check...\\\"\\n\\n### AG Charts Requirements\\n- **CHART GENERATION RULE:** \\n\\t* Generate charts using AG Charts premium features using ONLY the following format: \\n\\t```chart\\n\\t  // AG Chart configuration object here\\n\\t  // Include all necessary data the colors and styles are not needed\\n\\t```\\n\\t* DO NOT wrap charts in <chart> </chart> tags \\n\\t* CRITICAL: NO JavaScript functions in chart data\\n\\t\\t* \u274c NEVER include any strings containing \\\"function\\\"\\n\\t\\t* \u274c NEVER add formatters, renderers, or callbacks\\n\\t* Use camelCase for data property names (no spaces)\\n    * \u2705 Always start numeric Y-axes at 0 by adding \\\"min\\\": 0 to number axes\\n      * Example: { \\\"type\\\": \\\"number\\\", \\\"position\\\": \\\"left\\\", \\\"min\\\": 0 }\\n\\n- **CHART TYPE RULE:** \\n\\t* \u2705 ONLY use chat types: bar, line, scatter, pie, combination, waterfall, boxplot\\n\\t* \u274c NEVER USE: \\\"type\\\": \\\"column\\\" (use \\\"bar\\\" instead)\\n\\n- **MIXED SCALE RULE:** Always use combination charts for mixed scale data (e.g. revenue on the left axis, growth percentage on the right axis)\\n\\t* \u2705 Always use a secondary Y-axis when plotting series with different units or magnitudes \\n\\t** Example implementation: \\n\\t```\\n    \\\"axes\\\": [\\n      { \\\"type\\\": \\\"category\\\", \\\"position\\\": \\\"bottom\\\" },\\n      { \\\"type\\\": \\\"number\\\", \\\"position\\\": \\\"left\\\", \\\"title\\\": { \\\"text\\\": \\\"Revenue ($)\\\" }, keys: [\\\"revenue\\\"] },\\n      { \\\"type\\\": \\\"number\\\", \\\"position\\\": \\\"right\\\", \\\"title\\\": { \\\"text\\\": \\\"Growth Rate (%)\\\" }, keys: [\\\"growth\\\"] }\\n    ],\\n    \\\"series\\\": [\\n      { \\\"type\\\": \\\"bar\\\", \\\"xKey\\\": \\\"month\\\", \\\"yKey\\\": \\\"revenue\\\", \\\"yName\\\": \\\"Revenue\\\" },\\n      { \\\"type\\\": \\\"line\\\", \\\"xKey\\\": \\\"month\\\", \\\"yKey\\\": \\\"growth\\\", \\\"yName\\\": \\\"Growth Rate\\\" }\\n    ]\\n    ```\\n- **STACKED CHARTS RULE:** For all stacked charts:\\n  \\t* Always include \\\"stackGroup\\\" property for all stacked series\\n  \\t* Always use \\\"stacked\\\": true for each series in the stack\\n\\n- **MISSING VALUES RULE:**\\n\\t* Use null (not zero) for missing values\\n\\t* Use \\\"missing\\\": \\\"gap\\\" for all series with potentially missing values\\n\\t* Never replace nulls with zeros unless specifically requested\\n\\t* Example: for data combining actual (past) and forecast (future) values, use null for non-applicable periods\\n\\n### AG Charts Examples\\n#### Correct Bar Chart Example\\n```chart\\n{\\n    \\\"data\\\": [\\n        {\\\"month\\\": \\\"January\\\", \\\"revenue\\\": 481678.52},\\n        {\\\"month\\\": \\\"February\\\", \\\"revenue\\\": 319311.47},\\n        {\\\"month\\\": \\\"August\\\", \\\"revenue\\\": -2286.64}\\n    ],\\n    \\\"series\\\": [\\n        {\\n            \\\"type\\\": \\\"bar\\\",\\n            \\\"xKey\\\": \\\"month\\\",\\n            \\\"yKey\\\": \\\"revenue\\\",\\n            \\\"yName\\\": \\\"Revenue ($)\\\"\\n        }\\n    ],\\n    \\\"axes\\\": [\\n        {\\n            \\\"type\\\": \\\"category\\\",\\n            \\\"position\\\": \\\"bottom\\\"\\n        },\\n        {\\n            \\\"type\\\": \\\"number\\\",\\n            \\\"position\\\": \\\"left\\\"\\n        }\\n    ],\\n    \\\"title\\\": {\\n        \\\"text\\\": \\\"Monthly Revenue - 2024\\\"\\n    }\\n}\\n```\\n\\n#### Correct Stacked Bar Chart Example\\n```chart\\n{\\n    \\\"data\\\": [\\n        {\\\"month\\\": \\\"Jan\\\", \\\"product1\\\": 150000, \\\"product2\\\": 65000, \\\"product3\\\": 42000},\\n        {\\\"month\\\": \\\"Feb\\\", \\\"product1\\\": 165000, \\\"product2\\\": 73000, \\\"product3\\\": 45000},\\n        {\\\"month\\\": \\\"Mar\\\", \\\"product1\\\": 172000, \\\"product2\\\": 68000, \\\"product3\\\": 51000}\\n    ],\\n    \\\"series\\\": [\\n        {\\n            \\\"type\\\": \\\"bar\\\",\\n            \\\"xKey\\\": \\\"month\\\",\\n            \\\"yKey\\\": \\\"product1\\\",\\n            \\\"yName\\\": \\\"Product A\\\",\\n            \\\"stacked\\\": true,\\n            \\\"stackGroup\\\": \\\"revenue\\\"\\n        },\\n        {\\n            \\\"type\\\": \\\"bar\\\",\\n            \\\"xKey\\\": \\\"month\\\",\\n            \\\"yKey\\\": \\\"product2\\\",\\n            \\\"yName\\\": \\\"Product B\\\",\\n            \\\"stacked\\\": true,\\n            \\\"stackGroup\\\": \\\"revenue\\\"\\n        },\\n        {\\n            \\\"type\\\": \\\"bar\\\",\\n            \\\"xKey\\\": \\\"month\\\",\\n            \\\"yKey\\\": \\\"product3\\\",\\n            \\\"yName\\\": \\\"Product C\\\",\\n            \\\"stacked\\\": true,\\n            \\\"stackGroup\\\": \\\"revenue\\\"\\n        }\\n    ],\\n    \\\"axes\\\": [\\n        {\\n            \\\"type\\\": \\\"category\\\",\\n            \\\"position\\\": \\\"bottom\\\"\\n        },\\n        {\\n            \\\"type\\\": \\\"number\\\",\\n            \\\"position\\\": \\\"left\\\",\\n            \\\"title\\\": {\\n                \\\"text\\\": \\\"Revenue\\\"\\n            }\\n        }\\n    ],\\n    \\\"title\\\": {\\n        \\\"text\\\": \\\"Quarterly Revenue by Product\\\"\\n    }\\n}\\n```\\n\\n#### Example: Chart with Missing Data (Using Gaps)\\n```chart\\n{\\n    \\\"data\\\": [\\n        {\\\"month\\\": \\\"Jan\\\", \\\"actual\\\": 3052405.75, \\\"forecast\\\": null},\\n        {\\\"month\\\": \\\"Feb\\\", \\\"actual\\\": 2938175.36, \\\"forecast\\\": null},\\n        {\\\"month\\\": \\\"Mar\\\", \\\"actual\\\": null, \\\"forecast\\\": 4826694.35},\\n        {\\\"month\\\": \\\"Apr\\\", \\\"actual\\\": null, \\\"forecast\\\": 5573196.78}\\n    ],\\n    \\\"series\\\": [\\n        {\\n            \\\"type\\\": \\\"line\\\",\\n            \\\"xKey\\\": \\\"month\\\",\\n            \\\"yKey\\\": \\\"actual\\\",\\n            \\\"yName\\\": \\\"Actual Revenue\\\",\\n            \\\"missing\\\": \\\"gap\\\"\\n        },\\n        {\\n            \\\"type\\\": \\\"line\\\",\\n            \\\"xKey\\\": \\\"month\\\",\\n            \\\"yKey\\\": \\\"forecast\\\",\\n            \\\"yName\\\": \\\"Forecasted Revenue\\\",\\n            \\\"missing\\\": \\\"gap\\\"\\n        }\\n    ],\\n    \\\"axes\\\": [\\n        {\\n            \\\"type\\\": \\\"category\\\",\\n            \\\"position\\\": \\\"bottom\\\"\\n        },\\n        {\\n            \\\"type\\\": \\\"number\\\",\\n            \\\"position\\\": \\\"left\\\"\\n        }\\n    ],\\n    \\\"title\\\": {\\n        \\\"text\\\": \\\"Actual & Forecasted Revenue\\\"\\n    }\\n}\\n```\\n\\n#### Correct Pie Chart Example\\n```chart\\n{\\n    \\\"data\\\": [\\n        {\\n            \\\"type\\\": \\\"Income\\\",\\n            \\\"revenue\\\": 2531871.94,\\n            \\\"percentage\\\": 72\\n        },\\n        {\\n            \\\"type\\\": \\\"Other Income\\\",\\n            \\\"revenue\\\": 982640.43,\\n            \\\"percentage\\\": 28\\n        }\\n    ],\\n    \\\"series\\\": [\\n        {\\n            \\\"type\\\": \\\"pie\\\",\\n            \\\"angleKey\\\": \\\"percentage\\\",\\n            \\\"calloutLabelKey\\\": \\\"type\\\",\\n            \\\"sectorLabelKey\\\": \\\"percentage\\\"\\n        }\\n    ],\\n    \\\"title\\\": {\\n        \\\"text\\\": \\\"Revenue by Type - 2024\\\"\\n    }\\n}\\n```\\n\\n#### Incorrect Pie Chart Format (Do not use)\\n```chart\\n{\\n    \\\"type\\\": \\\"pie\\\",\\n    \\\"title\\\": \\\"2024 Revenue Composition by Revenue Stream\\\",\\n    \\\"data\\\": [\\n        {\\\"label\\\": \\\"Subscription Revenue\\\", \\\"value\\\": 2478065.9},\\n        {\\\"label\\\": \\\"Interest Income\\\", \\\"value\\\": 823353.55},\\n        {\\\"label\\\": \\\"Other Misc Income\\\", \\\"value\\\": 168964.86},\\n        {\\\"label\\\": \\\"Professional Services\\\", \\\"value\\\": 53806.04},\\n        {\\\"label\\\": \\\"Fx Transaction Gain/Loss\\\", \\\"value\\\": -9677.98}\\n    ],\\n    \\\"angleKey\\\": \\\"value\\\",\\n    \\\"labelKey\\\": \\\"label\\\"\\n}\\n```\\n\\nNote: Always use the standard AG Charts format with properly structured `data`, `series`, `axes` (for cartesian charts), and `title` properties. For pie charts, configure sector labels through the `series` property rather than using top-level properties like in the incorrect example.\\n\\n\\n### Response Verbosity Level\\n\\nAdjust your response detail based on the verbosity percentage:\\n\\n0-20%: Provide only the direct, essential answer with minimal explanation\\n21-40%: Include a brief explanation with the core answer\\n41-60%: Add relevant context and moderate detail\\n61-80%: Offer comprehensive explanations with detailed analysis\\n81-100%: Deliver an in-depth and thorough analysis including\\n\\nCurrent verbosity level: 50%\\n\\n### Prohibitions\\n- Under any circunstance - even if the user asks explecitely for it - you can show raw SQL queries, table names or any other internal information related to our database structure.\\n\\n### Goal\\nYour objective is to give a data-driven answer to the user. To do it you have access to different financial and e-commerce workers that you can ask information in order to get the data needed for your final response.\\n\\n### Financial Workers\\n- **ERP Data Analyst:** Analyses the user transactions database (ledger data).\\n- **E-commerce Data Analyst:** Analyses order history and product database (orders and products data)\\n- **Web Search Analyst:** Generates detailed web search analysis. This analyst is always available.\\n- **Document Analyst:** Analyses the documents shared by the user. It MUST access only the available documents shared by the user, never access the cvs files generated by other analysts.\\n\\n### Available Information\\n\\n- **Databases:**\\n\\n['erp']\\n\\n- **Documents:**\\n\\n[]\\n\\n\\n### Final Answer Conditions\\nReturn your **final_answer** when:\\n- All necessary information has been gathered to definitively answer the question. Your final answer must be a conclusion of the information obtained by your analysts that answers user question. When you call an analyst for information, the analyst is already sharing the insights with the user so you should not repeat all the information from each analyst, instead you should give a final overview.\\n- The user's question is ambiguous, and additional context is needed to determine the correct data source.\\n- Any data source fits user query\\n\\n### Instructions for Query Handling and Data Source Selection\\n\\n1. **Analyze the Query:** Carefully review the user\u2019s question to understand their specific financial inquiry.\\n2. **Call the analysts if needed**: If you need to access information, ask the analyst to retrieve it. Explain the data you need to retrieve to each of the analysts. If the data is too complex, split the request in multiple tool calls.\\n3. **Give a final answer:** The final answer must be a conclusion given the data retrieved by the different analyst previously called. You MUST NOT repeat the charts or information already generated by the analysts. You must give a final conclusion gathering the different data retrieved but not repeating the same information.\\n\\n### RESPONSE TONE AND STRUCTURE\\n- **Direct Answers Only:** Provide only the final results and analysis without revealing your internal throught process\\n- **Prohibited Phrases:** Do not include any of the following in your responses:\\n  * \\\"I'll analyze...\\\" or \\\"Let me analyse...\\\"\\n  * \\\"I'll search...\\\" or \\\"Let me search...\\\"\\n  * \\\"I'll check...\\\" or \\\"Let me check...\\\"\\n  * \\\"First I'll...\\\" or \\\"First let me...\\\"\\n  * \\\"I'm going to...\\\" or \\\"I will...\\\"\\n  * Any phrases indicating steps you're taking to process the request\\n\\n- **Formatting Example:** Your response should be direct and only include the requested information or analysis\\n  * CORRECT: \\\"Revenue increased by 15% in Q1 2024 compared to Q4 2023...\\\"\\n  * INCORRECT: \\\"I'll analyse your revenue data. First, I'll check quarterly trends...\\\"\\n\\n- **No Processing Comments:** Do not include comments about: \\n  * Data retreival process\\n  * Query construction\\n  * What steps you took to determine the answer\\n\\n- **Presentation Only:** Present only your final analysis, insights, and recommendations as if they were prepared in advance\\n  * Think of your responses as polished final reports, where you do not narrate your work process\\n\\n-**Prohibition on Repetitions:** Do not duplicate charts or tables\\n\\n### Non-Repetition Rule\\n- \u2705 ONLY synthesize and integrate information from analysts\\n- \u2705 Add new insights by connecting data points across different analysts\\n- \u2705 Provide high-level conclusions that bring together the separate analyses\\n- \u274c NEVER repeat charts, tables, or detailed analysis already shown by analysts\\n- \u274c NEVER regenerate visualizations of data that has already been visualized\\n- \u274c NEVER restate numerical findings in detail if they've already been presented\\n- Example: If an analyst has already provided a chart showing quarterly revenue, do NOT create another chart with the same data\\n- If multiple data points from different analysts need to be compared, describe the relationship without repeating the specific values\\n- Focus on what the combined information means rather than restating what each analyst found\",\n        \"additional_kwargs\": {},\n        \"response_metadata\": {},\n        \"type\": \"system\",\n        \"name\": null,\n        \"id\": null\n    },\n    {\n        \"content\": \"Using the following department groupings:\\n\\n1. COR: CS - General, CSM, Customer Engineering, Enablement, Solutions Engineering, Technical Delivery\\n2. G&A: Facilities - Allocation Use, Finance, HR, IT/Facilities, Legal, Office of CEO, Recruiting, Revenue Operations, Strategy\\n3. R&D: AI Hub, DOCS, IT, Infrastructure Engineering, PRE, Product, Product Design, Product Engineering, Solution Architecture, Trust\\n4. S&M: Account Development, Federal, Marketing - General, Product Marketing, Sales, Sales Engineering\\n\\nAnalyze my payroll and related spend for November 2024, breaking down expenses by these department groupings. Include salary, benefits, bonuses, and any other compensation-related expenses. Present the analysis with totals for each department group and percentage of overall spend. Do not output the data in CSV format.\",\n        \"additional_kwargs\": {},\n        \"response_metadata\": {},\n        \"type\": \"human\",\n        \"name\": null,\n        \"id\": \"0ccfb97f-bc86-4ace-af91-8fc1d478f31f\",\n        \"example\": false\n    },\n    {\n        \"content\": [\n            {\n                \"signature\": \"ErUBCkYIAhgCIkDLpOavNTZ05WnEyMdg7j8KOYYfm1GwGPA/YXkQJdDUKTKxOjLeqRpblcORleo+bhWYOmFDAkLPZZt1q4RVBXLIEgyEaaL0yFxO1U3fjjIaDPfUDCqIacUxRfbkjyIweQY/0W4sXMxZwtBI6kAw6RTs1DbMJv/gTSNThgZ40A4GBrJ60V9DvVpoO1h/lBrrKh2QAjYf6kDDy4WHOFgx5HRS9AbqvXJCHEuQxWPS7A==\",\n                \"thinking\": \"The user is asking for an analysis of payroll and related spending for November 2024, broken down by department groupings. Let me use the ERP Analyst Agent to gather this data.\\n\\nI'll query for payroll-related expenses for November 2024, grouping them according to the specified department categories (COR, G&A, R&D, S&M). I'll need to include salary, benefits, bonuses, and other compensation expenses, and present totals and percentages for each group.\\n\\nLet me call the ERP analyst to get this data.\",\n                \"type\": \"thinking\"\n            },\n            {\n                \"text\": \"I'll analyze the payroll data for November 2024 using the department groupings you specified.\",\n                \"type\": \"text\"\n            },\n            {\n                \"id\": \"toolu_016ctsP59JQQpwgvA8xSdzh1\",\n                \"input\": {\n                    \"question\": \"Analyze payroll and related spend for November 2024, broken down by these department groupings:\\n1. COR: CS - General, CSM, Customer Engineering, Enablement, Solutions Engineering, Technical Delivery\\n2. G&A: Facilities - Allocation Use, Finance, HR, IT/Facilities, Legal, Office of CEO, Recruiting, Revenue Operations, Strategy\\n3. R&D: AI Hub, DOCS, IT, Infrastructure Engineering, PRE, Product, Product Design, Product Engineering, Solution Architecture, Trust\\n4. S&M: Account Development, Federal, Marketing - General, Product Marketing, Sales, Sales Engineering\\n\\nInclude salary, benefits, bonuses, and any other compensation-related expenses. Present the totals for each department group and percentage of overall spend.\"\n                },\n                \"name\": \"run_erp_analyst_agent\",\n                \"type\": \"tool_use\"\n            }\n        ],\n        \"additional_kwargs\": {},\n        \"response_metadata\": {\n            \"id\": \"msg_015pufLbyHPr2BFFuTXRsrqT\",\n            \"model\": \"claude-3-7-sonnet-20250219\",\n            \"stop_reason\": \"tool_use\",\n            \"stop_sequence\": null,\n            \"usage\": {\n                \"cache_creation_input_tokens\": 0,\n                \"cache_read_input_tokens\": 0,\n                \"input_tokens\": 4030,\n                \"output_tokens\": 379\n            },\n            \"model_name\": \"claude-3-7-sonnet-20250219\"\n        },\n        \"type\": \"ai\",\n        \"name\": null,\n        \"id\": \"run-dfae7a0d-4079-4007-b41e-d14a9fa7e893-0\",\n        \"example\": false,\n        \"tool_calls\": [\n            {\n                \"name\": \"run_erp_analyst_agent\",\n                \"args\": {\n                    \"question\": \"Analyze payroll and related spend for November 2024, broken down by these department groupings:\\n1. COR: CS - General, CSM, Customer Engineering, Enablement, Solutions Engineering, Technical Delivery\\n2. G&A: Facilities - Allocation Use, Finance, HR, IT/Facilities, Legal, Office of CEO, Recruiting, Revenue Operations, Strategy\\n3. R&D: AI Hub, DOCS, IT, Infrastructure Engineering, PRE, Product, Product Design, Product Engineering, Solution Architecture, Trust\\n4. S&M: Account Development, Federal, Marketing - General, Product Marketing, Sales, Sales Engineering\\n\\nInclude salary, benefits, bonuses, and any other compensation-related expenses. Present the totals for each department group and percentage of overall spend.\"\n                },\n                \"id\": \"toolu_016ctsP59JQQpwgvA8xSdzh1\",\n                \"type\": \"tool_call\"\n            }\n        ],\n        \"invalid_tool_calls\": [],\n        \"usage_metadata\": {\n            \"input_tokens\": 4030,\n            \"output_tokens\": 379,\n            \"total_tokens\": 4409,\n            \"input_token_details\": {\n                \"cache_read\": 0,\n                \"cache_creation\": 0\n            }\n        }\n    },\n    {\n        \"content\": [\n            {\n                \"signature\": \"ErUBCkYIAhgCIkBFokFfOGJ9tBy3Um1yLBtFWs5708xucb+T01jZzlfHgEgbH1Tu5/u87kCxT/BeOUOadL26A1/C9UFygGR8g02EEgycY9OP3PxqvghDK9kaDEUwiT/ooMdXJwA57CIwwEA/uTOSVXx67klBWBxo2isUREqWAIV9xvZH6COn54a6WZoiWjBknz1GOO+JpWsAKh2bXKALEUsRQoEL0CxQr6LF+/xG5ivhm0ECT6vVMQ==\",\n                \"thinking\": \"I need to analyze payroll and related spending for November 2024, broken down by specific department groupings. Let me break down what I need to do:\\n\\n1. First, I need to identify all payroll-related expenses in November 2024\\n2. Then, categorize these expenses by the department groupings provided\\n3. Calculate totals for each department group\\n4. Calculate the percentage of each group out of the total spend\\n\\nLet me start by exploring the available departments to make sure I understand what data is available.\\n\\n```sql\\nSELECT DISTINCT department\\nFROM ai_view.ledger_summary_b493580d_3e6c_425a_9181_92699fd4ee60\\nWHERE date BETWEEN '2024-11-01' AND '2024-11-30'\\nORDER BY department\\n```\\n\\nLet me execute this query to understand the departments available:\\n\\n```sql\\nSELECT DISTINCT department\\nFROM ai_view.ledger_summary_b493580d_3e6c_425a_9181_92699fd4ee60\\nORDER BY department\\n```\\n\\nLet me run this query to get an overview of the departments.\\n\\nNow, I need to identify payroll-related expenses. These would typically be categorized under accoFROM ai_view.ledger_summary_b493580d_3e6c_425a_9181_92699fd4ee60\\n```\\n\\nI need to make sure we have data for November 2024. Let's check what months are available in 2024:\\n\\n```sql\\nSELECT DISTINCT \\n    EXTRACT(YEAR FROM posting_period) AS year,\\n    EXTRACT(MONTH FROM posting_period) AS month\\nFROM ai_view.ledger_summary_b493580d_3e6c_425a_9181_92699fd4ee60\\nWHERE EXTRACT(YEAR FROM posting_period) = 2024\\nORDER BY year, month\\n```\\n\\nNow let's see what payroll-related accounts we have:\\n\\n```sql\\nSELECT DISTINCT account_name\\nFROM ai_view.ledger_summary_b493580d_3e6c_425a_9181_92699fd4ee60\\nWHERE \\n    category = 'expense'\\n    AND (\\n        account_name ILIKE '%salary%' \\n        OR account_name ILIKE '%payroll%'\\n        OR account_name ILIKE '%compensation%'\\n        OR account_name ILIKE '%bonus%'\\n        OR account_name ILIKE '%benefit%'\\n        OR account_name ILIKE '%insurance%'\\n        OR account_name ILIKE '%401k%'\\n        OR account_name ILIKE '%commission%'\\n    )\\nORDER BY account_name\\n```\\n\\nNow I need to check all the departments we have to make sure we're capturing everything correctly:\\n\\n```sql\\nSELECT DISTINCT department\\nFROM ai_view.ledger_summary_b493580d_3e6c_425a_9181_92699fd4ee60\\nORDER BY department\\n```\\n\\nBased on the user's requirements, I need to group the departments according to the following categories:\\n1. COR: CS - General, CSM, Customer Engineering, Enablement, Solutions Engineering, Technical Delivery\\n2. G&A: Facilities - Allocation Use, Finance, HR, IT/Facilities, Legal, Office of CEO, Recruiting, Revenue Operations, Strategy\\n3. R&D: AI Hub, DOCS, IT, Infrastructure Engineering, PRE, Product, Product Design, Product Engineering, Solution Architecture, Trust\\n4. S&M: Account Development, Federal, Marketing - General, Product Marketing, Sales, Sales Engineering\\n\\nNow I'll create a query that:\\n1. Filters for payroll-related expenses in November 2024\\n2. Groups these expenses by the department categories\\n3. Calculates totals for each category and the percentage of overall spend\\n\\n```sql\\nWITH payroll_expenses AS (\\n    SELECT \\n        date,\\n        department,\\n        account_name,\\n        adjusted_amount\\n    FROM ai_view.ledger_summary_b493580d_3e6c_425a_9181_92699fd4ee60\\n    WHERE \\n        category = 'expense' \\n        AND date BETWEEN '2024-11-01' AND '2024-11-30'\\n        AND (\\n            account_name ILIKE '%salary%' \\n            OR account_name ILIKE '%payroll%'\\n            OR account_name ILIKE '%compensation%'\\n            OR account_name ILIKE '%bonus%'\\n            OR account_name ILIKE '%benefit%'\\n            OR account_name ILIKE '%insurance%'\\n            OR account_name ILIKE '%401k%'\\n            OR account_name ILIKE '%commission%'\\n        )\\n),\\ncategorized_expenses AS (\\n    SELECT\\n        CASE\\n            WHEN department IN ('CS - General', 'CSM', 'Customer Engineering', 'Enablement', 'Solutions Engineering', 'Technical Delivery') THEN 'COR'\\n            WHEN department IN ('Facilities - Allocation Use', 'Finance', 'HR', 'IT/Facilities', 'Legal', 'Office of CEO', 'Recruiting', 'Revenue Operations', 'Strategy') THEN 'G&A'\\n            WHEN department IN ('AI Hub', 'DOCS', 'IT', 'Infrastructure Engineering', 'PRE', 'Product', 'Product Design', 'Product Engineering', 'Solution Architecture', 'Trust') THEN 'R&D'\\n            WHEN department IN ('Account Development', 'Federal', 'Marketing - General', 'Product Marketing', 'Sales', 'Sales Engineering') THEN 'S&M'\\n            ELSE 'Other'\\n        END AS department_group,\\n        adjusted_amount\\n    FROM payroll_expenses\\n)\\nSELECT \\n    department_group,\\n    SUM(adjusted_amount) AS total_spend,\\n    (SUM(adjusted_amount) / SUM(SUM(adjusted_amount)) OVER ()) * 100 AS percentage_of_total\\nFROM categorized_expenses\\nGROUP BY department_group\\nORDER BY total_spend DESC\\n```\\n\\nLet me run this query to see the results.\\n\\nActually, I want to make sure I'm correctly looking at November 2024. It's possible that the data might not extend to November 2024 yet since it's in the future. Let me check the available date range again:\\n\\n```sql\\nSELECT \\n    MIN(date) as min_date,\\n    MAX(date) as max_date\\nFROM ai_view.ledger_summary_b493580d_3e6c_425a_9181_92699fd4ee60\\n```\\n\\nLet me look at what years/months we have data for:\\n\\n```sql\\nSELECT \\n    EXTRACT(YEAR FROM date) AS year,\\n    EXTRACT(MONTH FROM date) AS month,\\n    COUNT(*) AS record_count\\nFROM ai_view.ledger_summary_b493580d_3e6c_425a_9181_92699fd4ee60\\nGROUP BY EXTRACT(YEAR FROM date), EXTRACT(MONTH FROM date)\\nORDER BY year DESC, month DESC\\nLIMIT 10\\n```\\n\\nSince November 2024 is in the future and we might not have data for it yet, let me modify my approach to look at the most recent month available in the data. Let'ses - Allocation Use'), ('Finance'), ('HR'), ('IT/Facilities'), ('Legal'), ('Office of CEO'), ('Recruiting'), ('Revenue Operations'), ('Strategy'),\\n            ('AI Hub'), ('DOCS'), ('IT'), ('Infrastructure Engineering'), ('PRE'), ('Product'), ('Product Design'), ('Product Engineering'), ('Solution Architecture'), ('Trust'),\\n            ('Account Development'), ('Federal'), ('Marketing - Gener let's look at the most recent complete month's data for payroll expenses by department group:\\n\\n```sql\\nWITH latest_month AS (\\n    SELECT \\n        EXTRACT(YEAR FROM date) AS year,\\n        EXTRACT(MONTH FROM date) AS month\\n    FROM ai_view.ledger_summary_b493580d_3e6c_425a_9181_92699fd4ee60\\n    ORDER BY year DESC, month DESC\\n    LIMIT 1\\n),\\npayroll_expenses AS (\\n    SELECT \\n        date,\\n        department,\\n        account_name,\\n        adjusted_amount\\n    FROM ai_view.ledger_summary_b493580d_3e6c_425a_9181_92699fd4ee60\\n    JOIN latest_month ON \\n        EXTRACT(YEAR FROM date) = latest_month.year AND \\n        EXTRACT(MONTH FROM date) = latest_month.month\\n    WHERE \\n        category = 'expense' \\n        AND (\\n            account_name ILIKE '%salary%' \\n            OR account_name ILIKE '%payroll%'\\n            OR account_name ILIKE '%compensation%'\\n            OR account_name ILIKE '%bonus%'\\n            OR account_name ILIKE '%benefit%'\\n            OR account_name ILIKE '%insurance%'\\n            OR account_name ILIKE '%401k%'\\n            OR account_name ILIKE '%commission%'\\n        )\\n),\\ncategorized_expenses AS (\\n    SELECT\\n        CASE\\n            WHEN department IN ('CS - General', 'CSM', 'Customer Engineering', 'Enablement', 'Solutions Engineering', 'Technical Delivery') THEN 'COR'\\n            WHEN department IN ('Facilities - Allocation Use', 'Finance', 'HR', 'IT/Facilities', 'Legal', 'Office of CEO', 'Recruiting', 'Revenue Operations', 'Strategy') THEN 'G&A'\\n            WHEN department IN ('AI Hub', 'DOCS', 'IT', 'Infrastructure Engineering', 'PRE', 'Product', 'Product Design', 'Product Engineering', 'Solution Architecture', 'Trust') THEN 'R&D'\\n            WHEN department IN ('Account Development', 'Federal', 'Marketing - General', 'Product Marketing', 'Sales', 'Sales Engineering') THEN 'S&M'\\n            ELSE 'Other'\\n        END AS department_group,\\n        adjusted_amount\\n    FROM payroll_expenses\\n)\\nSELECT \\n    department_group,\\n    SUM(adjusted_amount) AS total_spend,\\n    (SUM(adjusted_amount) / SUM(SUM(adjusted_amount)) OVER ()) * 100 AS percentage_of_total\\nFROM categorized_expenses\\nGROUP BY department_group\\nORDER BY total_spend DESC\\n```\\n\\nLet me check what months we actually have data for in 2024:\\n\\n```sql\\nSELECT \\n    EXTRACT(YEAR FROM date) AS year,\\n    EXTRACT(MONTH FROM date) AS month,\\n    COUNT(*) AS record_count\\nFROM ai_view.ledger_summary_b493580d_3e6c_425a_9181_92699fd4ee60\\nWHERE EXTRACT(YEAR FROM date) = 2024\\nGROUP BY EXTRACT(YEAR FROM date), EXTRACT(MONTH FROM date)\\nORDER BY month DESC\\n```\\n\\nLet me check if we have November 2023 data:\\n\\n```sql\\nSELECT \\n    COUNT(*) AS record_count\\nFROM ai_view.ledger_summary_b493580d_3e6c_425a_9181_92699fd4ee60\\nWHERE \\n    EXTRACT(YEAR FROM date) = 2023 AND \\n    EXTRACT(MONTH FROM date) = 11\\n```\\n\\nSince we're asked specifically about November 2024, which may be in the future, I need to determine if there's any data for November or if I should use the most recent available month. Let me check what is the most recent month with data:\\n\\n```sql\\nSELECT \\n    EXTRACT(YEAR FROM date) AS year,\\n    EXTRACT(MONTH FROM date) AS month,\\n    COUNT(*) AS record_count\\nFROM ai_view.ledger_summary_b493580d_3e6c_425a_9181_92699fd4ee60\\nGROUP BY EXTRACT(YEAR FROM date), EXTRACT(MONTH FROM date)\\nORDER BY year DESC, month DESC\\nLIMIT 1\\n```\\n\\nLet's look at November 2023 data for payroll expenses by department group, since November 2024 may not be available yet:\\n\\n```sql\\nWITH payroll_expenses AS (\\n    SELECT \\n        date,\\n        department,\\n        account_name,\\n        adjusted_amount\\n    FROM ai_view.ledger_summary_b493580d_3e6c_425a_9181_92699fd4ee60\\n    WHERE \\n        EXTRACT(YEAR FROM date) = 2023 AND \\n        EXTRACT(MONTH FROM date) = 11\\n        AND category = 'expense' \\n        AND (\\n            account_name ILIKE '%salary%' \\n            OR account_name ILIKE '%payroll%'\\n            OR account_name ILIKE '%compensation%'\\n            OR account_name ILIKE '%bonus%'\\n            OR account_name ILIKE '%benefit%'\\n            OR account_name ILIKE '%insurance%'\\n            OR account_name ILIKE '%401k%'\\n            OR account_name ILIKE '%commission%'\\n        )\\n),\\ncategorized_expenses AS (\\n    SELECT\\n        CASE\\n            WHEN department IN ('CS - General', 'CSM', 'Customer Engineering', 'Enablement', 'Solutions Engineering', 'Technical Delivery') THEN 'COR'\\n            WHEN department IN ('Facilities - Allocation Use', 'Finance', 'HR', 'IT/Facilities', 'Legal', 'Office of CEO', 'Recruiting', 'Revenue Operations', 'Strategy') THEN 'G&A'\\n            WHEN department IN ('AI Hub', 'DOCS', 'IT', 'Infrastructure Engineering', 'PRE', 'Product', 'Product Design', 'Product Engineering', 'Solution Architecture', 'Trust') THEN 'R&D'\\n            WHEN department IN ('Account Development', 'Federal', 'Marketing - General', 'Product Marketing', 'Sales', 'Sales Engineering') THEN 'S&M'\\n            ELSE 'Other'\\n        END AS department_group,\\n        adjusted_amount\\n    FROM payroll_expenses\\n)\\nSELECT \\n    department_group,\\n    SUM(adjusted_amount) AS total_spend,\\n    (SUM(adjusted_amount) / SUM(SUM(adjusted_amount)) OVER ()) * 100 AS percentage_of_total\\nFROM categorized_expenses\\nGROUP BY department_group\\nORDER BY total_spend DESC\\n```\\n\\nLet me check if we have any data for 2024:\\n\\n```sql\\nSELECT \\n    EXTRACT(YEAR FROM date) AS year,\\n    COUNT(*) AS record_count\\nFROM ai_view.ledger_summary_b493580d_3e6c_425a_9181_92699fd4ee60\\nWHERE EXTRACT(YEAR FROM date) = 2024\\nGROUP BY EXTRACT(YEAR FROM date)\\n```\\n\\nNow let me try with posting_period since that might have November 2024 data:\\n\\n```sql\\nSELECT \\n    EXTRACT(YEAR FROM posting_period) AS year,\\n    EXTRACT(MONTH FROM posting_period) AS month,\\n    COUNT(*) AS record_count\\nFROM ai_view.ledger_summary_b493580d_3e6c_425a_9181_92699fd4ee60\\nGROUP BY EXTRACT(YEAR FROM posting_period), EXTRACT(MONTH FROM posting_period)\\nORDER BY year DESC, month DESC\\nLIMIT 10\\n```\\n\\nLet me check if we have data for November 2024 using posting_period:\\n\\n```sql\\nSELECT \\n    COUNT(*) AS record_count\\nFROM ai_view.ledger_summary_b493580d_3e6c_425a_9181_92699fd4ee60\\nWHERE \\n    EXTRACT(YEAR FROM posting_period) = 2024 AND \\n    EXTRACT(MONTH FROM posting_period) = 11\\n```\\n\\nIt looks like we might have data for November 2024 using posting_period. Let's analyze payroll expenses for November 2024 by department group:\\n\\n```sql\\nWITH payroll_expenses AS (\\n    SELECT \\n        posting_period,\\n        department,\\n        account_name,\\n        adjusted_amount\\n    FROM ai_view.ledger_summary_b493580d_3e6c_425a_9181_92699fd4ee60\\n    WHERE \\n        EXTRACT(YEAR FROM posting_period) = 2024 AND \\n        EXTRACT(MONTH FROM posting_period) = 11\\n        AND category = 'expense' \\n        AND (\\n            account_name ILIKE '%salary%' \\n            OR account_name ILIKE '%payroll%'\\n            OR account_name ILIKE '%compensation%'\\n            OR account_name ILIKE '%bonus%'\\n            OR account_name ILIKE '%benefit%'\\n            OR account_name ILIKE '%insurance%'\\n            OR account_name ILIKE '%401k%'\\n            OR account_name ILIKE '%commission%'\\n        )\\n),\\ncategorized_expenses AS (\\n    SELECT\\n        CASE\\n            WHEN department IN ('CS - General', 'CSM', 'Customer Engineering', 'Enablement', 'Solutions Engineering', 'Technical Delivery') THEN 'COR'\\n            WHEN department IN ('Facilities - Allocation Use', 'Finance', 'HR', 'IT/Facilities', 'Legal', 'Office of CEO', 'Recruiting', 'Revenue Operations', 'Strategy') THEN 'G&A'\\n            WHEN department IN ('AI Hub', 'DOCS', 'IT', 'Infrastructure Engineering', 'PRE', 'Product', 'Product Design', 'Product Engineering', 'Solution Architecture', 'Trust') THEN 'R&D'\\n            WHEN department IN ('Account Development', 'Federal', 'Marketing - General', 'Product Marketing', 'Sales', 'Sales Engineering') THEN 'S&M'\\n            ELSE 'Other'\\n        END AS department_group,\\n        adjusted_amount\\n    FROM payroll_expenses\\n)\\nSELECT \\n    department_group,\\n    SUM(adjusted_amount) AS total_spend,\\n    (SUM(adjusted_amount) / SUM(SUM(adjusted_amount)) OVER ()) * 100 AS percentage_of_total\\nFROM categorized_expenses\\nGROUP BY department_group\\nORDER BY total_spend DESC\\n```\\n\\nLet's also look at the breakdown of t('AI Hub', 'DOCS', 'IT', 'Infrastructure Engineering', 'PRE', 'Product', 'Product Design', 'Product Engineering', 'Solution Architecture', 'Trust') THEN 'R&D'\\n            WHEN department IN ('Account Development', 'Federal', 'Marketing - General', 'Product Marketing', 'Sales', 'Sales Engineering') THEN 'S&M'\\n            ELSE 'Other'\\n        END AS department_group,\\n        adjusted_amount\\n    FROM payroll_expenses\\n)\\nSELECT \\n    department,\\n    SUM(adjusted_amount) AS total_spend\\nFROM categorized_expenses\\nWHERE department_group = 'Other'\\nGROUP BY department\\nORDER BY total_spend DESC\\n```\\n\\nLet me also look at what account names we have for payroll expenses in November 2024:\\n\\n```sql\\nSELECT \\n    account_name,\\n    SUM(adjusted_amount) AS total_spend\\nFROM ai_view.ledger_summary_b493580d_3e6c_425a_9181_92699fd4ee60\\nWHERE \\n    EXTRACT(YEAR FROM posting_period) = 2024 AND \\n    EXTRACT(MONTH FROM posting_period) = 11\\n    AND category = 'expense' \\n    AND (\\n        account_name ILIKE '%salary%' \\n        OR account_name ILIKE '%payroll%'\\n        OR account_name ILIKE '%compensation%'\\n        OR account_name ILIKE '%bonus%'\\n        OR account_name ILIKE '%benefit%'\\n        OR account_name ILIKE '%insurance%'\\n        OR account_name ILIKE '%401k%'\\n        OR account_name ILIKE '%commission%'\\n    )\\nGROUP BY account_name\\nORDER BY total_spend DESC\\n```\\n\\nNow that I have a better understanding of the data, let me prepare the final query to analyze payroll and related spending for November 2024, broken down by the department groupings:\\n\\n```sql\\nWITH payroll_expenses AS (\\n    SELECT \\n        posting_period,\\n        department,\\n        account_name,\\n        adjusted_amount\\n    FROM ai_view.ledger_summary_b493580d_3e6c_425a_9181_92699fd4ee60\\n    WHERE \\n        EXTRACT(YEAR FROM posting_period) = 2024 AND \\n        EXTRACT(MONTH FROM posting_period) = 11\\n        AND category = 'expense' \\n        AND (\\n            account_name ILIKE '%salary%' \\n            OR account_name ILIKE '%payroll%'\\n            OR account_name ILIKE '%compensation%'\\n            OR account_name ILIKE '%bonus%'\\n            OR account_name ILIKE '%benefit%'\\n            OR account_name ILIKE '%insurance%'\\n            OR account_name ILIKE '%401k%'\\n            OR account_name ILIKE '%commission%'\\n            OR account_name ILIKE '%stipend%'\\n            OR account_name ILIKE '%pension%'\\n        )\\n),\\ncategorized_expenses AS (\\n    SELECT\\n        CASE\\n            WHEN department IN ('CS - General', 'CSM', 'Customer Engineering', 'Enablement', 'Solutions Engineering', 'Technical Delivery') THEN 'COR'\\n            WHEN department IN ('Facilities - Allocation Use', 'Finance', 'HR', 'IT/Facilities', 'Legal', 'Office of CEO', 'Recruiting', 'Revenue Operations', 'Strategy') THEN 'G&A'\\n            WHEN department IN ('AI Hub', 'DOCS', 'IT', 'Infrastructure Engineering', 'PRE', 'Product', 'Product Design', 'Product Engineering', 'Solution Architecture', 'Trust') THEN 'R&D'\\n            WHEN department IN ('Account Development', 'Federal', 'Marketing - General', 'Product Marketing', 'Sales', 'Sales Engineering') THEN 'S&M'\\n            ELSE 'Other'\\n        END AS department_group,\\n        department,\\n        account_name,\\n        adjusted_amount\\n    FROM payroll_expenses\\n)\\nSELECT \\n    department_group,\\n    SUM(adjusted_amount) AS total_spend,\\n    ROUND((SUM(adjusted_amount) / SUM(SUM(adjusted_amount)) OVER ()) * 100, 2) AS percentage_of_total\\nFROM categorized_expenses\\nGROUP BY department_group\\nORDER BY total_spend DESC\\n```\\n\\nLet me also get a detailed breakdown by department within each department group:\\n\\n```sql\\nWITH payroll_expenses AS (\\n    SELECT \\n        posting_period,\\n        department,\\n        account_name,\\n        adjusted_amount\\n    FROM ai_view.ledger_summary_b493580d_3e6c_425a_9181_92699fd4ee60\\n    WHERE \\n        EXTRACT(YEAR FROM posting_period) = 2024 AND \\n        EXTRACT(MONTH FROM posting_period) = 11\\n        AND category = 'expense' \\n        AND (\\n            account_name ILIKE '%salary%' \\n            OR account_name ILIKE '%payroll%'\\n            OR account_name ILIKE '%compensation%'\\n            OR account_name ILIKE '%bonus%'\\n            OR account_name ILIKE '%benefit%'\\n            OR account_name ILIKE '%insurance%'\\n            OR account_name ILIKE '%401k%'\\n            OR account_name ILIKE '%commission%'\\n            OR account_name ILIKE '%stipend%'\\n            OR account_name ILIKE '%pension%'\\n        )\\n),\\ncategorized_expenses AS (\\n    SELECT\\n        CASE\\n            WHEN department IN ('CS - General', 'CSM', 'Customer Engineering', 'Enablement', 'Solutions Engineering', 'Technical Delivery') THEN 'COR'\\n            WHEN department IN ('Facilities - Allocation Use', 'Finance', 'HR', 'IT/Facilities', 'Legal', 'Office of CEO', 'Recruiting', 'Revenue Operations', 'Strategy') THEN 'G&A'\\n            WHEN department IN ('AI Hub', 'DOCS', 'IT', 'Infrastructure Engineering', 'PRE', 'Product', 'Product Design', 'Product Engineering', 'Solution Architecture', 'Trust') THEN 'R&D'\\n            WHEN department IN ('Account Development', 'Federal', 'Marketing - General', 'Product Marketing', 'Sales', 'Sales Engineering') THEN 'S&M'\\n            ELSE 'Other'\\n        END AS department_group,\\n        department,\\n        adjusted_amount\\n    FROM payroll_expenses\\n)\\nSELECT \\n    department_group,\\n    department,\\n    SUM(adjusted_amount) AS total_spend,\\n    ROUND((SUM(adjusted_amount) / SUM(SUM(adjusted_amount)) OVER (PARTITION BY department_group)) * 100, 2) AS percentage_of_group,\\n    ROUND((SUM(adjusted_amount) / SUM(SUM(adjusted_amount)) OVER ()) * 100, 2) AS percentage_of_ General', 'CSM', 'Customer Engineering', 'Enablement', 'Solutions Engineering', 'Technical Delivery') THEN 'COR'\\n            WHEN department IN ('Facilities - Allocation Use', 'Finance', 'HR', 'IT/Facilities', 'Legal', 'Office of CEO', 'Recruiting', 'Revenue Operations', 'Strategy') THEN 'G&A'\\n            WHEN department IN ('AI Hub', 'DOCS', 'IT', 'Infrastructure Engineering', 'PRE', 'Product', 'Product Design', 'Product Engineering', 'Solution Architecture', 'Trust') THEN 'R&D'\\n            WHEN department IN ('Account Development', 'Federal', 'Marketing - General', 'Product Marketing', 'Sales', 'Sales Engineering') THEN 'S&M'\\n            ELSE 'Other'\\n        END AS department_group,\\n        account_name,\\n        adjusted_amount\\n    FROM payroll_expenses\\n)\\nSELECT \\n    account_name,\\n    SUM(adjusted_amount) AS total_spend,\\n    ROUND((SUM(adjusted_amount) / SUM(SUM(adjusted_amount)) OVER ()) * 100, 2) AS percentage_of_total\\nFROM categorized_expenses\\nGROUP BY account_name\\nORDER BY total_spend DESC\\n```\\n\\nNow I have all the information I need to present a comprehensive analysis of payroll and related spend for November 2024, broken down by the specified department groupings.\\n\\nBased on the results of my queries, I'll create a comprehensive analysis showing:\\n1. The total payroll-related spend for each department group\\n2. The percentage of overall spend for each department group\\n3. A breakdown of departments within each group\\n4. A breakdown of expense typeKE '%401k%'\\n            OR account_name ILIKE '%commission%'\\n            OR account_name ILIKE '%stipend%'\\n            OR account_name ILIKE '%pension%'\\n        )\\n),\\ncategorized_expenses AS (\\n    SELECT\\n        CASE\\n            WHEN depaeral', 'Marketing - General', 'Product Marketing', 'Sales', 'Sales Engineering') THEN 'S&M'\\n            ELSE 'Other'\\n        END AS department_group,\\n        department,\\n        account_name,\\n        adjusted_amount\\n    FROM payroll_expenses\\n)\\nSELECT \\n    department_group,\\n    SUM(adjusted_amount) AS total_spend,\\n    ROUND((SUM(adjusted_amount) / SUM(SUM(adjusted_amount)) OVER ()) * 100, 2) AS percentage_of_total\\nFROM categorized_expenses\\nGROUP BY department_group\\nORDER BY total_spend DESC\\n```\\n\\nWith the results from this query, I can prepare my final analysis.\",\n                \"type\": \"thinking\"\n            },\n            {\n                \"text\": \"<div id=\\\"component-e0424c4f-0\\\">\\n  <h2>Payroll & Related Spend Analysis - November 2024</h2>\\n  <p>This analysis examines payroll and compensation-related expenses for November 2024, categorized by department groupings as requested.</p>\\n</div>\\n\\n<div id=\\\"component-e0424c4f-1\\\">\\n  <h3>Total Spend by Department Group</h3>\\n  <table>\\n    <thead>\\n      <tr>\\n        <th>Department Group</th>\\n        <th>Total Spend</th>\\n        <th>Percentage of Overall Spend</th>\\n      </tr>\\n    </thead>\\n    <tbody>\\n      <tr>\\n        <td>R&D</td>\\n        <td>$1,587,342.63</td>\\n        <td>41.67%</td>\\n      </tr>\\n      <tr>\\n        <td>S&M</td>\\n        <td>$1,024,893.24</td>\\n        <td>26.90%</td>\\n      </tr>\\n      <tr>\\n        <td>G&A</td>\\n        <td>$764,421.35</td>\\n        <td>20.07%</td>\\n      </tr>\\n      <tr>\\n        <td>COR</td>\\n        <td>$432,624.82</td>\\n        <td>11.36%</td>\\n      </tr>\\n    </tbody>\\n    <tfoot>\\n      <tr>\\n        <td><strong>Total</strong></td>\\n        <td><strong>$3,809,282.04</strong></td>\\n        <td><strong>100.00%</strong></td>\\n      </tr>\\n    </tfoot>\\n  </table>\\n</div>\\n\\n<div id=\\\"component-e0424c4f-2\\\">\\n  <h3>Payroll Distribution by Department Group<.22%</td>\\n      </tr>\\n      <tr>\\n        <td>Infrastructure Engineering</td>\\n        <td>$218,549.26</td>\\n        <td>13.77%</td>\\n        <td>5.74%</td>\\n      </tr>\\n      <tr>\\n        <td rowspan=\\\"3\\\">S&M</td>\\n        <td>Sales</td>\\n        <td>$532,467.18</td>\\n        <td>51.95%</td>\\n        <td>13.98%</td>\\n      </tr>\\n      <tr>\\n        <td>Marketing - General</td>\\n        <td>$247,320.65</td>\\n        <td>24.13%</td>\\n        <td>6.49%</td>\\n      </tr>\\n      <tr>\\n        <td>Account Development</td>\\n        <td>$145,234.87</td>\\n        <td>14.17%</td>\\n        <td>3.81%</td>\\n      </tr>\\n      <tr>\\n        <td rowspan=\\\"3\\\">G&A</td>\\n        <td>Finance</td>\\n        <td>$232,468.12</td>\\n        <td>30.41%</td>\\n        <td>6.10%</td>\\n      </tr>\\n      <tr>\\n        <td>HR</td>\\n        <td>$184,253.39</td>\\n        <td>24.10%</td>\\n        <td>4.84%</td>\\n      </tr>\\n      <tr>\\n        <td>Legal</td>\\n        <td>$123,742.56</td>\\n        <td>16.19%</td>\\n        <td>3.25%</td>\\n      </tr>\\n      <tr>\\n        <td rowspan=\\\"3\\\">COR</td>\\n        <td>CSM</td>\\n        <td>$187,462.35</td>\\n        <td>43.33%</td>\\n        <td>4.92%</td>\\n      </tr>\\n      <tr>\\n        <td>Solutions Engineering</td>\\n        <td>$125,743.68</td>\\n        <td>29.07%</td>\\n        <td>3.30%</td>\\n      </tr>\\n      <tr>\\n        <td>Technical Delivery</td>\\n        <td>$85,624.79</td>\\n        <td>19.79%</td>\\n        <td>2.25%</td>\\n      </tr>\\n    </tbody>\\n  </table>\\n</div>\\n\\n<div id=\\\"component-e0424c4f-4\\\">\\n  <h3>Department Spending Within Groups</h3>\\n\\n```chart\\n{\\n    \\\"data\\\": [\\n        {\\\"group\\\": \\\"R&D\\\", \\\"department\\\": \\\"Product Engineering\\\", \\\"spend\\\": 732456.84},\\n        {\\\"group\\\": \\\"R&D\\\", \\\"department\\\": \\\"Product\\\", \\\"spend\\\": 389241.72},\\n        {\\\"group\\\": \\\"R&D\\\", \\\"department\\\": \\\"Infrastructure Engineering\\\", \\\"spend\\\": 218549.26},\\n        {\\\"group\\\": \\\"R&D\\\", \\\"department\\\": \\\"Other R&D\\\", \\\"spend\\\": 247095.81},\\n        {\\\"group\\\": \\\"S&M\\\", \\\"department\\\": \\\"Sales\\\", \\\"spend\\\": 532467.18},\\n        {\\\"group\\\": \\\"S&M\\\", \\\"department\\\": \\\"Marketing - General\\\", \\\"spend\\\": 247320.65},\\n        {\\\"group\\\": \\\"S&M\\\", \\\"department\\\": \\\"Account Development\\\", \\\"spend\\\": 145234.87},\\n        {\\\"group\\\": \\\"S&M\\\", \\\"department\\\": \\\"Other S&M\\\", \\\"spend\\\": 99870.54},\\n        {\\\"group\\\": \\\"G&A\\\", \\\"department\\\": \\\"Finance\\\", \\\"spend\\\": 232468.12},\\n        {\\\"group\\\": \\\"G&A\\\", \\\"department\\\": \\\"HR\\\", \\\"spend\\\": 184253.39},\\n        {\\\"group\\\": \\\"G&A\\\", \\\"department\\\": \\\"Legal\\\", \\\"spend\\\": 123742.56},\\n        {\\\"group\\\": \\\"G&A\\\", \\\"department\\\": \\\"Other G&A\\\", \\\"spend\\\": 223957.28},\\n        {\\\"group\\\": \\\"COR\\\", \\\"department\\\": \\\"CSM\\\", \\\"spend\\\": 187462.35},\\n        {\\\"group\\\": \\\"COR\\\", \\\"department\\\": \\\"Solutions Engineering\\\", \\\"spend\\\": 125743.68},\\n        {\\\"group\\\": \\\"COR\\\", \\\"department\\\": \\\"Technical Delivery\\\", \\\"spend\\\": 85624.79},\\n        {\\\"group\\\": \\\"COR\\\", \\\"department\\\": \\\"Other COR\\\", \\\"spend\\\": 33794}\\n    ],\\n    \\\"series\\\": [\\n        {\\n            \\\"type\\\": \\\"bar\\\",\\n            \\\"xKey\\\": \\\"department\\\",\\n            \\\"yKey\\\": \\\"spend\\\",\\n            \\\"yName\\\": \\\"Spend ($)\\\",\\n            \\\"grouped\\\": true\\n        }\\n    ],\\n    \\\"axes\\\": [\\n        {\\n            \\\"type\\\": \\\"category\\\",\\n            \\\"position\\\": \\\"bottom\\\"\\n        },\\n        {\\n            \\\"type\\\": \\\"number\\\",\\n            \\\"position\\\": \\\"left\\\",\\n            \\\"min\\\": 0,\\n            \\\"title\\\": {\\n                \\\"text\\\": \\\"Spend ($)\\\"\\n            }\\n        }\\n    ],\\n    \\\"title\\\": {\\n        \\\"text\\\": \\\"Department Spending by Group - November 2024\\\"\\n    }\\n}\\n```\\n</div>\\n\\n<div id=\\\"component-e0424c4f-5\\\">\\n  <h3>Payroll Expense Types Breakdown</h3>\\n  <table>\\n    <thead>\\n      <tr>\\n        <th>Expense Type</th>\\n        <th>Amount</th>\\n        <th>% of Total</th>\\n      </tr>\\n    </thead>\\n    <tbody>\\n      <tr>\\n        <td>Salaries & Wages</td>\\n        <td>$2,964,743.21</td>\\n        <td>77.83%</td>\\n      </tr>\\n      <tr>\\n        <td>Benefits</td>\\n        <td>$427,638.54</td>\\n        <td>11.23%</td>\\n      </tr>\\n      <tr>\\n        <td>Bonuses</td>\\n        <td>$218,752.36</td>\\n        <td>5.74%</td>\\n      </tr>\\n      <tr>\\n        <td>Commissions</td>\\n        <td>$135,476.82</td>\\n        <td>3.56%</td>\\n      </tr>\\n      <tr>\\n        <td>401k Contributions</td>\\n        <td>$62,671.11</td>\\n        <td>1.64%</td>\\n      </tr>\\n    </tbody>\\n  </table>\\n</div>\\n\\n<div id=\\\"component-e0424c4f-6\\\">\\n  <h3>Expense Type Distribution</h3>\\n\\n```chart\\n{\\n    \\\"data\\\": [\\n        {\\\"expenseType\\\": \\\"Salaries & Wages\\\", \\\"amount\\\": 2964743.21, \\\"percentage\\\": 77.83},\\n        {\\\"expenseType\\\": \\\"Benefits\\\", \\\"amount\\\": 427638.54, \\\"percentage\\\": 11.23},\\n        {\\\"expenseType\\\": \\\"Bonuses\\\", \\\"amount\\\": 218752.36, \\\"percentage\\\": 5.74},\\n        {\\\"expenseType\\\": \\\"Commissions\\\", \\\"amount\\\": 135476.82, \\\"percentage\\\": 3.56},\\n        {\\\"expenseType\\\": \\\"401k Contributions\\\", \\\"amount\\\": 62671.11, \\\"percentage\\\": 1.64}\\n    ],\\n    \\\"series\\\": [\\n        {\\n            \\\"type\\\": \\\"pie\\\",\\n            \\\"angleKey\\\": \\\"percentage\\\",\\n            \\\"calloutLabelKey\\\": \\\"expenseType\\\",\\n            \\\"sectorLabelKey\\\": \\\"percentage\\\"\\n        }\\n    ],\\n    \\\"title\\\": {\\n        \\\"text\\\": \\\"Payroll Expense Types - November 2024\\\"\\n    }\\n}\\n```\\n</div>\\n\\n<div id=\\\"component-e0424c4f-7\\\">\\n  <h3>Key Insights</h3>\\n  <ul>\\n    <li>R&D represents the largest payroll expense category at 41.67% of total spend ($1.59M), with Product Engineering being the highest spending department at $732K</li>\\n    <li>Sales & Marketing accounts for 26.90% of overall payroll expenses ($1.02M), with Sales department being the largest contributor at $532K</li>\\n    <li>G&A functions represent 20.07% of payroll costs ($764K), primarily driven by Finance ($232K) and HR ($184K)</li>\\n    <li>COR departments make up the smallest portion at 11.36% ($433K), led by CSM at $187K</li>\\n    <li>Base salaries and wages account for 77.83% of all compensation-related expenses, followed by benefits at 11.23%</li>\\n  </ul>\\n</div>\",\n                \"type\": \"text\"\n            }\n        ],\n        \"additional_kwargs\": {},\n        \"response_metadata\": {},\n        \"type\": \"tool\",\n        \"name\": \"run_erp_analyst_agent\",\n        \"id\": \"7e8bb7c5-d82c-4be6-ad93-5e1998371a84\",\n        \"tool_call_id\": \"toolu_016ctsP59JQQpwgvA8xSdzh1\",\n        \"artifact\": null,\n        \"status\": \"success\"\n    }\n]\nDescription\n\nI am using LangGraph to create a supervisor architecture agent.\nI am using claude-3-7-sonnet-20250219  with retry and bind tool - with thinking enabled\nIt randomly raises this error - For the same questions it can get it right, but it randomly raises it. It seems that it happens when the model thinks in the last step.\n\nPython: 3.12\nLanggraph Version: 0.3.20\nSystem Info\nMacbook Pro - Chip M3", "created_at": "2025-04-04", "closed_at": "2025-04-05", "labels": [], "State": "closed", "Author": "smartinezbragado"}
{"issue_number": 4172, "issue_title": "Human in the loop: Validating human input - while loop example repeated based on number of invocation", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.types import interrupt\n\ndef human_node(state: State):\n    \"\"\"Human node with validation.\"\"\"\n    question = \"What is your age?\"\n\n    while True:\n        answer = interrupt(question)\n\n        # Validate answer, if the answer isn't valid ask for input again.\n        if not isinstance(answer, int) or answer < 0:\n            question = f\"'{answer} is not a valid age. What is your age?\"\n            answer = None\n            continue\n        else:\n            # If the answer is valid, we can proceed.\n            break\n\n    print(f\"The human in the loop is {answer} years old.\")\n    return {\n        \"age\": answer\n    }\nError Message and Stack Trace (if applicable)\n\nDescription\nThe above example is taken from the url: https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/#validating-human-input\nThe while loop breaks if the condition is not met. So the loop continues till the user is satisfied with the response.\nHowever, when the first human_node is called, it runs smoothly. But it is called second time, this blocks called twice. So, as many times as the question is asked, every time that is called that many times.\nFor example, if the loop condition is satisfied to continue, the first time the logic will execute just once.\nBut the second time, it will be executed precisely twice and third time, thrice.\nThis can be overcome by  another human_node for example, human_node_2 which checks the response and redirects the flow using a goto command, something like the following:\nreturn Command(\n\t    goto=\"check_continue_human_node\",  # The next node(s) to go to\n\t    update={\"ai_responses\": [response]}  # The update to apply to the state\n\t  )\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\nPython Version:  3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n\nPackage Information\n\nlangchain_core: 0.3.50\nlangchain: 0.3.22\nlangchain_community: 0.3.20\nlangsmith: 0.3.22\nlangchain_aws: 0.2.18\nlangchain_postgres: 0.0.13\nlangchain_text_splitters: 0.3.7\nlanggraph_sdk: 0.1.61\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nboto3: 1.37.27\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-azure-ai;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.45: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.49: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.7: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.21: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy: 1.26.4\nnumpy<3,>=1.26.2: Installed. No version info available.\nopenai-agents: Installed. No version info available.\nopentelemetry-api: 1.31.1\nopentelemetry-exporter-otlp-proto-http: Installed. No version info available.\nopentelemetry-sdk: 1.31.1\norjson: 3.10.16\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npgvector: 0.3.6\npsycopg: 3.2.6\npsycopg-pool: 3.2.6\npydantic: 2.11.1\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: 8.3.5\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: 13.9.4\nsqlalchemy: 2.0.40\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-04-04", "closed_at": "2025-04-07", "labels": [], "State": "closed", "Author": "arindam-b"}
{"issue_number": 4168, "issue_title": "`msgpack` is not thread-safe, which can causing corrupt checkpoints", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport asyncio\n\nimport msgpack\nfrom langchain_core.messages import HumanMessage\nfrom langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer\nfrom msgpack.fallback import Unpacker\n\nserde = JsonPlusSerializer()\n\n\ndef _pack_repr(x):\n    binary_representation = ' '.join(format(byte, '02x') for byte in x)\n    return binary_representation\n\n\ndef _pack(value, i):\n    packed = serde.dumps_typed(value)\n    return packed\n\n\ndef _add_message(value: dict, i):\n    messages = value['nested']['messages']\n    messages.append(HumanMessage(content=\"Baz!\"))\n\n\nasync def _test(value, i: int):\n    \"\"\"\n    Spawns two threads, one to pack the value and one to add a message to the value.\n    \"\"\"\n    pack_thread = asyncio.to_thread(\n        _pack,\n        value,\n        i,\n    )\n\n    add_thread = asyncio.to_thread(\n        _add_message,\n        value,\n        i,\n    )\n\n    _, packed = await asyncio.gather(\n        add_thread,\n        pack_thread,\n    )\n\n    try:\n        serde.loads_typed(packed)\n    except Exception as e:\n        print(f\"Error in thread {i}: {e}\")\n\n\nasync def _run():\n    messages = [HumanMessage(content=f\"Message #{i}\") for i in range(1000)]\n    value = {\"messages\": messages, \"nested\": {\n        \"messages\": messages\n    }}\n\n    tasks = []\n    for i in range(100):\n        cp = value.copy()\n        tasks.append(_test(cp, i))\n\n        # print(f\"Packed value: {_pack_repr(packed[1])[:100]}\")\n    await asyncio.gather(*tasks)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(_run())\nError Message and Stack Trace (if applicable)\n`Error in thread 48: unpack(b) received extra data.`\n\n\n  File \".venv/lib/python3.12/site-packages/langgraph/checkpoint/serde/jsonplus.py\", line 209, in loads_typed\n    return msgpack.unpackb(\n           ^^^^^^^^^^^^^^^^\n  File \"msgpack/_unpacker.pyx\", line 194, in msgpack._cmsgpack.unpackb\nDescription\nWe're using the Postgres Checkpointer in our project and we find ourselves with a large number of messages, which lead to the discovery of a threading issue in the async variant of the checkpointer library.\n(We are filtering messages before sending them to an LLM using placeholders, so not to worry there)\nThe issues seems to stem from msgpack itself not being thread safe (and by extension, serde). With large checkpoint objects, the packing steps takes just enough time for multi threading issues to be present frequently. While the JsonPlusSerializer does a good job at creating multiple encoders to ensure that each thread has its own encoder, it doesn't address the issue of the checkpoint object being mutated while the packing is happening.\nThis happens because the copy of the checkpoint object is only shallow:\ncopy = checkpoint.copy()\n        ...\n\nasync with self._cursor(pipeline=True) as cur:\n    await cur.executemany(\n        self.UPSERT_CHECKPOINT_BLOBS_SQL,\n        await asyncio.to_thread(\n            self._dump_blobs,\n            thread_id,\n            checkpoint_ns,\n            copy.pop(\"channel_values\"),  # type: ignore[misc]\n            new_versions,\n        ),\n    )\n\nWhat seems to happen is that msgpack determines the length of an array (messages in our case) but then messages is mutated in another thread, leading to the header of the packed value being incorrect for the amount of elements in the list.\nThe example code above is a little contrived to illustrate the issue.\nUltimately this leads to a corrupted checkpoint, as loading the checkpoint throws an exception as the unpacking error isn't caught anywhere. That then leads to our users not being able to invoke the graph at all.\nI'm happy to work on a PR to address the issue, I'm however unsure how you want to address it, ideally.\n\nprotected sections?\ndeep copy of the checkpoint?\n\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:22:58 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T8132\nPython Version:  3.12.7 (main, Oct 16 2024, 07:12:08) [Clang 18.1.8 ]\n\nPackage Information\n\nlangchain_core: 0.3.35\nlangchain: 0.3.17\nlangchain_community: 0.3.0\nlangsmith: 0.1.144\nlangchain_openai: 0.3.6\nlangchain_postgres: 0.0.12\nlangchain_text_splitters: 0.3.5\nlangchainhub: 0.1.20\nlanggraph_sdk: 0.1.51\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp: 3.11.7\nasync-timeout: Installed. No version info available.\ndataclasses-json: 0.6.7\nhttpx: 0.27.2\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.35: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nnumpy: 1.26.4\nopenai<2.0.0,>=1.58.1: Installed. No version info available.\norjson: 3.10.11\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npgvector: 0.2.5\npsycopg: 3.2.3\npsycopg-pool: 3.2.4\npydantic: 2.10.0\npydantic-settings: 2.6.1\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\nPyYAML: 6.0.2\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nsqlalchemy: 2.0.36\nSQLAlchemy: 2.0.36\ntenacity: 8.5.0\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntypes-requests: 2.32.0.20241016\ntyping-extensions>=4.7: Installed. No version info available.\n", "created_at": "2025-04-03", "closed_at": "2025-04-04", "labels": [], "State": "closed", "Author": "marcammann"}
{"issue_number": 4166, "issue_title": "Setting LangGraph run_id in config does not appear to be picked up properly by LangSmith and different ID is used in the dashboard.", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langsmith import traceable\nimport uuid\nfrom langgraph import StateGraph, START, END\n\n@dataclass\nclass OverallState:\n    response: Optional[str] = None\n\ngraph_builder = StateGraph(OverallState)\ngraph_builder.add_edge(START, END)\ncompiled_graph = graph_builder.compile()\n\nresponse_message_key = \"4\"\nconversation_key = \"8\"\nmy_uuid = uuid.uuid4()\noutput_state = await sync_to_async(\n    traceable(\n        compiled_graph.invoke,\n        metadata={\n            \"user\": settings.LANGSMITH_USERNAME,\n            \"message_id\": response_message_key,\n            \"conversation_id\": conversation_key,\n        },\n   )\n)(input_state, config = {\"run_id\": my_uuid})\nError Message and Stack Trace (if applicable)\nNo error message is generated but in the LangSmith Dashboard the UUID for the invoke call is different from the uuid provided to run_id.\nDescription\nI am expecting the invoke call as traced in LangSmith to have the uuid from the run_id passed to LangGraph. It currently has a generated id instead.\nSystem Info\nI'm on Windows 11 and this was tested in a Django application using python 3.10.11", "created_at": "2025-04-03", "closed_at": "2025-04-03", "labels": [], "State": "closed", "Author": "TimCapes"}
{"issue_number": 4154, "issue_title": "DOC(langgraph_adaptive_rag.ipynb): the example code has logic false", "issue_body": "This graph of the full code is shown below, You will find that the generate node points to the transform_query node, so if the web_search tool is called and it gets irrelevance information about the question from the user, then it will lead to enter to the dead loop between the node of transform_query, retrieve and grade_documents!!!\n\nSo how to fix it? Designing another new generate node necessary for web_search? at least it's the true solution according to the architecture of the adaptive agent:\n\nThe file path : docs/docs/tutorials/rag/langgraph_adaptive_rag.ipynb", "created_at": "2025-04-03", "closed_at": null, "labels": [], "State": "open", "Author": "GoogTech"}
{"issue_number": 4140, "issue_title": "Functional API breaks mypy with Never type", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport asyncio\n\nfrom langgraph.func import entrypoint, task\n\n\n@task\nasync def my_task(number: int) -> int:\n    return number + 1\n\n\n@entrypoint()\nasync def my_workflow(number: int) -> int:\n    return await my_task(number)\n\n\nprint(asyncio.run(my_workflow.ainvoke(1)))\nError Message and Stack Trace (if applicable)\n(.venv) \u279c  langgraph_test mypy main.py                     \nmain.py:6: error: Argument 1 to \"task\" has incompatible type \"Callable[[int], Coroutine[Any, Any, int]]\"; expected \"Callable[[VarArg(Never), KwArg(Never)], Awaitable[Never]] | Callable[[VarArg(Never), KwArg(Never)], Never]\"  [arg-type]\nmain.py:13: error: Argument 1 to \"my_task\" has incompatible type \"int\"; expected \"Never\"  [arg-type]\nFound 2 errors in 1 file (checked 1 source file)\nDescription\n\nI'm trying to use Functional API in my project and it seems to break mypy.\nIt looks like the task decorator changes all the input types to Never.\n\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:22 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6041\nPython Version:  3.11.9 (v3.11.9:de54cf5be3, Apr  2 2024, 07:12:50) [Clang 13.0.0 (clang-1300.0.29.30)]\n\nPackage Information\n\nlangchain_core: 0.3.49\nlangsmith: 0.1.146\nlanggraph_sdk: 0.1.59\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nhttpx: 0.28.1\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\norjson: 3.10.16\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\n", "created_at": "2025-04-02", "closed_at": null, "labels": [], "State": "open", "Author": "RustamIbragimov"}
{"issue_number": 4138, "issue_title": "Chat Completion could not continue from the prefix if tool calling is involved", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langchain_core.tools import tool\nfrom langgraph.prebuilt import create_react_agent\n\n# First we initialize the model using init_chat_model. Replace with your LLM chat model\nllm = model.get_llm_chatmodel(\"meta-llama/llama-3-3-70b-instruct\")\n\n@tool\ndef get_example_word_for_alphabet(alphabet: str):\n    \"\"\"Use this to get an example word for any alphabet\"\"\"\n    return 'Jawahar(\u0b9c\u0bb5\u0bb9\u0bb0\u0bcd)' if alphabet in ('j', 'J', '\u0b9c') else 'I know you will not trust this'\n\n\ntools = [get_example_word_for_alphabet]\n\ngraph = create_react_agent(llm, tools=tools)\nchat_history = [\n    SystemMessage(\n        \"\"\"You are Nik, a compassionate assistant that has access to tools.\n        You should trust tool outputs and should not validate.\"\"\"\n    ),\n    HumanMessage(\n        \"What's the 10th alphabet in English? Can you show an example word that starts with it.\"),\n    # AIMessage(\"The alphabet asked for is\"),\n]\ngraph.invoke({\"messages\": chat_history})\nError Message and Stack Trace (if applicable)\nExpected Output is:\n\n{'messages': [SystemMessage(content='You are Nik, a compassionate assistant that has access to tools.\\n        You should trust tool outputs and should not validate.', additional_kwargs={}, response_metadata={}, id='246408e5-3ef9-4c37-a3b3-f40fdf604425'),\n  HumanMessage(content=\"What's the 10th alphabet in English? Can you show an example word that starts with it.\", additional_kwargs={}, response_metadata={}, id='be33b365-428b-4606-96c5-47e18fb1a60e'),\n  AIMessage(content='The alphabet asked for is', additional_kwargs={}, response_metadata={}, id='e2f57f46-f37f-4247-b238-feaa97ea42f3'),\n  AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'chatcmpl-tool-0939ada717524403929127f6ea9cc942', 'type': 'function', 'function': {'name': 'get_example_word_for_alphabet', 'arguments': '{\"alphabet\": \"J\"}'}}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'meta-llama/llama-3-3-70b-instruct'}, id='chatcmpl-4d815e8eca25940785fe6eb34fb6af35', tool_calls=[{'name': 'get_example_word_for_alphabet', 'args': {'alphabet': 'J'}, 'id': 'chatcmpl-tool-0939ada717524403929127f6ea9cc942', 'type': 'tool_call'}], usage_metadata={'input_tokens': 226, 'output_tokens': 27, 'total_tokens': 253}),\n  ToolMessage(content='Jawahar(\u0b9c\u0bb5\u0bb9\u0bb0\u0bcd)', name='get_example_word_for_alphabet', id='6144c9e5-5b55-4e93-b4a0-2fb859eb2d8b', tool_call_id='chatcmpl-tool-0939ada717524403929127f6ea9cc942'),\n  AIMessage(content=\"J. An example word that starts with 'J' is 'Jawahar(\u0b9c\u0bb5\u0bb9\u0bb0\u0bcd)'.\", additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'meta-llama/llama-3-3-70b-instruct'}, id='chatcmpl-d60f16dc067fe7ff49085d0356c83e63', usage_metadata={'input_tokens': 276, 'output_tokens': 38, 'total_tokens': 314})]}\n\nBut got\n\n{'messages': [SystemMessage(content='You are Nik, a compassionate assistant that has access to tools.\\n        You should trust tool outputs and should not validate.', additional_kwargs={}, response_metadata={}, id='44e33392-bf50-4847-91a3-2151fdd7ab76'),\n  HumanMessage(content=\"What's the 10th alphabet in English? Can you show an example word that starts with it.\", additional_kwargs={}, response_metadata={}, id='b9f572eb-196c-42da-b188-d0dd3f1f6161'),\n  AIMessage(content='The alphabet asked for is', additional_kwargs={}, response_metadata={}, id='0865198c-ba50-4916-b70b-7efa1300ab2d'),\n  AIMessage(content='', additional_kwargs={}, response_metadata={}, id='chatcmpl-6310b46639daa4e93b2db941c48fd94f', usage_metadata={'input_tokens': 236, 'output_tokens': 6, 'total_tokens': 242})]}\nDescription\nChat Completion Ignores Provided Prefix in Final AIMessage\nI'm trying to use LangGraph to create an AI agent with tools.\nRun the provided example code after uncommenting AIMessage(\"The alphabet asked for is\")\nThe response should be an AIMessage whose content string starts with the continuation of the last message of the input chat_history. For the example above, it should look something like:\n# continuation of \"The alphabet that you asked is \"\nAIMessage(content=\"'J'. An example word that starts with 'J' is 'Jawahar(\u0b9c\u0bb5\u0bb9\u0bb0\u0bcd)'\")\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:16 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6000\nPython Version:  3.12.9 (main, Feb 12 2025, 15:09:19) [Clang 19.1.6 ]\n\nPackage Information\n\nlangchain_core: 0.3.41\nlangchain: 0.3.20\nlangsmith: 0.3.10\nlangchain_chroma: 0.2.2\nlangchain_ibm: 0.3.6\nlangchain_milvus: 0.1.8\nlangchain_text_splitters: 0.3.6\nlanggraph_sdk: 0.1.53\n", "created_at": "2025-04-02", "closed_at": "2025-04-02", "labels": [], "State": "closed", "Author": "Jawahars"}
{"issue_number": 4130, "issue_title": "creating ChatGoogleGenerativeAI raises blockbuster.BlockingError", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nasync def route(state: AgentState, config: Configuration):\n    \"\"\"Decide which agent to use based on the user's prompt.\"\"\"\n    next_agent = state.get(\"next_agent\")\n    if next_agent:\n        return {\n            \"next_agent\": next_agent,\n        }\n\n    system_message = SystemMessage(content=router_prompt)\n\n    gemini_model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n    structured_model = gemini_model.with_structured_output(RouterResponse)\n    structured_model = structured_model.with_config({\"tags\": [\"langsmith:nostream\"]})\n\n    messages = [system_message] + list(state.get(\"messages\"))\n    response = await structured_model.ainvoke(messages)\n\n    return {\n        \"next_agent\": response.next_agent,\n    }\nError Message and Stack Trace (if applicable)\nFile \"/Users/tomassipko/Documents/projects/math-agent/.venv/lib/python3.11/site-packages/langgraph_api/worker.py\", line 149, in worker\n    await asyncio.wait_for(consume(stream, run_id), BG_JOB_TIMEOUT_SECS)\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py\", line 479, in wait_for\n    return fut.result()\n           ^^^^^^^^^^^^\n  File \"/Users/tomassipko/Documents/projects/math-agent/.venv/lib/python3.11/site-packages/langgraph_api/stream.py\", line 268, in consume\n    raise e from None\n  File \"/Users/tomassipko/Documents/projects/math-agent/.venv/lib/python3.11/site-packages/langgraph_api/stream.py\", line 258, in consume\n    async for mode, payload in stream:\n  File \"/Users/tomassipko/Documents/projects/math-agent/.venv/lib/python3.11/site-packages/langgraph_api/stream.py\", line 209, in astream_state\n    event = await wait_if_not_done(anext(stream, sentinel), done)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/tomassipko/Documents/projects/math-agent/.venv/lib/python3.11/site-packages/langgraph_api/asyncio.py\", line 80, in wait_if_not_done\n    raise e.exceptions[0] from None\n  File \"/Users/tomassipko/Documents/projects/math-agent/.venv/lib/python3.11/site-packages/langgraph/pregel/__init__.py\", line 2615, in astream\n    async for _ in runner.atick(\n  File \"/Users/tomassipko/Documents/projects/math-agent/./src/math_agent/graph.py\", line 93, in route\n    gemini_model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/tomassipko/Documents/projects/math-agent/.venv/lib/python3.11/site-packages/langchain_core/load/serializable.py\", line 125, in __init__\n    super().__init__(*args, **kwargs)\n  File \"/Users/tomassipko/Documents/projects/math-agent/.venv/lib/python3.11/site-packages/pydantic/main.py\", line 243, in __init__\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/tomassipko/Documents/projects/math-agent/.venv/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 935, in validate_environment\n    client_info = get_client_info(\"ChatGoogleGenerativeAI\")\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/tomassipko/Documents/projects/math-agent/.venv/lib/python3.11/site-packages/langchain_google_genai/_common.py\", line 139, in get_client_info\n    client_library_version, user_agent = get_user_agent(module)\n                                         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/tomassipko/Documents/projects/math-agent/.venv/lib/python3.11/site-packages/langchain_google_genai/_common.py\", line 121, in get_user_agent\n    langchain_version = metadata.version(\"langchain-google-genai\")\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/metadata/__init__.py\", line 1008, in version\n    return distribution(distribution_name).version\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/metadata/__init__.py\", line 981, in distribution\n    return Distribution.from_name(distribution_name)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/metadata/__init__.py\", line 563, in from_name\n    return next(cls.discover(name=name))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/metadata/__init__.py\", line 915, in <genexpr>\n    path.search(prepared) for path in map(FastPath, paths)\n    ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/metadata/__init__.py\", line 813, in search\n    return self.lookup(self.mtime).search(name)\n                       ^^^^^^^^^^\n  File \"/opt/homebrew/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/metadata/__init__.py\", line 818, in mtime\n    return os.stat(self.root).st_mtime\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/tomassipko/Documents/projects/math-agent/.venv/lib/python3.11/site-packages/blockbuster/blockbuster.py\", line 109, in wrapper\n    raise BlockingError(func_name)\nblockbuster.blockbuster.BlockingError: Blocking call to os.stat\nDuring task with name 'router' and id '19f71450-9745-c945-e312-35288765c2ac'\nDescription\nI'm using ChatGoogleGenerativeAI to route the request, however, it raises Blockbuster error: Blocking call to os.stat. It seems the error appears in the initialization of ChatGoogleGenerativeAI.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.6.0: Thu Dec 19 20:44:10 PST 2024; root:xnu-10063.141.1.703.2~1/RELEASE_ARM64_T6000\nPython Version:  3.11.5 (main, Aug 24 2023, 15:09:45) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n\nPackage Information\n\nlangchain_core: 0.3.49\nlangchain: 0.3.22\nlangchain_community: 0.3.20\nlangsmith: 0.3.21\nlangchain_anthropic: 0.3.10\nlangchain_google_genai: 2.1.2\nlangchain_openai: 0.3.11\nlangchain_text_splitters: 0.3.7\nlanggraph_api: 0.0.41\nlanggraph_cli: 0.1.82\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.60\nlanggraph_storage: Installed. No version info available.\nlanggraph_swarm: 0.0.8\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nanthropic<1,>=0.49.0: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nblockbuster: 1.5.24\nclick: 8.1.8\ncloudpickle: 3.1.1\ncryptography: 43.0.3\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nfiletype: 1.2.0\ngoogle-ai-generativelanguage: 0.6.17\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\njsonschema-rs: 0.29.1\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-azure-ai;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<0.4.0,>=0.3.40: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.45: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.49: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.7: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.21: Installed. No version info available.\nlanggraph: 0.3.22\nlanggraph-checkpoint: 2.0.23\nlanggraph<0.4.0,>=0.3.5: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<3,>=1.26.2: Installed. No version info available.\nopenai-agents: Installed. No version info available.\nopenai<2.0.0,>=1.68.2: Installed. No version info available.\nopentelemetry-api: Installed. No version info available.\nopentelemetry-exporter-otlp-proto-http: Installed. No version info available.\nopentelemetry-sdk: Installed. No version info available.\norjson: 3.10.16\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.11.1\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npyjwt: 2.10.1\npytest: Installed. No version info available.\npython-dotenv: 1.1.0\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: Installed. No version info available.\nSQLAlchemy<3,>=1.4: Installed. No version info available.\nsse-starlette: 2.1.3\nstarlette: 0.46.1\nstructlog: 25.2.0\ntenacity: 9.0.0\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nuvicorn: 0.34.0\nwatchfiles: 1.0.4\nzstandard: 0.23.0\n", "created_at": "2025-04-02", "closed_at": "2025-04-11", "labels": [], "State": "closed", "Author": "tomas-sipko"}
{"issue_number": 4121, "issue_title": "ImportError: cannot import name 'LATEST_VERSION' from 'langgraph.checkpoint.base' in LangChain Academy notebook", "issue_body": "When running the LangChain Academy module-1/router.ipynb notebook, I encountered the following import error:\nImportError: cannot import name 'LATEST_VERSION' from 'langgraph.checkpoint.base' (/Users/gafnts/Documents/Github/langchain-academy/academy/lib/python3.12/site-packages/langgraph/checkpoint/base/__init__.py)\nEnvironment Information\nPython version: 3.12.9\nLangGraph version: 0.3.22\nOS: macOS", "created_at": "2025-04-02", "closed_at": "2025-04-02", "labels": [], "State": "closed", "Author": "gafnts"}
{"issue_number": 4120, "issue_title": "astream does not return subgraph output and llm tokens like stream", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing import TypedDict\nfrom langgraph.constants import END, START\nfrom langgraph.graph import StateGraph, add_messages\nfrom langgraph.types import Send\n\n\n# Define subgraph\nclass SubgraphState(TypedDict):\n    # note that none of these keys are shared with the parent graph state\n    bar: str\n    baz: str\n\n\nasync def subgraph_node_1(state: SubgraphState):\n    return {\"baz\": \"baz\"}\n\n\nasync def subgraph_node_2(state: SubgraphState):\n    return {\"bar\": state[\"bar\"] + state[\"baz\"]}\n\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph = subgraph_builder.compile()\n\n\n# Define parent graph\nclass ParentState(TypedDict):\n    foo: str\n\n\nasync def node_1(state: ParentState):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\n\nasync def node_2(state: ParentState):\n    # transform the state to the subgraph state\n    response = await subgraph.ainvoke({\"bar\": state[\"foo\"]})\n    # transform response back to the parent state\n    return {\"foo\": response[\"bar\"]}\n\n\nbuilder = StateGraph(ParentState)\nbuilder.add_node(\"node_1\", node_1)\n# note that instead of using the compiled subgraph we are using `node_2` function that is calling the subgraph\nbuilder.add_node(\"node_2\", node_2)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\ngraph = builder.compile()\n\nasync for chunk in graph.astream({\"foo\": \"foo\"}, subgraphs=True):\n    print(chunk)\nError Message and Stack Trace (if applicable)\n\nDescription\nI'm trying to stream the subgraph output in with async api. but the output does not like sync api.\nI tried the example code in tutorial, with graph.stream(input, subgraphs=True, stream_mode=['updates']), I got the subgraph out, with graph.astream(input, subgraph=True, stream_mode=['updates']) I got only the output of parent graph.\nThis also exist for stream_mode=['messages'] to stream the llm token.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:22:00 PST 2025; root:xnu-11215.81.4~3/RELEASE_X86_64\nPython Version:  3.10.16 (main, Mar 18 2025, 09:30:41) [Clang 16.0.0 (clang-1600.0.26.6)]\n\nPackage Information\n\nlangchain_core: 0.3.49\nlangchain: 0.3.21\nlangchain_community: 0.3.20\nlangsmith: 0.3.19\nlangchain_huggingface: 0.1.2\nlangchain_milvus: 0.1.8\nlangchain_openai: 0.3.11\nlangchain_text_splitters: 0.3.7\nlanggraph_sdk: 0.1.60\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\nhuggingface-hub: 0.29.3\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-azure-ai;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.45: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.49: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.7: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.21: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<3,>=1.26.2: Installed. No version info available.\nopenai-agents: Installed. No version info available.\nopenai<2.0.0,>=1.68.2: Installed. No version info available.\nopentelemetry-api: 1.31.0\nopentelemetry-exporter-otlp-proto-http: Installed. No version info available.\nopentelemetry-sdk: 1.31.0\norjson: 3.10.15\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.3\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npymilvus: 2.5.3\npytest: 8.3.5\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: 13.9.4\nsentence-transformers: 3.3.1\nSQLAlchemy<3,>=1.4: Installed. No version info available.\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntokenizers: 0.21.1\ntransformers: 4.49.0\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-04-02", "closed_at": "2025-04-02", "labels": [], "State": "closed", "Author": "XiaoLiuAI"}
{"issue_number": 4119, "issue_title": "The LangGraph streaming example fails with 'Error in ConsoleCallbackHandler.on_llm_new_token'", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nJust like the example in https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/\nimport asyncio\nimport os\nimport sys\nfrom typing import TypedDict\nfrom langgraph.graph import START, StateGraph\nimport langchain\nlangchain.debug = True\n\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))\nfrom app.services.llm_service import llm_service\njoke_model = llm_service.get_llm(\"deepseek-r1-70b\")\npoem_model = llm_service.get_llm(\"deepseek-r1-70b\")\n\nclass State(TypedDict):\n    topic: str\n    joke: str\n    poem: str\n\nasync def call_model(state, config):\n    topic = state[\"topic\"]\n    print(\"Writing joke...\")\n    # Note: Passing the config through explicitly is required for python < 3.11\n    # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks\n    joke_response = await joke_model.ainvoke(\n        [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\n        config,\n    )\n    print(\"\\n\\nWriting poem...\")\n    poem_response = await poem_model.ainvoke(\n        [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}],\n        config,\n    )\n    return {\"joke\": joke_response.content, \"poem\": poem_response.content}\n\nasync def run_example():\n    graph = StateGraph(State).add_node(call_model).add_edge(START, \"call_model\").compile()\n    print(\"Starting stream example...\")\n    async for msg, metadata in graph.astream(\n        {\"topic\": \"cats\"},\n        stream_mode=\"messages\",\n    ):\n        if msg.content:\n            print(msg.content, end=\"|\", flush=True)\n\nif __name__ == \"__main__\":\n    asyncio.run(run_example())\n\n\nThe astream_events function also fails with an AssertionError\n \nasync for msg in graph.astream_events(\n        {\"topic\": \"cats\"},\n        stream_mode=\"messages\",\n    ):\n#        if msg.content:\n            print(msg, end=\"|\", flush=True)\n\nError in _AstreamEventsCallbackHandler.on_llm_new_token callback: AssertionError('Run ID 13a8c64a-3d02-497f-ba77-2ad7317828cd not found in run map.')\nError Message and Stack Trace (if applicable)\nWriting joke...\nError in ConsoleCallbackHandler.on_llm_new_token callback: TracerException('No indexed run ID bf46f8d0-aae4-4d49-95c8-bedd79dbc85a.')\nError in ConsoleCallbackHandler.on_llm_new_token callback: TracerException('No indexed run ID bf46f8d0-aae4-4d49-95c8-bedd79dbc85a.')\nError in ConsoleCallbackHandler.on_llm_new_token callback: TracerException('No indexed run ID bf46f8d0-aae4-4d49-95c8-bedd79dbc85a.')\nError in ConsoleCallbackHandler.on_llm_new_token callback: TracerException('No indexed run ID bf46f8d0-aae4-4d49-95c8-bedd79dbc85a.')\nError in ConsoleCallbackHandler.on_llm_new_token callback: TracerException('No indexed run ID bf46f8d0-aae4-4d49-95c8-bedd79dbc85a.')\nError in ConsoleCallbackHandler.on_llm_new_token callback: TracerException('No indexed run ID bf46f8d0-aae4-4d49-95c8-bedd79dbc85a.')\nError in ConsoleCallbackHandler.on_llm_new_token callback: TracerException('No indexed run ID bf46f8d0-aae4-4d49-95c8-bedd79dbc85a.')\nDescription\nI want to see the LLM node of the graph invoking streaming progress in real-time to improve the UX in the complex workflow.\nSystem Info\npython -m langchain_core.sys_info\nSystem Information\nOS: Darwin\nOS Version: Darwin Kernel Version 24.3.0: Thu Jan 2 20:24:06 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T8103\nPython Version: 3.12.5 (main, Aug 6 2024, 19:08:49) [Clang 15.0.0 (clang-1500.0.40.1)]\nPackage Information\nlangchain_core: 0.3.44\nlangchain: 0.3.19\nlangchain_community: 0.3.18\nlangsmith: 0.3.13\nlangchain_experimental: 0.3.4\nlangchain_ollama: 0.2.3\nlangchain_text_splitters: 0.3.6\nlanggraph_sdk: 0.1.57", "created_at": "2025-04-02", "closed_at": null, "labels": [], "State": "open", "Author": "qmz"}
{"issue_number": 4112, "issue_title": "langgraph_api's validation.py cannot find \"openapi.json\"", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nlanggraph dev\nError Message and Stack Trace (if applicable)\nFile \"/Users/tomassipko/Documents/projects/math-agent/.venv/lib/python3.11/site-packages/langgraph_api/server.py\", line 16, in <module>\n    from langgraph_api.api.openapi import set_custom_spec\n  File \"/Users/tomassipko/Documents/projects/math-agent/.venv/lib/python3.11/site-packages/langgraph_api/api/__init__.py\", line 12, in <module>\n    from langgraph_api.api.assistants import assistants_routes\n  File \"/Users/tomassipko/Documents/projects/math-agent/.venv/lib/python3.11/site-packages/langgraph_api/api/assistants.py\", line 17, in <module>\n    from langgraph_api.validation import (\n  File \"/Users/tomassipko/Documents/projects/math-agent/.venv/lib/python3.11/site-packages/langgraph_api/validation.py\", line 6, in <module>\n    with open(pathlib.Path(__file__).parent.parent / \"openapi.json\") as f:\nDescription\nI have a Langgraph app which was working fine. However, now, when I try to start it locally, It errors, because it cannot find the \"openapi.json\". The error came out of nowhere as I did not install/update any additional packages.\nWhat I've tried so far:\n\nremove UV cache\nreinstall virtual environment\nupdate \"langgraph-cli[inmem]\"\n\nThe app starts, errors and I cannot connect to it via frontend client application.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.6.0: Thu Dec 19 20:44:10 PST 2024; root:xnu-10063.141.1.703.2~1/RELEASE_ARM64_T6000\nPython Version:  3.11.5 (main, Aug 24 2023, 15:09:45) [Clang 14.0.3 (clang-1403.0.22.14.1)]\n\nPackage Information\n\nlangchain_core: 0.3.49\nlangchain: 0.3.22\nlangchain_community: 0.3.20\nlangsmith: 0.3.21\nlangchain_anthropic: 0.3.10\nlangchain_google_genai: 2.1.2\nlangchain_openai: 0.3.11\nlangchain_text_splitters: 0.3.7\nlanggraph_api: 0.0.39\nlanggraph_cli: 0.1.82\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.60\nlanggraph_storage: Installed. No version info available.\nlanggraph_swarm: 0.0.8\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nanthropic<1,>=0.49.0: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nblockbuster: 1.5.24\nclick: 8.1.8\ncloudpickle: 3.1.1\ncryptography: 43.0.3\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nfiletype: 1.2.0\ngoogle-ai-generativelanguage: 0.6.17\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\njsonschema-rs: 0.29.1\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-azure-ai;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<0.4.0,>=0.3.40: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.45: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.49: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.7: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.21: Installed. No version info available.\nlanggraph: 0.3.22\nlanggraph-checkpoint: 2.0.23\nlanggraph<0.4.0,>=0.3.5: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<3,>=1.26.2: Installed. No version info available.\nopenai-agents: Installed. No version info available.\nopenai<2.0.0,>=1.68.2: Installed. No version info available.\nopentelemetry-api: Installed. No version info available.\nopentelemetry-exporter-otlp-proto-http: Installed. No version info available.\nopentelemetry-sdk: Installed. No version info available.\norjson: 3.10.16\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.11.1\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npyjwt: 2.10.1\npytest: Installed. No version info available.\npython-dotenv: 1.1.0\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: Installed. No version info available.\nSQLAlchemy<3,>=1.4: Installed. No version info available.\nsse-starlette: 2.1.3\nstarlette: 0.46.1\nstructlog: 25.2.0\ntenacity: 9.0.0\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nuvicorn: 0.34.0\nwatchfiles: 1.0.4\nzstandard: 0.23.0\n", "created_at": "2025-04-01", "closed_at": "2025-04-01", "labels": [], "State": "closed", "Author": "tomas-sipko"}
{"issue_number": 4111, "issue_title": "Tools returning a `Command` are missing from messages-streaming", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing import Annotated, Any\n\nfrom langchain_core.messages import ToolMessage\nfrom langchain_core.runnables.config import RunnableConfig\nfrom langchain_core.tools import tool\nfrom langchain_core.tools.base import InjectedToolCallId\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\nfrom langgraph.types import Command\n\nUSER_INFO = [\n    {\"user_id\": \"1\", \"name\": \"Bob Dylan\", \"location\": \"New York, NY\"},\n    {\"user_id\": \"2\", \"name\": \"Taylor Swift\", \"location\": \"Beverly Hills, CA\"},\n]\n\nUSER_ID_TO_USER_INFO = {info[\"user_id\"]: info for info in USER_INFO}\n\n\nclass State(AgentState):\n    # updated by the tool\n    user_info: dict[str, Any]\n\n\ndef main() -> None:\n    @tool\n    def lookup_user_info(tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig):\n        \"\"\"Use this to look up user information to better assist them with their questions.\"\"\"\n        user_id = config.get(\"configurable\", {}).get(\"user_id\")\n        if user_id is None:\n            raise ValueError(\"Please provide user ID\")\n\n        if user_id not in USER_ID_TO_USER_INFO:\n            raise ValueError(f\"User '{user_id}' not found\")\n\n        user_info = USER_ID_TO_USER_INFO[user_id]\n        return Command(\n            update={\n                # update the state keys\n                \"user_info\": user_info,\n                # update the message history\n                \"messages\": [\n                    ToolMessage(\n                        \"Successfully looked up user information\", tool_call_id=tool_call_id\n                    )\n                ],\n            }\n        )\n\n    def prompt(state: State):\n        user_info = state.get(\"user_info\")\n        if user_info is None:\n            return state[\"messages\"]\n\n        system_msg = (\n            f\"User name is {user_info['name']}. User lives in {user_info['location']}\"\n        )\n        return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n\n    model = ChatOpenAI(model=\"gpt-4o\")\n\n    agent = create_react_agent(\n        model,\n        # pass the tool that can update state\n        [lookup_user_info],\n        state_schema=State,\n        # pass dynamic prompt function\n        prompt=prompt,\n    )\n\n    agent_input = {\"messages\": [(\"user\", \"hi, where do I live?\")]}\n    agent_config = {\"configurable\": {\"user_id\": \"1\"}}\n\n    invoke_result = agent.invoke(\n        agent_input,\n        agent_config,\n    )\n\n    # print(invoke_result)\n\n    for chunk in agent.stream(agent_input, agent_config, stream_mode='messages'):\n        print(chunk)\n\n\nif __name__ == '__main__':\n    main()\nError Message and Stack Trace (if applicable)\n\nDescription\nAs the title says, if you define a tool returning a Command to update the state, there is no ToolMessage for the tool call when using the messages streaming mode.\nThis is easily reproducible using the example in the How to update graph state from tools doc page.\nSystem Info\nSystem Information\n\nOS:  Windows\nOS Version:  10.0.19045\nPython Version:  3.13.0 (main, Oct 16 2024, 00:33:24) [MSC v.1929 64 bit (AMD64)]\n\nPackage Information\n\nlangchain_core: 0.3.49\nlangsmith: 0.3.21\nlangchain_openai: 0.3.11\nlanggraph_sdk: 0.1.60\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nhttpx: 0.28.1\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.49: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nopenai-agents: Installed. No version info available.\nopenai<2.0.0,>=1.68.2: Installed. No version info available.\nopentelemetry-api: Installed. No version info available.\nopentelemetry-exporter-otlp-proto-http: Installed. No version info available.\nopentelemetry-sdk: Installed. No version info available.\norjson: 3.10.16\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.11.1\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: Installed. No version info available.\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrich: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-04-01", "closed_at": "2025-04-14", "labels": ["investigate"], "State": "closed", "Author": "Martin19037"}
{"issue_number": 4108, "issue_title": "`ValueError: Channel names configurable are reserved`", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nI suddenly hit this error: ValueError: Channel names configurable are reserved on StateGraph.compile(). I only have the following references to configurable in the following:\nExample Code\n@dataclass\nclass EmailRAGState(TypedDict):\n    message: str\n    extract: EmailModel | None\n\nfrom pydantic import BaseModel, Field, computed_field\n\nclass EmailModel(BaseModel):\n    date: str | None = Field(\n        default=None,\n        exclude=True,\n        repr=False,\n        description=\"\"\"The date of the notice (if any) reformatted\n        to match YYYY-mm-dd\"\"\",\n    )\n    entity_name: str | None = Field(\n        default=None,\n        description=\"\"\"The name of the entity sending the notice (if present\n        in the message)\"\"\",\n    )\n    entity_phone: str | None = Field(\n        default=None,\n        description=\"\"\"The phone number of the entity sending the notice\n        (if present in the message)\"\"\",\n    )\n    entity_email: str | None = Field(\n        default=None,\n        description=\"\"\"The email of the entity sending the notice\n        (if present in the message)\"\"\",\n    )\n\n\ninit_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\", configurable_fields=(\"user_id\", \"graph\", \"email_state\"), streaming=True)\n\n    async def ParseEmail(self, config: RunnableConfig, state: EmailRAGState) -> EmailRAGState:\n        \"\"\"\n        Use the EmailModel LCEL to extract fields from email\n        \"\"\"\n        logging.info(f\"\\n=== {self.ParseEmail.__name__} ===\")\n        state[\"extract\"] = await self._email_parser_chain.with_config(config).ainvoke({\"message\": state[\"message\"]}) if state[\"message\"] else None\n        return state\n\n    async def NeedsEscalation(self, config: RunnableConfig, state: EmailRAGState) -> EmailRAGState:\n        \"\"\"\n        Determine if an email needs escalation\n        \"\"\"\n        logging.info(f\"\\n=== {self.NeedsEscalation.__name__} ===\")\n        result: EscalationCheckModel = await self._escalation_chain.with_config(config).ainvoke({\"message\": state[\"message\"], \"escalation_criteria\": state[\"escalation_text_criteria\"]}) if state and state[\"message\"] else None\n        state[\"escalate\"] = (result.needs_escalation or state[\"extract\"].max_potential_fine >= state[\"escalation_dollar_criteria\"])\n        return state\n\n    graph_builder = StateGraph(EmailRAGState)\n    graph_builder.add_node(\"ParseEmail\", self.ParseEmail)\n    graph_builder.add_node(\"NeedsEscalation\", self.NeedsEscalation)\n    graph_builder.add_edge(START, \"ParseEmail\")\n    graph_builder.add_edge(\"ParseEmail\", \"NeedsEscalation\")\n    graph_builder.add_edge(\"NeedsEscalation\", END)\n    self._graph = graph_builder.compile(store=InMemoryStore(), checkpointer=MemorySaver(), name=self._name)\ncreate_react_agent(self._llm, [email_processing_tool], store=InMemoryStore(), checkpointer=MemorySaver(), config_schema=EmailConfiguration, state_schema=EmailAgentState, name=self._name, prompt=self._prompt)\nself._agent = create_react_agent(self._llm, [email_processing_tool], store=InMemoryStore(), checkpointer=MemorySaver(), config_schema=EmailConfiguration, state_schema=EmailAgentState, name=self._name, prompt=self._prompt)\n\n\n    email_state = {\n            \"escalation_dollar_criteria\": 100_000,\n            \"escalation_emails\": [\"me@abc.com\", \"me1@def.com\"],\n    }\n    async for step in self._agent.with_config({\"graph\": self._graph, \"email_state\": email_state}).astream(\n            {\"messages\": [{\"role\": \"user\", \"content\": message_with_criteria}]},\n            #{\"configurable\": {\"graph\": self._graph, \"email_state\": email_state}},\n            stream_mode=\"values\",\n            #config = config\n    ):\n        step[\"messages\"][-1].pretty_print()\nError Message and Stack Trace (if applicable)\nself._graph = graph_builder.compile(store=InMemoryStore(), checkpointer=MemorySaver(), name=self._name)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/graph/state.py\", line 676, in compile\n    return compiled.validate()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 577, in validate\n    validate_graph(\n  File \"/home/khteh/.local/share/virtualenvs/rag-agent-YeW3dxEa/lib/python3.12/site-packages/langgraph/pregel/validate.py\", line 20, in validate_graph\n    raise ValueError(f\"Channel names {chan} are reserved\")\nValueError: Channel names configurable are reserved\nDescription\nI don't find anywhere in my code which uses this reserved configurable keyword.\nSystem Info\nSystem Information\n------------------\n> OS:  Linux\n> OS Version:  #21-Ubuntu SMP PREEMPT_DYNAMIC Wed Feb 19 16:50:40 UTC 2025\n> Python Version:  3.12.7 (main, Feb  4 2025, 14:46:03) [GCC 14.2.0]\n\nPackage Information\n-------------------\n> langchain_core: 0.3.45\n> langchain: 0.3.20\n> langchain_community: 0.3.19\n> langsmith: 0.3.15\n> langchain_google_genai: 2.1.0\n> langchain_google_vertexai: 2.0.9\n> langchain_openai: 0.3.8\n> langchain_text_splitters: 0.3.6\n> langgraph_api: 0.0.28\n> langgraph_cli: 0.1.75\n> langgraph_license: Installed. No version info available.\n> langgraph_sdk: 0.1.57\n> langgraph_storage: Installed. No version info available.\n\nOptional packages not installed\n-------------------------------\n> langserve\n\nOther Dependencies\n------------------\n> aiohttp<4.0.0,>=3.8.3: Installed. No version info available.\n> anthropic[vertexai]: Installed. No version info available.\n> async-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\n> click: 8.1.8\n> cryptography: 43.0.3\n> dataclasses-json<0.7,>=0.5.7: Installed. No version info available.\n> filetype: 1.2.0\n> google-ai-generativelanguage: 0.6.16\n> google-cloud-aiplatform: 1.84.0\n> google-cloud-storage: 2.19.0\n> httpx: 0.27.2\n> httpx-sse: 0.4.0\n> httpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\n> jsonpatch<2.0,>=1.33: Installed. No version info available.\n> jsonschema-rs: 0.20.0\n> langchain-anthropic;: Installed. No version info available.\n> langchain-aws;: Installed. No version info available.\n> langchain-cohere;: Installed. No version info available.\n> langchain-community;: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.34: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.41: Installed. No version info available.\n> langchain-core<1.0.0,>=0.3.42: Installed. No version info available.\n> langchain-deepseek;: Installed. No version info available.\n> langchain-fireworks;: Installed. No version info available.\n> langchain-google-genai;: Installed. No version info available.\n> langchain-google-vertexai;: Installed. No version info available.\n> langchain-groq;: Installed. No version info available.\n> langchain-huggingface;: Installed. No version info available.\n> langchain-mistralai: Installed. No version info available.\n> langchain-mistralai;: Installed. No version info available.\n> langchain-ollama;: Installed. No version info available.\n> langchain-openai;: Installed. No version info available.\n> langchain-text-splitters<1.0.0,>=0.3.6: Installed. No version info available.\n> langchain-together;: Installed. No version info available.\n> langchain-xai;: Installed. No version info available.\n> langchain<1.0.0,>=0.3.20: Installed. No version info available.\n> langgraph: 0.3.11\n> langgraph-checkpoint: 2.0.20\n> langsmith-pyo3: Installed. No version info available.\n> langsmith<0.4,>=0.1.125: Installed. No version info available.\n> langsmith<0.4,>=0.1.17: Installed. No version info available.\n> numpy<3,>=1.26.2: Installed. No version info available.\n> openai-agents: Installed. No version info available.\n> openai<2.0.0,>=1.58.1: Installed. No version info available.\n> orjson: 3.10.15\n> packaging: 24.2\n> packaging<25,>=23.2: Installed. No version info available.\n> pydantic: 2.9.2\n> pydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\n> pydantic<3.0.0,>=2.5.2;: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4: Installed. No version info available.\n> pydantic<3.0.0,>=2.7.4;: Installed. No version info available.\n> pyjwt: 2.10.1\n> pytest: 8.3.2\n> python-dotenv: 1.0.1\n> PyYAML>=5.3: Installed. No version info available.\n> requests: 2.32.3\n> requests-toolbelt: 1.0.0\n> requests<3,>=2: Installed. No version info available.\n> rich: 13.9.4\n> SQLAlchemy<3,>=1.4: Installed. No version info available.\n> sse-starlette: 2.1.3\n> starlette: 0.46.1\n> structlog: 25.1.0\n> tenacity: 9.0.0\n> tenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\n> tenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\n> tiktoken<1,>=0.7: Installed. No version info available.\n> typing-extensions>=4.7: Installed. No version info available.\n> uvicorn: 0.34.0\n> watchfiles: 1.0.4\n> zstandard: 0.23.0\n", "created_at": "2025-04-01", "closed_at": "2025-04-01", "labels": [], "State": "closed", "Author": "khteh"}
{"issue_number": 4084, "issue_title": "`@task`s always return `None` when running via `astream_events`", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom langgraph.func import entrypoint, task\n\n\n@task\nasync def async_task():\n    return \"Task 2\"\n\n\n@entrypoint()\nasync def run_async(inputs):\n    _ = inputs\n    task_value = await async_task()\n    print(f\"Got: ({task_value})\")\n    return task_value\n\n\nasync def main():\n    print(\"\\n\\nainvoke works\")\n    await run_async.ainvoke(input={})\n\n    print(\"\\n\\nastream works\")\n    async for event in run_async.astream(input={}):\n        _ = event\n\n    print(\"\\n\\nastream_events does NOT work\")\n    async for event in run_async.astream_events(input={}):\n        _ = event\n\n\nif __name__ == \"__main__\":\n    import asyncio\n\n    asyncio.run(main())\nError Message and Stack Trace (if applicable)\nainvoke works\nGot: (Task 2)\n\n\nastream works\nGot: (Task 2)\n\n\nastream_events does NOT work\nGot: (None)\nDescription\nAll three methods of running the entrypoint should behave the same... Instead, when running with astream_events, the value returned from any task is None.\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #21~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Feb 24 16:52:15 UTC 2\nPython Version:  3.13.2 (main, Feb 12 2025, 14:51:17) [Clang 19.1.6 ]\n\nPackage Information\n\nlangchain_core: 0.3.49\nlangsmith: 0.3.19\nlanggraph_sdk: 0.1.60\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nhttpx: 0.28.1\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nopenai-agents: Installed. No version info available.\nopentelemetry-api: Installed. No version info available.\nopentelemetry-exporter-otlp-proto-http: Installed. No version info available.\nopentelemetry-sdk: Installed. No version info available.\norjson: 3.10.16\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.11.1\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npytest: Installed. No version info available.\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrich: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nzstandard: 0.23.0\n", "created_at": "2025-03-30", "closed_at": null, "labels": ["investigate"], "State": "open", "Author": "TimChild"}
{"issue_number": 4083, "issue_title": "DOC:  The link the docs leads to iteself", "issue_body": "Issue with current documentation:\nThe link in the top level ReadMe.md on line 25 leads to https://langchain-ai.github.io/langgraphjs and this is fine from the github repo, but on https://langchain-ai.github.io/langgraphjs just leads to itself.\nIdea or request for content:\nMaybe line 25 could be a little more clear like: \"To learn more about how to use LangGraph, check out the github repo or the docs. We show a simple example below of how to create a ReAct agent.\"  Happy to open a PR.", "created_at": "2025-03-30", "closed_at": "2025-03-31", "labels": [], "State": "closed", "Author": "antonioortegajr"}
{"issue_number": 4075, "issue_title": "ToolMessage fails to rehydrate using ToolNode and PostgresSaver", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\ndef get_conversation_state(self, conversation_id: str) -> Optional[AgentState]:\n    \"\"\"\n    Get the most recent state of a conversation from persistence.\n    \n    Args:\n        conversation_id: The conversation ID to retrieve\n        \n    Returns:\n        AgentState if found, None otherwise\n    \"\"\"\n    try:\n        # Create config with the conversation ID\n        config = {\"configurable\": {\"thread_id\": conversation_id}}\n        \n        # Retrieve state directly from graph\n        state = self.graph.get_state(config)\n        \n        # Convert to AgentState\n        agent_state = AgentState.model_validate(state.values)\n\n        return agent_state\n    except Exception as e:\n        current_app.logger.error(f\"Error retrieving state for {conversation_id}: {str(e)}\")\n\n#Creation of tool node\ntool_node = ToolNode(\n                self.tools,\n                handle_tool_errors=True,  # Catch all errors and format them nicely\n                messages_key=\"messages\"   # Use standard messages key\n            )\ngraph.add_node(\"tools\", tool_node)\n\n#Tool function def\n@tool\ndef search_docs(\n    search_context: str = None,\n    state: Annotated[Any, InjectedState] = None\n) -> Dict:\n\n# How the states are being streamed\nfor state_update in self.graph.stream(state_dict, config, stream_mode=\"values\"):\n   agent_state = AgentState.model_validate(state_update)\n   yield agent_state\nError Message and Stack Trace (if applicable)\n# Error Details\nSeverity: ERROR\nTimestamp: 2025-03-28 12:11:26,844\nLogger: main\nModule: operator\nFunction: stream\nLine: 559\n\n# Error Message\nError during graph streaming\n\n# Traceback (most recent call last)\nFile \"[redacted]/src/operations/operator.py\", line 534, in stream\n    for state_update in self.graph.stream(state_dict, config, stream_mode=\"values\"):\n\nFile \"[redacted]/.venv/lib/python3.13/site-packages/langgraph/pregel/__init__.py\", line 2330, in stream\n    while loop.tick(input_keys=self.input_channels):\n\nFile \"[redacted]/.venv/lib/python3.13/site-packages/langgraph/pregel/loop.py\", line 435, in tick\n    mv_writes, updated_channels = apply_writes(\n                                  self.checkpoint,\n                                  ...<2 lines>...\n                                  self.checkpointer_get_next_version,\n                                 )\n\nFile \"[redacted]/.venv/lib/python3.13/site-packages/langgraph/pregel/algo.py\", line 317, in apply_writes\n    if channels[chan].update(vals) and get_next_version is not None:\n\nFile \"[redacted]/.venv/lib/python3.13/site-packages/langgraph/channels/binop.py\", line 89, in update\n    self.value = self.operator(self.value, value)\n\nFile \"[redacted]/.venv/lib/python3.13/site-packages/langgraph/graph/message.py\", line 36, in *add*messages\n    return func(left, right, **kwargs)\n\nFile \"[redacted]/.venv/lib/python3.13/site-packages/langgraph/graph/message.py\", line 173, in add_messages\n    for m in convert_to_messages(right)\n\nFile \"[redacted]/.venv/lib/python3.13/site-packages/langchain_core/messages/utils.py\", line 365, in convert_to_messages\n    return [_convert_to_message(m) for m in messages]\n\nFile \"[redacted]/.venv/lib/python3.13/site-packages/langchain_core/messages/utils.py\", line 338, in *****convert*****to_message\n    *****message = *****create_message_from_message_type(msg_type, msg_content, **msg_kwargs)\n\nFile \"[redacted]/.venv/lib/python3.13/site-packages/langchain_core/messages/utils.py\", line 281, in *create*message_from_message_type\n    message = ToolMessage(content=content, artifact=artifact, **kwargs)\n\nFile \"[redacted]/.venv/lib/python3.13/site-packages/langchain_core/messages/tool.py\", line 140, in init\n    super().__init__(content=content, **kwargs)\n\nFile \"[redacted]/.venv/lib/python3.13/site-packages/langchain_core/messages/base.py\", line 77, in **init**\n    super().__init__(content=content, **kwargs)\n\nFile \"[redacted]/.venv/lib/python3.13/site-packages/langchain_core/load/serializable.py\", line 125, in **init**\n    super().__init__(*args, **kwargs)\n\nFile \"[redacted].venv/lib/python3.13/site-packages/pydantic/main.py\", line 214, in **init**\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n\nFile \"[redacted].venv/lib/python3.13/site-packages/langchain_core/messages/tool.py\", line 132, in coerce_args\n    tool_call_id = values[\"tool_call_id\"]\n\n# Root Cause\nKeyError: 'tool_call_id'\nDescription\nThis was a tricky one to track down! Not really doing anything special here, using all of the standard LangGraph libraries, with the exception being my own class implementation of the LLM APIs.\nI'm registering tools by gathering a list of tools which have the tool decorator, then creating a ToolNode with that list of ToolCall instances.\nThis works all well and dandy, until the conversation history is stored in persistent storage (in this case I'm using the PostgresSaver). When I invoke the graph once again using a prior conversation id and it rehydrates the state from persistent storage, if there was a prior ToolMessage in the state it fails to instantiate, saying the ToolMessage is missing a tool_call_id.\nThe weird part of this is that when I look at the rehydrated messages the ToolMessage has the tool_call_id populated properly.\nToolMessage(content='{\"user_id\": \"[id]\", \"profile_summary\": \"[content]', name='get_user_profile', id='db1cdb71-b178-4fd7-9ad0-64c820fb2699', tool_call_id='call_aAEfgLIO5pD0pIOQG3GX2VvG')\nThe problem seems to be that the tool_call_id is not being properly preserved when messages are being merged in the add_messages function from langgraph.graph.message:\n\nThe ToolNode executes a tool and creates a ToolMessage\nThe message is added to the state's messages list\nWhen the state is updated, the add_messages function from langgraph.graph.message tries to merge the messages\nDuring this merge, the tool_call_id is being lost\n\nThis is specifically where the exception is thrown in tool.py (line 132):\ntool_call_id = values[\"tool_call_id\"]\nif isinstance(tool_call_id, (UUID, int, float)):\n    values[\"tool_call_id\"] = str(tool_call_id)\nreturn values\n\nOutside of this it's very hard to determine where the tool_call_id might be getting dropped.\nLet me know if you need any more detail!\nSystem Info\n[devbox]\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:22:58 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T8132\nPython Version:  3.13.2 (main, Feb  4 2025, 14:51:09) [Clang 16.0.0 (clang-1600.0.26.6)]\n\nPackage Information\n\nlangchain_core: 0.3.49\nlangchain: 0.3.21\nlangchain_community: 0.3.20\nlangsmith: 0.1.147\nlangchain_cli: 0.0.36\nlangchain_text_splitters: 0.3.7\nlanggraph_sdk: 0.1.59\nlangserve: 0.3.1\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nfastapi: 0.115.12\ngitpython<4,>=3: Installed. No version info available.\ngritql<1.0.0,>=0.2.0: Installed. No version info available.\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-azure-ai;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.45: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.7: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.21: Installed. No version info available.\nlangserve[all]>=0.0.51: Installed. No version info available.\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<3,>=1.26.2: Installed. No version info available.\norjson: 3.10.16\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nSQLAlchemy<3,>=1.4: Installed. No version info available.\nsse-starlette: 2.2.1\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntomlkit>=0.12: Installed. No version info available.\ntyper[all]<1.0.0,>=0.9.0: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nuvicorn<1.0,>=0.23: Installed. No version info available.\n", "created_at": "2025-03-28", "closed_at": null, "labels": ["investigate"], "State": "open", "Author": "brandohelios"}
{"issue_number": 4074, "issue_title": "`field_validator` in state schema can raise error but won't apply modification on values.", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n# Case 1: `ValidationError` is raised as expected\nfrom pydantic import BaseModel, field_validator, model_validator\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(BaseModel):\n    name: str\n    text: str = \"\"\n\n    @field_validator(\"name\", mode=\"after\")\n    @classmethod\n    def validate_name(cls, value):\n        if value[0].islower():\n            raise ValueError(\"Name must start with a capital letter\")\n        return value\n\ndef process_node(state: State):\n\n    return {\"text\": \"Hello, \" + state.name + \"!\"}\n\n\nbuilder = StateGraph(state_schema=State)\nbuilder.add_node(\"process\", process_node)\nbuilder.add_edge(START, \"process\")\nbuilder.add_edge(\"process\", END)\ng = builder.compile()\n\ninput_state = {'name': 'john'}\n\ng.invoke(input_state)\n# ValidationError\n\n# ---\n\n# Case 2: Modification is not applied\nfrom pydantic import BaseModel, field_validator, model_validator\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(BaseModel):\n    name: str\n    text: str = \"\"\n\n    @field_validator(\"name\", mode=\"after\")\n    @classmethod\n    def validate_name(cls, value):\n        if value[0].islower():\n            return value.capitalize()\n        return value\n\ndef process_node(state: State):\n\n    return {\"text\": \"Hello, \" + state.name + \"!\"}\n\n\nbuilder = StateGraph(state_schema=State)\nbuilder.add_node(\"process\", process_node)\nbuilder.add_edge(START, \"process\")\nbuilder.add_edge(\"process\", END)\ng = builder.compile()\n\ninput_state = {'name': 'john'}\n\ng.invoke(input_state)  # modification is not applied\n# {'name': 'john', 'text': 'Hello, john!'}\n\nState.model_validate(input_state)\n# State(name='John', text='')\nError Message and Stack Trace (if applicable)\n\nDescription\nWhen using pydantic.BaseModel as state schema. State validation in node is different with regular pydantic.BaseModel.model_validate(). Field validator can raise error but cannot modify the validated value.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.1.0: Mon Oct  9 21:33:00 PDT 2023; root:xnu-10002.41.9~7/RELEASE_ARM64_T6031\nPython Version:  3.12.8 (main, Dec 19 2024, 14:22:58) [Clang 18.1.8 ]\n\nPackage Information\n\nlangchain_core: 0.3.48\nlangchain: 0.3.21\nlangchain_community: 0.3.20\nlangsmith: 0.3.19\nlangchain_anthropic: 0.3.10\nlangchain_google_vertexai: 2.0.17\nlangchain_neo4j: 0.4.0\nlangchain_openai: 0.3.10\nlangchain_text_splitters: 0.3.7\nlanggraph_api: 0.0.34\nlanggraph_cli: 0.1.80\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.59\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nanthropic<1,>=0.49.0: Installed. No version info available.\nanthropic[vertexai]: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nclick: 8.1.8\ncryptography: 43.0.3\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\ngoogle-cloud-aiplatform: 1.85.0\ngoogle-cloud-storage: 2.19.0\nhttpx: 0.28.1\nhttpx-sse: 0.4.0\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\njsonschema-rs: 0.29.1\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-azure-ai;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.45: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.48: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.7: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.21: Installed. No version info available.\nlanggraph: 0.3.20\nlanggraph-checkpoint: 2.0.23\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nneo4j: 5.28.1\nneo4j-graphrag: 1.6.1\nnumpy<3,>=1.26.2: Installed. No version info available.\nopenai-agents: Installed. No version info available.\nopenai<2.0.0,>=1.68.2: Installed. No version info available.\nopentelemetry-api: 1.31.1\nopentelemetry-exporter-otlp-proto-http: 1.31.1\nopentelemetry-sdk: 1.31.1\norjson: 3.10.16\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npyjwt: 2.10.1\npytest: 8.3.5\npython-dotenv: 1.1.0\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: 13.9.4\nSQLAlchemy<3,>=1.4: Installed. No version info available.\nsse-starlette: 2.1.3\nstarlette: 0.46.1\nstructlog: 25.2.0\ntenacity: 9.0.0\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nuvicorn: 0.34.0\nvalidators: 0.34.0\nwatchfiles: 1.0.4\nzstandard: 0.23.0\n", "created_at": "2025-03-28", "closed_at": "2025-04-21", "labels": [], "State": "closed", "Author": "hon-gyu"}
{"issue_number": 4073, "issue_title": "DOC: LangGraph.prebuilt functions are not working as given in the documentation", "issue_body": "Issue with current documentation:\nHi, I am using the latest version of LangGraph 0.3.21 and I see the prebuilt methods are not working as expected in the documentation. For example: ToolNode, tool_conditions are not working and getting import error stating the unknown location.\nIdea or request for content:\nNo response", "created_at": "2025-03-28", "closed_at": "2025-03-28", "labels": [], "State": "closed", "Author": "Vishnumundlapudii"}
{"issue_number": 4060, "issue_title": "Incorrect state validation when there's generic types in `pydantic.BaseModel` state schema", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom pydantic import BaseModel\nfrom langgraph.graph import StateGraph, START, END\n\nclass A(BaseModel):\n    a: str\n\nclass B(BaseModel):\n    b: str\n\nclass C[AorB](BaseModel):\n    c: AorB\n\nclass State(BaseModel):\n    text: str\n    count: int\n    c: C[A]\n    \ndef process_node(state: State):\n    new_text = \", the type of c is \" + str(type(state.c.c))\n    print(state.c.c)\n\n    return {\"text\": state.text + new_text, \"count\": state.count + 1}\n\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"process\", process_node)\nbuilder.add_edge(START, \"process\")\nbuilder.add_edge(\"process\", END)\ng = builder.compile()\n\ninput_state = {'text': '1', 'count': 0, 'c': {'c': {'a': '1'}}}\n\ng.invoke(input_state)\n\n# output\n# {'a': '1'}\n# .../pydantic/main.py:426: UserWarning: Pydantic serializer warnings:\n#   Expected `A` but got `dict` with value `{'a': '1'}` - serialized value may not be as expected\n#   return self.__pydantic_serializer__.to_python(\n\n# {'text': \"1, the type of c is <class 'dict'>\",\n#  'count': 1,\n#  'c': {'c': {'a': '1'}}}\n\ng.invoke(State.model_validate(input_state))\n# output\n# a='1'\nError Message and Stack Trace (if applicable)\npydantic/main.py:426: UserWarning: Pydantic serializer warnings:\n  Expected `A` but got `dict` with value `{'a': '1'}` - serialized value may not be as expected\n  return self.__pydantic_serializer__.to_python(\nDescription\nWhen graph state schema has pydantic.BaseModel with generic types invovled. The validation does not perform correctly.\nIn the repro example provided, C instead of C[A] is used in validation, resulting in the type of state.c.c being dict instead of A.\nManual validation (g.invoke(State.model_validate(input_state))) gives the correct result.\nThe output of g.invoke(input_state) should be consistent with g.invoke(State.model_validate(input_state))\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 23.1.0: Mon Oct  9 21:33:00 PDT 2023; root:xnu-10002.41.9~7/RELEASE_ARM64_T6031\nPython Version:  3.12.8 (main, Dec 19 2024, 14:22:58) [Clang 18.1.8 ]\n\nPackage Information\n\nlangchain_core: 0.3.48\nlangchain: 0.3.21\nlangchain_community: 0.3.20\nlangsmith: 0.3.19\nlangchain_anthropic: 0.3.10\nlangchain_google_vertexai: 2.0.17\nlangchain_neo4j: 0.4.0\nlangchain_openai: 0.3.10\nlangchain_text_splitters: 0.3.7\nlanggraph_api: 0.0.34\nlanggraph_cli: 0.1.80\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.59\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\nanthropic<1,>=0.49.0: Installed. No version info available.\nanthropic[vertexai]: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nclick: 8.1.8\ncryptography: 43.0.3\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\ngoogle-cloud-aiplatform: 1.85.0\ngoogle-cloud-storage: 2.19.0\nhttpx: 0.28.1\nhttpx-sse: 0.4.0\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\njsonschema-rs: 0.29.1\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-azure-ai;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.45: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.48: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.7: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.21: Installed. No version info available.\nlanggraph: 0.3.20\nlanggraph-checkpoint: 2.0.23\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nneo4j: 5.28.1\nneo4j-graphrag: 1.6.1\nnumpy<3,>=1.26.2: Installed. No version info available.\nopenai-agents: Installed. No version info available.\nopenai<2.0.0,>=1.68.2: Installed. No version info available.\nopentelemetry-api: 1.31.1\nopentelemetry-exporter-otlp-proto-http: 1.31.1\nopentelemetry-sdk: 1.31.1\norjson: 3.10.16\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npyjwt: 2.10.1\npytest: 8.3.5\npython-dotenv: 1.1.0\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nrich: 13.9.4\nSQLAlchemy<3,>=1.4: Installed. No version info available.\nsse-starlette: 2.1.3\nstarlette: 0.46.1\nstructlog: 25.2.0\ntenacity: 9.0.0\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nuvicorn: 0.34.0\nvalidators: 0.34.0\nwatchfiles: 1.0.4\nzstandard: 0.23.0\n", "created_at": "2025-03-27", "closed_at": null, "labels": [], "State": "open", "Author": "hon-gyu"}
{"issue_number": 4051, "issue_title": "StateGraph.add_node \"destinations\" arg has incorrect type", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nfrom typing import Literal, Optional, TypedDict\n\nfrom langgraph.graph import MessagesState\nfrom langgraph.types import Command\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.graph import END, START, StateGraph\n\ndef first_node(state: MessagesState) -> Command[Literal[\"second_node\", \"__end__\"]]:\n    if len(state[\"messages\"]) == 1:\n        return Command(goto=\"second_node\")\n\n    return Command(goto=END)\n\ndef second_node(state: MessagesState) -> MessagesState:\n    return state\n\n\nclass Config(TypedDict):\n    foo: Optional[str]\n\n\nmemory = MemorySaver()\n\n# Graph\nbuilder = StateGraph(MessagesState, Config)\n\n\nbuilder.add_node(\n    \"first_node\",\n    first_node,\n    # mypy reports an error on the line below\n    destinations=(END, \"second_node\"),\n)\nbuilder.add_node(\"second_node\", second_node)\n\nbuilder.add_edge(START, \"first_node\")\nbuilder.add_edge(\"second_node\", END)\n\ngraph = builder.compile(checkpointer=memory)\nError Message and Stack Trace (if applicable)\nagent/repro.py:32: error: Argument \"destinations\" to \"add_node\" of \"StateGraph\" has incompatible type \"tuple[str, str]\"; expected \"dict[str, str] | tuple[str] | None\"  [arg-type]\nFound 1 error in 1 file (checked 82 source files)\nDescription\nWhen passing a tuple with multiple values to the destinations arg of StateGraph.add_node(), mypy reports an error (see error message).\nIt looks like this argument should use tuple[str, ...] instead of tuple[str]\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:16 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6000\nPython Version:  3.11.11 (main, Dec  5 2024, 17:56:59) [Clang 16.0.0 (clang-1600.0.26.4)]\n\nPackage Information\n\nlangchain_core: 0.3.49\nlangchain: 0.3.21\nlangchain_community: 0.3.20\nlangsmith: 0.2.11\nlangchain_anthropic: 0.3.10\nlangchain_fireworks: 0.2.8\nlangchain_google_genai: 2.1.1\nlangchain_milvus: 0.1.8\nlangchain_openai: 0.3.11\nlangchain_text_splitters: 0.3.7\nlangchainhub: 0.1.21\nlanggraph_api: 0.0.37\nlanggraph_cli: 0.1.80\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.59\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\naiohttp<4.0.0,>=3.8.3: Installed. No version info available.\naiohttp<4.0.0,>=3.9.1: Installed. No version info available.\nanthropic<1,>=0.49.0: Installed. No version info available.\nasync-timeout<5.0.0,>=4.0.0;: Installed. No version info available.\nclick: 8.1.8\ncloudpickle: 3.1.1\ncryptography: 43.0.3\ndataclasses-json<0.7,>=0.5.7: Installed. No version info available.\nfiletype: 1.2.0\nfireworks-ai>=0.13.0: Installed. No version info available.\ngoogle-ai-generativelanguage: 0.6.17\nhttpx: 0.28.1\nhttpx-sse<1.0.0,>=0.4.0: Installed. No version info available.\njsonpatch<2.0,>=1.33: Installed. No version info available.\njsonschema-rs: 0.29.1\nlangchain-anthropic;: Installed. No version info available.\nlangchain-aws;: Installed. No version info available.\nlangchain-azure-ai;: Installed. No version info available.\nlangchain-cohere;: Installed. No version info available.\nlangchain-community;: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.45: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.47: Installed. No version info available.\nlangchain-core<1.0.0,>=0.3.49: Installed. No version info available.\nlangchain-deepseek;: Installed. No version info available.\nlangchain-fireworks;: Installed. No version info available.\nlangchain-google-genai;: Installed. No version info available.\nlangchain-google-vertexai;: Installed. No version info available.\nlangchain-groq;: Installed. No version info available.\nlangchain-huggingface;: Installed. No version info available.\nlangchain-mistralai;: Installed. No version info available.\nlangchain-ollama;: Installed. No version info available.\nlangchain-openai;: Installed. No version info available.\nlangchain-text-splitters<1.0.0,>=0.3.7: Installed. No version info available.\nlangchain-together;: Installed. No version info available.\nlangchain-xai;: Installed. No version info available.\nlangchain<1.0.0,>=0.3.21: Installed. No version info available.\nlanggraph: 0.3.20\nlanggraph-checkpoint: 2.0.23\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nlangsmith<0.4,>=0.1.17: Installed. No version info available.\nnumpy<3,>=1.26.2: Installed. No version info available.\nopenai<2.0.0,>=1.10.0: Installed. No version info available.\nopenai<2.0.0,>=1.68.2: Installed. No version info available.\norjson: 3.10.16\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic-settings<3.0.0,>=2.4.0: Installed. No version info available.\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npyjwt: 2.10.1\npymilvus: 2.5.6\npython-dotenv: 1.1.0\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrequests<3,>=2: Installed. No version info available.\nSQLAlchemy<3,>=1.4: Installed. No version info available.\nsse-starlette: 2.1.3\nstarlette: 0.46.1\nstructlog: 25.2.0\ntenacity: 9.0.0\ntenacity!=8.4.0,<10,>=8.1.0: Installed. No version info available.\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntypes-requests: 2.32.0.20250306\ntyping-extensions>=4.7: Installed. No version info available.\nuvicorn: 0.34.0\nwatchfiles: 1.0.4\nzstandard: Installed. No version info available.\n", "created_at": "2025-03-27", "closed_at": "2025-03-27", "labels": [], "State": "closed", "Author": "mobiware"}
{"issue_number": 4032, "issue_title": "Getting error while executing a subgraph from a node and interrupt is called in the subgraph", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\ntest.py\n\nfrom typing import TypedDict\n\nfrom langgraph.constants import START, END\nfrom langgraph.errors import GraphInterrupt\nfrom langgraph.graph import StateGraph\nfrom langgraph.types import interrupt, Command\n\nfrom utils import get_new_thread_id, get_remote_graph\n\n\nclass SubNodeData(TypedDict):\n    test: True\n\ndef subnode1(state: SubNodeData):\n    print(\"subnode 1 called initially\")\n    interrupt_data = {\n        \"test\": True\n    }\n    result = interrupt(interrupt_data)\n    print(\"subnode 1 called after interrupt\")\n    return state\n\ndef build_subgrapph(state: dict):\n    workflow = StateGraph(SubNodeData)\n    workflow.add_node(\"subnode1\", subnode1)\n    workflow.add_edge(START, \"subnode1\")\n    workflow.add_edge(\"subnode1\", END)\n    return workflow.compile()\n\n\nclass MainNodeData(TypedDict):\n    test: True\n\ndef main_graph_node(state: MainNodeData):\n    print(\"main graph node called\")\n    return state\n\ndef subgraph_caller(state: MainNodeData) -> MainNodeData:\n    g = get_remote_graph(\"subagent\")\n    thread_id = get_new_thread_id()\n    g.invoke({\"test\": True}, config={\"configurable\": {\"thread_id\": thread_id}})\n    return state\n\ndef build_main_graph():\n    workflow = StateGraph(MainNodeData)\n    workflow.add_node(\"subgraph\", subgraph_caller)\n    workflow.add_node(\"main_node\", main_graph_node)\n    workflow.add_edge(START, \"main_node\")\n    workflow.add_edge(\"main_node\", \"subgraph\")\n    workflow.add_edge(\"subgraph\", END)\n    return workflow.compile()\n\nif __name__ == \"__main__\":\n    g = get_remote_graph(\"main_agent\")\n    thread_id = get_new_thread_id()\n    print(f\"Thread id: {thread_id}\")\n    try:\n        g.invoke({\"test\": True})\n    except GraphInterrupt as e:\n        print(e)\n        config = {\"configurable\": {\"thread_id\": thread_id}}\n        g.invoke(Command(resume={\"thread_id\": thread_id}), config=config)\n\n\nlanggraph.json\n\n\n{\n    \"dependencies\": [\n        \"./test\"\n\n\n    ],\n    \"graphs\": {\n        \"main_agent\": \"./test/test.py:build_main_graph\",\n        \"subagent\": \"./test/test.py:build_subgrapph\"\n    }\n}\n\n\nlanggraph-util\n\n\nimport os\n\nfrom langgraph_sdk import get_client, get_sync_client\nfrom langgraph.pregel.remote import RemoteGraph\n\n\ndef get_api_key():\n    return os.getenv(\"LANGSMITH_API_KEY\")\n\n\ndef get_url():\n    return \"http://127.0.0.1:2024\"\n    \n\n\ndef get_langgraph_sync_client():\n    return get_sync_client(url=get_url(), api_key=get_api_key())\n\n\ndef get_langgraph_client():\n    client = get_client(url=get_url(), api_key=get_api_key())\n    return client, get_langgraph_sync_client()\n\n\ndef get_remote_graph(graph_name: str):\n    return RemoteGraph(graph_name, url=get_url())\n\n\ndef get_new_thread_id():\n    client = get_langgraph_sync_client()\n    thread = client.threads.create()\n    return thread[\"thread_id\"]\nError Message and Stack Trace (if applicable)\nThe error in console\n\nThread id: 44e5a28a-5349-474f-b41a-9849719e6c71\nTraceback (most recent call last):\n  File \"/Users/dineshsingh/dev/topmate/test-subgraph/test/test.py\", line 58, in <module>\n    g.invoke({\"test\": True})\n  File \"/Users/dineshsingh/dev/topmate/test-subgraph/venv/lib/python3.12/site-packages/langgraph/pregel/remote.py\", line 808, in invoke\n    for chunk in self.stream(\n                 ^^^^^^^^^^^^\n  File \"/Users/dineshsingh/dev/topmate/test-subgraph/venv/lib/python3.12/site-packages/langgraph/pregel/remote.py\", line 659, in stream\n    raise RemoteException(chunk.data)\nlanggraph.pregel.remote.RemoteException: {'error': 'TypeError', 'message': 'asdict() should be called on dataclass instances'}\n\nProcess finished with exit code 1\n\n\nlogs\n\n\nmain graph node called\n2025-03-26T10:39:55.547068Z [info     ] HTTP Request: POST http://127.0.0.1:2024/threads \"HTTP/1.1 200 OK\" [httpx] api_variant=local_dev thread_name=asyncio_1\n2025-03-26T10:39:55.547319Z [info     ] POST /threads 200 0ms          [langgraph_api.server] api_variant=local_dev latency_ms=0 method=POST path=/threads path_params={} proto=1.1 query_string= req_header={'host': '127.0.0.1:2024', 'accept': '*/*', 'accept-encoding': 'gzip, deflate, zstd', 'connection': 'keep-alive', 'user-agent': 'langgraph-sdk-py/0.1.58', 'content-length': '2', 'content-type': 'application/json'} res_header={'content-length': '204', 'content-type': 'application/json'} route=/threads status=200 thread_name=asyncio_0\n2025-03-26T10:39:55.549475Z [info     ] Created run                    [langgraph_storage.ops] api_variant=local_dev run_id=1f00a2ea-837d-6df4-8ed5-de50373bf2dc thread_id=59d9db7d-db84-487b-bfe5-08adc4235933 thread_name=MainThread\n2025-03-26T10:39:55.550963Z [info     ] HTTP Request: POST http://127.0.0.1:2024/threads/59d9db7d-db84-487b-bfe5-08adc4235933/runs/stream \"HTTP/1.1 200 OK\" [httpx] api_variant=local_dev thread_name=asyncio_1\n2025-03-26T10:39:56.505064Z [info     ] Starting background run        [langgraph_api.worker] api_variant=local_dev run_attempt=1 run_created_at=2025-03-26T10:39:55.549452+00:00 run_id=1f00a2ea-837d-6df4-8ed5-de50373bf2dc run_queue_ms=955 run_started_at=2025-03-26T10:39:56.504847+00:00 thread_name=asyncio_0\nsubnode 1 called initially\n2025-03-26T10:39:56.511934Z [info     ] Background run succeeded       [langgraph_api.worker] api_variant=local_dev run_attempt=1 run_created_at=2025-03-26T10:39:55.549452+00:00 run_ended_at=2025-03-26T10:39:56.511783+00:00 run_exec_ms=6 run_id=1f00a2ea-837d-6df4-8ed5-de50373bf2dc run_started_at=2025-03-26T10:39:56.504847+00:00 thread_name=asyncio_0\n2025-03-26T10:39:56.512273Z [info     ] POST /threads/59d9db7d-db84-487b-bfe5-08adc4235933/runs/stream 200 963ms [langgraph_api.server] api_variant=local_dev latency_ms=963 method=POST path=/threads/59d9db7d-db84-487b-bfe5-08adc4235933/runs/stream path_params={'thread_id': '59d9db7d-db84-487b-bfe5-08adc4235933'} proto=1.1 query_string= req_header={'host': '127.0.0.1:2024', 'accept': '*/*', 'accept-encoding': 'gzip, deflate, zstd', 'connection': 'keep-alive', 'user-agent': 'langgraph-sdk-py/0.1.58', 'content-length': '237', 'content-type': 'application/json'} res_header={'location': '/threads/59d9db7d-db84-487b-bfe5-08adc4235933/runs/1f00a2ea-837d-6df4-8ed5-de50373bf2dc/stream', 'cache-control': 'no-store', 'connection': 'keep-alive', 'x-accel-buffering': 'no', 'content-type': 'text/event-stream; charset=utf-8'} route=/threads/{thread_id}/runs/stream status=200 thread_name=asyncio_1\n2025-03-26T10:39:56.514847Z [info     ] POST /runs/stream 200 1762ms   [langgraph_api.server] api_variant=local_dev latency_ms=1762 method=POST path=/runs/stream path_params={} proto=1.1 query_string= req_header={'host': '127.0.0.1:2024', 'accept': '*/*', 'accept-encoding': 'gzip, deflate, zstd', 'connection': 'keep-alive', 'user-agent': 'langgraph-sdk-py/0.1.58', 'content-length': '189', 'content-type': 'application/json'} res_header={'location': '/threads/b2b3d02c-715b-4ac3-85a4-8d1d4e581167/runs/1f00a2ea-7be4-66a6-a078-e89b310c7603/stream', 'cache-control': 'no-store', 'connection': 'keep-alive', 'x-accel-buffering': 'no', 'content-type': 'text/event-stream; charset=utf-8'} route=/runs/stream status=200 thread_name=asyncio_2\n2025-03-26T10:39:56.513067Z [error    ] Background run failed          [langgraph_api.worker] api_variant=local_dev run_attempt=1 run_created_at=2025-03-26T10:39:54.752956+00:00 run_ended_at=2025-03-26T10:39:56.513011+00:00 run_exec_ms=1010 run_id=1f00a2ea-7be4-66a6-a078-e89b310c7603 run_started_at=2025-03-26T10:39:55.502821+00:00 thread_name=asyncio_1\nTraceback (most recent call last):\n  File \"/Users/dineshsingh/dev/topmate/test-subgraph/venv/lib/python3.12/site-packages/langgraph_api/worker.py\", line 128, in worker\n    await asyncio.wait_for(consume(stream, run_id), BG_JOB_TIMEOUT_SECS)\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n    return await fut\n           ^^^^^^^^^\n  File \"/Users/dineshsingh/dev/topmate/test-subgraph/venv/lib/python3.12/site-packages/langgraph_api/stream.py\", line 267, in consume\n    raise e from None\n  File \"/Users/dineshsingh/dev/topmate/test-subgraph/venv/lib/python3.12/site-packages/langgraph_api/stream.py\", line 257, in consume\n    async for mode, payload in stream:\n  File \"/Users/dineshsingh/dev/topmate/test-subgraph/venv/lib/python3.12/site-packages/langgraph_api/stream.py\", line 208, in astream_state\n    event = await wait_if_not_done(anext(stream, sentinel), done)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dineshsingh/dev/topmate/test-subgraph/venv/lib/python3.12/site-packages/langgraph_api/asyncio.py\", line 72, in wait_if_not_done\n    raise e.exceptions[0] from None\n  File \"/Users/dineshsingh/dev/topmate/test-subgraph/venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py\", line 2607, in astream\n    async for _ in runner.atick(\n  File \"/Users/dineshsingh/dev/topmate/test-subgraph/venv/lib/python3.12/site-packages/langgraph/pregel/runner.py\", line 279, in atick\n    self.commit(t, exc)\n  File \"/Users/dineshsingh/dev/topmate/test-subgraph/venv/lib/python3.12/site-packages/langgraph/pregel/runner.py\", line 383, in commit\n    self.put_writes()(task.id, writes)  # type: ignore[misc]\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dineshsingh/dev/topmate/test-subgraph/venv/lib/python3.12/site-packages/langgraph/pregel/loop.py\", line 341, in put_writes\n    self._output_writes(task_id, writes)\n  File \"/Users/dineshsingh/dev/topmate/test-subgraph/venv/lib/python3.12/site-packages/langgraph/pregel/loop.py\", line 860, in _output_writes\n    self._emit(\n  File \"/Users/dineshsingh/dev/topmate/test-subgraph/venv/lib/python3.12/site-packages/langgraph/pregel/loop.py\", line 822, in _emit\n    for v in values(*args, **kwargs):\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dineshsingh/dev/topmate/test-subgraph/venv/lib/python3.12/site-packages/langgraph/pregel/debug.py\", line 141, in map_debug_task_results\n    asdict(v)\n  File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/dataclasses.py\", line 1328, in asdict\n    raise TypeError(\"asdict() should be called on dataclass instances\")\nTypeError: asdict() should be called on dataclass instances\nDescription\nI am getting this weird error.\nlanggraph.pregel.remote.RemoteException: {'error': 'TypeError', 'message': 'asdict() should be called on dataclass instances'}\n\nConditions:\nI am running graphs(both main and subgraph) on a local server. Subgraph is direct a part of the main graph, it is being executed via a node in the main graph.\nThe subgraph calls the interrupt. I am not catching the GraphInterrupt in the main graph however, I am catching it from the place I am executing the main graph.\nThe error\nGetting\nlanggraph.pregel.remote.RemoteException: {'error': 'TypeError', 'message': 'asdict() should be called on dataclass instances'}\n\nI even tried passing dataclass in the interrupt, but even that didn't work. Wanted to know what exactly is the issue.\nAlso, is there a basic level flaw in the architecture which is creating this issue, if yes, then can someone please explain.\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.4.0: Sat Feb 15 22:50:54 PST 2025; root:xnu-11417.100.533.501.4~3/RELEASE_ARM64_T6000\nPython Version:  3.12.9 (main, Feb  4 2025, 14:38:38) [Clang 16.0.0 (clang-1600.0.26.6)]\n\nPackage Information\n\nlangchain_core: 0.3.47\nlangsmith: 0.3.18\nlangchain_openai: 0.3.9\nlanggraph_api: 0.0.32\nlanggraph_cli: 0.1.79\nlanggraph_license: Installed. No version info available.\nlanggraph_sdk: 0.1.58\nlanggraph_storage: Installed. No version info available.\n\nOptional packages not installed\n\nlangserve\n\nOther Dependencies\n\nclick: 8.1.8\ncryptography: 43.0.3\nhttpx: 0.28.1\njsonpatch<2.0,>=1.33: Installed. No version info available.\njsonschema-rs: 0.29.1\nlangchain-core<1.0.0,>=0.3.45: Installed. No version info available.\nlanggraph: 0.3.18\nlanggraph-checkpoint: 2.0.23\nlangsmith-pyo3: Installed. No version info available.\nlangsmith<0.4,>=0.1.125: Installed. No version info available.\nopenai-agents: Installed. No version info available.\nopenai<2.0.0,>=1.66.3: Installed. No version info available.\nopentelemetry-api: Installed. No version info available.\nopentelemetry-exporter-otlp-proto-http: Installed. No version info available.\nopentelemetry-sdk: Installed. No version info available.\norjson: 3.10.15\npackaging: 24.2\npackaging<25,>=23.2: Installed. No version info available.\npydantic: 2.10.6\npydantic<3.0.0,>=2.5.2;: Installed. No version info available.\npydantic<3.0.0,>=2.7.4;: Installed. No version info available.\npyjwt: 2.10.1\npytest: Installed. No version info available.\npython-dotenv: 1.0.1\nPyYAML>=5.3: Installed. No version info available.\nrequests: 2.32.3\nrequests-toolbelt: 1.0.0\nrich: Installed. No version info available.\nsse-starlette: 2.1.3\nstarlette: 0.46.1\nstructlog: 25.2.0\ntenacity: 9.0.0\ntenacity!=8.4.0,<10.0.0,>=8.1.0: Installed. No version info available.\ntiktoken<1,>=0.7: Installed. No version info available.\ntyping-extensions>=4.7: Installed. No version info available.\nuvicorn: 0.34.0\nwatchfiles: 1.0.4\nzstandard: 0.23.0\n", "created_at": "2025-03-26", "closed_at": "2025-03-27", "labels": [], "State": "closed", "Author": "dinesh1301"}
{"issue_number": 4028, "issue_title": "Unable to resume multiple interrupts from a single graph invoke", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport operator\nimport uuid\nfrom typing import Optional, Annotated, List\n\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.constants import START, END\nfrom langgraph.graph import StateGraph\nfrom langgraph.types import interrupt, Send, Interrupt, Command\nfrom pydantic import BaseModel, Field\n\n\n# --- CHILD GRAPH ---\n\nclass ChildState(BaseModel):\n    prompt: str = Field(\n        ...,\n        description=\"What is going to be asked to the user?\"\n    )\n    human_input: Optional[str] = Field(\n        None,\n        description=\"What the human said\"\n    )\n    human_inputs: Annotated[List[str], operator.add] = Field(\n        default_factory=list,\n        description=\"All of my messages\"\n    )\n\ndef get_human_input(state: ChildState):\n    human_input = interrupt(state.prompt)\n\n    return dict(\n        human_input=human_input,  # update child state\n        human_inputs=[human_input],  # update parent state\n    )\n\nchild_graph_builder = StateGraph(ChildState)\nchild_graph_builder.add_node(\"get_human_input\", get_human_input)\nchild_graph_builder.add_edge(START, \"get_human_input\")\nchild_graph_builder.add_edge(\"get_human_input\", END)\nchild_graph = child_graph_builder.compile(checkpointer=True)\n\n# --- PARENT GRAPH ---\n\nclass ParentState(BaseModel):\n    prompts: List[str] = Field(\n        ...,\n        description=\"What is going to be asked to the user?\"\n    )\n    human_inputs: Annotated[List[str], operator.add] = Field(\n        default_factory=list,\n        description=\"All of my messages\"\n    )\n\ndef assign_workers(state: ParentState):\n    return [\n        Send(\n            \"child_graph\",\n            dict(\n                prompt=prompt,\n            )\n        )\n        for prompt in state.prompts\n    ]\n\ndef cleanup(state: ParentState):\n    assert len(state.human_inputs) == len(state.prompts)\n\nparent_graph_builder = StateGraph(ParentState)\nparent_graph_builder.add_node(\"child_graph\", child_graph)\nparent_graph_builder.add_node(\"cleanup\", cleanup)\n\nparent_graph_builder.add_conditional_edges(START, assign_workers, [\"child_graph\"])\nparent_graph_builder.add_edge(\"child_graph\", \"cleanup\")\nparent_graph_builder.add_edge(\"cleanup\", END)\n\nparent_graph = parent_graph_builder.compile(checkpointer=MemorySaver())\n\n\n# --- CLIENT INVOCATION ---\n\nif __name__ == \"__main__\":\n    thread_config = dict(\n        configurable=dict(\n            thread_id=uuid.uuid4(),\n        )\n    )\n    current_input = dict(\n        prompts=['a', 'b'],\n    )\n\n    done = False\n    while not done:\n        # reset interrupt\n        current_interrupt: Optional[Interrupt] = None\n\n        # start / resume the graph\n        for event in parent_graph.stream(\n            input=current_input,\n            config=thread_config,\n            stream_mode=\"updates\",\n        ):\n            print(event)\n            # handle the interrupt\n            if \"__interrupt__\" in event:\n                current_interrupt: Interrupt = event[\"__interrupt__\"][0]\n                # assume that it breaks here, because it is an interrupt\n\n\n        # get human input and resume\n        if (\n            current_interrupt is not None\n            and current_interrupt.resumable is True  # make sure it is resumable\n        ):\n            response = \"Test Input\"\n            current_input = Command(resume=response)\n\n        # not more human input required, must be completed\n        else:\n            done = True\n\n    exit(0)\nError Message and Stack Trace (if applicable)\n\nDescription\nMoving from LangGraph 3.13.0 to 3.14.0 we noticed a change in behavior that we had come to expect and rely on specifically related to this PR: #3889 and this issue: #3398.\nThe behavior that we had been relying on was the ability to start a graph using ainvoke with command resume to restart the graph from multiple interrupts that reside within parallel subgraphs.\nFor this example, assume we had 2 parallel subgraphs. The first subgraph has an interrupt in a node toward the beginning of the subgraph while the second subgraph has an interrupt in a node toward the end. Utilizing ainvoke we were able to wait until both subgraphs either complete or raise an interrupt. If the first subgraph raised an interrupt toward the beginning of its execution, the graph would still process the rest of the second subgraph until it too either hit an interrupt or completed. At that point we could then present any interrupts to the user at one singular time allowing them to provide all of their feedback before continuing the graphs execution.\nIn LangGraph 3.14.0 we are still able to gather the interrupts that were hit in the parallel subgraphs, but we are unable to resume all of the interrupts through a single graph invoke. We are aware that we could simply invoke the graph and hit the waiting interrupt again, but we would then need to wait for the subgraph that we just started with the resume to complete, instead of simply resuming both at the same time. This would also force the call to invoke the graph multiple times and force the reload of the correct state from the checkpointer multiple times. This problem gets worse as the size and concurrency of the subgraphs grow.\nThis behavior can be seen in the outputs from the above code that was initially provided in this issue:\n#3398\nOutput From Code Above:\nIn LangGraph 3.13.0 you see both interrupts get hit in the parallel subgraphs and they are able to be resumed through a single resume.\n{'interrupt': (Interrupt(value='a', resumable=True, ns=['child_graph', 'get_human_input:f1c95e13-34de-3127-a6cf-7945d349df2f'], when='during'),)}\n{'interrupt': (Interrupt(value='b', resumable=True, ns=['child_graph', 'get_human_input:09753ebf-d018-7591-45f0-a718d2fc7f7f'], when='during'),)}\n{'child_graph': {'human_inputs': ['Test Input']}}\n{'child_graph': {'human_inputs': ['Test Input']}}\n{'cleanup': None}\nIn LangGraph 3.14.0 you will see that both interrupts are still hit on the first pass through the graph, but when you resume, you only restart from the first interrupt and must wait to call the graph to resume the second subgraph, once that interrupt gets raised again.\n{'interrupt': (Interrupt(value='a', resumable=True, ns=['child_graph', 'get_human_input:eea06b75-9d01-2d5f-8bef-b39884443394']),)}\n{'interrupt': (Interrupt(value='b', resumable=True, ns=['child_graph', 'get_human_input:2d8ef817-08cc-8346-a2f8-4240273e50cf']),)}\n{'interrupt': (Interrupt(value='b', resumable=True, ns=['child_graph', 'get_human_input:2d8ef817-08cc-8346-a2f8-4240273e50cf']),)}\n{'child_graph': {'human_inputs': ['Test Input']}}\n{'child_graph': {'human_inputs': ['Test Input']}, 'metadata': {'cached': True}}\n{'child_graph': {'human_inputs': ['Test Input']}}\n{'cleanup': None}\nThe considerations made in the original issue are still valid, if you wanted to break the graph and return to the user after each interrupt is encountered regardless of whether it is in parallel subgraphs. Due to the long running nature and high parallelization of our system we have found it to be the optimal user experience to wait until each parallel subgraph has either completed or hit an interrupt, giving the user the ability to provide their input all at once instead of sending them a new message and waiting for their input after each one.\nI believe it is possible for both methods to be supported. Currently to get around the limitations of only being able to send a single resume value when restarting all of the interrupts, we have had to utilize a dict, mapping the interrupt node name to the relevant responses from the user. Ideally, we would like the ability to specify multiple resume values or commands that would then map to each encountered interrupt. This would also fit with the previous issue raised as they could simply respond with a single resume to restart from each interrupt that is encountered.\nHappy to provide more code examples to support this methodology.\nSystem Info\nSystem Information\n\nOS:  Linux\nOS Version:  #1 SMP Tue Nov 5 00:21:55 UTC 2024\nPython Version:  3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0]\n\nPackage Information\n\nlangchain_core: 0.3.48\nlangchain: 0.3.4\nlangchain_community: 0.3.3\nlangsmith: 0.1.147\nlangchain_anthropic: 0.2.3\nlangchain_cohere: 0.3.4\nlangchain_experimental: 0.3.4\nlangchain_openai: 0.3.3\nlangchain_text_splitters: 0.3.7\nlanggraph_sdk: 0.1.58\n", "created_at": "2025-03-26", "closed_at": null, "labels": [], "State": "open", "Author": "cnummer1"}
{"issue_number": 4026, "issue_title": "in the fan-out and fan-in with extra steps, there are execution steps that do not meet expectations", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport operator\nfrom typing import Annotated, Any\n\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import StateGraph, START, END\n\n\nclass State(TypedDict):\n    # The operator.add reducer fn makes this append-only\n    aggregate: Annotated[list, operator.add]\n    t:Annotated[int, operator.add]\n\n\ndef a(state: State):\n    print(f'Adding \"A\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"A\"], \"t\":1}\n\n\ndef b(state: State):\n    print(f'Adding \"B\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B\"], \"t\":1}\n\n\ndef c(state: State):\n    print(f'Adding \"C\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"C\"], \"t\":1}\n\n\ndef d(state: State):\n    print(f'Adding \"D\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"D\"], \"t\":1}\n\n\ndef b_2(state: State):\n    print(f'Adding \"B_2\" to {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B_2\"]}\n\n\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\nbuilder.add_node(b_2)\nbuilder.add_node(c)\nbuilder.add_node(d)\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"a\", \"c\")\nbuilder.add_edge(\"b\", \"b_2\")\nbuilder.add_edge(\"b_2\", \"d\")\nbuilder.add_edge(\"c\", \"d\")\nbuilder.add_edge(\"d\", END)\ngraph = builder.compile()\n\nres = graph.invoke({\"aggregate\": []}, {\"configurable\": {\"thread_id\": \"foo\"}})\n\nprint(res)\nError Message and Stack Trace (if applicable)\nAdding \"A\" to []\nAdding \"C\" to ['A']\nAdding \"B\" to ['A']\nAdding \"B_2\" to ['A', 'B', 'C']\nAdding \"D\" to ['A', 'B', 'C']\nAdding \"D\" to ['A', 'B', 'C', 'B_2', 'D']\n{'aggregate': ['A', 'B', 'C', 'B_2', 'D', 'D'], 't': 5}\nDescription\nnode \"d\" was executed one more time\uff0cthis is not expected\nSystem Info\nSystem Information\n\nOS:  Darwin\nOS Version:  Darwin Kernel Version 24.3.0: Thu Jan  2 20:24:24 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T6030\nPython Version:  3.11.8 | packaged by conda-forge | (main, Feb 16 2024, 20:49:36) [Clang 16.0.6 ]\n\nPackage Information\n\nlanggraph: 0.3.20\n...\n", "created_at": "2025-03-26", "closed_at": "2025-03-26", "labels": [], "State": "closed", "Author": "lean-zone"}
{"issue_number": 4010, "issue_title": "ormsgpack breaks integration with alpine OS", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\n# On alpine os\n\npip install -U langgraph\nError Message and Stack Trace (if applicable)\nCollecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph~=0.3.5->-r requirements.txt (line 8))\n Downloading .../99/98/e709bdcc729995eb8bc8006429bb5396bf891b3089fdc201bfdeb395b608/ormsgpack-1.9.0.tar.gz (56 kB)\n Installing build dependencies: started\n Installing build dependencies: finished with status 'done'\n Getting requirements to build wheel: started\n Getting requirements to build wheel: finished with status 'done'\n Preparing metadata (pyproject.toml): started\n Preparing metadata (pyproject.toml): finished with status 'error'\n error: subprocess-exited-with-error\n \n \u00d7 Preparing metadata (pyproject.toml) did not run successfully.\n \u2502 exit code: 1\n \u2570\u2500> [6 lines of output]\n \n Cargo, the Rust package manager, is not installed or is not on PATH.\n This package requires Rust and Cargo to compile extensions. Install it through\n the system's package manager or via https://rustup.rs/\n \n Checking for Rust toolchain....\n [end of output]\n \n note: This error originates from a subprocess, and is likely not a problem with pip.\n error: metadata-generation-failed\n \n \u00d7 Encountered error while generating package metadata.\n \u2570\u2500> See above for output.\n \n note: This is an issue with the package mentioned above, not pip.\nDescription\nSince 0.3.20, i can't use anymore os alpine to build my project which uses langgraph.\nSystem Info\njust an alpine os", "created_at": "2025-03-25", "closed_at": "2025-03-28", "labels": [], "State": "closed", "Author": "gregoryboue"}
{"issue_number": 4007, "issue_title": "Duplicate items in state with subgraph and operator.add", "issue_body": "Checked other resources\n\n This is a bug, not a usage question. For questions, please use GitHub Discussions.\n I added a clear and detailed title that summarizes the issue.\n I read what a minimal reproducible example is (https://stackoverflow.com/help/minimal-reproducible-example).\n I included a self-contained, minimal example that demonstrates the issue INCLUDING all the relevant imports. The code run AS IS to reproduce the issue.\n\nExample Code\nimport operator\nfrom typing_extensions import Annotated, TypedDict\n\nfrom langgraph.graph import StateGraph, START, END\n\nclass OverallState(TypedDict):\n    list_a: Annotated[list[str], operator.add]\n\ndef node_1(state: OverallState) -> OverallState:\n    output = { \"list_a\": [\"node_1\"] }\n    print(f\"Entered node `node_1`:\\n\\tInput: {state}.\\n\\tReturned: {output}\")\n    return output\n\ndef node_2(state: OverallState) -> OverallState:\n    output = { \"list_a\": [\"node_2\"] }\n    print(f\"Entered node `node_2`:\\n\\tInput: {state}.\\n\\tReturned: {output}\")\n    return output\n\ndef sub_node_1(state: OverallState) -> OverallState:\n    output = { \"list_a\": [\"sub_node_1\"] }\n    print(f\"Entered node `sub_node_1`:\\n\\tInput: {state}.\\n\\tReturned: {output}\")\n    return output\n\nsubgraph_builder = StateGraph(OverallState)\nsubgraph_builder.add_node(sub_node_1)\nsubgraph_builder.add_edge(START, sub_node_1.__name__)\nsubgraph_builder.add_edge(sub_node_1.__name__, END)\nsubgraph = subgraph_builder.compile(name=\"subgraph\")\n\nbuilder = StateGraph(OverallState)\nbuilder.add_node(node_1)\nbuilder.add_node(node_2)\nbuilder.add_node(subgraph)\nbuilder.add_edge(START, node_1.__name__)\nbuilder.add_edge(node_1.__name__, node_2.__name__)\nbuilder.add_edge(node_2.__name__, subgraph.name)\nbuilder.add_edge(subgraph.name, END)\ngraph = builder.compile(name=\"main_graph\")\n\nresponse = graph.invoke({\n    \"list_a\": []\n})\nprint(f\"Output of graph invocation: {response}\") \n\n### Output:\n# Entered node `node_1`:\n#         Input: {'list_a': []}.\n#         Returned: {'list_a': ['node_1']}\n# Entered node `node_2`:\n#         Input: {'list_a': ['node_1']}.\n#         Returned: {'list_a': ['node_2']}\n# Entered node `sub_node_1`:\n#         Input: {'list_a': ['node_1', 'node_2']}.\n#         Returned: {'list_a': ['sub_node_1']}\n# Output of graph invocation: {'list_a': ['node_1', 'node_2', 'node_1', 'node_2', 'sub_node_1']}\nError Message and Stack Trace (if applicable)\n\nDescription\nIs Duplicate in the output expected because of the nature of how operator.add works ? Is there any solution to avoid this?\nSystem Info\nlangchain==0.3.18\nlangchain-core==0.3.35\nlanggraph==0.2.72", "created_at": "2025-03-25", "closed_at": "2025-03-25", "labels": [], "State": "closed", "Author": "kmprasad4u"}
